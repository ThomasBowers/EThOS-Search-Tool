Text-mining of experimental methods in phylogenetics 
A thesis submitted to the University of Manchester for the degree of 
In the Faculty of Life Sciences 
James Matthew Eales 
List of Contents 
LIST OF FIGURES ......... 6 
LIST OF ABBREViATIONS .. 8 
FORMATTING CONVENTIONS .. 19 
2. SOFTWARE FOR COLLECTING AND PROCESSING LARGE NUMBERS OF FULL-
4. METHODOLOGY CAPTURE: DISCRIMINATING BETWEEN THE "BEST" AND THE 
WORD COUNT: 42,846 
List of Figures 
Figure 3.3 - Results section from example article showing figure legend 
Figure 4.5 - Co-authorship network highlighting 'expert' authors.114 
Figure 5.6 - Analysis of methods used in an example workflow ... 148 
List of Tables 
Table 1.1 - List of currently used quality metrics grouped by 
Table 3.1 - Frequency of occurrence (per 1,000 words) of discriminatory 
Table 4.1 - Expert evolutionary biology methodological 
Table 5.1 - Correlation coefficient values between all pairs of metric 
Table 5.2 - Correlation coefficient values between all pairs of metric 
List of Abbreviations 
BMC - BioMed Central 
CV - Controlled Vocabulary 
DaM - Document Object Model 
DTD - Document Type Definition 
GUI - Graphical User Interface 
HKY - Hasegawa, Kishino and Yano (model of nucleotide substitution) 
HTML - HyperText Markup Language 
IP - Internet Protocol 
JPEG - Joint Photographic Experts Group (refers to a JPEG Image) 
JTT - Jones, Taylor and Thornton (amino acid substitution matrix) 
ME - Minimum Evolution 
ML - Maximum Likelihood 
MRC - Medical Research Council 
NCBI - the National Centre for Biotechnology Information 
NICE - National Institute for health and Clinical Excellence 
NJ - Neighbour-Joining 
NLM - National Library of Medicine 
NLP - Natural Language Processing 
OAI-PMH - Open Archives Initiative Protocol for Metadata Harvesting 
OTMI - Open Text-Mining Initiative 
PDF - Portable Document Format 
PMC - PubMed Central 
SOAP - Simple Object Access Protocol 
UKPMC - UK PubMed Central 
UPGMA - Unweighted Pair Group Method with Arithmetic Mean 
URI - Uniform Resource Identifier 
URL - Uniform Resource Locator 
WSDL - Web Services Description Language 
XML - eXtensible Markup Language 
XPath - XML Path language 
Abstract 
The University of Manchester 
James Matthew Eales 
Doctor of Philosophy (PhD) 
Text-mining of experimental methods in phyl.ogenetics 
26/08/2010 
The published scientific literature represents a vast investment of time and 
money into scientific research. It is therefore a huge information resource that 
can be used to answer many research questions. This thesis presents work 
on analysing methodological practice in the published literature and it 
demonstrates how this can be used to inform decision-making in the design of 
phylogenetic methodologies. A software infrastructure has been developed for 
collecting and structuring scientific articles so that they can be intelligently 
interrogated. The infrastructure includes an article download agent that was 
able to locate the correct PDF article from 88% of an original sample of 2,396. 
There is also software to classify article text into one of the standard scientific 
article sections (i.e., Introduction, Methods, Results, Discussion); this was 
able to correctly label 85% of 94,424 sections of text. A study of 
methodological practice in phylogenetics found it to be highly variable 
between its practitioners but also strongly field-specific, and that this tendency 
has increased in the last 10 years. The methodological practice of well-
published 'expert' authors and others in their field was found to be very 
similar, thus highlighting the importance of 'expert' scientists as practice 
indicators for their field. By combining knowledge of phylogenetic 
methodological practice with information on the articles it is declared in, four 
new metrics of quality for methods have been derived; these are popularity, 
recentness, expertness and citation. These metrics were used to score all of 
the major phylogenetic tree-inference and sequence alignment methods, and 
there is an investigation of methods for normalising and combining these 
scores. Results show that Bayesian tree-inference (a relatively new method) 
has the best combination of metric scores, and Clustal X (a long established 
method) has the highest combined metric score of the sequence alignment 
methods. In summary, the aim of this work was to objectively inform and 
clarify the design of phylogenetic analyses, a difficult task, but one which can 
be significantly simplified by making use of the published literature in novel 
ways. 
Declaration 
No portion of the work referred to in this thesis has been submitted in support 
of an application for another degree or qualification of this or any other 
university or other institute of learning. 
Copyright 
The author of this thesis (including any appendices and/or schedules to this 
thesis) owns any copyright in it (the "Copyright") and s/he has given The 
University of Manchester the right to use such Copyright for any 
administrative, promotional, educational and/or teaching purposes. 
Copies of this thesis, either in full or in extracts, may be made only in 
accordance with the regulations of the John Rylands University Library of 
Manchester. Details of these regulations may be obtained from the Librarian. 
This page must form part of any such copies made. 
The ownership of any patents, designs, trade marks and any and all other 
intellectual property rights except for the Copyright (the "Intellectual Property 
Rights") and any reproductions of copyright works, for example graphs and 
tables ("Reproductions"), which may be described in this thesis, may not be 
owned by the author and may be owned by third parties. Such Intellectual 
Property Rights and Reproductions cannot and must not be made available 
for use without the prior written permission of the owner(s) of the relevant 
Intellectual Property Rights and/or Reproductions. 
Further information on the conditions under which disclosure, publication and 
exploitation of this thesis, the Copyright and any Intellectual Property Rights 
and/or Reproductions described in it may take place is available from the 
Dean of the Faculty of Life Sciences. 
Acknowledgements 
Firstly I would like to thank my PhD supervisors David Robertson and Robert 
Stevens; they provided a truly inspiring and particularly creative environment 
in which to work and learn. I would also like to thank Goran Nenadic for his 
positive and encouraging influence on my work. 
I thank the J-room members: John Archer, Jonathan Dickerson, Jamie 
MacPherson and Jeongmin Woo, for their advice, opinion, and many useful 
discussions. 
Finally, I would like to thank my wife, Jenny, for making home life so enjoyable 
during my PhD. For the last few years I've been an overtly happy person, and 
all of this was because of her. Furthermore, her dedication to her own work, 
and her appreciation of mine, has been a constant source of energy and 
excitement throughout the work presented here. Sometimes, the best reason 
to work long into the night, solving a difficult problem, isn't so you can publish 
a paper or present at a conference, but is instead the approval you get from 
those who care for you. 
Rationale for submission in the alternative format 
When work commenced on this thesis, all chapters, were either; published in 
a journal, presented at a conference, or formed into a draft manuscript for 
submission to journals. Chapter 4 had been published in a journal, chapter 3 
had been partially presented at a conference and chapters 2 and 5 had been 
formed into draft manuscripts to be submitted for publication. It was therefore 
felt that the alternative format was best suited to the presentation of this work. 
Since that time the publication status of chapters 2 and 5 has not changed, 
this is owing to supervisor time constraints. It is, however, still our intention to 
publish these pieces of work, and to continue work in this area of research. 
Thesis structure 
The work presented in this thesis forms a single cohesive research project. 
Chapter 1 highlights the potential research areas and summarises the 
literature, whilst chapters 2 and 3 outline the technical research challenges 
approached and software created to enable the further work of chapters 4 and 
5. Chapter 5 would not have been possible without the software from 
chapters 2 and 3. The central research question of chapter 5 was a direct 
response to the work in chapter 4. Chapter 4 allowed us to capture the 
methodologies used by a research community, but we needed to do further 
work on how to choose between them, thus leading to chapter 5. A summary 
of content, author contribution and publication status of thesis chapters 2-6 
follows this section. . 
Chapter 1 - Introduction 
Chapter 1 provides additional background to chapters 2-5, it summarises 
some of the overall aims of the thesis, and describes the approach used to 
address the aims. 
Chapter 2 - Software for collecting and processing large numbers of 
full-text articles 
Publication type: Software Article (in preparation for BMC Bioinformatics) 
Authors: James M Eales, Goran Nenadic, Robert D Stevens and David L 
Robertson 
Author contributions: JME, GN, RDS and DLR designed the work. JME 
carried out the programming, data analysis, online resource development and 
drafted the manuscript. GN supervised the text retrieval, and agent-navigation 
methods. RDS and DLR supervised the project. All authors edited the final 
manuscript. 
Chapter Summary: This chapter highlights the technical research challenges 
in gaining access to the literature. It describes the software developed to 
enable literature collection, and also describes the importance of gaining 
automated access to full-text literature. The results from this chapter show 
that the software is able to collect a large proportion (88%) of an original 
article set without human intervention. The results of this chapter enable the 
work of the subsequent chapters, and also provide open source, freely 
available software for other researchers to use in their work. 
Chapter 3 - Looking in the right place: classifying manuscript sections 
Publication type: Research / Software Article (presented as part of a 
conference presentation, ISMB 2008) 
Authors: James M Eales, Goran Nenadic, Robert D Stevens and David L 
Robertson 
Author contributions: JME, GN, RDS and DLR designed the work. JME did 
the data analysis, programming and drafted the manuscript. GN informed the 
classifier training and error analysis. RDS and DLR supervised the project and 
all authors edited and approved the final manuscript. 
Chapter Summary: Chapter 3 describes the importance of being able to 
determine the structure of a research article from its plain-text form. It 
discusses the technical challenges in doing this, and describes software 
developed to perform this task (a text classifier). There is an analysis of the 
ability of the software to classify text into one of the 4 common article sections 
(introduction, methods, results, discussion). The classifier was found to 
perform well, correctly classifying 85% of a large set (94,424) of text sections. 
The results of this chapter enable later work in the thesis, and also provide 
software resources for other researchers. 
Chapter 4 - Methodology capture: discriminating between the "best" 
and the rest of community practice 
Publication type: Fully published research article (Eales, Pinney et al. 2008), 
BMC Bioinformatics 9:359, 2008, doi:10.1186/1471-2105-9-359. 
Authors: James M Eales, John W Pinney, Robert D Stevens and David L 
Robertson. 
Author Contributions: JME, RDS and DLR designed the work. JME carried 
out the text-mining, network construction, error analysis and drafted the 
manuscript. JWP participated in the design of the simulation and statistical 
analysis of networks. All authors edited and approved the final manuscript. 
Chapter Summary: This work describes an approach to capture 
methodological practice from the phylogenetics literature using text-mining 
methods. A methodology for capturing methodologies is outlined and its 
accuracy is also assessed. Community structure is inferred from the literature 
and patterns of methodological practice are studied across communities. 
Phylogenetic practice is found to be highly variable both between and within 
communities. However there are also a few very commonly used core 
methodologies that change little through time. The influence of scientific 
authority on practice is also studied and it is found that well-published authors 
use the same methods as other authors. This work is theoretically applicable 
to other research domains and the wider implications and requirements for 
this are discussed. 
Chapter 5 - Metrics on methods, valuing the process 
Publication type: Research article (submitted to Genome Biology in an 
abridged form) 
Authors: James M Eales, Goran Nenadic, Robert D Stevens and David L 
Robertson 
Author Contributions: JME, GN, RDS and DLR designed the work. JME 
carried out the programming, data analysis and drafted the manuscript. GN 
advised on text-mining and bibliometrics. RDS and DLR supervised the 
project, and all authors edited and approved the final manuscript. 
Chapter Summary: In this chapter, four new metrics of methodological 
quality are described and implemented. There is also a description of 
possible metric score normalisation and combination methods. This chapter 
also includes the analysis and scoring of the elements of an existing 
phylogenetic analysis. There is also a discussion of the complications of 
existing bibliometric indicators, and how the new metrics can be used to tailor 
methodological choices to the preferences of the individual researcher. 
Chapter 6  Discussion 
Chapter Summary: Chapter 6 is a concluding chapter that discusses the 
outcomes of all the previous chapters, and considers future work. 
Submission in Alternative Format 
This thesis has been submitted in alternative format with permission from the 
graduate office at the Faculty of Life Sciences. 
Formatting conventions 
The following formatting conventions have been followed throughout the 
thesis. 
 Each chapter has its own bibliography. This can be found at the end of 
the chapter. 
 Each bibliography lists references according to the alphabetical 
ordering of the first author's surname. 
 Because of the bibliography style (which requires an author name), all 
uniform resource locator (URL) references are provided as footnotes 
on the page where they are declared. 
 Footnotes are denoted in the text as superscript numbers (e.g., \ If 
multiple footnotes are required in a single location they are separated 
by a superscript comma (e.g., 1,2,3). 
 Footnotes are numbered sequentially within each chapter. Each 
chapter begins its footnote numbering at 1. 
 Figures and tables are numbered according to their chapter and their 
order within the chapter. For example, the second figure in chapter 1 is 
referred to as "Figure 1.2". This numbering style is used so that a 
figure and table list can be provided at the start of the thesis (as 
required by the guidelines) that refers to all chapters 
 Figure and table captions are formatted in 1 Opt, bold, single-spaced 
Arial (the main text is 12pt Arial). 
1. Introduction 
This thesis presents an investigation of methodological practice in the field of 
molecular phylogenetics; we use a text-mining approach to analyse methods 
used in the published journal literature from the field. The overall aim of this 
work was to capture methodological practice from the literature and to use this 
to inform the choice of methods in phylogenetic analyses. The objectives 
necessary to achieve the overall aim, are listed below: 
1. To collect full-text journal articles in large numbers. 
2. To structure the text of an article so the Methods section text can be 
identified. 
3. To capture a representation of the phylogenetic protocol reported in an 
article. 
4. To study the possibilities for deriving methodological best-practice 
information from the literature. 
5. To use literature-derived information to inform the choice of 
phylogenetic methodologies. 
This chapter outlines the background to four key elements of this work: 
1. The importance of methods in science 
2. The field of molecular phylogenetics and why it has been studied 
3. The implications of using information from published literature 
4. The acquisition and analysis of methodological practice 
1.1. Definition of terms 
There are several key terms that are used commonly throughout this thesis. 
A single definition of these terms, as used in the thesis, is provided here to aid 
comprehension for the reader. 
1.1.1. Task: A task is a component part of an experiment; examples of 
phylogenetic tasks are "tree-inference" and "sequence alignment". 
1.1.2. Method: A method is a way to perform a task; an example would be 
"Neighbour-Joining" which is a method used to perform the task of "tree-
inference" . 
1.1.3. Protocol: A way to perform an experiment; a protocol details the 
methods used to perform the tasks that make up an experiment. 
1.1.4. Workflow: A representation of a protocol; a workflow contains links to 
implementations of its methods. Workflows describe purely in silico protocols. 
1.1.5. Implementation of a protocol: This could be a workflow or a physical 
lab protocol performed in a lab by a scientist. 
1.1.6. Experiment: An experiment is an investigation into the validity of an 
hypothesis. It wi" most often be represented as a protocol; however there 
may be several distinct protocols inside an experiment. 
1.2. Methods are key to science 
Science is the acquisition of knowledge through enquiry. A scientist's process 
of investigation is broadly defined by the scientific method, whereby we first 
suggest a hypothesis and then devise experiments to test the validity of that 
hypotheSiS. This suggests that the design of the experiment, as we" as the 
. results it generates, wi" determine the accuracy with which the hypothesis is 
assessed. _ 
Science relies on communication; by publishing our work we make it available 
for others to consider. Our work can then be built upon to answer research 
questions more accurately or to inform new research questions. This leads to 
an iterative process of communication and improvement that is controlled 
almost universally through the dissemination of scientific research articles. 
A research article is the textual representation of an application of the 
scientific method. There is a discussion of current thinking, hypotheses 
proposed, results derived to address these hypotheses and then a discussion 
of whether the results support or refute the hypotheses. The scientific method 
also requires as full a declaration as possible of the methods used to derive 
those results; this is because there is a causal link between the two. The 
results of an experiment will always depend on the methods used to derive 
them. Therefore, we must always consider results in the light of the methods 
used to derive them. Much effort in recent times has looked to extract, collect 
and derive new information from results declared in scientific literature 
(Feldman and Sanger 2007). Here, an approach using information from the 
literature in a new way is described. Much of this work is devoted to analysis 
of experimental methods, mainly because they are an important prerequisite 
to the consideration of results, and yet their analysis remains under-
developed and under-explored. 
Communication is profoundly intertwined with the efficient functioning of 
scientific research in the modern world. We now have more and better ways 
to transfer information between researchers than has ever existed before. 
Unfortunately, it can still be very difficult to communicate elements of research 
(ideas, data, results) between researchers. Science has always had good 
intentions in this area; the archetypal scientific report contains all the 
ingredients necessary for communication of every element ofa research 
project. The introduction provides the context and previous work, the 
methods tell others how the work was done, the results communicate the raw 
data and the discussion provides inferences made from those data. However, 
in recent times, science, let alone methodological details, has become 
prohibitively complex, and it is often very difficult to gain a real understanding 
of how the work was performed without extensive field-specific knowledge. 
Furthermore, it is now common that, for any given research question, we can 
derive our answer using any of a range of tools .. 
These problems with dissemination of methodological detail represent a 
serious threat to. the way we work. The scientific method is fundamentally 
dependent on replication of results. The replication of results enables a 
rigorous assessment of how an experiment was performed and whether it 
may have incorporated any source of bias or error. There has been 
significant interest in this problem from publishing groups; their emphasis is 
on how replication differs between disciplines (Nature-Editorial 2006), the 
difficulty of methodological communication, and the reporting of replication 
results (Giles 2006). Two journals have also been created that are entirely 
dedicated to publishing experimental methods 1,2. Clearly, the publishing 
industry is aware of the lack of efficient communication, and is also aware that 
modern methodological details have become so complex that they require 
independent publication. This is an interesting direction for science in that we 
can now, hopefully, begin to assess the methodological detail of a given piece 
of research in a more efficient manner. 
As scientists, we are taught to be objective; to look at a choice without bias or 
preconceived notions, and then to justify our choice with further objective 
detail. This work proposes that this is becoming an increasingly difficult task 
for any researcher who wants to use tools from a field that is not exactly the 
same as their own, and of which they do not. have an encyclopaedic 
knowledge. This is the point at which this proposition can be linked with real 
research projects. The UK e-Science programme3 has broken down many 
barriers between disciplines by the application of collaborative technologies 
developed in computer science. Many fields can now benefit from better 
communication, dissemination of results and the improved efficiency of 
computational automation. 
1 http://www.nature.com/nprot/index.html 
2 http://cshprotocols.cshlp.org/ 
3 http://www.rcuk.ac. ukl escience/ 
Here, an approach to solving the problems outlined above within the field of 
molecular phylogenetics is proposed. In the following section, a description of 
why phylogenetics is a good test case for this approach is presented. 
1.3. Phylogenetics, a field with problems 
Molecular phylogenetics is the inference of evolutionary relationships between 
organisms using genomic data (Nei and Kumar 2000; Felsenstein 2004). 
Often these ideas are extended to relatedness between genes and entire 
populations, but the basic idea of examining the relationship between a set of 
samples holds true throughout the field. Relationships are quantified using 
information from genetic material, such as DNA, RNA or protein sequence 
data. A value of relatedness (or unrelatedness) is then calculated between 
all pairs of samples, and these values are then used to infer a phylogenetic 
tree that visually describes the relationships between all of the samples. 
Phylogenomics is a recently derived offshoot of phylogenetics that makes use 
of very large quantities of genetic material from genomic data, the intention 
being that this amount accurately represents, or is the total genetic 
complement of each sample, whereas phylogenetics uses only specific 
regions of the genome. Phylogenomics is only possible because of the huge 
advances in speed and accuracy of whole genome sequencing projects 
(Lander, Linton et al. 2001). 
Clearly, there is a need for phylogenetic methodologies to be chosen 
appropriately; the field has already influenced a broad group of research 
areas, including ecology (Eggleton and Vane-Wright 1994), epidemiology 
(Harvey and Nee 1994) and, of course, evolutionary biology (Harvey and 
Pagel 1991 ). Phylogenetics, therefore, has a responsibility to many 
practitioners, and this must be to provide clear, supportable information that 
can be used for hypothesis testing. The vast swathes of bio-data currently 
populating database servers all over the world only amplifies this 
responsibility; with the data-to-information step of an investigation being a 
significant bottle-neck, there must not be a reduction in quality and 
accountability in scientific analysis. 
1.3.1. Problem 1: Complexity 
Phylogenetics is complex; researchers create their own often highly personal 
experiments using favourite methodologies. This creates two problems: an 
easy question: how can we capture how phylogenetics is done? and a hard 
one: how can we then reproduce the experiment? One of the central scientific 
tenets is that experiments must be repeatable in order to test the 
reproducibility of results. Phylogenetics is mostly carried out with computers, 
and computers are good at storing information. This solves the problem of 
how to capture what has been done. To solve the second problem, we have 
to help computers to gain some degree of "insight" about what they are 
capturing. To illustrate this distinction, the Google search index captures a 
large proportion of all Web pages in the world; it stores them and allows us to 
look for words and phrases within those pages. However, Google search 
does not "know" what kind of thing it is storing; it merely indexes the text and 
the hyperlink, and infers relationships between pages by the Web links 
between them. The semantic Web movement (Berners-Lee, Hendler et al. 
2001) is trying to address this problem by providing Web pages that, instead 
of being a recipe on how to draw the page in a browser, are documents of 
data and description: e.g., if we want to show a quotation, we call it a 
quotation and provide meaning to the word 'quotation'. Applying semantics to 
phylogenetics methodology is a far simpler prospect than semanticising the 
Web, because we only need to define the meanings of things relevant to 
phylogenetics (e.g., neighbour-joining tree-inference, sequence alignment, 
DNA sequences or pairwise distances). We can then imagine this leading to 
an environment within which a standardised semantic ontology of 
phylogenetic entities exists that we can use. to generate standardised 
repeatable experiments that can be tested for reproducible results. 
1.3.2. Problem 2: Plumbing between software 
In this section, 'plumbing' is referred to as the purely practical elements of an 
experiment. A major challenge to standardised phylogenetics is the existence 
of a large number of data-:file formats; these are clearly defined as 'plumbing' 
problems. These formats usually have a predetermined file structure that is 
related to how the particular bioinformatics software in use reads in the data 
or prints it out. For example, the Phylip file format4 is a commonly used file 
format; an example is presented in Figure 1.1. 
10 705; 
Chicken 
Human 
Loach 
Mouse 
Whale 
ATGGCATATCCCATACAACTAGGATTCCAAGATGCAACATCACCAATCATAGAAGAACTAl 
ATGGCACACCCAACGCAACTAGGTTTCAAGGACGCGGCCATACCCGTTATAGAGGAACTTl 
ATGGCCAACCACTCCCAACTAGGCTTTCAAGACGCCTCATCCCCCATCATAGAAGAGCTCl 
ATGGCACATGCAGCGCAAGTAGGTCTACAAGACGCTACTTCCCCTATCATAGAAGAGCTTl 
ATGGCACATCCCACACAATTAGGATTCCAAGACGCGGCCTCACCCGTAATAGAAGAACTTl 
ATGGCCTACCCATTCCAACTTGGTCTACAAGACGCCACATCCCCTATTATAGAAGAGCTAl 
ATGGCTTACCCATTTCAACTTGGCTTACAAGACGCTACATCACCTATCATAGAAGAACTTl 
ATGGCATACCCCCTACAAATAGGCCTACAAGATGCAACCTCTCCCATTATAGAGGAGTTAl 
ATGGCATATCCATTCCAACTAGGTTTCCAAGATGCAGCATCACCCATCATAGAAGAGCTCl 
ATGGCACACCCATCACAATTAGGTTTTCAAGACGCAGCCTCTCCAATTATAGAAGAATTAl 
Figure 1.1 - Example of Phylip file format, showing the correct organisation of 
alignment meta data and sequence data. On the first line, separated by a space, the 
number of sequences and the length of the alignment are reported. Sequence name 
and sequence data are reported on subsequent lines. Names of sequences should be a 
maximum of 10 characters long, with shorter names being lengthened to 10 characters 
by spaces. Spaces are presented as dots (".") and line breaks by the new paragraph 
character ("11"). 
The file is read in sequentially by the Phylip software with the "10" denoting 
the number of sequences and the "705" denoting the length of the DNA 
sequences. There must be 1 0 spaces before the start of the DNA sequence 
and these spaces provide the location of the sequence identifier. The 
problem with this strategy is that a single "tab" after "Cow" would make the file 
unreadable to any of the Phylip programs. Also, the reliance of sequential 
reading and writing to file makes it difficult to read write and display specific 
elements of the data-file individually. An example of a difficult task to perform 
would be to find the 400th base in the "Cow" sequence and change it to an "A". 
4 http://evolution.genetics.washington.edulphylip/doc/main.html. 
This would require the whole file being read into a data structure, modified 
and then totally rewritten.to the file again. 
In preference to the unnecessary difficulties described above, we can imagine 
a different strategy with sets of identifiers and type information being defined 
for each section of the file: for example, a section that says the length of the 
DNA sequence that is called "length" and that the value of "length" is a whole 
number, and that that number is "705". We can now see how the structure of 
a file using this methodology could contain the information in many different 
orders (that are sequentially independent), and would totally remove the 
unnecessary annoyances of typographical mistakes. 
Data-files encoded using eXtensible Markup Language (XML) have several 
advantages over plain-text files: first, XML-based data-files can be shared 
between different software and computer systems (e.g., Macintosh, Windows 
or Linux) without complications, this is because the software used to save and 
read XML files is standardised against a common specificationS that requires 
detail on how to read the file to be included as part of the file; second, the 
same standardised software can be used by any third-party software 
application to generate, read and save XML to and from files in a manner that 
is repeatable between applications. XML is in common use in bioinformatics: 
for example, the NCBI Website6 offers biological data for download in their 
own predefined XML format. Clearly, the favourable qualities of XML have 
been recognised, and they are becoming more popular in phylogenetics 
(Gilmour 2000), with tree-inference software such as BEAST (Drummond and 
Rambaut 2007), which only accepts XML input files. PhyloXML is an XML 
format used to describe phylogenetic trees and associated metadata all in a 
single structured file. NESCENT7, an evolutionary science research centre, 
5 http://www.w3.org/TRlREC-xml/ 
6 http://www.ncbi.nlm.nih.gov/ 
7 http://www.nescent.org 
has also been active in the development of XML-based file formats8 in 
phylogenetics9. However, a standardised file format for exchanging 
phylogenetic data through XML (or in any other form) currently does not exist, 
given that there are already at least three different XML formats for the 
purpose. One attempt to standardise phylogenetics at a level above data is 
the "Minimum Information About a Phylogenetic Analysis" (MIAPA) standard 
(Leebens-Mack, Vision ef al. 2006) that is being developed by a group of 
phylogeneticists to create a single way to declare a description of a 
phylogenetic analysis. MIAPA is, however, currently incomplete, and it is 
unclear whether it will be adopted as a standard in a similar way to its 
microarray equivalent (Brazma, Hingamp ef al. 2001). 
Although this problem may superficially seem limited in its scope, data-file 
format problems are extremely common in bioinformatics, and are often 
difficult to resolve. It is interesting that the programming language Perl is so 
popular in bioinformatics and phylogenetics: with its shorthand syntax and 
powerful text-processing abilities, it is an ideal language for quick 
development of file- and report-processing code. However, researchers often 
rely on ad hoc Perl scripts that are specific to their needs, and usually only 
implement one step in an experiment. This provides a very clear argument for 
integration of phylogenetic methods with standardised data-transfer methods. 
There is no need for individual stages of an experiment to fail, because of an 
incorrect sequence name or tab symbol in the wrong place: the tools to avoid 
this are available and are in use (Oinn, Addis ef al. 2004), and they now need 
to be brought into phylogenetics. 
8 https:/ /www .nescent.org/wg~hy loinfonnatics/Phy loSoC:Phy logenetic _ XML 
9 https://www.nescent.org/wg_evoinfolNeXML 
1.3.3. Problem 3: Architectural concerns: How to choose? 
A publication-quality set of phylogenetic analyses from sequence alignment to 
tree-inference and statistical testing will almost always involve more than one 
software package, and will most commonly involve several. This is because 
there is no standardised format for performing an analysis, and this, in turn, is 
owing to a complete lack of consensus between researchers on which 
methods to implement. In addition, at every step in the process, researchers 
must choose a particular software package to perform whichever methods 
they have chosen. There is extensive redundancy in the software packages 
available: some perform a variety of tasks (Kumar, Tamura et al. 2004); 
others perform just one (Posada and Crandall 1998). This environment of 
redundancy and repetition is supported by the way in which users choose 
their tools. Researchers regularly mix and match software tools from different 
packages to fit their individual needs or preference, and the resulting 
workflow, from alignment to trees, is often a highly bespoke solution. This 
selection of tools compounds the problems of software interoperability, as 
data must be able to cross barriers not only between individual tools but also 
between different software packages. 
To compound this confusion, the phylogenetic community is still unable to 
provide good clear guidelines on the choice of methods, although it is agreed 
. that use of multiple methods in combination is always favourable and 
essential for publication-standard work. A single method will not suffice for 
any experiment, but to have a set of guidelines, or at least general heuristics, 
to aid in the choice of method would be invaluable. In order to reach this goal, 
there must be a systematic review of the underlying nature of phylogenetic 
methods; they should be assessed for statistical and phylogenetic validity 
when applied in a number of different situations. This review will attempt to 
begin this process by highlighting the most important properties of each of the 
most common methods. It is hoped that an expansion of this will produce the 
guidelines or heuristics discussed above. 
1.3.4. Potential solutions to the problems 
1.3.4.1. Workflows 
The term workflow is commonly used in a business environment; indeed, a 
large quantity of commercial and academic research effort (Hwang and Yang 
2002; Herbst and Karagiannis 2004; Schimm 2004; van der Aalst, Weijters et 
al. 2004) has been devoted to the characteristics of workflows and their 
application to business. The reason for this interest is attributable to the 
potential for efficiency monitoring and optimisation within a working 
environment. The activity of a working unit (e.g., an office) can be mapped 
onto a workflow; this workflow (essentially a model) can then be manipulated 
and tested for efficiency, time and time again, without the need for major and 
costly reorganisation in the real world. This is particularly apparent in the 
manufacturing industry, where it is far cheaper to optimise and assess the 
manufacturing process than the output of the process. 
If workflows are good at improving the efficiency (Oinn, Greenwood et al. 
2006), understanding and communication (Gentleman, Carey et al. 2004) of a 
process, then how can workflows be used to benefit bioinformatics and 
phylogenetics? And will a workflow model of a phylogenetic analysis allow us 
to improve the efficiency and repeatability of our experiments? A workflow is 
an intuitive creation; it involves the organisation of an experiment into a map 
of individual work units or processes, and describes how the movement of 
activity is transferred between these processes (see Figure 1.2); the simplicity 
of this approach allows human comprehension of large, and often complex 
and dynamic, processes. Workflows are a good way to provide standardised 
best-practice information (Stevens, Robinson et al. 2003), because they 
describe the process in detail. If we are able to communicate, declare and 
scrutinise our experimental workflows in a more efficient manner than is 
currently used, then this must make a good case for the validity of workflows 
in phylogenetics. 
STAGE 1 STAGE 2 STAGE 3 STAGE 4 STAGE 5 
= experimental process = input/output link 
Figure 1.2 - Abstract representation of a workflow. Coloured circles represent 
processes and lines between circles represent input going from the process at the 
plain end of the line to become input to the process at the arrowed end of the line. 
One of the most important components of the workflow model is that it allows 
multiple inpuUoutput connections, as well as the assumption that output from 
one entity will ultimately become input for another. This feature is very useful 
for e-Science experiments, where it is common to run several, often quite 
similar, pieces of software, with the same inputs, and then compare the 
combined outputs. Examples of this are found in gene finding, protein 
structure prediction, phylogenetic tree-inference and microarray data analysis. 
Entities in this context are referred to as discrete elements of the experimental 
workflow. We are particularly interested in only defining entities as the truly 
important components of the experimental method. We are not currently 
interested in how the entities communicate, but. rather whether they are able 
to communicate at all. It is vital to solve issues with file formats and data-
communication problems (see Problem 2: File formats). At this stage, 
however, we want to exclude the details of these steps, because they have 
implicit problems that we believe should be solved when we reach the stage 
of turning a workflow back into an experiment. This strategy will allow us to 
provide a total solution to the problem that is not dependent on the problems 
that we currently have with data communication. Essentially, we do not want 
to perpetuate the problems we currently have (see Problems 1, 2 and 3 
above). We would rather start again and provide a novel solution to the 
problem. 
1.3.5. Phylogenetic methods 
The primary aim of this work is to objectively explore phylogenetic 
methodologies without excessive reference to their empirical properties; this is 
because this work has already been done (Felsenstein 2004), and it does not 
solve the problems inherent to phylogenetic analysis. However, a brief 
discussion of the broad tasks often performed in phylogenetics will help to 
define the field better, and will also provide a basis of understanding when 
methods are discussed in this chapter, and in chapters 4 and 5. 
1.3.5.1. Sequence Alignment 
By far the most common form of input data in phylogenetics is sequence data, 
with DNA and amino acid sequences being the most commonly used. The 
approach is then to derive a measure of relatedness or distance between the 
sequences. However, because the evolution of biological sequences (Nei 
and Kumar 2000) often involves insertions and deletions, sequences are 
regularly of different lengths, and we need to be able to align positions on one 
sequence with those on another. This step is commonly known as multiple 
sequence alignment, or just sequence alignment. A discussion of some of the 
technical implications of choosing a sequence-alignment tool is presented in 
Chapters 4 and 5, and therefore, here, a short summary of the background 
behind the most commonly applied sequence-alignment tools is provided. 
Sequence alignment is a key first step in phylogenetic analysis. This is 
because it provides the basis for all further analysis. Bias or error introduced 
at this stage is propagated to all further analyses. There are a wide variety of 
different sequence alignment methods and tools 10. One of the earliest 
alignment tools is Clustal (Higgins and Sharp 1988), which has gone through 
several iterations to reach its current forms as Clustal W (command line 
version) and Clustal X (graphical user interface version) (Larkin, Blackshields 
et al. 2007). Clustal is a very well known tool, with the first Clustal W paper 
(Thompson, Higgins et al. 1994) being cited 28,229 times in Google Scholar, 
27,159 times in Scopus and 26,938 times in lSI web of knowledge (all 
accessed 01/10/08). However, its theoretical basis has only been refined 
(Larkin, Blackshields et al. 2007) rather than renewed during its development. 
This has encouraged other researchers to create tools based on other 
methods of alignment. Furthermore, Clustal can be very slow to execute on 
larger data sets (Edgar 2004; Edgar and Batzoglou 2006), which has limited 
its utility as data-sets have grown in size. T-Coffee (Notredame, Higgins et al. 
2000) was an attempt to improve on Clustal; it applied a similar basic 
methodology, but attempted to optimise the speed and accuracy of alignment 
using heuristic refinements. Muscle uses a different basic approach from 
Clustal (Edgar 2004), which can be faster, but also less accurate; it is 
therefore often applied to data-sets containing large numbers of sequences 
(>1,000 sequences). 
10 http://evolution.genetics.washington.edulphylip/software.html 
1.3.5.2. Tree-inference 
Tree-inference is the process of transforming a set of aligned sequences into 
a hierarchical structure (often referred to as a dendrogram, phylogram or 
cladogram) that describes the relationships between those sequences. It is 
the relatedness values that are used to draw the trees that are most 
significant. The question, "How do we calculate these values?" has many 
answers; it would be best to clarify the question first. Essentially, the question 
is asking, "How related (or unrelated) is each sequence to each other?" 
Simple methods do this qualitatively; other, more complicated, methods 
calculate relatedness and give it a quantitative value. The most complicated 
methods analyse the properties of each difference in turn, and assign each 
with a different value. Therefore, one difference between two sequences 
does not always equal a distance of one between the sequences in the tree. 
In chapters 4 and 5, tree-inference methods are discussed, and therefore, 
here, a short summary of each of the most common methods is provided. 
The intention of this section is not to review how each method has been used, 
and how accurate the results were in comparison with each other, or at 
reconstructing a simulated phylogeny. This is too specific and has been 
covered in a wealth of previous research articles (Jin and Nei 1990; Douady, 
Delsuc et al. 2003; Erixon, Svennblad et al. 2003; Hagstrom, Hang et al. 
2004). It is important to highlight properties of each method in relation to its 
statistical basis, robustness, power and assumptions, as well as 
computational concerns, properties relating to reproducibility, and the 
avoidance of both over- and under-simplification. 
1.3.5.2.1. Distance Methods 
For distance methods, there are roughly 3 stages: list all the pairwise 
differences between samples using a model of molecular evolution; work out 
the quantitative value of the differences; and then use a method to draw the 
tree using the distances. We now review some of the most common distance 
methods, with a particular emphasis on their objective and statistical qualities. 
1.3.5.2.1.1. Unweighted Pair Group Method with Arithmetic mean 
(UPGMA) 
UPGMA (Sokal and Michener 1958; Felsenstein 2004) finds the nearest pair 
of distance values and then recalculates the distances using the arithmetic 
mean to combine the values for the two sequences in the pair. This method 
produces leaves that are all equal distances from the root, which leads to 
biased distances and tree topologies (Felsenstein 2004). Despite its 
tendency to bias with data-sets that vary in their rate of evolution, UPGMA is a 
computationally "cheap" method and should not be excluded off-hand; it could 
be useful for very large intraspecific phylogenies, where the rate of evolution 
between sequences is expected to be very similar. Indeed, in this situation, it 
would be biasing to read too much into very small differences between 
sequences. In such c~ses, phylogenetic signal is often difficult to detect, and 
applying methods with complicated assumptions would be incorrect. 
In most cases, UPGMA is not the best method: uniform branch lengths (from 
leaf to root) remove lineage-specific information on rates of evolution, which 
also leads to a bias in tree topology that cannot be justified. UPGMA can, 
however, be used to infer trees that describe relationships between 
sequences that are known to evolve at a constant rate. Justifiable 
applications of UPGMA would be for intra-specific phylogenies from a species 
that is little-influenced by selection; for example, a species that is evolving 
mainly owing to genetic drift, after range expansion into a largely native 
environment with uninterrupted gene flow between all members. 
Mitochondrial DNA is also known to exhibit clock-like evolutionary behaviour 
and is therefore suited to UPGMA i.nvestigation. In practice, this is unlikely in 
a natural environment (Hasegawa, Kishino et al. 1985). 
1.3.5.2.1.2. Neighbour-Joining 
The Neighbour-Joining (NJ) method (Saitou and Nei 1987; Felsenstein 2004) 
is a clustering method (as is UPGMA) that seeks to group sequences together 
according to algorithmic rules. NJ is an approximation to the Minimum 
Evolution (ME) method (mentioned below); it does not seek to optimise a 
single criterion (e.g., the ME criterion of short tree distances) - instead, it 
applies its algorithm whilst moving to the nearest neighbour in the distance 
matrix. NJ therefore does not guarantee to find the true tree, but it is 
computationally tractable for large data-sets, although it has been proposed 
(Strimmer and von Haeseler 1996) that the accuracy of NJ decreases 
exponentially as the number of taxa increases. 
NJ has good qualities as an all-purpose tree-construction method: it is quick in 
relation to other methods; it regularly achieves results as good as methods 
that rely on tree-searChing (e.g., parsimony, ME); it also provides good quality 
branch-length information when used with an appropriate model; and it can 
use a model of molecular evolution that fits well with the data and the needs 
of the investigator. 
1.3.5.2.1.3. General points on distance methods 
As Felsenstein (Felsenstein 2004) points out, distance methods are 
. repeatable. An NJ tree drawn by any piece of software, or by hand, should be 
exactly the same every time, as long as the data-set is identical. 
Unfortunately, not all implementations of the NJ algorithm are exactly the 
same, but most stick closely to the original algorithm rules defined by Saitou 
and Nei (Saitou and Nei 1987). Repeatability is vital in science: we rely on it 
for independent verification of results. NJ is a good method for repetition 
because it does not have parameters (apart from those used to make the 
distance matrix) to record, and as long as the correct algorithm is used, it will 
always produce the same tree from the same set of data. 
1.3.5.2.2. Parsimony 
Parsimony methods of tree-inference are varied in their nature and 
implementation; however, the most common and most general 
implementation comes from Camin and Sokal (Camin and Sokal 1965), and it 
is this version that is referred to here. Parsimony has an intuitive basis to its 
methodology: it simply tries to find the tree that explains the patterns in the 
data with the least amount of evolution, from all of the leaves to the root of the 
tree. Parsimony is unlike distance methods in that it explicitly seeks to 
optimise a criterion, the criterion being the amount of evolution shown in the 
tree, and its optimal value is always the lowest discovered. Parsimony is 
theoretically an absolute process; in other words, all the most parsimonious 
trees should always be found. In practice, a complete search can only be 
completed for small phylogenies: for example, if 10 sequences are to be 
studied, there are 34,459,425 possible trees to search through in order to find 
all the most parsimonious trees. Searching through that many trees is 
practicable on desktop computers, but more than 10 sequences is generally 
considered to be too lengthy a process (Felsenstein 2004, page 23). The 
problem arises with post hoc tests, such as bootstrapping (Felsenstein 1985), 
which require multiple different permutations of the same analysis. Searching 
through that many trees would not only take an inordinate amount of time, but 
is also a waste of time as most possible trees are poor. The number of 
possible trees increases explosively with respect to the number of sequences 
to be studied (n). When we reach 50 sequences, the number of possible 
trees becomes larger than the predicted number of electrons that occur in the 
perceived universe (also know as Eddington's number). Clearly, searching 
through that number of trees is not achievable on a desktop computer; neither 
is it achievable by supercomputing: despite vast technological improvements 
since the first phylogenies were inferred, we are still unable to absolutely 
resolve a tree with more than 10 sequences in a reasonable period of time on 
a desktop-computer. Phylogenies regularly contain more than 10 sequences, 
and therefore we need a solution to this problem of scale. Luckily, many 
different methods have been devised, and all cut down the time required 
massively. 
A derivation of Parsimony, known as weighted parsimony (Williams and Fitch 
1990), follows the same basic methodology defined above, except that it 
scores the value of evolutionary changes according to a weighting scheme. 
Whereas regular parsimony scores all evolutionary changes equally, weighted 
parsimony will derive different weights for each possible change and then will 
seek to minimise the sum of these weights in a tree. In practice, the weights 
are derived from the data-set and fulfil a role very similar to the model of 
molecular evolution used by distance methods (Marshall 1992). 
1.3.5.2.3. Likelihood Methods 
Likelihood methods employ a function to derive a likelihood for a given tree 
topology, based on a model of evolution and a sequence alignment. The 
assumptions of the statistics are very important. In this case, the assumption 
is made that the sample data-set contains phylogenetic information that would 
be found again if the experiment were repeated. This allows the assumption 
that the tree constructed will be a good estimate of the true tree. This is a 
very common assumption that is applied to most tree-inference methods that 
do not absolutely resolve their trees. In addition to this, there are two 
likelihood-specific assumptions: that the evolution of different sites and 
different sequences are independent of one another. In practice, these are 
hard assumptions to meet; clearly, changes in pairs of sites that are important 
in RNA secondary structure folding will not evolve independently; natural 
selection will instead cause their evolution to be closely tied in order to 
maintain the functionality of the RNA. This is discussed by Huelsenbeck and 
Rannala (1997) in their critical, yet optimistic, presentation of maximum 
likelihood (ML) as an accountable and impartial methodology that is useful for 
more than just tree-construction. Within the same article, the origins of 
likelihood as applied to phylogenetics are described; beginning with Fisher's 
foundation statistical and theoretical work (Fisher, 1922), leading to the first 
practical uses by Edwards and Cavalli-Sforza (1964), and Felsenstein's 
practical work on ML (Felsenstein 1981), which made ML a computational 
reality for nucleotide sequence data-sets of a reasonable size. 
ML always makes use of an explicit model of molecular evolution; the model 
is required in order to calculate the probabilities of the specific mutation 
events, and therefore their biological significance. As has been discussed in 
the distance method section, models potentially act as a source of bias. 
However, in most cases, there is good evidence for the use of a model, as 
most mutational change does not occur at a uniform rate across biological 
sequences or between states. 
The most important property of likelihood as a tree-construction methodology 
is its basis in statistics. Not only can one derive a measure of confidence for 
a given tree topology, but it is also easy to set up simple hypothesis tests 
concerning any property of the tree (given that the property can be 
represented mathematically). The statistical basis and ease of hypothesis 
testing provide likelihood with robustness that is easily communicated (i.e., 
through a journal publication), as well as flexibility for application to any 
number of phylogenetic research questions. 
1.3.5.2.4. Bayesian Phylogenetics 
The Bayesian approach to inference is applied in many areas of science; this 
is mainly because of its grounding in statistics, and the attractive qualities that 
this provides ( e.g., statistical support values, objectivity). Bayesian inference 
is very similar to the likelihood approach; the two differ in only one respect: 
likelihood does not use a prior distribution (also known as just "the prior") 
describing the probability of the quantity (in phylogenetics, a tree topology) 
being examined. The next step is to multiply the prior by the likelihood 
function, and then to normalise the data; this produces the posterior 
probability distribution (also known as just "the posterior"), which is the result 
of the analysis. The posterior now provides us with all the probabilities of all 
the different hypotheses (described by the prior), given our observed data. So 
to summarise, the likelihood of a hypothesis is calculated by combining the 
prior knowledge about the quantity (the tree) with the data that have been 
observed. The advantage of the Bayesian methodology is that it allows us to 
investigate many different hypotheses at the same time. 
Implementations of Bayesian inference are usually combined with a Markov 
Chain Monte Carlo sampling process that allows the software to capture a 
sample of trees from the posterior distribution of trees. This sampling allows 
the software to summarise the trees and highlight areas in the tree with strong 
or weak statistical support Oust as bootstrap resampling does (Felsenstein 
2004)). The Monte Carlo algorithms implemented in common software 
packages (Ronquist and Huelsenbeck 2003; Drummond and Rambaut 2007) 
can be very complex are often not described in detail. Although this does 
reduce the level of complexity in the use of the software, it does make it 
difficult to compare trees generated by different software. The lack of 
specification of the Monte Carlo algorithms has been highlighted and criticised 
by some (Felsenstein 2004). 
The use of a prior seems a small difference, but it is potentially a source of 
bias. It is common for a prior to be formed by investigators using their own 
instincts on the matter. The best way to choose a prior is not particularly well 
discussed in phylogenetic literature, but mention is usually made of its 
calculation, even if the reasoning behind the calculation is not apparent (for 
example, see (Jow, Hudelot ef al. 2002)). This is not to say that choice 
without definitive reasoning is always incorrect (indeed, the authors do 
provide good justification for their prior distribution). However, it may be 
unnecessary to do so when likelihood could address the same issue without 
the possible introduction of error. Suzuki (Suzuki, Glazko ef al. 2002) gave an 
in-depth description of Bayesian inference employing Markov Chain Monte 
Carlo simulation, and found that posterior probabilities were commonly over-
estimated when compared to bootstrap support values (which were slightly 
underestimated). This kind of work is needed in order to provide justification, 
or criticism, for tree-construction methods that incorporate possible sources of 
error, or bias, without strong statistical evidence. 
In summary, a Bayesian approach to phylogenetic tree-construction has a 
good basis, being rooted in statistical probability theory. The methods provide 
a good framework for estimating the quality of tree topologies produced, but 
some questions still remain as to the choice of prior distributions and the 
specification of Monte Carlo algorithms. 
1.4. Methods in the literature 
In order to study the methods used in phylogenetic research, we can capture 
the methods reported in research publications. This information will provide 
an overview of how the community as a whole performs their experiments and 
can then be used to make informed decisions concerning the choice of 
methods for different phylogenetic tasks. To elucidate how an entire research 
community performs its work requires the collection of a large amount of 
literature (see chapters 2 and 4) and appropriate literature-mining tools that 
can extract which methods are reported in an article. There are, however, 
several challenges associated with the collection and analysis of a 
community-wide set of research articles. First, these challenges will be 
outlined; and then potential solutions will be described. 
1.4.1. Challenges in analysing large numbers of articles 
To analyse the use of methods in a community-wide set of publications (see 
Figure 1.3) we need four things: 
1. Better access to the literature, (can we actually read it); 
2. Structured literature: articles should not just be a file of plain-text, but 
should be split into sections, images, tables, citations; 
3. Better metadata: what do we know about this article? Has it been 
cited? Who are the authors? 
4. To be able to organise the collected literature. 
1: Access 4: Organisation 
How do I get the PDF? 
How do I cite this? 
Where can I get it? 
2 : Internal Structure 
Introduction 
Authors 
Affiliations 
References 
-. ----
, .. ~ -.--
I want to cite this result 
Do I have this 
reference already? 
~. Who are the authors? 
Cited, how many times? 
Where was it published? 
\----===--------'--
3: Metadata 
Figure 1.3 - Challenges of literature analysis. This illustrates some of the wider 
implications of the four challenges described above. The four main challenges in 
literature analysis are numbered. Potential research questions that are complicated by 
these challenges are presented in red font. A blue arrow highlights the part of an 
article (if any) to which they relate. 
1.4.2. Access to literature 
Access to full-text scientific literature has increased massively in recent years 
with the use of electronic delivery of the full-text. Almost all publishing houses 
and journals now make use of electronic full-text delivery, so much so that it is 
now becoming commonplace for journals to have online-only articles as well 
as entire journals that are only published online, the most notable collection of 
which being those published by BioMed Central11 (BMC). Articles can be 
delivered in several different electronic formats; for example, Portable 
Document Format (PDF) and the HyperText Markup Language (HTML), with 
a recent increase in the availability of XML-based (eXtensible Markup 
Language) document formats. All of these formats contain the full-text, as 
well as formatting information. They therefore require processing to extract 
the pure textual content. 
The availability of full-text to the research community is still restricted by 
licensing and subscription issues imposed by publishers and journals, with the 
notable and commendable exception of the data-mining set provided by 
BMC12 and the Open Archives Initiative Protocol for Metadata Harvesting 
(OAI-PMH) interface to PubMed Central (PMC)13. However, even when the 
availability issues can be surmounted, there is very limited scope for bulk 
retrieval of full-text (again PMC and BMC must be commended for their full-
text. retrieval systems). It appears that the majority of fUll-text providers 
perceive little need for bulk download services. 
The use of newly available automated tools can allow us to collect relatively 
large data-sets (i.e., thousands and millions of articles). Notable examples of 
these tools include the proprietary Quosa Information Manager14 and the 
GetltFull tool (Natarajan, Haines et al. 2006), as well as an academically-
derived tool known as BioRat (Corney, Buxton et al. 2004) that is free for 
academic research purposes. There are, however, difficulties with all these 
tools, that limit their utility for literature-mining purposes (see chapter 2). 
II http://www.biomedcentral.comlhome/ 
12 http://www.biomedcentral.comlinfo/aboutdatamining/ 
13 http://www .ncbi.nlm.nih.gov Ipmcl about oai.html 
14 http://www.quosa.coml 
The successful retrieval of full-text articles is heavily dependent on Internet 
Protocol (IP) address authentication and library subscription rights and URL 
accuracy. However, the publisher BMC has made a full-text corpus of peer-
reviewed fUll-text articles available for download15. These articles are heavily 
annotated, using XML tags that mark up the different sections, titles, 
headings, etc., in the text. Interestingly, Nature Publishing Group 16 has 
submitted a draft proposal (the Open Text-Mining Interface, OTMI17) to make 
full-text from its journals available for research purposes. Under the OTMI, 
publishers would be free to provide text in whatever form they wish, with NPG 
offering individual sentences but out of paragraph order: given that most text-
mining processes occur at the sentence level, this seems a promising 
approach. Nature Publishing Group (NPG) also included an open call to other 
publishers for cooperation in the initiative. The level of interest in the OTMI 
has been limited, with only one article (currently reported by Google Scholar 
[01/10108]) making reference to it (Altman, Bergman et al. 2008); furthermore, 
NPG are no longer actively pursuing the project17. This demonstrates that 
publishers (e.g., NPG) are keen to provide access to their text, albeit with 
some access restrictions, but that currently there is no interest from the 
research community. The lack of coverage for phylogenetics publications and 
the presentation as out-of-order sentences (rather than as a structured 
document with article sections) precluded the use of OTMI data in chapters 3 
and 4. Perhaps, similar reasons have also reduced the utility of OTMI to 
others and therefore limited its adoption by other publishers. 
Both restrictions on access have created a less than favourable environment 
for text- and data-mining research on scientific literature. Current research 
has been mainly limited to the use of the Medline database of abstracts and 
article metadata, which although information-dense in nature, does heavily 
restrict the range of research questions we can address with literature 
15 http://www.biomedcentral.comlinfo/about/datamining/ 
16 http://npg.nature.com 
17 http://opentextmining.org/wikilMain _Page 
analysis. There is good evidence that abstracts are good quality summaries 
of an article (Schuemie, Weeber et a/. 2004; Cohen and Hersh 2005; 
Krallinger and Valencia 2005), but that is all they are: they do not provide 
space for the detail of the experimental conditions. This means that the 
validity of information derived from abstracts can be unknown: some results 
are well supported by data, others are conjecture. When we consider the 
size, scope and value of resources built upon Medline text (e.g., Medie 
(Miyao, Ohta et a/. 2006), Info-PubMed18, iHOP (Hoffmann and Valencia 
2004)), it seems prudent that we move to qualify the extracted results. 
1.4.3. Structured Literature 
As anyone who has ever written a research article will know, a lot of work 
goes into each publication, but, more importantly, a lot of information is forced 
into what is essentially a compact form of communication. This requires some 
non-trivial compression of the complexity and quality of information to encode 
it into the right forms to deliver a cogent research publication. The different 
elements of an article that could be of use for future analysis are sparsely 
distributed, being surrounded by explanatory detail and text that is both 
necessary to aid human understanding but also clouds comprehension by 
automated methods. 
As was previously discussed, all currently available file formats for electronic 
articles require some processing to extract the article text. This makes a 
plain-text conversion step necessary; the most common file format, PDF, 
presents the greatest challenge to plain-text extraction. However, there are a 
number of tools that do a good job of extracting text, and can even retain 
some of the formatting, formatting being an important prerequisite when we 
want to reconstruct the content of the article. 
18 http://www-tsujii.is.s.u-tokyo.ac.jp/info-pubmed/ 
HTML articles are no more structured than plain-text; however the pattern and 
type' of HTML tags could be used to infer article structure, if the formatting 
was found to be consistent across articles. In practice, however, this 
formatting varies significantly between journals and articles, and therefore , 
only general trends can be relied upon (Hollingsworth, Lewin ef al. 2005). 
XML articles are the best suited to analysis: their formatting is directly linked 
to the article structure, and the large collections provided by PMC19 and 
BMC20 follow a standardised format for each article. 
1.4.4. Metadata 
Metadata, as applied to articles, . usually includes information on the where, 
when and who of an article: e.g., who are the authors of the article? 
Unfortunately, articles and their metadata are often widely separated (Hull, 
Pettifer ef al. 2008), with the article being a file on your computer and the 
metadata being somewhere on a database. Additionally, it is not a simple 
task to match the two up; there is no single system for associating metadata 
with an article, and most researchers resort to maintaining metadata and 
articles separately, only manually linking the two when necessary. Accurate 
metadata is a powerful resource; if we can link an article with its metadata 
reliably, then we can link articles with their authors, their publishing journal, 
their methods, their data and anything else. This opens up a whole host of 
possibilities that are currently only feasible in small numbers of highly 
structured articles (Page 2008b); for example, Page (2008b; 2008a) has 
created summary web pages for a set of full-text articles, each including Web 
links to all the species, cited-papers, authors and geographic coordinates 
present in the article. 
19 http://www.ncbi.nlm.nih.gov/pmcl 
20 http://www.biomedcentral.com 
1.4.5. Organisation of literature 
To an observer with no first-hand experience of the task, collecting and 
organising appropriate research articles, would appear simple, with many 
different helpful tools to ease the process. There is software available to 
organise citations (e.g., Endnote21 ), there are huge Web resources that 
catalogue millions of articles (PubMed22 , SCOpUS23 , Web of Knowledge24) and 
there are also Websites and tools that will organise your library of articles and 
help you to download electronic copies (Endnote web25 , Papers26 , pubget2\ 
However, it is difficult to get these tools to communicate with each other, and 
there is not a single tool that can do all of the necessary tasks. This is the first 
problem with good access to research literature. 
1.5. Analysing the use of methods 
If we can successfully tame the problems previously described in relation to 
the literature, then we can move on to analysing it. Because published 
literature is the basis of all modern research, we can immediately begin to 
explore what is being done now in the light of what has come before. This will 
enable us to study temporal patterns of methodological practice in 
phylogenetics; by capturing which methods the community have used as well 
as when they did so. 
21 http://www.endnote.com 
22 http://www .ncbi.nlm.nih.gov/pubmedl 
23 http://www.scopus.com 
24 http://wok.mimas.ac.uk/ 
25 http://www.endnoteweb.com 
26 http://mekentosj .compapers/ 
27 http://pubget.com 
1.5.1. Text-mining of methodologies 
Text-mining in a simple form has been a part of bioinformatics for a long time. 
But it is only with the more recent interest from the Natural Language 
Processing (NLP) community, in the application of methods from their field to 
literature from the biomedical domain, that research in this area has become 
known in its own right. All of the major bioinformatics journals (Bioinformatics, 
BMC Bioinformatics, Nucleic Acids Research) now include text-mining 
research in most issues. 
When reporting the accuracy of text-mining methods, it is common to report 
the precision (often abbreviated to P), recall (abbreviated to R) and the F-
score or F-measure (F1). Precision measures the proportion of results 
reported by an analysis that are correct. Recall is the proportion of all correct 
results that were recovered by the approach. The F-score is the harmonic 
mean of the precision and recall values (Feldman and Sanger 2007). 
There are two major approaches to text-mining as applied to bioscience: the 
NLP or computer science approach, and the bioinformatics approach - the 
first concentrates on advancing the accuracy of current methods and the 
second centres on generating new data. Both complement each other well, 
with improving methods leading to better results. However, the range of tasks 
to which text-mining has been applied are limited. This is probably the result 
of the literature analysis problems previously defined, but is also because of 
lack of communication between people who do text-mining and people who 
benefit from the results of it (Altman, Bergman et al. 2008). The availability of 
software and tools to perform biomedical text-mining has also been limited, for 
example, a central repository for text mining software does not currently exist. 
This is in direct contrast with other elements of bioinformatics, where software 
availability is seen as a necessity for increased citation and successful 
publication. There are, however, many Web resources that expose the 
results of biomedical text-mining (Hoffmann and Valencia 2004; Miyao, Ohta 
et al. 2006). These represent an important step in raising awareness of the 
value of text-derived data. 
Most text-mining studies analyse article abstracts rather than full articles 
(Cohen and Hersh 2005; Zweigenbaum, Demner-Fushman et al. 2007; 
Altman, Bergman et al. 2008). Abstracts are a summary of their article and 
must be kept short; therefore, they contain text on the background, results 
and conclusions of the work, with little attention given to the methods. This, in 
turn, has led to most studies looking to identify results in the text (Jenssen, 
Laegreid et al. 2001; Cohen and Hersh 2005), without considering the 
methodology used to derive them. As already mentioned, results cannot be 
considered in isolation, and must be evaluated with knowledge of the methods 
used to derive them. This makes a very strong case for the use of full-text 
articles in biomedical text-mining, and for any results identified being 
intrinsically linked with the methodology of the article. Very little has been 
published on the mining of experimental methods, the most notable example 
being the protein interaction method (IMS) SUb-task from the BioCreative II 
challenge (Krallinger, Leitner et al. 2008). The task asked participants to 
identify the method used to confirm a sample of 847 protein-protein 
interactions. The task provided the participants with a controlled vocabulary 
(CV) of possible methods and asked them to assign one to each interaction. 
Tbis cannot be regarded as a particularly difficult task, given that the CV 
provides all possible methods, and that only one is required for each result. 
Even in this restricted setting, the task proved to be difficult for approximate 
string-matching techniques (the best F-score was only 45%, with only 2 out of 
30 teams attempting the task), indicating that more complex text-mining 
techniques are needed to capture experimental methods and their context in 
research articles (Soldatova, Batchelor et al. 2007). 
1.5.2. Analysing communities of practice 
Research communities change through time (Hopcroft, Khan et al. 2004), and 
most significantly for this work, so do the methodologies that these 
communities use. This is one of the main subjects of chapter 4. There are 
two sets of literature in this area: the first comes from network analysis of 
scientific collaboration networks (Barabasi, Jeong et al. 2002; Newman 2004; 
Wagner and Leydesdorff 2005) and the second takes a more socio-economic 
approach (Brown and Duguid 2001; Metcalfe, James et al. 2005; Mina, 
Ramlogan et al. 2007). Several analysis options are described in the 
collaboration network literature for identifying clusters and other structures in 
networks of collaborating scientists. For example, the assortativity coefficient 
(Newman 2003) (as used in chapter 4) calculates the level of mixing in the 
network, where a highly mixed network is one that contains edges that mostly 
connect nodes of different types (or in the case of author collaboration 
networks, nodes from different communities). This method works well at 
defining the level of community structure, as long as the network is modelled 
correctly, with edges representing a level of relatedness between nodes. The 
second set of literature give examples of the evolution of knowledge in a given 
domain, highlighting the propensity for communities to remain disjointed even 
when pursuing a common purpose (Mina, Ramlogan et al. 2007). There is 
also discussion of theoretical reasons why groups of workers remain 
disjointed despite a common purpose, the suggestion of Brown (Brown and 
Duguid 2001) being that the membership of each group is defined by 
differences in practice. 
1.5.3. Metrics of quality 
Metrics have been defined to assess quality of all parts of science; from 
authors to research institutions, there is a metric available to assess quality. 
Table 1.1 presents a summary of currently used metrics. 
,- THr-
i I 'L .. 
 t ~ .:. ,"j \" r. f\ ~ ~r"'to 
l .I I I ~ t J"\ > ~ 1_ 0 
~";:'i\ : ~~-)':TY 
I I " .. -. , ; "1 ~_):)r~.f<.-~ j '_._---
Metric Level 
Author 
Article 
Journal 
Institution 
Metric Name 
h-index 
Mean citations per paper 
Papers per year 
Count of citations 
Impact factor of publishing 
journal 
SClmago SJR 
Impact factor 
Mean citations 
Research power (RAE, REF) 
Research quality (RAE, REF) 
Reference 
(Hirsch 2005) 
(van Raan 2006a) 
(van Raan 2006b) 
Very common, 
reference 
(van Raan 2006a) 
~O.31 
3031 
no single 
Table 1.1 - List of currently used quality metrics grouped by application. Metrics are 
listed according to their application domain (i.e., author, articles or journal). The name 
of the metric and references discussing the metric are also provided. 
There is a large but rather widely distributed literature devoted to metrics. 
Articles on the subject can appear in general science journals, such as 
Science32 to Scientometrics33 , a journal devoted to research on metrics. 
There are also resources that provide metric data, with Thompson lSI 
providing the most widely used data, namely journal impact factors and article 
citation counts. Newer resources have attempted to expand the range of 
metric data, with SCOpUS34 providing author h-index scores and SCImag035 
using their own Google Page Rank-based journal and country scoring system 
(Falagas, Kouranos et al. 2008). In a recent, high-profile, publication 
28 http://www.thomsonreuters.comlproducts _ services/science/ 
academic/impact_ factor/ 
29 http://www.scimagojr.comlSClmagoJoumalRank.pdf 
30 http://www.rae.ac.uk/Results/intro.aspx 
31 http://www.hefce.ac. uk/Research/ref/ 
32 http://www.sciencemag.org/ 
33 http://www.springerlink.comlcontentl0138-9130 
34 http://www.scopus.coml 
35 http://www.scimagojr.coml 
(Lehmann, Jackson et al. 2006), it was suggested that, for author quality 
mean citations per published paper provided a fairer measure of quality than 
h-index and papers per year. 
This increase in metrics has led to more research on how the metrics 
compare with each other in measuring quality (Bollen, de Sompel et al. 2005; 
Lehmann, Jackson et al. 2006; van Raan 2006b; van Raan 2006a). 
Obviously, this presents the problem of how to compare metric scores with 
each other (van Raan 2006a) and with other more independent measures of 
quality (Lehmann, Jackson et al. 2006; Harnad 2008). Approaches so far 
have included probabilistic inference (Lehmann, Jackson et al. 2006), ranking 
by peers (Harnad 2008) and ranking by questionnaire survey (Nederhof, 
Luwel et al. 2001). These approaches all work well, and help us to better 
understand the nature of metrics that are becoming increasingly powerful in 
science, especially in the UK36. However, more work should be put into 
deriving metrics that can be tuned by the people who use them: chapter 5 
presents work in this area, where the aim is not to provide the best match with 
a comparative score of quality, but is instead to better capture information that 
reflects the current needs of the research community. 
All of the metrics in Table 1.1 provide a score based on citation data; this 
seems reasonable, because to have your work cited shows that ano~her 
scientist has read arid shown interest in your work. However, this interest 
could be positive or negative, because articles can be cited to highlight 
erroneous results or to provide an exemplar of undesirable scientific practice. 
Citations can be said to be an inference of the level of interest in an article. 
However, it seems there may be room for metrics that assess science, not on 
overall quality but on individual components of it. If we consider an article 
demonstrating a new method to perform an existing task, it would not be well 
cited until that method becomes established. However, the article is 
36 http://www.hefce.ac. uk/Research/ref/ 
presenting a piece of innovative and novel work. Often, when considering 
articles or authors, we do not want those who are the 'best' overall, but 
instead we want to know what is newest or what is most tried and tested. 
Researchers have different aims when considering the work of others, and 
this should be catered for. There is a risk that, if quality metrics remain bound 
to citation data, innovation will be hampered by the Matthew effect, which is a 
common problem in science. The Matthew effece7 (Merton 1968; Merton 
1988) is a biasing influence on the number of citations an article will receive, 
whereby the well cited are continually cited and the uncited remain so. This is 
something that is explored more fully in chapter 5. 
This chapter has presented the background to the aim of this thesis. The 
importance of methods in science has been highlighted and this has been 
linked with problems inherent to phylogenetic analysis, specifically, how to 
choose which methods to use for a given analytical task? We then described 
how this question could be answered using information from the literature. 
We also outlined the challenges of collecting and analysing text from the 
published literature. Finally, we outlined approaches for using this text (with 
its supporting metadata) to inform experimental design choices in 
phylogenetics. The next 4 chapters provide full details (including 
implementation and testing) of our approach to inform experimental design 
decisions in phylogenetics using information derived from the published 
literature. 
37 http://blogs. b bsrc.ac. uk/index. php/2009/03/the-matthew-effect -in -science/ 
1.6. References 
Altman, R. 8., C. M. Bergman, J. Blake, C. Blaschke, A Cohen, F. Gannon, L. 
Grivell, U. Hahn, W. Hersh, L. Hirschman, L. J. Jensen, M. Krallinger, 
B. Mons, S. I. O'Donoghue, M. C. Peitsch, D. Rebholz-Schuhmann, H. 
Shatkay and A Valencia (2008). "Text mining for biology - the way 
forward: opinions from leading scientists." Genome Biology 9 Suppl 2: 
Barabasi, A. L., H. Jeong, Z. NEda, E. Ravasz, A Schubert and T. Vicsek 
(2002). "Evolution of the social network of scientific collaborations." 
Physica A: Statistical Mechanics and its Applications 311 (3-4): 590-
Berners-Lee, T., J. Hendler and O. Lassila (2001). "The Semantic Web - A 
new form of Web content that is meaningful to computers will unleash a 
revolution of new possibilities." Scientific American 284(5): 34. 
Bollen, J., H. V. de Sompel, J. A Smith and R. Luce (2005). "Toward 
alternative metrics of journal impact: A comparison of download and 
citation data." Information Processing & Management 41 (6): 1419-
1440. 
Brazma, A, P. Hingamp, J. Quackenbush, G. Sherlock, P. Spellman, C. 
Stoeckert, J. Aach, W. Ansorge, C. A Ball, H. C. Causton, T. 
Gaasterland, P. Glenisson, F. C. Holstege, I. F. Kim, V. Markowitz, J. 
C. Matese, H. Parkinson, A Robinson, U. Sarkans, S. Schulze-Kremer, 
J. Stewart, R. Taylor, J. Vilo and M. Vingron (2001). "Minimum 
information about a microarray experiment (MIAME)-toward standards 
for microarray data." Nature Genetics 29(4): 365-71. 
Brown, J. S. and P. Duguid (2001). "Knowledge and Organization: A Social-
Practice Perspective." Organization Science 12(2): 198-213. 
Camin, J. H. and R. R. Sokal (1965). "A method for deducing branching 
sequences in phylogeny." Evolution 19: 311-326. 
Cohen, A M. and W. R. Hersh (2005). "A survey of current work in biomedical 
text mining." Briefings in Bioinformatics 6(1): 57-71. 
Corney, D. P., 8. F. Buxton, W. B. Langdon and D. T. Jones (2004). "BioRAT: 
extracting biological information from full-length papers." Bioinformatics 
20( 17): 3206-13. 
Douady, C. J., F. Delsuc, Y. Boucher, W. F. Doolittle and E. J. P. Douzery 
(2003). "Comparison of Bayesian and maximum likelihood bootstrap 
measures of phylogenetic reliability." Molecular Biology and Evolution 
20(2): 248-254. 
Drummond, A. J. and A. Rambaut (2007). "BEAST: Bayesian evolutionary 
analysis by sampling trees." BMC Evolutionary Biollogy 7: 214. 
Edgar, R. C. (2004). "MUSCLE: a multiple sequence alignment method with 
reduced time and space complexity." BMC Bioinformatics 5: 113. 
Edgar, R. C. and S. Batzoglou (2006). "Multiple sequence alignment." Current 
Opinion in Structural Biology 16(3): 368-73. 
Edwards, A. W. F. and L. L. Cavalli-Sforza (1964). "Reconstruction of 
evolutionary trees." Phenetic and Phylogenetic Classification 6: 67-76. 
Eggleton, P. and R. I. Vane-Wright (1994). "Phylogenetics and ecology." 
Linnean Society symposium series. 
Erixon, P., B. Svennblad, T. Britton and B. Oxelman (2003). "Reliability of 
Bayesian posterior probabilities and bootstrap frequencies in 
phylogenetics." Systematic Biology 52(5): 665-673. 
Falagas, M. E., V. D. Kouranos, R. Arencibia-Jorge and D. E. 
Karageorgopoulos (2008). "Comparison of SClmago journal rank 
indicator with journal impact factor." FASEB Journal 22(8): 2623-8. 
Feldman, R. and J. Sanger (2007). The Text Mining Handbook. Cambridge, 
Cambridge university press. 
Felsenstein, J. (1981). "Evolutionary Trees from DNA-Sequences - a 
Maximum-Likelihood Approach." Journal of Molecular Evolution 17(6): 
368-376. 
Felsenstein, J. (1985). "Confidence-Limits on Phylogenies - an Approach 
Using the Bootstrap." Evolution 39(4): 783-791. 
Felsenstein, J. (2004). Inferring phylogenies, SinauerAssociates Sunderland, 
Mass., USA. 
Gentleman, R. C., V. J. Carey, D. M. Bates, B. Bolstad, M. Dettling, S. Dudoit, 
B. Ellis, L. Gautier, Y. Ge, J. Gentry, K. Hornik, T. Hothorn, W. Huber, 
S. lacus, R. Irizarry, F. Leisch, C. Li, M. Maechler, A J. Rossini, G. 
Sawitzki, C. Smith, G. Smyth, L. Tierney, J. Y. Yang and J. Zhang 
(2004). "Bioconductor: open software development for computational 
biology and bioinformatics." Genome Biology 5(10): R80. 
Giles, J. (2006). "The trouble with replication." Nature 442(7101): 344-347. 
Gilmour, R. (2000). "Taxonomic markup language: applying XML to 
systematic data." Bioinformatics 16(4): 406-407. 
Hagstrom, G. I., D. H. Hang, C. Of ria and E. Torng (2004). "Using Avida to 
test the effects of natural selection on phylogenetic reconstruction 
methods." Artificial Life 10(2): 157-166. 
Harnad, S. (2008). "Validating research performance metrics against peer 
rankings." Ethics in Science and Environmental Politics. 
Harvey, P. H. and S. Nee (1994). "Phylogenetic Epidemiology Lives." Trends 
in Ecology & Evolution 9(10): 361-363. 
Harvey, P. H. and M. D. Pagel (1991). The comparative method in 
evolutionary biology. Oxford, Oxford University Press. 
Hasegawa, M., H. Kishino and T. A Yano (1985). "Dating of the Human Ape 
Splitting by a Molecular Clock of Mitochondrial-DNA" Journal of 
Molecular Evolution 22(2): 160-174. 
Herbst, J. and D. Karagiannis (2004). "Workflow mining with InWoLvE." 
Computers in Industry 53(3): 245-264. 
Higgins, D. G. and P. M. Sharp (1988). "CLUSTAL: a package for performing 
multiple sequence alignment on a microcomputer." Gene 73(1): 237-
Hirsch, J. E. (2005). "An index to quantify an individual's scientific research 
output." Proceedings of the National Academy of Sciences of the 
United States of America 102(46): 16569-72. 
Hoffmann, R. and A Valencia (2004). "A gene network for navigating the 
literature." Nature Genetics 36(7): 664. 
Hollingsworth, B., I. Lewin and D. Tidhar (2005). Retrieving Hierarchical Text 
Structure from Typeset Scientific Articles-a Prerequisite for E-Science 
Text Mining. Proceedings of the 4th E-Science All Hands Meeting. 
Hopcroft, J., O. Khan, B. Kulis and B. Selman (2004). "Tracking evolving 
communities in large linked networks." Proceedings of the National 
Academy of Sciences of the United States of America 101 (Suppl 1): 
5249-5253. 
Huelsenbeck, J. P. and B. Rannala (1997). "Phylogenetic methods come of 
age: Testing hypotheses in an evolutionary context." Science 
276(5310): 227-232. 
Hull, D., S. R. Pettifer and D. B. Kell (2008). "Defrosting the digital library: 
bibliographic tools for the next generation web." PLoS Computational 
Biology 4(10): e1000204. 
Hwang, S. Y. and W. S. Yang (2002). "On the discovery of process models 
from their instances." Decision Support Systems 34( 1 ): 41-57. 
Jenssen, T. K., A. Laegreid, J. Komorowski and E. Hovig (2001). "A literature 
network of human genes for high-throughput analysis of gene 
expression." Nature Genetics 28(1): 21-8. 
Jin, L. and M. Nei (1990). "Limitations of the Evolutionary Parsimony Method 
of Phylogenetic Analysis." Molecular Biology and Evolution 7(1): 82-
Jow, H., C. Hudelot, M. Rattray and P. G. Higgs (2002). "Bayesian 
phylogenetics using an RNA substitution model applied to early 
mammalian evolution." Molecular Biology and Evolution 19(9): 1591-
1601. 
Krallinger, M., F. Leitner, C. Rodriguez-Penagos and A. Valencia (2008). 
"Overview of the protein-protein interaction annotation extraction task 
of BioCreative 11." Genome Biology 9 Suppl 2: S4. 
Krallinger, M. and A. Valencia (2005). "Text-mining and information-retrieval 
services for molecular biology." Genome Biology. 
Kumar, S., K. Tamura and M. Nei (2004). "MEGA3: Integrated software for 
molecular evolutionary genetics analysis and sequence alignment." 
Briefings in Bioinformatics 5(2): 150-163. 
Lander, E. S., L. M. Linton, B. Birren, C. Nusbaum, M. C. Zody, J. Baldwin, K. 
Devon, K. Dewar, M. Doyle, W. FitzHugh, R. Funke, D. Gage, K. 
Harris, A. Heaford, J. Howland, L. Kann, J. Lehoczky, R. LeVine, P. 
McEwan, K. McKernan, J. Meldrim, ,J. P. Mesirov, C. Miranda, W. 
Morris, J. Naylor, C. Raymond, M. Rosetti, R. Santos, A. Sheridan, C. 
Sougnez, N. Stange-Thomann, N. Stojanovic, A. Subramanian, D. 
Wyman, J. Rogers, J. Sulston, R. Ainscough, S. Beck, D. Bentley, J. 
Burton, C. Clee, N. Carter, A. Coulson, R. Deadman, P. Deloukas, A. 
Dunham, I. Dunham, R. Durbin, L. French, D. Grafham, S. Gregory, T. 
Hubbard, S. Humphray, A. Hunt, M. Jones, C. Lloyd, A. McMurray, L. 
Matthews, S. Mercer, S. Milne, J. C. Mullikin, A. Mungall, R. Plumb, M. 
Ross, R. Shownkeen, S. Sims, R. H. Waterston, R. K. Wilson, L. W. 
Hillier, J. D. McPherson, M. A. Marra, E. R. Mardis, L. A. Fulton, A. T. 
Chinwalla, K. H. Pepin, W. R. Gish, S. L. Chissoe, M. C. Wendl, K. D. 
Delehaunty, T. L. Miner, A. Delehaunty, J. B. Kramer, L. L. Cook, R. S. 
Fulton, D. L. Johnson, P. J. Minx, S. W. Clifton, T. Hawkins, E. 
Branscomb, P. Predki, P. Richardson, S. Wenning, T. Slezak, N. 
Doggett, J. F. Cheng, A. Olsen, S. Lucas, C. Elkin, E. Uberbacher, M. 
Frazier, R. A. Gibbs, D. M. Muzny, S. E. Scherer, J. B. Bouck, E. J. 
Sodergren, K. C. Worley, C. M. Rives, J. H. Gorrell, M. L. Metzker, S. 
L. Naylor, R. S. Kucherlapati, D. L. Nelson, G. M. Weinstock, Y. 
Sakaki, A. Fujiyama, M. Hattori, T. Yada, A. Toyoda, T. Itoh, C. 
Kawagoe, H. Watanabe, Y. Totoki, T. Taylor, J. Weissenbach, R. 
Heilig, W. Saurin, F. Artiguenave, P. Brottier, T. Bruls, E. Pelletier, C. 
Robert, P. Wincker, D. R. Sl)1ith, L. Doucette-Stamm, M. Rubenfield, K. 
Weinstock, H. M. Lee, J. Dubois, A. Rosenthal, M. Platzer, G. 
Nyakatura, S. Taudien, A. Rump, H. Yang, J. Yu, J. Wang, G. Huang, 
J. Gu, L. Hood, L. Rowen, A. Madan, S. Qin, R. W. Davis, N. A. 
Federspiel, A. P. Abola, M. J. Proctor, R. M. Myers, J. Schmutz, M. 
Dickson, J. Grimwood, D. R. Cox, M. V. Olson, R. Kaul, N. Shimizu, K. 
Kawasaki, S. Minoshima, G. A. Evans, M. Athanasiou, R. Schultz, B. A. 
Roe, F. Chen, H. Pan, J. Ramser, H. Lehrach, R. Reinhardt, W. R. 
McCombie, M. de la Bastide, N. Dedhia, H. Blocker, K. Hornischer, G. 
Nordsiek, R. Agarwala, L. Aravind, J. A. Bailey, A. Bateman, S. 
Batzoglou, E. Birney, P. Bork, D. G. Brown, C. B. Burge, L. Cerutti, H. 
C. Chen, D. Church, M. Clamp, R. R. Copley, T. Doerks, S. R. Eddy, E. 
E. Eichler, T. S. Furey, J. Galagan, J. G. Gilbert, C. Harmon, Y. 
Hayashizaki, D. Haussler, H. Hermjakob, K. Hokamp, W. Jang, L. S. 
Johnson, T. A. Jones, S. Kasif, A. Kaspryzk, S. Kennedy, W. J. Kent, 
P. Kitts, E. V. Koonin, I. Korf, D. Kulp, D. Lancet, T. M. Lowe, A. 
McLysaght, T. Mikkelsen, J. V. Moran, N. Mulder, V. J. Pollara, C. P. 
Ponting, G. Schuler, J. Schultz, G. Slater, A. F. Smit, E. Stupka, J. 
Szustakowski, D. Thierry-Mieg, J. Thierry-Mieg, L. Wagner, J. Wallis, 
R. Wheeler, A. Williams, Y. I. Wolf, K. H. Wolfe, S. P. Yang, R. F. Yeh, 
F~ Collins, M. S. Guyer, J. Peterson, A. Felsenfeld, K. A. Wetterstrand, 
A. Patrinos, M. J. Morgan, P. de Jong, J. J. Catanese, K. Osoegawa, 
H. Shizuya, S. Choi and Y. J. Chen (2001). "Initial sequencing and 
analysis of the human genome." Nature 409(6822): 860-921. 
Larkin, M. A., G. Blackshields, N. P. Brown, R. Chenna, P. A. McGettigan, H. 
McWilliam, F. Valentin, I. M. Wallace, A. Wilm, R. Lopez, J. D. 
Thompson, T. J. Gibson and D. G. Higgins (2007). "Clustal Wand 
Clustal X version 2.0." Bioinformatics 23(21): 2947-8. 
Leebens-Mack, J., T. Vision, E. Brenner, J. E. Bowers, S. Cannon, M. J. 
Clement, C. W. Cunningham, C. dePamphilis, R. deSalle, J. J. Doyle, 
J. A. Eisen, X. Gu, J. Harshman, R. K. Jansen, E. A. Kellogg, E. V. 
Koonin, B. D. Mishler, H. Philippe, J. C. Pires, Y. L. Qiu, S. Y. Rhee, K. 
Sjolander, D. E. Soltis, P. S. Soltis, D. W. Stevenson, K. Wall, T. 
Warnow and C. Zmasek (2006). "Taking the first steps towards a 
standard for reporting on phylogenies: Minimum Information About a 
Phylogenetic Analysis (MIAPA)." Omics : a journal of integrative 
biology 10(2): 231-7. 
Lehmann, S., A. D. Jackson and B. E. Lautrup (2006). "Measures for 
measures." Nature 444(7122): 1003-4. 
Marshall, C. R. (1992). "Substitution bias, weighted parsimony, and amniote 
phylogeny as inferred from 18S rRNA sequences." Molecular Biology 
and Evolution 9(2): 370-7; author reply 378. 
Merton, R. K. (1968). "The Matthew effect in science. The reward and 
communication systems of science are considered." Science 159(810): 
56-63. 
Merton, R. K. (1988). "The Matthew Effect in Science, II: Cumulative 
Advantage and the Symbolism of Intellectual Property." Isis 79(4): 606-
Metcalfe, J. S., A James and A Mina (2005). "Emergent innovation systems 
and the delivery of clinical services: The case of intra-ocular lenses." 
Research Policy 34(9): 1283-1304. 
Mina, A, R. Ramlogan, G. Tampubolon and J. S. Metcalfe (2007). "Mapping 
evolutionary trajectories: Applications to the growth and transformation 
of medical knowledge." Research Policy 36(5): 789-806. 
Miyao, Y., T. Ohta, K. Masuda, Y. Tsuruoka, K. Yoshida, T. Ninomiya and J. 
Tsujii (2006). Semantic Retrieval for the Accurate Identification of 
Relational Concepts in Massive Textbases. Proceedings of COLlNG-
ACL 2006. Sydney, Australia. 
Natarajan, J., C. Haines, 8. Berglund, C. DeSesa, C. Hack, W. Dubitzky and 
E. G. Bremer (2006). GetitFull - A Tool for Downloading and Pre-
processing Full-Text Journal Articles. Lecture Notes in Computer 
Science. Heidelberg, Springer Berlin. 3886: 139-145. 
Nature-Editorial (2006). "Let's replicate." Nature 442(7101): 330-330. 
Nederhof, A J., M. Luwel and H. F. Moed (2001). "Assessing the quality of 
scholarly journals in Linguistics: An alternative to citation-based journal 
impact factors." Scientometrics 51 (1): 241-265. 
Nei, M. and S. Kumar (2000). Molecular Evolution and Phylogenetics. Oxford, 
Oxford University Press. 
Newman, M. E. (2003). "Mixing patterns in networks." Physical review. E, 
Statistical. nonlinear, and soft matter physics 67(2 Pt 2): 026126. 
Newman, M. E. (2004). "Coauthorship networks and patterns of scientific 
collaboration." Proceedings of the National Academy of Sciences of the 
United States of America 101 Suppl 1: 5200-5. 
Notredame, C., D. G. Higgins and J. Heringa (2000). "T-Coffee: A novel 
method for fast and accurate multiple sequence alignment." Journal of 
Molecular Biology 302(1): 205-17. 
~inn, T., M. Addis, J. Ferris, D. Marvin, M. Senger, M. Greenwood, T. Carver, 
K. Glover, M. R. Pocock, A Wipat and P. Li (2004). "Taverna: a tool for 
the composition and enactment of bioinformatics workflows." 
Bioinformatics 20(17): 3045-3054. 
~inn, T., M. GreenWOOd, M. Addis, M. N. Alpdemir, J. Ferris, K. Glover, C. 
Goble, A Goderis, D. Hull, D. Marvin, P. Li, P. Lord, M. R. Pocock, M. 
Senger, R. Stevens, A. Wipat and C. Wroe (2006). "Taverna: lessons 
in creating a workflow environment for the life sciences." Concurrency 
and Computation: Practice and Experience 18(10): 1067-1100. 
Page, R. (2008a). "Towards realising Darwin's dream: setting the trees free. 
Available from Nature Precedings 
<http://dx.doLorg/10.1038/npre.2008.2217.1 >." 
Page, R. (2008b). "Visualising a scientific article. Available from Nature 
Precedings <http://dx.doLorg/10.1038/npre.2008.2579.1 > ." 
Posada, D. and K. A. Crandall (1998). "MODEL TEST: testing the model of 
DNA substitution." Bioinformatics 14(9): 817-818. 
Ronquist, F. and J. P. Huelsenbeck (2003). "MrBayes 3: Bayesian 
phylogenetic inference under mixed models." Bioinformatics 19( 12): 
1572-4. 
Saitou, N. and M. Nei (1987). "The Neighbor-Joining Method - a New Method 
for Reconstructing Phylogenetic Trees." Molecular Biology and 
Evolution 4(4): 406-425. 
Schimm, G. (2004). "Mining exact models of concurrent workflows." 
Computers in Industry 53(3): 265-281. 
Schuemie, M. J., M. Weeber, B. J. A. Schijvenaars, E. M. van Mulligen, C. C. 
van der Eijk, R. Jelier, B. Mons and J. A. Kors (2004). "Distribution of 
information in biomedical abstracts and fUll-text publications." 
Bioinformatics 20( 16): 2597-2604. 
Sokal, R. R. and C. D. Michener (1958). "A statistical method for evaluating 
systematic relationships." University of Kansas Science Bulletin 38: 
1409-1438. 
Soldatova, L., C. Batchelor, M. Liakata, H. Fielding, S. Lewis and R. King 
(2007). ART: An ontology based tool for the translation of papers into 
Semantic Web format. Proceedings of ontology workshop. ISMB 2007. 
Vienna, Austria. 
Stevens, R. D., A. J. Robinson and C. A. Goble (2003). "myGrid: personalised 
bioinformatics on the information grid." Bioinformatics 19 Suppl1: 
i302-4. 
Strimmer, K. and A. von Haeseler (1996). "Accuracy of Neighbor Joining for n-
Taxon Trees." Systematic Biology 45(4): 516-523. 
Suzuki, Y., G. V. Glazko and M. Nei (2002). "Overcredibility of molecular 
phylogenies obtained by Bayesian phylogenetics." Proceedings of the 
National Academy of Sciences of the United States of America 99(25): 
16138-16143. 
Thompson, J. D., D. G. Higgins and T. J. Gibson (1994). "CLUSTAL W: 
improving the sensitivity of progressive multiple sequence alignment 
through sequence weighting, position-specific gap penalties and weight 
matrix choice." Nucleic Acids Research 22(22): 4673-80. 
van der Aalst, W., T. Weijters and L. Maruster (2004). "Workflow mining: 
Discovering process models from event logs." IEEE Transactions on 
Knowledge and Data Engineering 16(9): 1128-1142. 
van Raan, A. F. J. (2006a). "Comparison of the Hirsch-index with standard 
bibliometric indicators and with peer judgment for 147 chemistry 
research groups." Scientometrics 67(3): 491-502. 
van Raan, A. F. J. (2006b). "Statistical properties of bibliometric indicators: 
Research group indicator distributions and correlations." Journal of the 
American SOCiety for Information Science and Technology 57(3): 408-
Wagner, C. S. and L. Leydesdorff (2005). "Network structure, self-
organization, and the growth of international collaboration in science." 
Research Policy 34(10): 1608-1618. 
Williams, P. L. and W. M. Fitch (1990). "Phylogeny determination using 
dynamically weighted parsimony method." Methods in Enzymology 
183: 615-626. 
Zweigenbaum, P., D. Demner-Fushman, H. Yu and K. B. Cohen (2007). 
"Frontiers of biomedical text mining: current progress." Briefings in 
Bioinformatics 8(5): 358-75. 
2. Software for collecting and processing large numbers of fUll-text 
articles 
2.1. Abstract 
2.1.1. Background 
Text-mining methods have reached a significant level of complexity and 
reliability, yet the text that they operate on remains mostly limited to article 
abstracts. Full-text articles contain all manner of varied information that 
abstracts do not, they therefore represent a huge information resource that is 
potentially very valuable to the research community. 
2.1.2. Results 
Here, we present software to enable the collection, storage and pre-
processing of large numbers of full-text articles. Our software can be used in 
multiple ways from a graphical user interface through to integration with other 
software via the Java API. We have tested our software on a set of three 
. article-collection tasks: namely, the collection of literature that is gene-
specific, field-specific and organism-specific. The software achieved an 88% 
conversion from PubMed full-text link to the correct PDF. 
2.1.3. Conclusions 
This software enables, for the first time, collection of full-text articles in the 
numbers required for intelligent analysis of the literature by advanced text-
mining methods. 
2.1.4. Availability: http://code.google.com/p/fulltextarticledownloader/ 
2.2. Background 
As text-mining methods have matured and become more widely known in 
biology, researchers from a wider range of backgrounds have started to 
consider whether their research question can be answered by intelligent 
interrogation of the literature (Natarajan, Berrar et al. 2006; Aerts, Haeussler 
et al. 2008; Altman, Bergman et al. 2008; Eales, Pinney et al. 2008; Garten 
and Altman 2009; Rodriguez-Esteban and lossifov 2009; Zaremba, Ramos-
Santacruz et a/. 2009). Access to, and analysis of, full-text journal article 
literature is vital to the continued success of text-mining, as applied to the life 
sciences (Cohen and Hersh 2005; Zweigenbaum, Demner-Fushman et al. 
2007; Altman, Bergman et al. 2008). Currently, there has been only limited 
use of full-text, but it has already enabled a wider set of scientifically relevant 
questions to be answered (Natarajan, Berrar et al. 2006; Crangle, Cherry et 
al. 2007; Aerts, Haeussler et al. 2008; Eales, Pinney et al. 2008; Krallinger, 
Leitner et al. 2008; Muller, Rangarajan et al. 2008). There is existing software 
to automate the download of article collections (Corney, Buxton et al. 2004; 
Natarajan, Haines et a/. 2006). There are, however, difficulties with these 
tools: Quosa38 is commercial software that can be costly to license when 
downloading large numbers of articles; BioRAT (Corney, Buxton et al. 2004) 
is not primarily aimed at collection of large numbers of articles. Furthermore, 
GetltFull (Natarajan, Haines et al. 2006) uses software that is limited to the 
Microsoft Windows platform, requires journal-specific scripting code to be 
generated, and does not appear to be currently available. 
Limited access to full-text articles is hampering their use in studies (Altman, 
Bergman et al. 2008); therefore, we have created a set of open-source 
software components that, together, form an infrastructure for obtaining and 
structuring fUll-text journal literature for use in biologically-oriented 
investigations. Here, we present a software-download agent that can retrieve 
large numbers of full-text articles in PDF form, and then automatically convert 
them to plain-text. The retrieval is based on an original PubMed search, and 
allows the download of all results, the first n results, or only those selected by 
the user. The software can be used via a graphical or command-line 
interface, or by direct programmatic interaction with our Java API. 
38 http://www.quosa.com/ 
2.3. Implementation 
Our software is available to download from its project site39 It is a Java 
application with a graphical interface; it includes a search interface into the 
PubMed database, an HTML parser, full-text link-extraction system, and an 
article-download engine, to automatically retrieve PDF files based on the 
original search. All of these elements are combined to form an application 
with a graphical interface, and are also accessible to other software 
applications by working directly with the code available at the project site. We 
also provide the Java source code and JavaDoc documentation for the use of 
third-party developers who wish to integrate elements of our software with 
their own. 
2.3.1. PubMed Search Interface 
The search interface makes use of the ESearch40 service provided by the 
NCBI Entrez Programming Utilities41 (EUtils). This service allows 
programmatic access (via URLs) to the PubMed database. Keyword 
searches in any of the database search fields40 are available in the same form 
they are via the Entrez interface to PubMed. Our PubMed search box allows 
users to construct both simple and complex queries, by building up their 
queries through keyword- and search-field pairs to the level of detail required. 
For example, a basic search of PubMed through Entrez will search for the 
entered keywords in all fields; this can be achieved in our software by typing 
the query keyword/s, selecting 'all fields' and appending this to the query list, 
then activating the search. The matching records from the search are listed, 
and the user can then specify which (if any) of the records are to be 
downloaded. 
39 http://code.google.com/p/fulltextarticledownloader/ 
40 http://www.ncbi.nlm.nih.gov/entrez/query/static/esearch _ help.html 
41 http://www.ncbi.nlm.nih.gov/entrez/query/static/eutils _ help.html 
2.3.2. HTML Parser 
To be able to navigate the Web to locate PDF files, the software needs to be 
able to extract and follow Web links declared in HTML. The software uses an 
HTML parser to create a structured model of a Web page in memory and it 
then searches this model for all possible Web links. The model used to 
represent the Web page in memory is the Document Object Model42 (DaM). 
The DaM is a way of representing the structure of markup language files (i.e., 
XML, HTML, XHTML), which can then be manipulated and analysed by 
accessing standardised programming interfaces defined by the World Wide 
Web Consortium43 (W3C). One of the ways to extract information from DOM 
representations is by the use of XML Path Language44 (XPath) queries; these 
can be written to extract content from a DOM or compute values based on 
information in a DOM. In this case, our software will use XPath queries to 
extract Web link information from a DOM. 
Given that a human user needs to be able to locate the appropriate PDF link 
for an article on a Web page in their Web browser, by using a similar HTML 
parser to those used in Web browser software, we can simulate the kind of 
activity a human user would employ to find the PDF through their browser. 
HTML parsing avoids problems associated with the simpler approach of 
analysing the raw HTML document as text and applying string-matching 
methods, such as regular expressions, to identify Web links. This simpler 
approach, often, either misses text associated with links, or is unable to 
identify all the true HTML links in the page. Our software makes use of the 
parser employed by the Firefox Web-browser produced by the Mozilla 
foundation45 Our software accesses the parser through Java code provided 
by the open source project known as 'mozillaparser' 46. 
42 http://www . w3 .org/DOM/ 
43 http://www.w3.org/ 
44 http://www.w3.orglTRlxpathl 
45 http://www.mozilla.org/ 
46 http://mozillaparser .sourceforge.netl 
2.3.3. Download Agent 
A base URL for access to the fUll-text of an article is provided by an 
automated query to another EUtiis service, ELink47. The download agent then 
accesses this URL, parses the subsequent HTML (if HTML is returned) into a 
DOM representation of the page; it then extracts the Web links from the page 
using an XPath query, these are then stored for further processing. The links 
are scored using a set of rules (see Link-scoring rules section) created to 
weight links based on how likely they are to pOint to a PDF file. The agent 
then accesses the highest quality link until it either locates a PDF file or it has 
moved 2 links away from the starting link. If a PDF file is not found (after 
following 2 links), it will then move back to the starting link and follow 2 more 
different sets of 3 links, before moving on to the next article. The agent will 
not proceed beyond this point, thus limiting the time required to ascertain 
whether a PDF file is available. The PDF-download process is multi-
threaded, with each agent thread being allocated a specific paper to 
download. All thread activity is logged and monitored and can be explored 
after download completion. 
2.3.4. Link-scoring rules 
The link-scoring rules are able to assess the ,quality of an HTML link based on 
the text associated with the link as well as its URL. Each rule has five 
properties; '10', 'field', 'pattern', 'weight' and 'polarity'. '10' is a string to identify 
the rule; 'field' determines which property (i.e., URL or link text) the rule 
should be applied to; 'pattern' is a regular expression that is used to 
determine if the link property matches the rule; 'weight' is the factor by which 
the link's score should be modified; and 'polarity' is whether the modification 
should be to reduce or increase the quality of the link. We supply a standard 
set of link-scoring rules with the software; these were found to work well 
during testing (see results). The rules are stored in an XML file (,rules.xml'), 
included with the software. This file can be easily updated or entirely replaced 
by users to alter the behaviour of the download agent. It is also possible, 
47 http://www.ncbi.nlm.nih.gov/entrez/query/static/elink_help.html 
given a small amount of extra work, to alter the activity of the download agent 
to download any kind of file (as long as it can be accurately differentiated from 
HTML). Each rule is applied to all links found in a page, and if the rule 
matches the link, then the links score is adjusted 'according to the rule. 
2.3.5. FileStore 
The software stores all downloaded files within a FileStore. A FileStore is 
specific to the software and is represented on the user's machine by a single 
directory containing an XML file ('filestore.xml', known as the index file), which 
lists the contents of the FileStore. The FileStore index file is used to prevent 
replication of downloads; a FileStore can be used repeatedly for different 
searches and the index file informs the software whether a requested 
download is present in the FileStore. The user can request that PDF, plain-
text and PubMed XML records for each paper are saved (or not) in the 
FileStore. PDF files are converted to plain-text using 'pdftotext' from the xpdf 
package (www.foolabs.com). Each of the file types (PDF, plain-text and XML) 
are stored in their own directories within the FileStore and can therefore be 
transferred to other locations easily for use with other applications. 
2.3.6. Finding PDF tests 
The main purpose of this software is to enable the automated collection of full-
text articles in numbers that are suitable for text-mining studies on a specific 
subject. We have defined and run three download tests to measure the ability 
of the software to navigate the Web and locate the required article in PDF 
form. To determine if the correct PDF was downloaded by the software (Table 
2.1, column 5), a thumbnail of the first page of every downloaded PDF file 
was generated; these were then inspected manually. 
2.3.6.1. Test 1 
The articles to download were identified using an automated query of PubMed 
via the EUtils service known as ELink. The query identified all PubMed 
records linked to the gene database entry with ID 92048, which is the human 
gene CD4 [GeneID:920]. 978 articles were identified by ELink (accessed 
22/12/08); an attempt was made to download all of these articles. 
2.3.6.2. Test 2 
The test articles were identified using the ESearch EUtils service to query the 
PubMed database for all records with a match to the search term 'phylogen*' 
in either the title or abstract field49  The use of the ,*, character implies that the 
term should match all words that have characters 'phylogen' and then any 
other characters immediately after this. The intention with this search term is 
to capture more of the phylogenetics-related literature by matching articles 
that use terms such as 'phylogeny' and 'phylogenomics'. 54,022 articles were 
identified from the search; 1,000 articles were randomly sampled from this set 
to test the accuracy of the download process. 
2.3.6.3. Test 3 
The final set of test articles was identified using another ELink query for all 
PubMed records linked to the taxonomy database entry with ID 1167650 
[Taxonomy:11676]. The query identified 5,657 records in PubMed. Of these, 
1,000 were sampled at random to test the accuracy of the download process. 
2.4. Results and Discussion 
We have tested the ability of our software to locate PDF documents using a 
set of three tests. The tests are designed to mirror the needs of current 
research activity on mining information from full-text journal articles. In this 
case, we are not considering how well we identify relevant literature, but 
instead how accurately we can identify and download articles that are 
48 http://view.ncbi.nlm.nih.gov/gene/920 
49 http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&field=tiab 
&term=phylogen * 
50 http://view.ncbi.nlm.nih.gov/taxonomy/11676 
assumed to be relevant. For each of the tests, the method for identifying the 
relevant article to download is declared in the Implementation section. 
The first test is based on a search for articles related to a specific gene (the 
human CD4 gene, Entrez Gene ID:920); this simulates activity that is very 
common and useful for text-mining studies: i.e., attempting to capture 
information specific to one or to a set of genes. The second test is the 
collection of articles related to a whole field (phylogenetics) via a simple 
PubMed search (see Implementation); this was used in a previous study 
(Eales, Pinney ef al. 2008), and represents the kind of query that could be 
used to identify trends and activity in a given field of research. The third test 
is organism specific, and attempts to identify articles related to HIV-1, a 
pathogen that is widely studied worldwide and significant to the work of many 
disparate research communities. 
Test PubMed Full-text PDFs found Correct 
PDF found 
(% of PDFs 
found) 
Conversion 
from full-
text link to 
correct PDF 
references Links (% of full-
to download available (% text links 
Test 1 
(Gene- 978 
specific) 
Test 2 
(Field- 1000 
specific) 
Test 3 
(Organism- 1000 
specific) 
of found) 
references 
found) 
767 (78.4%) 706 (92%) 703 (99.6%) 91.7% 
771 (77.1%) 729 (94.6%) 710 (97.3%) 92.1% 
858 (85.8%) 705 (82.1 %) 701 (99.4%) 81.7% 
Table 2.1 Results of PDF download tests. The three download tests are listed as rows, 
with several performance measure values in columns along each row. Percentage 
values are provided in parentheses. 
The purpose of these tests was to assess the ability of our software to identify 
and download PDF articles from an original fUll-text link. Given that the full-
text links were extracted directly from PubMed and not by our software, the 
percentage of fUll-text links found (77.1 %-85.8%) does not give us an 
indication of the abilities of the software, but instead supports our assertion 
that it is indeed possible and practical to collect large numbers of articles 
automatically. It should, however, be noted that human users would most 
. likely be able to find the full-text article (if it is available electronically) even if 
PubMed does not provide a link to it. 
An important measure of performance for the software is the percentage of 
PDF files found in relation to the number of fUll-text links provided. This value 
varied between 82% (test 3) and 95% (test 2). 
To assess whether the PDF files downloaded were actually the full-text of the 
original article (and not a different PDF), we manually inspected the first page 
of every PDF downloaded for each test. The results of this are presented in 
Table 2.1, column 5. When considering all the articles downloaded as one 
collection, the software achieved an overall conversion from full-text link to 
correct PDF downloaded of 88.2% (2,114 correct PDF files from 2,396 fUll-text 
links). 
When designing and implementing this software package, we intended for it to 
be extensible and tuneable by users. For example, our separation of the 
formulation of the PubMed search and the search itself, allows users to build 
their queries in the way that is necessary for their purpose, whilst also 
permitting the software to have full control over the search inputs and outputs. 
Additionally, users of the software are able to modify the search activity of the 
download agent, and can also alter the target of the agent to other types of file 
found on the Web. Furthermore, the source code is available from the project 
site39 , so that others with programming skills can make use of the code for 
their own tasks. 
As we begin to realise the huge wealth of information present in full-text 
articles and absent in abstracts, new improvements in software for providing 
and processing these articles is required. Currently, there is very little 
software to enable the move to fUll-text analysis. Here, we have presented 
software for automating the retrieval of large numbers of scientific articles; it is 
intended to be used in text- or data-mining studies, but could also be useful 
for literature surveys. We have demonstrated that our software is able to 
identify and download a significant proportion of articles (88.2%) from a test-
set of articles. Our software automates and enables the important first stage 
of any text-mining or data-mining task, that of corpus collection. 
2.5. References 
Aerts, S., M. Haeussler, S. van Vooren, O. L. Griffith, P. Hulpiau, S. J. Jones, 
S. B. Montgomery and C. M. Bergman (2008). "Text-mining assisted 
regulatory annotation." Genome Biology 9(2): R31. 
Altman, R. B., C. M. Bergman, J. Blake, C. Blaschke, A. Cohen, F. Gannon, L. 
Grive", U. Hahn, W. Hersh, L. Hirschman, L. J. Jensen, M. Kra"inger, 
B. Mons, S. I. O'Donoghue, M. C. Peitsch, D. Rebholz-Schuhmann, H. 
Shatkay and A. Valencia (2008). "Text mining for biology--the way 
forward: opinions from leading scientists." Genome Biology 9 Suppl 2: 
Cohen, A. M. and W. R. Hersh (2005). "A survey of current work in biomedical 
text mining." Briefings in Bioinformatics 6(1): 57-71. 
Corney, D. P., B. F. Buxton, W. B. Langdon and D. T. Jones (2004). "BioRAT: 
extracting biological information from fu"-Iength papers." Bioinformatics 
20(17): 3206-13. 
Crangle, C. E., J. M. Cherry, E. L. Hong and A. Zbyslaw (2007). "Mining 
experimental evidence of molecular function claims from the literature." 
Bioinformatics 23(23): 3232-40. 
Eales, J. M., J. W. Pinney, R. D. Stevens and D. L. Robertson (2008). 
"Methodology capture: discriminating between the "best" and the rest 
of community practice." BMC Bioinformatics 9: 359. 
Garten, Y. and R. B. Altman (2009). "Pharmspresso: a text mining tool for 
extraction of pharmacogenomic concepts and relationships from full 
text." BMC Bioinformatics 10 Suppl 2: S6. 
Kra"inger, M., F. Leitner, C. Rodriguez-Penagos and A. Valencia (2008). 
"Overview of the protein-protein interaction annotation extraction task 
of BioCreative II." Genome Biology 9 Suppl2: S4. 
Muller, H. M., A. Rangarajan, T. K. Teal and P. W. Sternberg (2008). 
"Textpresso for neuroscience: searching the full text of thousands of 
neuroscience research papers." Neuroinformatics 6(3): 195-204. 
Natarajan, J., D. Berrar, W. Dubitzky, C. Hack, Y. Zhang, C. DeSesa, J. R. 
Van Brocklyn and E. G. Bremer (2006). "Text mining of fUll-text journal 
articles combined with gene expression analysis reveals a relationship 
between sphingosine-1-phosphate and invasiveness of a glioblastoma 
cell line." BMC Bioinformatics 7: 373. 
Natarajan, J., C. Haines, B. Berglund, C. DeSesa, C. Hack, W. Dubitzky and 
E. G. Bremer (2006). GetltFull - A Tool for Downloading and Pre-
processing FUll-Text Journal Articles. Lecture Notes in Computer 
Science. Heidelberg, Springer Berlin. 3886: 139-145. 
Rodriguez-Esteban, R. and I. lossifov (2009). "Figure mining for biomedical 
research." Bioinformatics 25(16): 2082-4. 
Zaremba, S., M. Ramos-Santacruz,T. Hampton, P. Shetty, J. Fedorko, J. 
Whitmore, J. M. Greene, N. T. Perna, J. D. Glasner, G. Plunkett, 3rd, 
M. Shaker and D. Pot (2009). "Text-mining of PubMed abstracts by 
natural language processing to create a public knowledge base on 
molecular mechanisms of bacterial enteropathogens." BMC 
Bioinformatics 10: 177. 
Zweigenbaum, P., D. Demner-Fushman, H. Yu and K. B. Cohen (2007). 
"Frontiers of biomedical text mining: current progress." Briefings in 
Bioinformatics 8(5): 358-75. 
3. Looking in the right place: classifying manuscript sections 
3.1. Abstract 
3.1.1. Background 
Text-mining has the potential for being of great utility to the life sciences and 
particularly bioinformatics; it can support and inform studies with information 
that would otherwise be laborious to collect. The kind of information required . 
is, however, often not found in article abstracts (the traditional source for text-
mining studies). This is fuelling the move to analysis of full-text articles, which 
contain a wider range of more detailed information; however, there are also 
associated key difficulties, including those of computational costs and the 
variable accuracy of text-mining methods between different article sections. 
To address these difficulties, new software is required to provide the key 
infrastructural elements of full-text mining; here, we provide a text classifier 
that can label plain-text as coming from one of the four common article 
sections (Introduction, Methods, Results, Discussion). 
3.1.2. Results 
Our classifier can reduce computational costs by identifying or excluding 
specific article sections; it also enables software to 'know' which type of 
section it is analysing, and to adapt its approach accordingly, which could 
potentially improve accuracy. Our classifier correctly labelled 85.2% of a 
large set (94,424) of open-access article sections. The classifier enables 
greater efficiency and accuracy when mining full-text documents by providing 
extra contextual information on the kind of text being analysed. 
3.1.3. Conclusions 
Full-text mining has extensive potential for informing all manner of biological 
investigation, from identifying methods and protocols to discovering new 
avenues for research. The classifier software is available as a SOAP Web 
service, graphical interface application and as an API. 
3.1.4. Availability: http://code.google.com/p/articlesectionclassifier/ 
3.2. Background 
Scientific articles are a rich source of information; they contain experimental 
methods, results, conclusions, arguments, proposals, algorithms, hypotheses, 
evidence, citations and m.any more elements. The information in most 
scientific articles is, however, only easily available to single human readers of 
the article. Text-mining methods can allow us to capture information from text 
in a way that simulates elements of the action of human readers (albeit in a 
simplified and less flexible manner), thus making some of this information 
available to computational analysis (Yu, Hatzivassiloglou et a/. 2002; Shah, 
Perez-Iratxeta et a/. 2003; Rzhetsky, lossifov et a/. 2004; Natarajan, Serrar et 
a/.2006). 
In the past, most text-mining studies have made use of text content that is 
more readily available (abstracts), rather than the more informative full-text 
(Schuemie, Weeber et a/. 2004; Cohen and Hersh 2005; Krallinger and 
Valencia 2005). A major limiting factor in current text-mining work is access 
to the full-text of scientific articles (Dickman 2003; Cohen and Hersh 2005; 
Krallinger and Valencia 2005). Paper articles are not well suited to 
computational analysis, owing to the difficult transition from physical printed 
text to machine-readable characters; furthermore, almost all journals now 
publish their content electronically. Electronic content is, however, still 
restricted in its availability; this can be owing to licensing issues preventing 
large-scale content retrieval, but the difficulties are often augmented by 
technical issues relating to automation of document retrievals through 
interfaces designed for single human users. 
Despite the difficulties previously outlined, articles (in fUll-text form) that are 
published in an open-access format are becoming increasingly numerous and 
easily available for text- and data-mining studies in a highly structured XML 
format. This is mostly attributable to the open-access publishing movement, 
the publishers who support it51 ,52 and the initiatives53,54 and public 
51 http://www.plos.org/ 
repositories55,56 that support open access to the output of publicly funded 
scientific research. The amount of text-mining research published that makes 
use of full-text article collections is, however, still limited (Dickman 2003; 
Cohen and Hersh 2005); given the large range of useful information elements 
present in full-text but absent in abstracts, this is unsustainable (see Figure 
3.1 ). 
52 http://www.biomedcentral.com/ 
53 http://www.wellcome.ac.ukldoc_WTD002766.html 
54 http://www.rcuk.ac.uklcmsweb/downloads/rcukldocuments/2006statement.pdf 
55 http://www.pubmedcentral.nih.gov/ 
56 http://ukpmc.ac.ukl 
Abstract 
-Summary of importance of work 
-Summary of approach used 
-Description of most significant 
result(s) 
-Description of the importance of 
result(s} 
-Citations to important work 
-Previous literature 
-Hypotheses to be tested 
-Explanation of the problem 
-Outline of the approach 
-Definition of acronyms 
-Unks to online resources 
Figure 3.1 - A model of a typical scientific article. The different elements of a research 
article are listed inside each article section (coloured circles) and the abstract 
(bordered box) according to where they appear in an article. 
Scientific articles follow a structure first developed to support the efficient 
communication of a piece of research based on its fidelity to the scientific 
method. In most cases, it is therefore appropriate to 'include passages of text 
that can form the Introduction, Methods, Results and Discussion sections of 
the article. There are exceptions to this rule: for example, a review article 
may not describe any Methods. It should also be stressed that some journals 
do not provide titles for their sections and may even include text that could be 
in separate sections in a single paragraph; this is especially common in 
compact format journals, such as Science and Nature. 
Scientific authors do, in the majority of cases, follow the basic principles of the 
scientific article structure, and assign information accurately to each section. 
We can profit from this accuracy of information assignment by exploiting what 
we know about the structure of the archetypal scientific article to direct text-
mining activities to sections of text that are most likely to contain the 
information that we are interested in (Shah, Perez-Iratxeta et a/. 2003; 
Schuemie, Weeber et a/. 2004). The Introduction or Background section will 
contain information that is not unique to the article; it will most often discuss 
previous research in the area, often comparing and contrasting the results 
with subjective commentary on these results from the present authors. 
Furthermore the Introduction will commonly outline the aims and theoretical 
basis of the piece of research. The Methods or Materials and Methods 
section has a clear aim that involves the often highly technical description of 
the physical implementation of the piece' of research; it will also describe any 
assumptions or simplifications made. The Results section again has a clear 
aim: the presentation of the new scientific information derived specifically from 
the piece of research in question; this section will, on occasion, discuss 
relationships between these results, but will not refer to the wider meaning of 
the results. The Discussion section attempts to derive meaning from the data 
presented in the Results section; it also discusses how these results fit in with 
current thinking in the field, which may have been discussed in the 
Introduction. 
Text-mining methods have continued to increase in their sophistication and 
their computational complexity (Natarajan, Mulay et a/. 2005). Some of the 
most intensive methods, which are often performing natural-Ianguage-
processing (NLP) tasks, can take considerable time to complete, even on 
collections of abstracts. So, given a collection of full-text documents, which 
are commonly an order of magnitude longer than an abstract, we must look 
for ways to reduce the amount of computation required (Natarajan, Mulay et 
al. 2005). A text classifier that can assign one of the archetypal section labels 
to plain-text would enable us to direct concerted text-mining efforts to specific 
sections of an article without significantly increasing the computational 
requirements. Furthermore, reliable section labels would allow us to tailor our 
search to sections of an article where we know our information will be present, 
thus minimising the likelihood of false-positive and out-of-context matching. 
Further uses of reliable section labels would include the active exclusion of a 
type of section from analysis. For example, several authors (Tanabe and 
Wilbur 2002; Camon, Barrell et al. 2005) have suggested the special 
treatment or total exclusion of Methods text from their analyses. In our own 
research, we have applied the approach to a collection of over 20,000 articles 
to identify Methods section text; this was then tested for use in the exploration 
and definition of best practice methodologies in the field of molecular 
phylogenetics (Eales, Pinney et al. 2008; Eales, Stevens et al. 2008). 
Our work has some similarity with zone analysis (Mizuta and Collier 2004a; 
Mizuta and Collier 2004b; Mizuta, Korhonen et al. 2006), a method based on 
analysis of discourse (Teufel, Carletta et al. 1999) and argumentation. In this 
approach, fragments of text are classified into one of a detailed set of classes, 
described by an annotation scheme. A similar, yet also theoretically distinct, 
approach has been proposed (Wilbur, Rzhetsky et al. 2006), whereby a set of 
five dimensions (focus, polarity, certainty, evidence and trend) is used to 
characterise each fragment of text. We believe that these are valuable 
approaches that, when implemented, will definitely enhance the analysis of 
full-text articles. Another related approach, which has been implemented,. 
used a metadata schema (CISP (Soldatova and Liakata 2007 to describe 
the most important components of an article5? (e.g., the aims, hypotheses and 
methods) as they are described in the articles text. This implementation has 
been applied to a corpus of 225 biochemistry and physical chemistry articles 
and machine-learning methods have been trained on this data to enable 
automated CISP annotation (Liakata, Q et al. 2009). The annotated corpus 
(the ART corpus) has been made available to download58 and therefore 
represents 'a valuable resource for the community. All of these approaches, 
however, suffer from two main practical difficulties: first, they require manually 
annotated training data, and are therefore subject to the difficulties of any 
annotation task (namely, the problem of inter-annotator agreement and the 
investment of a significant amount of time by the annotators); second, the 
non-trivial issue of linking the output of these approaches to address likely 
research questions. Additionally, to the best of the author's knowledge 
neither of the first two approaches has been implemented or made available 
to the research community, this further reinforces the importance of the ART 
corpus. 
Our approach makes direct use of the common method of structuring the 
reporting of a piece of scientific research. This removes the problem of 
annotation tasks by making use of the large corpus of open-access and 
highly-structured XML articles available from UKPMC59 Additionally, we can 
remove the problem of mapping classifier output back to the original question 
by making use of this scientific article structure that is familiar to all scientists, 
who can then form their questions based directly on the structure of an article: 
for example, by specifying that a certain protein must be mentioned in the 
Results section, as well as that a certain lab protocol must have been 
reported in the Methods section for it to be included for further analysis. Our 
classifier allows simple, quick and accurate restructuring of an article from an 
unstructured plain-text form; this is functionality useful to any text-mining 
57 http://www.aber.ac.ukJenics/research/cb/projects/artlart:..corpusI 
58 http://www.aber.ac.ukJenics/research/cb/projects/artiart-corpus/ 
59 http://ukpmc.ac.ukJ 
study making use of full-text. Additionally, our classifier was trained in an 
automated manner, reducing difficulties with lengthy annotation tasks and 
inter-annotator agreement. Furthermore, our classifier was trained on all 
currently available open-access articles from UKPMC (on 05/08/08, 84,269 
articles from 696 different journals), and therefore has a set of training data 
applicable ~o most disciplines of biomedical science. Finally, we have made 
our classifier available through a SOAP Web service interface6o , a browser-
based interface for testing60 , and as a downloadable Java application (with a 
GUI) for local use61 . By making our software available in all these different 
forms, we are attempting to make it usable by a wide range of researchers, 
irrespective of their level of technical knowledge. 
In this paper, we present our software and the various ways we have created 
to access it, as well as a broad methodology to support targeted mining of full-
text articles. First, we present the abilities of our classifier to classify the text 
sections of a single article. Then we will discuss the importance of basic 
language in the classification of text from scientific articles. We also provide 
an error analysis of our classifier's abilities when applied to half of all currently 
available open-access full-text articles in the biomedical sciences domain. 
3.3. Implementation 
3.3.1. Access to the classifier 
Our classifier software can be accessed in several ways; these are broadly 
grouped into. network, local and programmatic interfaces. Full technical 
information on how to use each of these interfaces is available at our project 
Website62 . 
60 http://gnode 1.mib.man.ac. uk: 80801 Artic1eSectionClassifierWebAppl 
61 http://code.google.com/p/artic1esectionc1assifier/. 
62 http://code.google.com/p/artic1esectionc1assifier/ 
3.3.2. Network Usage 
The network interfaces to the classifier are intended for testing and for 
moderate intensity usage. There is a Web-browser interface available63 , 
which gives a quick, visual demonstration of the classification outputs. We 
have also created a Web service interface (described by a WSDL document) 
that can be used to integrate the classifier into other applications, scientific 
workflows or workflow enactment software (e.g., Taverna (Oinn, Addis et a/. 
2004)). We have provided an example workflow and instructions on the use 
of our software in Taverna at the project site62 . 
3.3.3. Local Usage 
We have created a graphical client program written in Java as well as 
command-line clients for the Web service, written in Perl and Ruby. Further 
details on local use of the software are available on the project site62  
3.3.4. Programmatic access 
We have made the Java source code, JavaDoc documentation and trained 
data-file for the classifier available for download64. This enables others to use 
our code in their own software, or to modify the software to address different 
problems. We have also included a set of java classes that automate access 
to the WSDL Web service from within any Java program. Further details on 
programmatic use of the software are available on the project site62. 
3.3.5. Error analysis 
The accuracy of text-mining methods is commonly quantified using 3 different 
measures of accuracy: Precision (commonly abbreviated to P), recall (R) and 
F-measure or F-score (F1). Precision is the fraction of the results provided by 
the method that are correct. Recall provides the fraction of all correct results 
that were recovered by the method and the F-score is calculated as the 
63 http://gnodel.mib.man.ac.uk:SOSO/ ArticleSectionClassifierWebAppl 
64 http://code.google.comp/articlesectionclassifier/downloads/list 
harmonic mean of precision and recall (Feldman and Sanger 2007). 
3.3.6. Open-access database construction 
We downloaded the full archive (84 1269 articles downloaded on 05/08/08) of 
the PubMed Central XML for data-mining65; this includes all articles from the 
PLoS and BMC, as well as those archived solely by PMC. The structure of 
markup language files (i.e., HTML, XML) can be represented using the 
Document Object Model66 (DOM). This DOM representation of the file can 
then be accessed using standardised interfaces provided and defined by the 
World Wide Web Conso~ium67 (W3C). DOM representations can be queried 
for information using the XML Path Language68 (XPath). XPath queries can 
be used to extract specific values from a DOM representation of a file and 
they can also be used to calculate new values based on the structure of a 
DOM. We queried the DOM representation of each XML document using a 
set of XPath expressions designed from the National Library of Medicine69 
(NLM) Journal Archiving and Interchange Document Type Definition (DTD) 
v2.370 to extract important metadata (e.g., author names, journal, publication 
date, unique ID references) and the sections of text from each article. This 
information as well as the extracted full text of each article, was archived for 
further analysis in a MySQL database. 
3.3.7. Section classifier technical specification 
Previous work has explored all manner of text-classifier design decisions and 
their relation to accuracy (Dumais, Platt et a/. 1998; Yang 1999). We used the 
na'ive Bayesian maximum-likelihood approach because it performed 
consistently well throughout these studies; it also benefits from a sound 
65 http://www.pubmedcentral.nih.gov/aboutlftp.html 
66 http://www.w3.orgIDOM/ 
67 http://www.w3.org/ 
68 http://www.w3.org/TRlxpathi 
69 http://www.nlm.nih.gov/ 
70 http://dtd.nlm.nih.gov/archivingl2.3/archivearticle.dtd 
statistical basis that can be explored thoroughly, post hoc. It scores each 
word in the supplied text using prior knowledge (the training data), selects a 
number of the most 'important' words, and uses these to assign a score to the 
whole text passage for each of the possible article sections (classes). First, 
we need to calculate the likelihood of each word occurring in each of the 
classes; this is done using the training data and, more specifically, the 
frequency of the word in each of the sections, as well as the likelihood of each 
article section occurring at random. Next, we sort these words according to 
informative data, and then take the top 50 words to classify the given text. In 
this case, we used a measure of disparate usage of words between section 
types (the coefficient of variation of the probability of occurrence, a measure 
of statistical dispersion) to sort the words. The coefficient of variation is 
essentially the standard deviation normalised with respect to the mean. The 
coefficient of variation is useful in this case, because it is a dimensionless 
number that can be compared between data-sets with very different means. 
We then calculate a combined probability of each of the 50 words appearing 
in each of the sections for each of the sections. These are then sorted, and 
the most likely section type is chosen. 
3.3.8. Classifier training 
The single most important aspect of any machine-learning task is how data 
are represented to the training algorithm and the classifier. Here, we create a 
vector of words and their counts for all unique words present in the text in 
question. This provides greater discriminatory power by converting the data 
from a binary (presence or absence) form to a discrete form on a finite scale. 
We used 2 separate data-sets to train the classifier: the first was used for 
error analysis (the testing data-set); the second was used for normal 
classification (the production data-set). We have only used those sections 
that could be automatically labelled as being of a specific type (188,705 
sections from a possible number of 334,766). The testing data-set was 
created from slightly less than half the available sections in our open-acc.ess 
text database (94,281 sections); these were chosen randomly. The 
production data-set was generated from all usable sections (188,705 
sections) from all open-access articles (84,269 articles) available from 
PubMed Central on 05/08/08. 
We employed 2 methods for labelling our training data: the first uses specific 
information extracted from section-type elements of the XML of the article, 
and the second relies on matching section headings. The DTD used to 
describe the markup model of UKPMC documents71 provides the capability 
(Attribute 'sec-type' 72) to type section elements speCifically with a limited but 
extendable range of types (e.g., 'methods'). However, these elements are not 
commonly used throughout the article collection (21.4% of sections are typed, 
and 19.6% have a type that can be linked to a specific section type) and most 
are left undefined. Our second method involved surveying and counting all 
section titles from all articles, and manually grouping them together according 
to which of the archetypal section types they correspond. For example, the 
section titles' Methods', 'Materials and methods' and 'Implementation' can be 
reliably mapped to the Methods section type. All section titles that occurred 
more than 50 times and could be reliably mapped to a section type were 
included. The full list of section titles grouped according to their section 
membership is available from the code site
 The first method, because of its 
reliable semantic properties, is always favoured over the second; we did not, 
however, find a single instance where the two methods disagreed. 
71 http://dtd.nlm.nih.gov/archivingl2.3/archivearticle.dtd 
72 http://dtd.nlm.nih.gov/archiving/tag-Iibraryl2.3/index.html 
73 http://code.google.comp/articlesectionclassifier/ 
3.4. Results 
3.4.1. Classifying an article, an example 
To produce the image in Figure 3.2, we used the text extracted from the XML 
for a short article (Martin, Wilkinson et al. 2006) that follows the basic 
structure of a scientific article. We ran each section of text (including the 
acknowledgements, abstract, author contributions, references and all the . 
Figure legends) through our classifier, and the output is represented as 
overlaid coloured boxes in Figure 3.2. All article sections were classified 
correctly: in addition, the Abstract was classified as Introduction; all Figure 
legends were classified as Methods; the Acknowledgements were classified 
as Introduction; Author Contributions were classified as Methods; and the 
References were labelled as Methods. Figure 3.3 presents a figure legend, 
which has been classified as Methods, but is located in the results section of 
the article. 
Virology Journal 
Short~ ~ 
?.lfF.~.Q 
::-..::.--====~:.:--
:~~-=~ 
rat'I..c: ~  ____ -Lftl_ C...-.rL "'~ M 
_..toe",,~~~I~L.'.~" 
_..toe~ ____ , 
_ .. --.-...--_ .. _-_.--.. --_ .. _ .. _-
Introduction 
'" ... .,. 
Primer s-ir VRUP and nut 
10 -.-.. _--- 
~ ~_= ~-. l' g 
imr ;=~=;ES~~ 
I:::: = ==::::=== 
~ . --_. ::-~:ffi 
---------~ 
:E" ========555 :==== = = = 
- --.- ... -=:::=:::. ... .=::, 
Discussion 
Figure 3.2 - Example article with sections labelled. Sections of text labelled by the 
classifier from the full-text of a short article (Martin, Wilkinson et al. 2006). The article 
is presented in greyscale, sections are coloured by the label chosen by the classifier: 
yellow, Introduction; blue, Methods; orange, Results; green, Discussion. 
Pancreatic tissue did not contain virus. Only PBMC were 
available for testing from animals 13653 and 15150. 
Sequencing performed in duplicate on samples from ani-
mals 13910 and t 5 149 revealed 98% homology with a 
previously published PERVAC sequence from Infected 
human cell. lines, AY364236.t (Fig. 2 and 3) [10). Sam' 
pie, from animal, 13653 and 15150 ,howed clo,er 
homology to another PERVAC clone, AF417230 (Fig. 2 
and 3) [21. 
Comparison of sequence, from 13653 and 15 I 50 
revealed a 0.2% difference (3 out of 1284 base pairs). 
Comparison of sequences from 3 different tlssues (spleen, 
M 2 3 4 
liver, and PBMC In 13910 and 'pleen and PBMC In 
15149) showed a difference of 0.4 1.2%. Some ofthese 
minute differences within multiple samples of the same 
tissues are wrthln the realm of PCR and sequenCing artl 
fact, though the presence of actual smail variation, cannot 
be ruled out. Nontransmlttlng swine 15578, 15579, 
t6181, and 12190 revealed no evidence of genomic DNA 
PERV-AC recombinant virus In lymph node, liver, spleen, 
lung, thymus, or PBMC (animal, 15578 and 15579) or 
just PBMC (t 2 I 90). 
USing prrmer pair VRBf and PERV-C reverse, a second 
PERVAC recombinant was detected In the DNA of 13910 
5 6 7 8 
Primer pair VRBF and TMR 
M 2 3 4 5 6 7 8 9 10 
--------------------------~~ 
Primer pair VRBF and C-reverse 
M 2 3 4 5 6 7 8 10 --- ___ -:.'.".at  Primer pair porcine mitochondria cytochrome oxidase II 
lows: I, 15149; 2. 13910; 3. 13653; 4. 15150; S. 15578; 6. 15579; 7. 11910; a. 16181: 9, ... 14-110; Ind 10. _. The ... 14-110 
DNA serves I. I positive control for PeR IrwoNtn& recombinant PERV-AC, n WI_ U. nopCM control. Banda __ wtIh 
primer poirI VRBf and THR are >wHnhe 1000 andl6SO baM pair lane 1'nIIi<ers. Banda seen wtIh primer poirI VR8F and 
C...-.. are between che 300 and 400 ba .. pair lane markers. 
Results 
Figure 3.3 - Results section from example article showing figure legend In results 
section. Text sections are coloured according to 'result of classification (blue for 
methods, orange for results). Page shown Is page 2 from example article (Martin, 
Wilkinson et al. 2006). Figure legend present In the results section has been classified 
as Methods. 
3.4.2.1 The importance of basic language 
The removal of stop words is a common practice to reduce computational 
effort in processing large quantities of text. These words are often referred to 
as 'basic', and are regularly regarded as conveying little or no information. 
However, in article section text-classification, these supposedly basic and 
information-poor words are actually very important in defining the nature of a 
piece of text. It is common for a certain style of language to prevail in each 
section. In addition, it is vital to monitor word frequency instead of word 
presence or absence. 
Each section has its own word signature, and this is best highlighted by those 
words that are used differentially between sections. In Table 3.1, we have 
listed the top 10 most useful (see Implementation) words for discriminating 
between sections that occur above an average rate of 1 mention per 1,000 
words across all sections. Most of these words are significantly more 
common in one section than the others; this explains why they have been 
scored as useful for discriminating between sections. 
The words 'may', 'were', 'each', 'our and 'been' are all common words in the 
English language and commonly occur in English stop word lists74 , yet they 
are all useful discriminatory words (Table 3.1). The top 4 most discriminatory 
words all appear most frequently in Results-section text; Methods-section text 
has 3 words in the top 10; Discussion-section text has 2; and Introduction text 
has just one. Usually the highest frequency of occurrence in a given section 
is a lot higher than its frequency in other sections. For example, the 
frequency (per 1,000 words) of the word 'were' in the Methods section is 
17.26, which is more than twice its frequency in Results sections (7.89), 
almost 5 times its frequency in Discussion-section text (3.55), and 
approximately 9 times higher than the frequency of 'were' in Introduction 
sections (1.91). 
74 http://ir.dcs.gla.ac.ukiresources/linguistic_utils/stop _words 
Word Introduction Methods Results Discussion Coefficient 
Variation 
'figure' 0.45 0.72 4.96 0.75 1.26 
'table' 0.2 0.79 2.84 0.41 1.14 
'-' 0.36 2.28 4.55 0.34 1.06 
'p' 0.4 1.32 3.63 0.43 1.05 
'may' 2.04 0.35 0.54 3.61 0.93 
'were' 1.91 17.26 7.89 3.55 0.90 
'our' 0.92 0.66 0.78 3.32 0.90 
'using' 1.17 4.88 1.38 1.05 0.87 
'each' 0.79 3.71 1.63 0.58 0.85 
'been' 4.08 0.75 0.70 2.64 0.80 
Table 3.1 - Frequency of occurrence (per 1,000 words) of discriminatory words by 
section. Frequency of each of the ten most discriminatory words (that have an average 
frequency per 1,000 words greater than 1) in each section. Words are sorted by the 
measure of discriminatory power, the coefficient of variation. 
3.4.3. Error analysis 
Our classifier performed as shown in Table 3.2, using our testing data-set of 
training data. In total, 85.2% of all sections were correctly classified. The 
best overall results come from the Methods class, with the highest F-measure 
(0.8905) and recall (0.9413) values. The best precision (0.9397) is achieved 
in the Discussion class, which also exhibits the lowest recall (0.7657). The 
lowest precision (0.7884) and F-measure (0.8320) values came from the 
Introduction class. 
Section Type 
Introduction 
Methods 
Results 
Discussion 
Precision 
0.7884 
0.8449 
0.8123 
0.9397t 
Recall 
0.8808 
0.9413t 
0.8780 
0.7657 
F-Measure 
0.8320 
0.8905t 
0.8439 
0.8438 
Table 3.2 Performance data for error analysis. Classification accuracy on 94,424 
random open-access text sections, using the testing data-set. 't' indicates the highest 
value in each column. 
3.5. Discussion 
We are, for the first time, providing a text classifier that enables researchers to 
target their text-mining activities to specific article sections. We have also 
provided both a service-oriented programmatic (via Simple Object Access 
Protocol (SOAP)) and a user-oriented browser-based interface to our 
resource. Furthermore, this classifier labels text according to its 'content' 
rather that its strict structural membership of any section. It is therefore 
possible to identify sections of text that may, for example, appear in a Figure 
legend in the Results section but are actually highly methodological in nature 
(as is often the case, see Figure 3.3). This opens up new avenues for 
discovering text of a particular semantic basis in any location in an article. 
These sections can then be further analysed by advanced techniques, thus 
minimising the computational overheads of such procedures by directing them 
to specific text sections, rather than the whole article. 
The value of this classifier is further increased by its generalised training 
basis. We have trained the classifier on all available XML-based open-access 
articles. This represents a set of 696 unique journals, covering a significant 
portion of the scope of all biological and biomedical research. Although the 
classifier is bio-medically oriented, any scientific article that uses a similar 
written style and form of language in the description of a piece of research 
should be accurately classifiable. 
Article sections not only differ in their usage of technical terminology and 
keywords (Shah, Perez-Iratxeta et al. 2003), but also in the kind of basic 
language they use to achieve the aim of each section. For example, the word 
'were' is very useful for discriminating between section types: by normalising 
for the number of sections in each class, and the size of those sections, we 
found it was used, on average, 18.3 times per Methods section, 10.2 times 
per Results section, and 2.2 and 1.4 times per Discussion and Introduction 
section respectively. This defines clearly the power of simple language in 
article-section discrimination. Stop-word lists are useful for limiting index 
sizes, and for improving search results where most documents contain stop 
words. Our text classifier, however, uses normalised counts of words to 
discriminate between sections; this removes the problem of very common 
words matching to all documents; instead, each word match increases the 
probability according to the word's prevalence in each section type. We did 
not use a stop word list when training our classifier for the simple reason that 
some of the most common words occur at the most variable frequencies 
between sections, and are therefore extremely useful discriminatory data. 
The ability to attach some level of meaning to free text opens up many 
possibilities for targeted text-mining activities, such as re-creation of 
structured-article representations from free text. This could be an important 
improvement, because, at the moment, the majority of electronic fUll-text 
articles are only available in PDF or HTML formats. PDF presents a particular 
problem because even the best text-extraction tools still suffer from problems 
with page breaks and the order of text sections in the extracted plain-text. 
This creates a fragmented plain-text document, where section titles may be 
spatially distant from the main text they are associated with. There are 
equally difficult problems with HTML full-text because of its highly variable 
mark-up. During the preliminary stages of this work, we found that mark-up 
varies between publishers, year of publication .and type of article. The only 
reliable solution to this is text classification that can apply section labels to 
passages of text based on their semantic 'message'. 
Clearly the semantic detail of research publications is difficult to regain after a 
manuscript has been published and converted into PDF or HTML. But 
several encouraging projects have provided new avenues to 'semantify' 
publications. Utopia documents75 (UD) is a PDF document reader that 
recreates the link between content provided in a PDF file and the data used to 
create this content (Attwood, Kell et al. 2009). UD is able to provide 
interactive views of the data reported in an article; examples of this are: 
images of protein structures that become interactive 3D models; tables of 
numbers redrawn on graph axes; and sequence alignments that can be 
manipulated inside UD. UD is also able to tightly link a PDF file with the 
bibliography of the published article that it presents; clickable links are 
available that provide access to the cited works. Some of this functionality is 
only fully available for a subset of articles from the Biochemical Journal76 , but 
it clearly demonstrates how tighter links between the authoring and publishing 
processes, can enable huge improvements in the communication of 
knowledge by publication. Just as UD re-establishes the link between a PDF 
article and its associated data and metadata, here we are attempting to link 
parts of a plain-text article with its original article section context. 
As we might expect, information is distributed heterogeneously .throughout an 
article (Shah, Perez-Iratxeta et al. 2003; Schuemie, Weeber et al. 2004) with 
some sections containing reliable information and some being very unreliable, 
dependent on the entity type in question. For example, mentions of gene 
names in Methods-section text are commonly unreliable (Shah, Perez-Iratxeta 
et al. 2003), and will often be a reference to other types of entities (e.g., 
75 http://getutopia.com 
76 http://www.biochemj.org 
vectors, primers or reagents), whilst mentions of the same gene names in the 
Results section are likely to be much more reliable (Shah, Perez-Iratxeta et al. 
2003; Schuemie, Weeber et al. 2004). It is therefore vital to inform text-
mining methods of the 'kind' of text they are analysing. The classifier we have 
developed provides this information, and enables software to make 'informed' 
choices on the kind of analysis to apply to a given piece of text. Furthermore, 
it becomes simple to restrict or limit the analysis procedures to a range of 
article sections (e.g., Methods and Results) or to a single type of section, 
therefore improving the quality of the information derived from the text. 
Most text-mining software has been designed and tested for operation on 
abstracts; it seems unlikely that this software will be suited to the analysis of 
much longer fUll-text documents. We suggest that a fUll-text section classifier 
can help to reduce computational costs (Cohen and Hersh 2005; 
Zweigenbaum, Demner-Fushman et al. 2007; Rzhetsky, Seringhaus et al. 
200B) by restricting analysis to those sections containing the kinds of 
information most pertinent to the text-mining task. 
3.6. References 
Attwood, T. K., D. B. Kell' P. McDermott, J. Marsh, S. R. Pettifer and D. 
Thorne (2009). "Calling International Rescue: knowledge lost in 
literature and data landslide" Biochemical Journal 424(3): 317-333. 
Camon, E. B., D. G. Barrell, E. C. Dimmer, V. Lee, M. Magrane, J. Maslen, D. 
Binns and R. Apweiler (2005). "An evaluation of GO annotation 
retrieval for BioCreAtlvE and GOA." BMC Bioinformatics 6 Suppl1: 
Cohen, A. M. and W. R. Hersh (2005). "A survey of current work in biomedical 
text mining." Briefings in Bioinformatics 6(1): 57-71. 
Dickman, S. (2003). "Tough mining: the challenges of searching the scientific 
literature." PLoS Biology 1 (2): E4B. 
Dumais, S., J. Platt, D. Heckerman and M. Sahami (1998). Inductive learning 
algorithms and representations for text categorization. Proceedings of 
CIKM. Bethesda. 
Eales, J. M., J. W. Pinney, R. D. Stevens and D. L. Robertson (2008). 
"Methodology capture: discriminating between the "best" and the rest 
of community practice." BMC Bioinformatics 9: 359. 
Eales, J. M., R. D. Stevens and D. L. Robertson (2008). FUll-Text Mining: 
Linking practice, protocols and articles in biological research. 
Proceedings of Biolink Special Interest Group, ISMB. Toronto. 
Feldman, R. and J. Sanger (2007). The Text Mining Handbook. Cambridge, 
Cambridge university press. 
Krallinger, M. and A. Valencia (2005). "Text-mining and information-retrieval 
services for molecular biology." Genome Biology. 
Liakata, M., C. Q and L. N. Soldatova (2009). Semantic annotation of papers: 
interface & enrichment tool (SAPIENT). Proceedings of the Workshop 
on BioNLP. Boulder, Colorado, Association for Computational 
Linguistics. 
Martin, S. I., R. Wilkinson and J. A. Fishman (2006). "Genomic presence of 
recombinant porcine endogenous retrovirus in transmitting miniature 
swine." Virology Journal 3: 91. 
Mizuta, Y. and N. Collier (2004a). An annotation scheme for a rhetorical 
analysis of biology articles. Proceedings of LREC. Lisbon. 
Mizuta, Y. and N. Collier (2004b). Zone identification in biology articles as a 
basis for information extraction. Proceedings of Coling 2004. Geneva. 
Mizuta, Y., A. Korhonen, T. Mullen and N. Collier (2006). "Zone analysis in 
biology articles as a basis for information extraction." International 
Journal of Medical Informatics 75(6): 468-487. 
Natarajan, J., D. Berrar, W. Dubitzky, C. Hack, Y. Zhang, C. DeSesa, J. R. 
Van Brocklyn and E. G. Bremer (2006). "Text mining of full-text journal 
articles combined with gene expression analysis reveals a relationship 
between sphingosine-1-phosphate and invasiveness of a glioblastoma 
cell line." BMC Bioinformatics 7: 373. 
Natarajan, J., N. Mulay, C. DeSesa, C. J. Hack, W. Dubitzky and E. G. 
Bremer (2005). A Grid Infrastructure for Text Mining of Full' Text 
Articles and Creation of a Knowledge Base of Gene Relations. Lecture 
Notes in Computer Science. Berlin I Heidelberg, Springer. 3745: 101-
~inn, T., M. Addis, J. Ferris, D. Marvin, M. Senger, M. Greenwood, T. Carver, 
K. Glover, M. R. Pocock, A. Wipat and P. Li (2004). "Taverna: a tool for 
the composition and enactment of bioinformatics workflows." 
Bioinformatics 20(17): 3045-54. 
Rzhetsky, A., I. lossifov, T. Koike, M. Krauthammer, P. Kra, M. Morris, H. Yu, 
P. A. Duboue, W. Weng, W. J. Wilbur, V. Hatzivassiloglou and C. 
Friedman (2004). "GeneWays: a system for extracting, analyzing, 
visualizing, and integrating molecular pathway data." Journal of 
Biomedical Informatics 37(1): 43-53. 
Rzhetsky, A., M. Seringhaus and M. Gerstein (2008). "Seeking a new biology 
through text mining." Cell 134(1): 9-13. 
Schuemie, M. J., M. Weeber, B. J. A. Schijvenaars, E. M. van Mulligen, C. C. 
van der Eijk, R. Jelier, B. Mons and J. A. Kors (2004). "Distribution of 
information in biomedical abstracts and fUll-text publications." 
Bioinformatics 20(16): 2597-2604. 
Shah, P., C. Perez-Iratxeta, P. Bork and M. Andrade (2003). "Information 
extraction from full text scientific articles: Where are the keywords?" 
BMC Bioinformatics 4(1): 20. 
Soldatova, L. and M. Liakata (2007). "An ontology methodology and CISP-
the proposed Core Information about Scientific Papers." JISC Project 
Report. 
Tanabe, L. and W. J. Wilbur (2002). "Tagging gene and protein names in 
biomedical text." Bioinformatics 18(8): 1124-1132. 
Teufel, S., J. Carletta and M. Moens (1999). An annotation scheme for 
discourse-level argumentation in research articles. Proceedings of 
EACL. Bergen. 
Wilbur, W. J., A. Rzhetsky and H. Shatkay (2006). "New directions in 
biomedical text annotation: ~efinitions, guidelines and corpus 
construction." BMC Bioinformatics 7: 356. 
Yang, Y. (1999). "An Evaluation of Statistical Approaches to Text 
Categorization." Information Retrieval 1 (1): 69-90. 
Yu, H., V. Hatzivassiloglou, C. Friedman, A. Rzhetsky and W. J. Wilbur 
(2002). Automatic extraction of gene and protein synonyms from 
MEDLINE and journal articles. Proceedings of AMIA Symposium. San 
Antonio. 919. 
Zweigenbaum, P., D. Demner-Fushman, H. Yu and K. B. Cohen (2007). 
"Frontiers of biomedical text mining: current progress." Briefings in 
Bioinformatics 8(5): 358-75. 
4. Methodology capture: discriminating between the "best" and the 
rest of community practice. 
4.1. Abstract 
4.1.1. Background 
The methodologies we use both enable and help define our research. 
Keeping track of available methods and software in a field is made more 
difficult by increasing experimental complexity. Therefore, designing a 
suitable experimental protocol in a specific research area can be difficult for 
non-experts. To remedy this, we present an approach for capturing 
methodology from literature in order to identify, and thus define, best practice 
within a field. 
4.1.2. Results 
Our approach is to implement data-extraction techniques on the full-text of 
scientific articles to obtain the set of experimental protocols used by an entire 
scientific discipline: molecular phylogenetics. Our methodology for identifying 
methodologies could, in principle, be applied to any scientific discipline, 
whether or not computer-based. We find a number of issues related to the 
nature of best practice, as opposed to community practice. We find that there 
is much heterogeneity in the use of molecular phylogenetic methods and 
software, some of which is related to poor specification of protocols. We also 
find that phylogenetic practice exhibits field-specific tendencies that have 
increased through time, despite the generic nature of the available software. 
We used the practice of well published and widely collaborative researchers 
("expert" researchers) to analyse the influence of authority on community 
practice. We find expert authors exhibit patterns of practice common to their 
field, and therefore act as useful field-specific practice indicators. 
4.1.3. Conclusions 
We have identified a structured community of phylogenetic researchers 
performing analyses that are customary in their own local community and 
significantly different from those in other areas. Best practice information can 
help to bridge such subtle differences by increasing communication of 
protocols to a wider audience. We propose that the practice of expert authors 
from the field of evolutionary biology is the closest to contemporary best 
practice in phylogenetic experimental design. Capturing best practice is, 
however, a complex task and should also acknowledge the differences 
between fields, such as the context of the analysis. 
4.2. Background 
As scientists, the methodologies we use both enable and help define our 
research. Furthermore, the methodologies we declare in articles permit 
others both to judge the merits of the research we have carried out and to 
replicate our experiments. As experimental complexity has increased, 
however, the choice of appropriate methodologies has become an 
increasingly difficult task (Giles 2006; Nature-editorial 2006), especially in 
fields that rely heavily on computational analysis. Indeed, there is now a 
bewildering array of software tools77 that often perform similar tasks using 
different methods, making it difficult for individual researchers to keep track of 
new and existing software, let alone the most suitable software in a specific 
research area. 
Best practice is the most efficient (and effective) declaration of the process 
that describes the implementation of a specific methodology. Although all of 
the elements of best practice are routinely considered by researchers, its 
explicit declaration in biological research is rarely performed. Recording of 
best practice is an important element of many disciplines, such as clinical 
medicine, e.g., evidence-based medicine (Sackett, Rosenberg et al. 1996) 
and NICE guidelines78; medical research, e.g., MRC guidelines79 and meta-
analyses (Egger, Smith et al. 1997); and in business, e.g., best practice 
77 http://evolution.genetics.washington.edulphylip/software.html 
78 http://www.nice.org.uklguidancelindex.jsp 
79 http://www.rnrc.ac. ukiPolicyGuidance/EthicsAndGovemance/ 
GoodResearchPractice/index.htm 
templates (Szulanski 1996) and best practice benchmarking8o  The 
commonality between these groups is evidence; in order to justify a decision, 
you need evidence upon which to base it. However, most of the current 
usage of best practice in biological research employs evidence in the context 
of results, with limited regard to the design of experimental protocols 
(Goldman 1998). 
To enable researchers to choose appropriate methodologies, we propose that 
systems to automatically suggest experimental design templates based on 
literature-based validation of best-practice information wi": (i) simplify the 
design process, and (ii) provide a sound scientific basis for the choice of the 
specific details of an experiment. In order to survey practice in a community 
and identify best practice, we must first be able to collect practice in general. 
From this data, we can then attempt to identify elements therein that may be 
considered for inclusion into a best-practice proposal. Experimental best 
practice is dependent on its contextual environment; this could constitute the 
size of the data-set to analyse, the field in which the researcher works, or the 
kind of research questions that the research wi" address. We aim to capture 
elements of context in relation to practice and to incorporate this into our best-
practice information. Specifically, we examine the impact of field allegiance, 
co-authorship patterns, and the overall popularity of methods on experimental 
practice. We refer to the complete set of a" different practices used by a" 
members of a research community as "community practice". Additiona"y, we 
define "best practice" as a subset of community practice, incorporating those 
elements most scientifically credible and providing the most appropriate 
choice for any practitioner from the field. 
80 http://www .cbi.org. uklndbs/ content.nsf/ 
802737 AED3E3420580256706005390AE/9AFOOADE5AF840E080256B97004CF 
Our implementation of methodology-capture involves the application of 
information-extraction techniques to full-text journal literature. Text analysis, 
text-mining and data-mining are becoming increasingly popular techniques for 
information conglomeration. They are suited to the large information 
resources that are currently available, for example, literature (Chaussabel and 
Sher 2002; Tanabe and Wilbur 2002) or gene expression (Bassett, Eisen et 
al. 1999; Creighton and Hanash 2003) databases. These techniques, for 
example, have been used extensively in the identification of protein 
interactions (Marcotte, Xenarios et al. 2001; Jang, Lim et al. 2006). When we 
combine text-mining with the increasing availability of fUll-text journal articles 
(Shah, Perez-Iratxeta et al. 2003; Rzhetsky, lossifov et al. 2004; Natarajan, 
Mulay et al. 2005; Natarajan, Berrar et al. 2006; Rzhetsky, lossifov et al. 
2006; Aerts, Haeussler et al. 2008), we find it facilitates the automatic 
identification, extraction and dissemination of experimental methods. 
To assess the utility of our approach, we selected the field of molecular 
phylogenetics to act as a test case. Phylogenetics was selected because: (i) 
the methods used are mainly computational and are implemented by a large, 
but well-defined, group of software programs, the names of which can be 
easily collected; (ii) there is significant variety in the methods that different 
researchers use; (iii) phylogenetic methods are employed by many 
scientifically distinct fields of research; (iv) there is debate over which 
methods should be employed; and (v) there is no standard way to 
communicate or declare the methods and software used in a phylogenetic 
analysis. 
The single largest source of phylogenetic and indeed scientific practice is 
journal literature. Because of the adherence to the scientific method, and 
therefore the need to declare the methods used, each article describing 
original research should contain text relating to the methods employed. Our 
approach makes use of this practice resource by operating on the fUll-text of 
journal articles. We then search this text for terms that are significant in the 
description of phylogenetic experiments (see Figure 4.1 for example). 
"ClustaIW', 
Sequence 
alignment software 
"Multiple alignments of HSVd sequences were obtained using ClustalW (Thompson et al., 1994). 
The alignment was corrected manually to maximize sequence homology. Phylogenetic analyses 
were performed using the following programs of the PHYLIP 35c package (Felsenstein, 1993). 
DNADIST was used to calculate genetic distances, NEIGHBOUR (UPGMA or neighbour-joining 
methods) to cluster the variants from the distance data, DRAWTREE to draw the resulting 
phylogenetic tree and SEQBOOT (100 repetitions) and CONSENSE to perform bootstrap 
analysis" 
Align 
sequence 
Text quoted from Amari et aI. (2001), PMID: 11257203 
Figure 4.1 - Model of archetypal phylogenetic experiment. A model of the archetypal 
phylogenetic experiment with an example representation of a protocol In text form. 
Protocol elements are coloured according to their stage (1 to 4) In the model. 
The set of important methodological terms found in anyone article can be 
said to be a description of the protocol employed in that piece of research. 
We divide the methodological terms, found in the text, between four key 
stages: (i) sequence alignment, (ii) tree-inference, (iii) statistical testing and 
data resampling, and (iv) tree visualisation and annotation (Figure 4.1). The 
individual protocols are thus a model of a scientific experiment that is inferred 
from the text of the methods described in an article. The phylogenetic terms 
found in the methods are inferred to describe a task, or part of a task, and the 
collection of these tasks is what we term the protocol. Note, our analytical 
protocol model (Figure 4.1) is only part of a fully declared workflow that also 
includes the more mundane aspects: data retrieval, reformatting, which may 
then be transformed into a computer-enabled workflow that can be 
implemented by a system such as Taverna (Stevens 2003) or Kepler 
(Ludascher, Altintas et al. 2005). These are stages that will need to be 
included for full automation (Stevens, Tipney et al. 2004; Fisher, Hedeler et al. 
2007). 
Our approach successfully retrieves a large number of phylogenetic protocols. 
Analysing this data, we find that phylogenetic practice, over the last 10 years, 
has varied both temporally and between different groups of practitioners. 
Distinct fields of phylogenetic practitioners can be identified that, although 
they overlap, are significantly divergent in the protocols they use. We have 
also identified, using a collaboration network, well published and widely 
collaborative researchers for each field. These "expert" researchers design 
their experiments in very similar ways to other members of their field, and 
therefore act as useful practice indicators for their field. Our recommendation 
for producing a best-practice proposal for phylogenetics involves a 
combination of expert practice from each of the most significant fields (for 
example, evolutionary biology) and the most sophisticated or appropriate 
practice from all fields. 
4.3. Results 
A PubMed81 search for "phylogen*" in title and abstracts identified 27,259 
results, which yielded 24,494 different articles in PDF format. This difference 
is attributable to incorrect PubMed "link out" data, and software difficulties with 
finding the PDF version of the article from the original link. After processing 
the 24,494 PDF files, 21,484 articles in plain-text form remained. Reasons for 
this difference include a number of PDF files being encrypted, while others 
contained only scanned images of text. 
The result of the community practice gathering and extraction process was 
861 unique phylogenetic protocols found in 17,732 different articles. The 
oldest available article to contain any terms in our data-set was published in 
1980, and 90% of all analysed articles were published after 1996; before 
1996, protocols were retrieved in fewer than 300 articles per year. Thus, we 
focussed our analysis on the 847 unique protocols identified from 1996 to 
2005. We found there are several very popular protocols, with most articles 
(62%) using one of the top 10 most utilised' (Appendix 1). This does, 
however, leave another 837 protocols that have on average, only been used 
seven times each. The 10 most popular protocols all include at least one 
reference to either neighbour-joining, maximum-likelihood, parsimony or the 
Unweighted Pair-Group Method with Arithmetic mean (UPGMA) as a method 
for phylogenetic tree-inference. When assessing the accuracy of protocol 
identification by comparing our approach to manually annotated text (see 
Methods), we found very high levels of protocol retrieval: precision 89.8%, 
recall 85.7% and F-measure (F-score) 87.7%. 
Now that we have a sample of community practice, we can address how 
practice varies with respect to contextual properties. To do this, we 
investigated the importance of field allegiance and scientific authority (as 
81 http://www.ncbi.nlm.nih.gov/pubmedl 
inferred by co-authorship patterns) in relation to community and best practice. 
Our first step, therefore, in identifying best practice is to define the fields for 
which it must be able to cater, and whether practice varies between them. 
847 different journals are represented in the 21,484 articles that we collected. 
Out of these, 723 are represented in the set of articles (17,732 articles) where 
a protocol was extracted. The 10 most commonly represented journals have 
published almost 40% of all the articles in our data-set. Excluding PNAS as a 
general interest journal, there are three defined journal groups relating to 
fields of research within these 10 journals: evolutionary biology, 
microbiology/bacteriology, and virology. In addition to the 10 most common 
journals, we have also classified all journals represented in our article set (see 
Methods) into these journal groups. We found that 17% (3,712 articles) of 
articles were published in evolutionary biology journals, 22% (4,625 articles) 
were published in microbiology or bacteriology journals, and 11 % (2,274 
articles) were from journals related to virology; the remaining 50% (10,873 
articles) were published in a wide variety of fields; this we term the 'other' 
journal group. 
Given that a best-practice proposal should be able to cater for all users of 
phylogenetics, we assessed the differences/similarities between these fields 
and how they have developed through time. Furthermore, we can use this to 
assess whether there is methodological communication between fields. To do 
this, we calculate the proportion of articles from each journal group that 
contained each of the protocols implemented in each year, and generate a 
series of networks (Figure 4.2) that map the methodological choices made by 
authors from three different fields during the last 10 years (Appendix 1). 
These networks indicate that, while there is overlap, a significant shift in 
methodological preference has occurred between fields. We have used 
calculations of the network assortativity coefficient (see Methods) (r) 
(Newman 2003) to highlight changes in methodological choice. In this case, a 
larger r-value indicates field-specific method choice. Overall, network 
assortativity, and some field-field assortativity, comparisons (specifically: 
evolutionary biology/microbiology and evolutionary biology/virology) have 
increased throughout this period (Figure 4.3). Compared to random networks, 
there is a significantly different increase in overall network assortativity (Figure 
4.3). This is presumably owing to the larger increase in assortativity between 
the evolutionary biology field and the other two fields (Figure 4.3). There was 
no change in assortativity between the microbiology and virology fields, the 
values being inside the 95% confidence interval from 1996-1998 and 2000, 
and, when outside the 95% confidence interval, only varying between -0.04 
and 0.04. 
1996 2000 
o ./J , ~"",o I 
 I.} 
 " .? f , . <f 
w~: .-'~~. ~    I ,  e -0' -'-  ~ e 
 e ., 0 
~ e   0 
e r 0 
(; , . ,"   I 0 '" 
 e 0 /' .. 
~ 0  
e 0  " 
0 2005 0   0 
   
0    ,~ e  
c II Microbiology 
 II Evolutionary Biology  
 eG D Virology 
0 ~ .~ ~ 
 ~~ e  .() o. . fe" 1) 0 0 
~ . e D Other  0  
Figure 4.2 - Usage of protocols by field and through time. Protocol networks for the 
years 1996, 2000 and 2005. Nodes represent Individual protocols and are sized 
according to the number of times they were used. Each node Is also a pie chart 
describing the proportion of all uses of that protocol by each field group. Edges 
denote an F-measure value of greater than 0.75 phylogenetic term similarity between 
the protocols. The networks shown are the largest connected component after the F-
measure threshold was applied. 
> . -
Whole network 
 Evol. BioI. and Microbiol. 
D Evol. BioI. and Virology 
Microbiol. and Virology 
1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 
Year of Publication 
Figure 4.3 - Field-field network assortatlvlty data. Bar chart showing the changes In 
whole network and field-field "network assortatlvlty coefficient calculations (r). Error 
bars show 95% confidence Interval of distribution of r values calculated from 1000 
simulated networks (see Methods). 
To analyse the pattern of divergence between evolutionary biology and the 
other two fields, we analysed the usage of terms relating to Bayesian 
phylogenetic analysis (a relatively new method in the field). Over 60% of 
evolutionary biology articles published in 2005 included one or more 
references to a term describing Bayesian phylogenetic analysis of some kind; 
this compares to 5% of microbiology and 11 % of virology articles. This 
demonstrates that the kind of protocols implemented by the three fields have 
diverged during the period 1996-2005 and that in particular, protocols 
published in evolutionary biology journals have become more distinct from 
those in the other two journal groups, during the same period. 
To further investigate the heterogeneous use of different phylogenetic 
software and methods between fields, we analysed, for each of our 
phylogenetic terms (see Methods); (i) the field or fields in which it was used in 
its first year (Figure 4.4A); and (ii) whether it was ever used in each of the 
fields (Figure 4.48). Interestingly, there are a large number of terms that are 
only used in their first year of reporting (Figure 4.4A) in evolutionary biology 
(49/207) and outside of the three fields (101/207). Very few were used in all 
fields in their first year (2/207), while many more terms (Figure 4.48, 89/207) 
are used by all fields at some point, and some are only ever used by one 
(35/207) or two (55/207) of the fields. Many terms are first published outside 
the three fields, with 20% (20/101) of these never used in the three fields. 
A Usage in first year. B Usage in any year 
 Microbiology Virology  Other 
Figure 4.4 - Usage of phylogenetic terms according to field. Venn diagrams showing 
the usage of phylogenetic terms in articles from all three fields and those from outside 
the fields. (A) shows in which field, or fields, a term was used during the year when it 
was first mentioned in our corpus; this demonstrates the origin of the term. (8) Shows 
usage of terms in the three fields (or outside the three fields) but measures usage 
across all years; this measures communication of the term between fields. 
It is commonly accepted that much scientific practice is influenced by authority 
of some kind, be it by role, citations or experience; certain people are listened 
to and emulated more than others. Therefore, authority can be seen as an 
indicator of common practice. Authorities are, however, almost always 
specific to a field: a virologist will tend to read virology literature more often 
than microbiology literature. Given that we have a strongly field-specific 
research community, we can make an inference over what might be 
considered "best", or, perhaps more accurately, commonly published and 
scrutinised practice, by capturing what is done by the experts in each field. 
We define our experts as those who are most widely collaborative and also . 
who contribute the most research of publication quality to the community. 
To explore expert practice, we constructed a collaboration network (Newman 
2001 c) from our articles, and overlaid collaboration metric data. We also 
labelled our authors according to the journal group in which they most 
frequently publish. We included only those authors that came from articles 
from which a protocol has been extracted. This resulted in 45,290 unique 
authors and 190,530 collaborations between them. Each author was 
represented as a node, and each collaboration as an edge. We assigned two 
sets of attributes to the edges: these were the number of collaborations (edge 
weights 1) between the two authors connected by the edge, and the number 
of collaborations divided by the number of authors on each article (edge 
weights 2) of which they were co-authors (Newman 2001 b). This weights 
collaboration between authors on those articles with small numbers of co-
authors more highly than those with many co-authors. 
In order to identify authors who were most active in this network, we restricted 
the node set to include only those that had co-authored three or more articles 
with one or more' other authors. This reduced the largest connected 
component of the resultant network to 1,112 nodes and 2,412 edges; we refer 
to this network as the reduced collaboration network (Figure 4.5). When we 
consider the reduced collaboration network of authors (Figure 4.5) who have 
published in our phylogenetics corpus, we see a field-specific pattern similar 
to that in Figure 4.2. There are many authors who do not collaborate regularly 
with others outside their field (visible as the clusters of nodes of a single 
colour), and then there are those authors who link the clusters through 
interdisciplinary collaborations. 
e Microbiology e Evolutionary Biology o Virology QOther 
Figure 4.5 - Co-authorship network highlighting most expert authors. Co-authorship 
network according to research field. Nodes represent individual authors; edges 
represent three or more co-authorships between the two connected authors. The 20 
expert authors (see Methods) are represented by larger nodes with numbered labels. 
Author Names, 1: Koonin, E.V., 2: Pace, N.R., 3: Wang, Y., 4: Zhang, Y., 5: Doolittle, 
W.F., 6: Hasegawa, M., 7: Okada, N., 8: Nel, M., 9: Roger, A.J., 10: Meyer, A., 11: Falsen, 
E., 12: Collins, M.D., 13: Stackebrandt, E., 14: Schumann, P., 15: Yoon, J.H., 16: Orito, 
E., 17: Mizokami, M., 18: Webster, R.G., 19: Sharp, P.M., 20: Gessaln, A. 
We quantified the field-specific structure in Figure 4.5 in the same way as 
Figure 4.2 (see Methods). Overall, network assortativity is 0.36, fields being 
almost equally assortative with respect to each other (evolutionary 
biology/microbiology: 1.0; evolutionary biology/virology: 0.977; and 
microbiology/virology: 0.959). Note, the assortativity value of 1.0 between 
evolutionary biology and microbiology indicates there are no co-authorships 
between authors from these fields in this reduced network. Using a metric of 
collaborative activity (see Methods), we have highlighted the five most active 
researchers from each of the three fields, as well as five from outside the 
fields. Interestingly, the protocols employed by these 20 most highly active 
authors in the network are very similar to those used by the rest of the 
community. The 10 most frequently used protocols by the non-experts (used 
73% of the time) are used almost as frequently by the experts (74%). The 
experts have co-authored 1,001 articles between them, and these articles 
have made use of 26 protocols unique to their group. 
4.4. Discussion 
Our survey of phylogenetic practice over the last 10 years has found a large 
range of experimental protocols declared at varying levels of detail. This has 
created an environment of both consensus and variation; we have authors 
reporting methods that are commonly used by hundreds of authors,' and 
others who create highly bespoke experimental protocols of their own design, 
and remain the sole practitioners of these protocols. Context is clearly 
important; it could be that these highly specific protocols are used to answer 
highly specific research questions. Until we are able to reliably capture 
detailed contextual information of this kind, we will not know whether this is 
the case. In addition, our results highlight the need for better recording and 
communication of experimental methods. 
The observed field-specific and temporal variations (Figures 4.2 and 4.3) in 
community practice suggest that an opinion of what constitutes best practice 
is changing through time, and will depend to a large extent on the field to 
which the individual belongs. In particular, we find there has been a 
significant shift in the methodologies used by the evolutionary biology field 
and the microbiology and virology fields, which is partly attributable to the 
differential use of Bayesian inference. We have analysed the emergence and 
spread of different phylogenetic terms in the three fields (Figure 4.4), and find 
a large number of terms unique to the evolutionary biology field (in their first 
year of usage, Figure 4.4A) and that a large number of terms are never used 
outside of this field (Figure 4.4B), except for when they are used in 'other' 
group journal articles. The number of terms being first used outside the three 
fields (101/207, Figure 4.48) is a reflection of the specialised nature of 
phylogenetic methods and software; the authors who develop new methods 
and software do not publish in the same kind of journals as those who use 
their innovations. 
Almost all of the 10 protocols used most commonly by the phylogenetics 
community represent a valid choice (except those using UPGMA) for a 
researcher new to the field. UPGMA has been criticised (Huelsenbeck 1995; 
Leitner, Escanilla et al. 1996) as a method of tree-inference because it always 
produces ultra metric trees (trees that have leaf nodes that are all the same 
distance from the root) and trees of this type rarely convey evolutionary 
history accurately (Felsenstein 2004, p56). Common community practice is 
therefore a good starting point upon which to build a best-practice proposal. It 
does, however, lack those features unique to specific fields, and the 
requirements of specific users. 
Our analysis suggests that communication of methods is not only difficult (i.e., 
researchers apparently use many different computer programs for generally 
the same types of analysis), but is also hampering information exchange 
between practitioners of phylogenetics. The latter appears to be owing to 
academic specialism, in that researchers will first look to others from their own 
field when choosing methods. Interestingly, expert authors do not use 
protocols that are distinct from others in their field. This makes the protocols 
used by experts in each field a valuable indicator of the kind of protocols 
commonly used in their respective fields, and thus a useful short-cut to 
identifying best practice for that specific field. 
Our model-based method for protocol extraction (Figure 4.1) has permitted 
the construction of representations of protocols that have a direct link with the 
physical implementation of that protocol. The use of full-text journal articles 
was necessary owing to the information we were endeavouring to capture 
(i.e., experimental methods). Many other text-mining projects would be 
significantly enhanced with the use of full-text. An abstract is an abbreviated 
summary that presents the most important findings from a piece of research, 
and very briefly places them in context. Its function is to act as a pOint of 
entry to a complete manuscript. Much potentially important information, and 
nuance, will only be found in the full manuscript. Thus, given the number of 
different SCientifically interesting elements (Shah, Perez-Iratxeta et al. 2003; 
Natarajan, Mulay et al. 2005; Natarajan, Serrar et 81. 2006; Aerts, Haeussler 
et 81. 2008) contained in a full article, which are usually not present in the 
abstract, researchers must make more use of full-text. 
The model-based method for protocol extraction also allows us to organise 
method terms according to the order in which they would have been used in 
the experiment, rather than their order in the text. This is a powerful element 
of our approach that could lead to further work on automated suggestion of 
methods, or software, for a given task or region of a protocol. The model can 
also allow us to account for missing information in our extracted protocols, so 
that if an extracted protocol does not contain any terms related to a single part 
of the model (Figure 4.1), we can still analyse the other parts of the protocol 
for which we do have information. This is an important feature for analysing 
information that has been automatically derived from text, which is often 
sparse, some elements being well described, easy to identify and analyse, 
and others being described indirectly via citation, Figure legends or 
supplementary information. We believe that our structured approach to 
capturing protocols from full-text articles could be applied to any discipline of 
science where the methods used can be broken down into individual 
sequential stages. For example, a simple molecular biology task to sequence 
a genic region from a single fruit fly could be broken down into: DNA 
extraction, purification, amplification, sequencing and chromatogram analysis. 
As with a phylogenetic protocol, several terms could map onto each one of 
these stages: for example, peR or bacterial cloning could be used in the 
amplification step. 
In the increasingly specialised world of scientific research, our results 
demonstrate the need for strong collaboration and communication between 
fields of research, especially between those implementing similar 
experimental designs. Best practice information derived from whole 
disciplines, rather than small research communities, allows us to share 
information between a larger number of researchers who may have no 
knowledge of new innovations in other fields. Best practice also supports 
replication of results and standardisation of practices by providing protocols 
that can be reused in many different research areas and that produce 
comparable results. Comparable results are of particular importance in 
phylogenetics at the moment, with the advent of phylogenomics (Eisen and 
Fraser 2003; Delsuc, Brinkmann ef al. 2005) and projects attempting to 
construct and represent .the full tree of life (Ciccarelli, Doerks ef al. 2006; 
Letunic and Bork 2007; Maddison, Schulz ef al. 2007). 
4.5. Conclusions 
Our capture of protocols from across a large group of researchers has 
allowed us to reliably survey the current state of practice in the design of 
protocols in the field of molecular phylogenetics. This information is useful for 
monitoring best practice versus new trends and directions in the community, 
as well as identifying where they originate. 
The capture of best practice is a non-trivial task; in this case, we have found 
that the practice of well-published authors acts as a good proxy for that of 
others in the same field. We have also defined how the main fields have 
altered methodologically over time. The evolutionary biology field, in 
particular, has diverged from the others, and these changes are characterised 
by the use of new and more advanced methods. This suggests that the 
practice of expert authors in evolutionary biology (Table 4.1) is the closest to 
contemporary best practice for phylogenetic experimental design. 
Notwithstanding data-specific issues, a protocol combining those 
methodological elements present in the protocols of the evolutionary biology 
group with elements from protocols of the experts in the specific field will offer 
an appropriate choice for any researcher. We also envisage the tailoring of 
best practice to individual users' needs. This would determine the user's 
experimental context, including such information as data-set size, 
idiosyncrasies of the data, the level of analytical detail required; and any time 
constraints, and would use this information to tailor a base protocol to the 
users' needs. 
Method/Model Combination 
Neighbour-joining, Parsimony, Maximum-likelihood 
Neighbour-joining, Parsimony, Maximum-likelihood, 
JTT model 
Maximum-likelihood 
Neighbour-joining 
Neighbour-joining, Parsimony, Maximum-likelihood, 
HKY model 
Neighbour-joining, Maximum-likelihood 
Maximum-likelihood, Bayesian 
Parsimony, Maximum-likelihood 
Neighbour-joining, Maximum-likelihood, HKY model 
Neighbour-joining, Parsimony, Maximum-likelihood, 
Bayesian 
Usage (number of 
articles) 
Table 4.1 - Expert evolutionary biology methods. A sample of the most commonly 
used tree-inference and evolutionary model combinations used by the top five expert 
authors from the evolutionary biology field. 
Currently, practice-capture requires extensive effort to identify community-
wide information from a field where the choices of methods are well defined 
and described. We propose that an explicit model of an experimental protocol 
represented as a workflow (Fisher, Hedeler et al. 2007) will help to improve 
communication, sharing and ranking of experimental protocols and will 
support one of the central tenets of the scientific method, that of reproducible 
results (Gentleman, Carey et al. 2004; Gentleman 2005; Giles 2006; Nature-
editorial 2006). Importantly, this computer-enabled workflow should contain 
all parameters and methods of all elements of the experiment, with stable 
connections to implementations of these methods that are accessible to all. 
Specific phylogenetic protocols (represented as implementable workflows) 
could then be associated with quality metrics: for example, quantification of 
usage based on the number of published articles using the protocol; protocols 
associated with specific authors; types of data; specialist protocols that 
integrate additional methodologies (for example, the detection of recombinant 
sequences). Work such as this will benefit from concerted effort on the 
subject of context capture and how to capture the real aims of a study. This 
will complete the linking of methods with aims, permitting researchers to 
efficiently tailor experimental solutions to specific research projects. 
4.6. Methods 
4.6.1. Article identification 
We collected a set of journal articles in PDF format identified by a PubMed 
search defined as "phylogen*[Title/Abstract] AND (full-text[sb]) AND 
("X"[PDat]:"Y"[PDat])" where X and Y are dates used to restrict the number of 
results returned. The search was performed 01/04/06. The Quosa 
Information Manager82 was used to download the PDF files. 
4.6.2. PDF file text extraction 
Text was extracted using the pdftotext executable from the xpdf package83 
provided by foolabs. The executable was run with default settings. 
4.6.3. Methodological term identification 
We used manually tested and designed regular-expression patterns to identify 
the methods declared in the text. A manually created controlled vocabulary of 
258 important names and terms84 was used to store and group these 
patterns. The controlled vocabulary is an XML document that groups terms 
according to their methodological nature. The software names were taken 
from Professor Joseph Felsenstein's page of phylogenetic programs85. Other 
terms were manually collated using the phylogenetics primary literature. 
82 http://www.quosa.com/ 
83 http://www.foolabs.com/xpdfl 
84 http://www.bioinf.manchester.ac.uk/robertsonijeales/contentiphylogenetic_cv.xml 
85 http://evo lution.genetics. washington.edulphylip/software.html 
4.6.4. Protocol Formation 
We constructed two forms of the protocol from each article. The first included 
the term matches for all terms in the vocabulary. The second form only 
includes terms that are classified as a type of "phylo_method" or 
"phylo_model" in the vocabulary document. 
4.6.5. 151 Journal Citation Report categorisation 
Classification of journals is determined by the lSI Journal Citation Report 
(JCR) service, which classifies journals by discipline. The three journal 
subject categories used are "Evolutionary Biology", "Microbiology" and 
"Virology" with 16, 51 and 15 of the journals in our data-set being present in 
these categories respectively. All journals not present in these groups were 
labelled "Ungrouped". 
4.6.6. Protocol similarity 
We use the F-measure or f-measure (Feldman and Sanger 2006) to 
determine similarity between protocols. The F-measure value gives an 
indication of the number of shared terms between two protocols as a fraction 
of all the terms found in the two protocols. This gives a measure of 
methodological similarity, while also normalising for the number of terms 
found. The F-measure is calculated between all pairs of protocols. The F-
measure is calculated as follows. 
F - measure = , 
n1 + n2 
where 
c = the number of terms common between the two protocols, 
n1 = the total number of terms in the first protocol, 
n2 = the total number of terms in the second protocol. 
4.6.7. Author names 
We used the 'surname and all initials' method of author-name construction 
(Newman 2001 a). This can lead to multiple author names that may refer to 
the same author. However, we felt that this would not bias the structure of the 
network significantly, given that most authors tend to co-author most of their 
articles with a similar group of collaborators. This method can also avoid the 
problem of common surname/first initial combinations referring to multiple 
authors. 
4.6.8. Protocol networks 
The protocols in the protocol networks in Figure 4.2 are present in the largest 
connected component from a network of all unique protocols found in this 
study that have an F-measure similarity score of greater than 0.75, and were 
captured from an article published in the given year. The protocols are, in this 
case, constructed from the terms found in the article that are classified as part 
of the "phylo_method" or "phylo_model" sections of our term vocabulary. This 
eliminates unimportant software implementation detail from the protocols, and 
allows us to analyse the specific methodological choices made in each article. 
We generated the pie node networks in Figure 4.2 using the GenePro 
(Vlasblom, Wu ef al. 2006) plugin for Cytoscape (Shannon, Markiel ef al. 
2003). Each pie node gives a visual representation of the proportional use of 
each protocol by each journal category. 
4.6.9. Author field/journal group affiliation 
Each author node present in the reduced collaboration network was assigned 
a journal group label. These were the same set of labels used in the protocol 
networks (Figure 4.2). If the author had published more than half (the 
majority) of their articles in one of the journal groups, then they were labelled 
with that group; otherwise, they were labelled as "ungrouped" and appear as 
white nodes in the network. 
4.6.10. Expert identification 
For each node in the reduced collaboration network, we calculated the sum of 
the values of edge weight 2 for every connected edge. We then used five 
nodes with the highest value of this metric from each of the three fields, and 
from the ungrouped authors, as our set of 20 experts. 
4.6.11. Term identification error analysis 
To test the accuracy of our term matching, we manually annotated the 
Methods section or section of text most descriptive of methodological detail 
for 50 randomly chosen articles from our corpus. We annotated all pieces of 
text that referred to any of the phylogenetic entities that are present in the 
controlled vocabulary. 
4.6.12. Network assortativity calculations 
The network assortativity coefficient (r) gives an indication of how mixed a 
network is; it measures the average proportion of edges that connect nodes of 
the same type, with nodes being typed according to their field. If all network 
edges only connect nodes with the same class label then the network will 
have an assortativity coefficient of 1; if all edges connect nodes with different 
labels than the coefficient drops to -1. The calculation of the network 
assortativity coefficient (r) requires that each node is given a class label 
(Newman 2002; Newman 2003). Because our nodes are composite 
structures that describe the number of articles from each field, using the 
particular protocol, we calculated r using a discrete model of our network. 
Each pie node becomes a set of nodes of size n, where n equals the number 
of articles that used the protocol. These nodes are then assigned class 
labels, based on the field composition of each pie node. We then create 
edges between all new nodes that were part of a pie node pair (Figure 4.6). 
-----3 
 Microbiology 
 Evolutionary Biology Virology o Other 
Figure 4.6 - Example of discrete network construction. Example of discrete network 
construction. Numbers denote the number of articles from each group In the pie node. 
(A) The starting pie node network. (8) The resultant discrete network model used for 
assortatlvlty calculations. 
4.6.13. Network simulations and error 
For each of the years between 1996 and 2005 (inclusive), we calculated the r 
value, the coefficient of network assortativity on the whole network, and for all 
pairwise field comparison sub-networks. To gain a measure of the error of 
these values, we simulated 1,000 randomised networks for each year, and 
performed the same network assortativity calculations on these networks. 
The randomisation process maintained everything from the original network, 
apart from the class labels assigned to each node. These labels were 
shuffled and reassigned to each node. We then calculated the 95% 
confidence interval of the distribution of simulation results, and these values 
are presented in Figure 4.3 as error bars. 
4.6.14. Author metadata 
Author metadata for each article was extracted from the eSummary NCB I 
eUtils service86 . We used the PubMed ID (PMID) as a unique identifier for 
each article. The PMID for each article was obtained by Quosa when the 
article was originally downloaded. 
4.6.15. Network visualisation 
The networks in Figure 4.2 were visualised using Cytoscape version 2.3.2 
(Shannon, Markiel et al. 2003), with yFiles organic layout and the GenePro 
plugin (Vlasblom, Wu et al. 2006). The network in Figure 4.5 was visualised 
using Cytoscape version 2.6.0, with force-directed layout and using default 
settings. 
86 http://www .ncbi.nlm.nih.gov/entrez/query/static/esummary _ help.html. 
4.7. References 
Aerts, S., M. Haeussler, S. van Vooren, O. Griffith, P. Hulpiau, S. Jones, S. 
Montgomery, C. Bergman and C. The Open Regulatory Annotation 
(2008). "Text-mining assisted regulatory annotation." Genome Biology 
9(2): R31. 
Bassett, D. E., M. B. Eisen and M. S. Boguski (1999). "Gene expression 
informatics-it's all in your mine." Nature Genetics 21 (1 Suppl): 51-55. 
Chaussabel, D. and A. Sher (2002). "Mining microarray expression data by 
literature profiling." Genome Biology 3(10). 
Ciccarelli, F. D., T. Doerks, C. von Mering, C. J. Creevey, B. Snel and P. Bork 
(2006). "Toward Automatic Reconstruction of a Highly Resolved Tree 
of Life." Science 311 (5765): 1283-1287. 
Creighton, C. and S. Hanash (2003). "Mining gene expression databases for 
association rules." Bioinformatics 19(1): 79-86. 
Delsuc, F., H. Brinkmann and H. Philippe (2005). "Phylogenomics and the 
reconstruction of the tree of life." Nature Reviews Genetics 6(5): 361-
Egger, M., G. D. Smith and A. N. Phillips (1997). "Meta-analysis: Principles 
and procedures." BMJ 315(7121): 1533-1537. 
Eisen, J. A. and C. M. Fraser (2003). Phylogeno~ics: Intersection of Evolution 
and Genomics. Science, American Association for the Advancement of 
Science. 300: 1706-1707. 
Feldman, R. and J. Sanger (2006). The Text Mining Handbook: advanced 
approaches in analyzing unstructured data. New York, Cambridge 
University Press. 
Felsenstein, J. (2004). Inferring phylogenies, Sinauer Associates Sunderland, 
Mass., USA. 
Fisher, P., C. Hedeler, K. Wolstencroft, H. Hulme, H. Noyes, S. Kemp, R. D. 
Stevens and A. Brass (2007). "A systematic strategy for large-scale 
analysis of genotype-phenotype correlations: identification of candidate 
genes involved in African trypanosomiasis." Nucl. Acids Res. 35(16): 
5625-5633. 
Gentleman, R (2005). "Reproducible research: A bioinformatics case study." 
Statistical Applications in Genetics and Molecular Biology 4(1): 2. 
Gentleman, R, V. Carey, D. Bates, B. Bolstad, M. Dettling, S. Dudoit, B. Ellis, 
L. Gautier, Y. Ge, J. Gentry, K. Hornik, T. Hothorn, W. Huber, S. lacus, 
R Irizarry, F. Leisch, C. Li, M. Maechler, A. Rossini, G. Sawitzki, C. 
Smith, G. Smyth, L. Tierney, J. Yang and J. Zhang (2004). 
"Bioconductor: open software development for computational biology 
and bioinformatics." Genome Biology 5(10): R80. 
Giles, J. (2006). "The trouble with replication." Nature 442(7101): 344-347. 
Goldman, N. (1998). "Phylogenetic information and experimental design in 
molecular systematics." Proceedings of the Royal Society of London. 
Series B. Biological sciences. 265(1407): 1779-1786. 
Huelsenbeck, J. P. (1995). "Performance of Phylogenetic Methods in 
Simulation." Systematic Biology 44( 1): 17-48. 
Jang, H., J.Lim, J. H. Lim, S. J. Park, K. C. Lee and S. H. Park (2006). 
"Finding the evidence for protein-protein interactions from PubMed 
abstracts." Bioinformatics 22(14): e220. 
Leitner, T., D. Escanilla, C. Franzen, M. Uhlen and J. Albert (1996). "Accurate 
reconstruction of a known HIV-1 transmission history by phylogenetic 
tree analysis." Proceedings of the National Academy of Sciences of the 
United States of America. 
Letunic, I. and P. Bork (2007). "Interactive Tree Of Life (iTOL): an online tool 
for phylogenetic tree display and annotation." Bioinformatics 23(1): 
Ludascher, 8., /. Altintas, C. Berkley, D. Higgins, E. Jaeger-Frank, M. Jones, . 
E. Lee, J. Tao and Y. Zhao (2005). "Scientific Workflow Management 
and the Kepler System." Concurrency and Computation: Practice & 
Experience 18(10): 1039-1065. 
Maddison, D. R, K. S. Schulz and W. P. Maddison (2007). "The tree of life 
web project." Zootaxa 1668: 19-40. 
Marcotte, E. M., /. Xenarios and D. Eisenberg (2001). "Mining literature for 
protein-protein interactions." Bioinformatics 17(4): 359-363. 
Natarajan, J., D. Berrar, W. Dubitzky, C. Hack, Y. Zhang, C. DeSesa, J. Van 
Brocklyn and E. Bremer (2006). "Text mining of fUll-text journal articles 
combined with gene expression analysis reveals a relationship 
between sphingosine-1-phosphate and invasiveness of a glioblastoma 
cell line." BMC Bioinformatics 7(1): 373. 
Natarajan, J., N. Mulay, C. DeSesa, C. J. Hack, W. Dubitzky and E. G. 
Bremer (2005). A Grid Infrastructure for Text Mining of Full Text 
Articles and Creation of a Knowledge Base of Gene Relations. Lecture 
Notes in Computer Science. 3745: 101. 
Nature-editorial (2006). "Let's replicate." Nature 442(7101): 330-330. 
Newman, M. E. J. (2001 a). "Scientific collaboration networks. I. Network 
construction and fundamental results." Physical Review E 64(1): 
016131. 
Newman, M. E. J. (2001 b). "Scientific collaboration networks. II. Shortest 
paths, weighted networks, and centrality." Physical Review E 64: 
016132. 
Newman, M. E. J. (2001c). "The structure of scientific collaboration networks." 
Proc Natl Acad Sci USA 98(2): 404-409. 
Newman, M. E. J. (2002). "Assortative Mixing in Networks." Physical Review 
Letters 89(20): 208701. 
Newman, M. E. J. (2003). "Mixing patterns in networks." Physical Review E 
67(2): 026126. 
Rzhetsky, A, I. lossifov, T. Koike, M. Krauthammer, P. Kra, M. Morris, H. Yu, 
P. A Duboue, W. Weng, W. J. Wilbur, V. Hatzivassiloglou and C. 
Friedman (2004). "GeneWays: a system for extracting, analyzing, 
visualizing, and integrating molecular pathway data." Journal of 
Biomedical Informatics 37(1): 43-53. 
Rzhetsky, A., I. lossifov, J. M. Loh and K. P. White (2006). "Microparadigms: 
Chains of collective reasoning in publications about molecular 
interactions." Proc Natl Acad Sci USA 103(13): 4940-4945. 
Sackett, D. L., W. M. C. Rosenberg, J. A M. Gray, R. B. Haynes and W. S. 
Richardson (1996). "Evidence based medicine: what it is and what it 
isn't." BMJ 312(7023): 71-72. 
Shah, P., C. Perez-I ratxeta , P. Bork and M. Andrade (2003). "Information 
extraction from full text scientific articles: Where are the keywords?" 
BMC Bioinformatics 4(1): 20. 
Shannon, P., A. Markiel, O. Ozier, N. S. Baliga, J. T. Wang, D. Ramage, N. 
Amin, B. Schwikowski and T. Ideker (2003). "Cytoscape: A Software 
Environment for Integrated Models of Biomolecular Interaction 
Networks." Genome Research 13(11): 2498-2504. 
Stevens, R., H. Tipney, C. Wroe, T. ~inn, M. Senger, P. Lord, C. Goble, A. 
Brass and M. Tassabehji (2004). "Exploring Williams Beuren Syndrome 
Using my Grid." Bioinformatics 20(suppl 1): i303-310. 
Stevens, R. D. (2003). "myGrid: personalised bioinformatics on the 
information grid." Bioinformatics 19(90001): 302-304. 
Szulanski, G. (1996). "Exploring Internal Stickiness: Impediments to the 
Transfer of Best Practice Within the Firm." Strategic Management 
Journal 17: 27-43. 
Tanabe, L. and W. J. Wilbur (2002). "Tagging gene and protein names in 
biomedical text." Bioinformatics 18(8): 1124-1132. 
Vlasblom, J., S. Wu, S. Pu, M. Superina, G. Liu, C. Orsi and S. J. Wodak 
(2006). "GenePro: a cytoscape plug-in for advanced visualization and 
analysis of interaction networks." Bioinformatics 22( 17): 2178-2179. 
5. Metrics on methods, valuing the process 
5.1. Abstract 
Well-designed experiments are the key to reliable data and well-supported 
conclusions. But in the multidisciplinary world of modern biology, how do we 
choose between the huge number of choices available to perform even the 
simplest of tasks? Here, we present an approach to improve experimental-
design decisions based on objective evidence provided by published 
research. We score choices of methods based on who uses them, how 
frequently, when they were used and in which articles. We quantitatively 
score methods in the multidisciplinary research domain of phylogenetics. We 
define four potential metrics of quality (popularity, recentness, expertness and 
citation), and apply possible normalisations to the scores. We define a 
method for combination of these metric scores, and explore the utility of our 
data by analysing the choice of methods in a currently available phylogenetic 
analysis. This work illustrates how novel use of the published literature and 
text-mining methods can bring a deeper level of evidence to the choice of 
experimental methods. The metrics we define provide that which is currently 
difficult to capture objectively: a quantitative measure of quality and 
confidence in a given methodology. 
5.2. Introduction 
Progress in science is almost entirely dependent on advances in the research 
techniques and tools that we have at our disposal. It is important, therefore, to 
know both what can be done in a given experimental setting and what should 
be done. Indeed, along with critical interpretation of results, scientific expertise 
is predominantly knowledge of methodologies, their appropriate design, 
invention and implementation. Thus, the method is all-important: a scientist 
will judge the validity of an experiment's findings by the specific protocol used, 
and experiments can only be replicated if methodologies are recorded 
accurately (Eales, Pinney et al. 2008). Furthermore, the choice of method 
can have an impact on the outcome of the analysis, to the extent that 
conflicting relationships are obtained (Worobey, Rambaut et al. 2002). 
In many research areas (e.g., sequence analysis, biological pathway 
modelling, data-mining or microarray normalisation), there are now a 
bewildering array of protocols available, many of which perform similar tasks 
using different methods, making it difficult for individual researchers to keep 
track of new and existing methods. Identifying a "preferred" methodology 
requires extensive effort and experience, as there is currently no way to 
comprehensively search and compare the array of methodologies used in a 
research domain for a particular task. So, how do we choose which methods, 
protocols, software, statistical tests to use, without knowledge of which is 
best, or even what all the options are? 
One possibility'is to ask a colleague with relevant expertise, an option that 
might not always be available, or the expertise may not be completely up-to-
date, an increasing problem as our science becomes more multi-disciplinary 
(Kostoff 2002). A common approach is relying on established authorities in 
the field. In the increasingly specialised world of scientific research (Kostoff 
2002), there is an unprecedented need for strong collaboration and 
communication between fields of research, especially between those 
implementing similar experimental designs. A default option, of course, is 
studying the published literature in which detailed domain knowledge has 
been recorded. Accessing the appropriate information, and piecing together 
an understanding of contemporary practice in a domain, however, can be 
extremely time-consuming, given the number of existing papers and the rate 
of new publications in biology. 
Here, we propose a new approach to these problems that is based on metrics 
of quality automatically inferred from the published literature. We capture 
metric data on a domain-wide level, to avoid individual bias and field-
specificity (Eales, Pinney et al. 2008). We capture the kind of information that 
we think is commonly considered by individual researchers as useful in 
determining reliability and quality. More specifically, these metrics will not be 
measuring empirical quality directly, but rather, inferring evidence for, and 
confidence in, the choices available to perform a certain task. The metrics are 
based on when, how often, and how well known, a certain method is in the 
published literature. 
Published literature has an inherent lower limit on quality; we can infer that a 
published methodology was good enough to be passed by the peer-review 
process, and therefore is sufficiently well designed and described to be 
eligible for publication. There are some difficulties with this argument, in that, 
studies have shown that peer-review does not identify papers to be accepted 
any differently from a random process (Weiner, Weiner et al. 2010). 
Furthermore, the contribution of peer review, in assuring the quality of 
accepted and therefore published papers, has not been fully quantified 
(Jefferson, Alderson et al. 2002) or widely studied (Wager and Jefferson 
2001). Despite these difficulties, the vast majority of published scientific 
research articles have been peer reviewed and, currently, this method of 
ascertaining quality is the most commonly used and accepted in scientific 
research. Peer review does not guarantee very high-quality methodologies, 
but it does impose a certain baseline, determined by how rigorous the original 
peer-review process was and the vagaries of the peer-review process in 
general. Our approach is to collect large numbers of journal articles and 
identify the methodological components (i.e., the methods, software, 
algorithms, statistical tests, reagents, kits and apparatus) declared in those 
articles, using text-mining methods. We then link the components with metric 
scores that are derived from not just one article, but all articles in which they 
are declared. Furthermore, we assign a role to each of the components. For 
example, a t-test would be categorised as a 'statistical test' for 'examining the 
difference in the mean of 2 distributions of data', and it cannot, for example, 
be used to 'buffer a solution against a change in pH'. The utility of knowing 
the role is that we can gather together all the different possibilities and choose 
the one most favoured by the metric scores. 
To the best of our knowledge, there has been no work on assigning 
bibliometric-like measures to specific methodologies to characterise their 
quality. Current bibliometric indicators (e.g., journal impact factor, article 
citation counts, author h-index) use citation data to derive a measure of 
quality. These metrics have been the subject of detailed scrutiny (Lehmann, 
Jackson et 81. 2006; Harnad 2008; Harnad 2009), mainly owing to their use as 
quality indicators for articles, individual researchers, journals, and, now, 
higher education institutions87 . Citation data, however, have an inherent bias 
referred to as the Matthew Effect (Merton 1968; Merton 1988; Havemann, 
Heinz et 81. 2005), whereby the likelihood of an article being cited is 
determined by how many times it has been cited previously, and is not 
determined by its inherent quality. Other work suggests a similar pattern of 
self-reinforcement in methodological practices (Sun 2004). There is, thus, a 
need for a more general metric-based framework that uses more than just 
citation data to derive quantitative characteristics objectively, in particular by 
providing metric scores that are tailored to an individual's needs. 
Capturing information on a range of properties (as opposed to a single 
measure, such as impact factor) is important, so that we can weight the 
properties differently given different requirements, and so that our process is 
able to better mimic what an individual might do to assess quality. It is 
important to note that our approach here is deliberately general; we have tried 
to capture a range of different information sources: some may prove to be 
effective at inferring quality, and some information sources that are absent 
may prove to be invaluable. We have identified and analysed data sources 
that are widely available, and that we think provide information relevant for the 
inference of scientific quality. Here, we propose 4 new metrics that we think 
can be informative for assessing the quality of the methods used. We will 
illustrate the outcome of applying each of these metrics to methods from a 
field of research that could benefit from objective methodological-quality 
metrics: the field of molecular phylogenetics (Eales, Pinney et 81. 2008). 
87 http://www.hefce.ac.uklResearchiref 
5.3. Results 
To calculate the metric scores for a set of methods, we have used a 
representative sample of the phylogenetics literature. The sample comprises 
21,484 fUll-text journal articles, 90% of which were published after 1996 
(Eales, Pinney et al. 2008). We have identified a set of methods that 
implement two very common tasks in a standard phylogenetic analysis, and 
we have scored each one using the 4 metrics defined below. The two tasks 
are multiple sequence alignment and tree-inference. We achieved this 
grouping of methods by using a structured Controlled Vocabulary (CV) that 
groups method terms (how methods appear in text) according to how they are 
used in an experiment (i.e., which of the 2 tasks they perform) (Eales, Pinney 
et al. 2008). For example Neighbour-Joining is in the 'tree-inference methods' 
group. Finally, in the calculation of the metric scores, we have also included 
potentially useful normalisations with which we intend to improve the 
approximation of an individual's assessment of quality. 
5.3.1. Potential Quality Metrics 
5.3.1.1. Popularity (is the method commonly used) 
Our intention with this metric is to highlight what is seen as a common or 
uncommon approach overall. We would nEwer consider this metric to provide 
a definitive answer, but it does highlight those choices that are seen as tried 
and trusted, and well known in the field. We assign a score for this metric by 
identifying the proportion of all articles in the collection that have declared use 
of the method. 
Figure 5.1 presents the raw popularity scores for both tasks. Clustal W 
(Thompson, Higgins et al. 1994) and Clustal X (Thompson, Gibson et al. 
1997) are two implementations (X has a graphical user interface (GUI) and W 
is command line only) of the same alignment algorithm and are referred to 
collectively as "Clustal W/X" in this work. Clustal W/X has the highest 
popularity score of 0.92; this is over 11 times higher than the next largest 
score (Clustal V, popularity = 0.08). The scores for tree-inference methods 
are more evenly spread, with Neighbour-Joining and Maximum Likelihood 
being the most popular methods. 
Multiple Sequence Alignment 
Q) ci 
Clustal WIX T-Coffee Muscle Clustal V 
Method 
Tree Inference 
"'" 0 ci en 
0 .;:: 
Q) ci 
ci Neighbour Maximum Weighted Maximum 
Joining Parsimony Parsimony Likelihood Bayesian UPGMA 
Method 
Figure 5.1 - Popularity metric scores. Bar chart of popularity metric scores (see 
Methods) for tree-inference and sequence alignment methods. 
5.3.1.2. 
years) 
Recentness (how has its popularity changed in recent 
Progress in science is often incremental, with old methods slowly going out of 
fashion as they are replaced by newer, more efficient and more accurate 
methods. Recentness highlights the change in popularity of a method in 
recent years. We track the usage of a method, year by year, and then 
determine how its popularity has changed in the literature. 
This metric scores increasingly popular methods highly, and scores methods 
declining in popularity with a low value. We evaluate the change in method 
popularity by taking the slope of the least squares regression model fitted to 
the popularity data for the period 2001 to 2006 inclusive. 
Figure 5.2 presents the raw recentness scores for both tasks. Although 
Clustal W/X is the most popular alignment method overall (see Figure 5.1), it 
also has the 2nd lowest recentness score (recentness = 0.0017), a reflection 
of its recent decline in popularity in favour of newer methods (such as T-
Coffee and Muscle). Bayesian tree-inference methods have seen a large 
recent increase in popularity, maximum likelihood has also seen a small 
increase, but all other methods have declined in popularity. 
0;:: -Q) 
Multiple Sequence Alignment 
Clustal W/X T -Coffee Muscle Clustal V 
Method 
Tree Inference 
I Neighbour 
Joining Parsimony 
Weighted 
Parsimony 
Maximum 
Likelihood Bayesian UPGMA 
Method 
Figure 5.2 - Recentness metric scores. Bar chart of recentness metric scores (see 
Methods) for tree-Inference and sequence alignment methods. 
5.3.1.3. Expertness (is the method linked to highly published 
'expert' authors) 
The expertness metric is our attempt to link a method with the scientists who 
make use of it. All authors of a research publication offer an indirect 
endorsement of the methods they report in the text. They have used the 
method in the research they are reporting, and therefore they must have at 
least considered the validity of the choice of method. We link the 'expertness' 
of authors with the methods they have declared in their research publications. 
Our measure of 'expertness' is calculated for all authors in the literature 
sample, and it is based on who has contributed the most original research 
publications. We normalise each score with respect to the number of authors 
on the article (e.g., authors who publish on their own receive more expertness 
per publication than those who publish with, for example, 10 others). We can 
then identify all authors who have published an article that makes use of a 
given method, and assign the median normalised author-expertness value to 
that method (see Methods). 
Figure 5.3 presents the raw expertness scores for both tasks. The scores for 
the sequence alignment methods are very similar, Muscle having the highest 
score. Results for the tree-inference methods are more varied, weighted 
parsimony and Bayesian methods having the highest scores. 
Q) ... 
Multiple Sequence Alignment 
Clustal W/X 
Neighbour Maximum 
Joining Parsimony 
T -Coffee Muscle 
Method 
Tree Inference 
Maximum 
Likelihood 
Method 
Bayesian 
Clustal V 
UPGMA 
Figure 5.3 - Expertness metric scores. Bar chart of expertness metric scores (see 
Methods) for tree-Inference and sequence alignment methods. 
5.3.1.4. Citation (is the method well cited) 
The perceived quality of a piece of research (and publications based on it) is 
very closely tied to the number of citations it receives. Although citation data 
do have implicit difficulties and biases (Merton 1968; Merton 1988; 
Havemann, Heinz et at. 2005), we felt that it must be included in our metric 
calculations so that we can truly attempt to reflect an individual's own 
measure of quality. 
The score for this metric is the median number of citations (as reported by 
Google Scho/ar88) attributed to all articles that have made use of the method. 
We, however, only assign a proportion of the citations for each article based 
on whether alternatives to the method in question were also used in the same 
article. For example, if an article with 10 citations has used Neighbour-Joining 
and Maximum Likelihood, the/'). each method is only assigned a normalised 
citation score of 5. We have deliberately avoided correcting for citations 
accumulated over time (i.e., the age .of the article), because we think this 
better represents the opinion of individual researchers. A well-cited but 
possibly out-dated methodology is very likely to be favoured over a totally 
unknown but potentially more accurate method, because it has been widely 
published and has a proven track record. 
Figure 5.4 presents the raw citation scores for both tasks. C/ustal V and 
Neighbour-Joining achieve the highest scores. Bayesian tree-inference and 
the Muscle alignment tool both were given the lowest score for their task. 
88 http://scholar.google.co.uk 
Q) ... 
0 0 u 
u c -Q) 
Q) ... 
Multiple Sequence Alignment 
Clustal W/X 
Neighbour Maximum 
Joining Parsimony 
T -Coffee Muscle 
Method 
Tree Inference 
Weighted 
Parsimony 
Maximum 
Likelihood 
Method 
Bayesian 
Clustal V 
UPGMA 
Figure 5.4 - Citation metric scores. Bar chart of citation metric scores (see Methods) 
for tree-Inference and sequence alignment methods. 
5.3.2. Combination and normalisation of metric scores 
The main difficulty with the raw metric scores in Figures 5.1-5.4 is that they 
are not directly comparable with the other. We need to be able to weight each 
one equally with respect to the other. We have made the metric scores 
comparable by transforming the values into the range 0.0 - 1.0 and summing 
the individual metric scores to derive an overall score (Figure 5.5). 
~ ll' 
t= L() 
Multiple Sequence Alignment 
Clustal WIX T-Cotfee Muscle 
Method 
Tree Inference 
Neighbour Maximum Weighted Maximum 
Joining Parsimony Parsimony Likelihood 
Method 
Clustal V 
C::=::JI Citation 
C::;Et] Expertness 
 _ Recentness 
__ Popularity 
Bayesian UPGMA 
Figure 5.5 - Transformed and combined metric scores. Stacked bar chart of all four 
transformed metric scores (see Methods) for tree-Inference and sequence alignment 
methods. 
The tree-inference method with the highest overall score is Bayesian (2.06); it 
has a high score for recentness (1.0) and expertness (1.0). Neighbour-Joining 
has an almost identically high score to Bayesian (2.05). The high overall 
score for Neighbour-Joining comes from the citation (1.0) and popularity (1.0) 
metrics, which is in direct contrast with the individual high scores for Bayesian 
(recentness and expertness). This result highlights the importance of the user 
in metric analysis; an established phylogeneticist is most likely to favour 
Bayesian for its recentness and expertness scores, however a new 
practitioner may prefer Neighbour-Joining with its high popularity and citation 
scores. We have deliberately weighted the metric scores equally to provide 
an overview of the results, but they are likely to be significantly more useful 
when weighted according to user requirements. The highest-scoring multiple 
sequence alignment tool is Clustal W/X (2.18), which has the highest 
popularity score (1.0) of the group and a good score for recentness (0.76). 
The scores for each method vary extensively between different metrics, with, 
for example Neighbour-Joining having the highest popularity value (1.0), but 
also an expertness score of O. Very few methods do well in all metrics. 
Maximum-Likelihood, performs the best across metric calculations, but its 
overall score (1.83) is still lower than Bayesian. 
5.3.3. Relationships between metric scores 
We assessed the relationships between the different metrics using a pairwise 
product-moment correlation analysis (Fowler, Cohen et 81. 1998) between all 
combinations of metric pairs across all tree-inference and multiple sequence 
alignment methods. We calculated correlations for both the untransformed 
(Figures 5.1, 5.2, 5.3, 5.4) and transformed metric scores (Figure 5.5). The 
correlation results for the untransformed metric scores are presented in Table 
5.1 and the transformed scores are in Table 5.2. We also tested the statistical 
significance (n=1 0, df=8) of each correlation value in Tables 5.1 and 5.2 using 
the alternative hypothesis that the true correlation is not equal to O. Only one 
correlation was found to be significant at the 5% (p<0.05) level; this was the 
strongly negative correlation between the transformed scores for recentness 
and citation. 
Popularity 
Recentness 
Expertness 
Citation 
Popularity 
-0.183 
-0.374 
0.102 
Recentness 
0.565 
-0.399 
Expertness 
-0.485 
Citation 
Table 5.1 - Correlation coefficient values between all pairs of metric scores using 
untransformed data. Values are calculated from all metric scores from all multiple 
sequence alignment and tree-inference methods (n=10). These values were calculated 
using the untransformed data presented in Figures 5.1, 5.2, 5.3 and 5.4. 
Popularity 
Recentness 
Expertness 
Citation 
Popularity 
-0.197 
-0.456 
Recentness 
-0.965 
Expertness Citation 
-0.522 1 
Table 5.2 - Correlation coefficient values between all pairs of metric scores using 
transformed data. Values are calculated from all metric scores from all multiple 
sequence alignment and tree-inference methods (n=10). These values were calculated 
using the transformed data presented in Figure 5.5. 
The results of the correlation analysis (Tables 1 and 2) only identify a single 
strong relationship between the individual metric scores; this is between the 
recentness and citation scores in the transformed data and it is strongly 
negative. Recentness and citation are likely to be negatively correlated 
because of the increasing accumulation of citations over time after publication 
(Adams 2005; Vanclay 2009). So, if a method has a low recentness score 
and therefore is still increasing in popularity, it is likely to have received fewer 
citations than a method that is well known in a field and therefore has a low 
recentness score. 
All but one of the pairwise metric comparisons were not significantly 
correlated at the 5% level; this provides good evidence that each of the 
metrics is measuring different things. It should, ' however, be noted that 
several of the metrics were found to have weak or moderate (coefficient 
values in the range 0.2 - 0.69 (Fowler, Cohen et al. 1998)) positive and 
negative correlations (Tables 5.1 and 5.2); this indicates that it is possible, 
with a larger data-set, that a greater number of significant correlations may be 
found. However, the strength of a correlation should not be confused with its 
statistical significance (i.e., its propensity to be the result of random chance or 
a real correlation) and a larger data-set (or a high correlation coefficient) does 
not guarantee a greater number of statistically significant correlations (Fowler, 
Cohen et al. 1998). 
5.3.4. Use of metrics (a 'real world' example) 
Scientific protocols and computational workflows are now available in many 
forms, from traditional journal article publications (e.g., Nature Methods89; 
SioTechniques90) to electronic forms (myExperiment91 ; Nature Protocols92; 
Protocol online93; OpenWetWare94 and MoIMeth95). The easy availability of 
these protocols promotes sharing and reuse, and also enables automated 
analysis of the protocol descriptions. 
We will use our proposed metric calculations to make suggestions for 
improvements to an existing phylogenetic analysis protocol, and we will also 
derive further information on the quality of the methods that are currently 
implemented. We take our example from the myExperiment virtual research 
environment, which currently contains 9 workflows that perform phylogenetic 
analyses. If we consider the workflow titled 'ES,-ClustaIW_alignment_tree,96, 
it currently makes use of Clustal W for multiple sequence alignment, and 
infers phylogenetic trees using the Neighbour-Joining method (which is also 
89 http://www.nature.comlnmethlindex.html 
90 http://www.biotechniques.coml 
91 http://www.myexperiment.org/ 
92 http://www.nature.comlnprotlindex.html 
93 http://www.protocol-online.org/ 
94 http://openwetware.org/wikilMain _Page 
95 http://www.molrneth.org/ 
96 http://www:myexperiment.org/workflows/207 
implemented by the Clustal W executable). Based on our metric data, we can 
recommend using an implementation of Bayesian (Figure 5.5) phylogenetic 
tree-inference (such as MrBayes (Ronquist and Huelsenbeck 2003) or BEAST 
(Drummond and Rambaut 2007)). Furthermore, using our metric data, we 
can also provide justification for the original choice of ClustalW and 
Neighbour-Joining: both represent the most popular choice (Popularity score 
of 1.0 for both), which increases the likelihood that the output of this workflow 
will be directly comparable with results from other phylogenetic analyses. 
ClustalW_a1igrvnent 
Task: Multiple sequence alignment 
hoice: ClustalW 
Task: Tree-inference 
hoice: Nei hbour-Joinin 
Figure 5.6 - Analysis of the methods used In an example myExperlment phylogenetic 
workflow. This figure shows an abstract representation of the main phylogenetic 
analysis steps performed In a Taverna workflow. The first task Is multiple sequence 
alignment and this Is performed by a ClustalW Web Service. The second task Is tree-
Inference and this Is performed using a Web Service that Implements the Nelghbour-
Joining algorithm. . This diagram is derived from the Taverna visualisation of 
myExperlment workflow number 207 (http://www.myexperiment .org/workflows/207). 
5.4. Discussion 
An important property of our proposed metrics is that they are based on 
information from whole communities of research rather than from one or a few 
key authority figures. Our intention with this is to deliberately capture 
information from as much literature as is achievable, and then combine, rank 
and weight this information to derive a set of metric scores. Recent interest in 
group intelligence (Ward, Sumpter et al. 2008) and the 'Wisdom of the Crowd' 
(Surowiecki and Silverman 2007; Petsko 2008) has highlighted the utility of 
considering opinion from whole communities rather than a single authority. 
Additionally, our previous work has highlighted a methodologically divided 
community structure in the field of molecular phylogenetics (Eales, Pinney et 
al. 2008). This is a problem that may be common in other fields, can hamper 
exchange of good quality methodologies in phylogenetics, and could be 
addressed by information drawn from a wider 'crowd' of phylogeneticists. It 
should be noted that the proposed metrics are not supposed to be a definitive 
set of methods for defining methodological quality. Instead, we are exploring 
novel ways of scoring research quality, and have deliberately chosen a small 
number of the most clear and obvious examples of quality metrics. 
We envisage these metrics being useful for researchers to explore and define 
methods relevant to their work. For example, search results from UK PubMed 
Central (UKPMC97) could be augmented with a methodological score, 
highlighting the properties of the methods used. The results could then be 
sorted by methodological recentness to highlight cutting-edge research. We 
also see this approach being useful in the communication of methods 
between authors, by applying automated methods to build a structured 
representation of the protocol. Each element of the protocol could then be 
linked to an ontology of protocol-specific components (Soldatova, Clare et al. 
2006; Soldatova and King 2006; Soldatova, Aubrey et al. 2008). This would 
make semantic querying of the literature (e.g., Textpresso (Mueller, Kenny et 
97 http://ukpmc.ac.uk/ 
al. 2004; Mueller, Rangarajan et al. 2008)) by methods both possible and of 
greater utility. Textpresso (Mueller, Kenny et al. 2004) is a literature search-
engine, which can be tailored to the literature pertinent to a particular 
organism. Currently, Textpresso allows literature searches using terms from 
semantic categories, and one of these includes method terms; however, the 
terms are not described in an ontology, and they are not uniquely identified, 
therefore limiting the utility of these search results for analysis outside of 
T extpresso. 
To effectively link methods with research literature, it would be extremely 
useful to have a model for deriving the broad aims of an experiment. Current 
research into the formal representation of experiments (Soldatova, Clare et al. 
2006; Soldatova and King 2006; Soldatova, Aubrey et al. 2008) is extremely 
promising, but currently lacks a means of automatically generating an 
ontological representation of an experiment. Some aims can, in part, be 
inferred from the methods used; but this is not a practicable solution for 
automated analysis. The utility of knowing the aims of an experiment is that 
we can use this to group and compare different approaches (i.e., the methods 
used) to address similar questions. This seems a simple solution to the 
opposing problems of academic specialisation and analytical generality. 
Another requirement for this entire approach is access to the full-text of an 
article. This is because the methods used in an experiment are only rarely 
declared in abstract text. Currently, UKPMC98 and PubMed Central (PMC99) 
provide the largest repositories of full-text open-access articles, all of which 
are in a standardised format that is suited to automated dissemination and 
analysis. There are, however, still many journals that do not archive their 
articles, or do not make their archived material available to download from 
these publicly accessible repositories. A reasonable approach would be to 
begin the process of analysis by only processing the open-access literature, 
and then later attempt to incorporate articles from other sources, if the owners 
98 http://ukpmc.ac.uk/ 
99 http://www . pubmedcentral.nih.gov / 
gave their consent. The full-text is required for processing, but it would not be 
necessary to disclose any part of the full-text after this point, only data that 
has been derived from it. The Textpresso (Mueller, Kenny et al. 2004) suite of 
resources, as well as Google Scholar10o , have taken a similar approach to 
this, whereby they only provide a small amount of the copyrighted article text 
for each search result (Dickman 2003), and also provide a link to the 
publisher's site. 
A further development of the method-centric approach is an automated 
methodology suggestion service. The requirements for this service are, as 
mentioned previously, the ability to derive the aims of a study and adequate 
metric data. There is also a need for technical information on the experiment: 
is it being performed in a test tube, on a laptop, or under a microscope? For 
example, the myExperiment (Figure 5.6) workflow is very likely to be run 
remotely, and therefore a suggestion of a GUI application is inappropriate, 
because it requires user input through the GUI. This highlights the importance 
of capturing contextual information about an experiment, and fitting method 
suggestions to it. With this added information, the methodology suggestion 
service could then be used to provide alternative methodologies to a given 
aim, or for exploring the alternative aims approached using a given 
methodology. It is common for users of a new method or protocol to want 
other examples of its use in the literature. This service could also provide for 
that need. 
5.5. Methods 
5.5.1. Derivation of metric scores 
All of the metric calculations produce values on different scales and within 
different intervals; to weight each of the metrics equally with respect to each 
other, we interpolate the values linearly to fit them to the interval (0.0-1.0), this 
involves a linear transformation to fit the data into a range of 1, and a 
100 http://scholar.google.co. uk 
translation to move the data points between 0 and 1. The smallest value is 
always set to zero and the highest value is always set to 1. Therefore, the 
transformed metric values are only directly comparable within a group of 
terms. 
5.5.2. Distribution of expertness scores 
Expertness is a measure of publishing activity in the field. For each of the 
articles (in the data-set) published by a single author, a score of 1 / n is 
attributed to the author, where n is equal to the number of authors listed on 
the article. 
The distribution of all author-expertness proportions superficially appears to 
follow a form of exponential distribution. Subsequent natural log 
transformation of the values, however, does not fit a normal distribution with 
the same mean and standard deviation (Figure 5.7). The log-transformed 
distribution contained multiple ties, and therefore direct tests for normality 
(e.g., the Anderson Darling test) were not appropriate. The distribution was, 
however, found to have a skewness value that was statistically significantly 
different from that expected of normally distributed data (D'Agostini test, n = 
53,411, skew = 0.821, z = 34.267, P < 2.2x10-16), and the kurtosis was also 
found to differ significantly (Anscombe-Glynn test, n = 53,411, kurt = 4.4017, z 
= 28.1714, p-value < 2.2x10-16). The mean, which is highly sensitive to 
asymmetry in extreme values, will therefore be a poor estimator of an average 
value of this distribution; this is why we use the median author expertness for 
deriving overall method expertness. 
...:t 
-4 -2 o 2 
Logged expenness score 
Figure 5.7 - Histogram of expertness scores (log-transformed). Blue line denotes 
density curve for normal distribution with mean=-1.435 and sd=O.797. 
5.5.3. Distribution of citation scores 
The distribution of citation proportions again appears to follow a form of 
exponential distribution (Figure 5.8). Once log transformed, the kurtosis of the 
distribution departs significantly from that expected of normally distributed 
data (Anscombe-Glynn, n = 5244, kurt = 2.8276, Z = -2.7888, p-value = 
0.00529). Therefore, (as with expertness) the median citation proportion is 
used instead of the mean. 
<1l 0 
-2 0 2 
Logged cltaUon score 
Figure 5.8 - Histogram of citation scores (log-transformed). Blue line denotes density 
curve for normal distribution with mean=1.705 and sd=1.325. 
5.6. References 
Adams (2005). "Early citation counts correlate with accumulated impact." 
Scientometrics 63(3): 567-581. 
Dickman, S. (2003). "Tough mining: the challenges of searching the scientific 
literature." PLoS Biology 1(2): E48. 
Drummond, 'A. J. and A. Rambaut (2007). "BEAST: Bayesian evolutionary 
analysis by sampling trees." BMC Evolutionary Biology 7: 214. 
Eales, J. M., J. W. Pinney, R. D. Stevens and D. L. Robertson (2008). 
"Methodology capture: discriminating between the "best" and the rest 
of community practice." BMC Bioinformatics 9: 359. 
Fowler, J., L. Cohen and P. Jarvis (1998). Practical statistics for field biology. 
Chichester, Wiley. 
Harnad, S. (2008). "Validating research performance metrics against peer 
rankings." Ethics in Science and Environmental Politics. 
Harnad, S. (2009). "Open access scientometrics and the UK Research 
Assessment Exercise." Scientometrics 79(1): 147-156. 
Havemann, F., M. Heinz and R. Wagner-Dobler (2005). "Firm-like behavior of 
journals? Scaling properties of their output and impact growth 
dynamics." Journal of the American Society for Information Science 
and Technology. 
Jefferson, T., P. Alderson, E. Wager and F. Davidoff (2002). "Effects of 
editorial peer review: a systematic review." JAMA : the journal of the 
American Medical Association 287(21): 2784-6. 
Kostoff, R. N. (2002). "Overcoming specialization." Bioscience 52(10): 937-
Lehmann, S., A. D. Jackson and B. E. Lautrup (2006). "Measures for 
measures." Nature 444(7122): 1003-4. 
Merton, R. K. (1968). "The Matthew effect in science. The reward and 
communication systems of science are considered." Science 159(810): 
56-63. 
Merton, R. K. (1988). "The Matthew Effect in Science, II: Cumulative 
Advantage and the Symbolism of Intellectual Property." Isis 79(4): 606-
Mueller, H. M., E. E. Kenny and P. W. Sternberg (2004). "Textpresso: An 
ontology-based information retrieval and extraction system for 
biological literature." PLoS Biology 2(11). 
Mueller, H. M., A. Rangarajan, T. K. Teal and P. W. Sternberg (2008). 
"Textpresso for Neuroscience: Searching the Full Text of Thousands of 
Neuroscience Research Papers." Neuroinformatics: 1-10. 
Petsko, G. A. (2008). "The wisdom, and madness, of crowds." Genome 
Biology 9(11): 112. 
Ronquist, F. and J. P. Huelsenbeck (2003). "MrBayes 3: Bayesian 
'phylogenetic inference under mixed models." Bioinformatics 19(12): 
1572-4. 
Soldatova, L. N., W. Aubrey, R. D. King and A. Clare (2008). "The EXACT 
description of biomedical protocols." Bioinformatics 24(13): i295-i303. 
Soldatova, L. N., A. Clare, A. Sparkes and R. D. King (2006). "An ontology for 
a Robot Scientist." Bioinformatics 22(14): e464-e471. 
Soldatova, L. N. and R. D. King (2006). "An ontology of scientific 
experiments." Journal of the Royal Society Interface 3(11): 795-803. 
Sun, T. T. (2004). "Excessive trust in authorities and its influence on 
experimental design." Nature Reviews Molecular Cell Biology 5(7): 
577-81. 
Surowiecki, J. and M. P. Silverman (2007). "The Wisdom of Crowds." 
American Journal of Physics. 
Thompson, J. D., T. J. Gibson, F. Plewniak, F. Jeanmougin and D. G. Higgins 
(1997). "The CLUSTAL_X windows interface: flexible strategies for 
multiple sequence alignment aided by quality analysis tools." Nucleic 
Acids Research 25(24): 4876-82. 
Thompson, J. D., D. G. Higgins and T. J. Gibson (1994). "CLUSTAL W: 
improving the sensitivity of progressive multiple sequence alignment 
through sequence weighting, position-specific gap penalties and weight 
matrix choice." Nucleic Acids Research 22(22): 4673-80. 
Vanclay (2009). "Bias in the journal impact factor." Scientometrics 78(1): 3-12. 
Wager, E. and T. Jefferson (2001). "Shortcomings of peer review in 
biomedical journals." Learned Publishing 14: 257-263. 
Ward, A. J., D. J. Sumpter, I. D. Couzin, P. J. Hart and J. Krause (2008). 
"Quorum decision-making facilitates information transfer in fish shoals." 
Proceedings of the National Academy of Sciences of the United States 
of America 1 05( 19): 6948-53. 
Weiner, B. K., J. P. Weiner and H. E. Smith (2010). "Spine journals: is 
reviewer agreement on publication recommendations greater than 
would be expected by chance?" The Spine Journal 10(3): 209-11. 
Worobey, M., A. Rambaut, O. G. Pybus and Robertson (2002). "Questioning 
the evidence for genetic recombination in the 1918 "Spanish flu" virus." 
Science 296(5566): 211 discussion 211. 
6. Discussion 
Biologists have become increasingly specialised in their knowledge 101 (Kostoff 
2002), and have begun to exist in research communities with highly 
specialised literatures (Kostoff 2002). At the same time, however, through 
encouragement by the research councils and funding bodies, and also 
because it is usually intellectually advantageous, interdisciplinarity has 
become more popular (Aboelela, Larson et al. 2007). So, as in chapter 4, we 
have a situation where there are a lot of people working on very similar things 
using highly disparate methodologies, whilst other groups of researchers use 
exactly the same methodology as each other. As scientists, we need to 
provide evidence to support our decisions, and we also need to declare all our 
decisions. A further consideration is that others will judge the reliability of our 
results, based on how reliable they consider our methods to be. This creates 
a problem, whereby highly specialised methodologies are communicated to. 
other researchers, who do not possess the specialised knowledge necessary 
to critically appraise them. 
The key targets of this thesis were to try to alleviate some of these problems 
in the discipline of phylogenetics (see chapter 1) by capturing and identifying 
good quality methodological practice. There was also work aiming to 
investigate whether measures of quality derived from the literature could be 
informative. Further work was performed to enable greater access to the pool 
of scientific literature to which all biologists contribute. Scientists all contribute 
so much time and effort to publishing their work, and yet most of the 
information disseminated in this way is not available to automated methods 
(Hull, Pettifer et al. 2008) or to other researchers who do not work in the field 
(Kostoff 2002). 
101 http://blogs.bbsrc.ac.uklindex.phpI2009/03/the-matthew-effect-in-sciencel 
Identifying the correct components to use in a phylogenetic analysis is a 
complex task. There is not a single solution, or even a small number of 
different ways, to perform a phylo-analysis; instead, it seemed there are many 
ways to do the same thing, with no consensus of opinion on which to choose. 
During a preliminary manual literature survey, it was found that almost all 
analyses made use of different components, and that very few articles 
declared why they had chosen to use these components. It was quickly 
realised that a methodology for surveying methodologies would have to be 
developed, and that this could be used to provide supporting evidence for why 
any chosen methodology was the .most appropriate. This led to the further 
inference that the information captured in the survey (chapter 4) would be 
useful to other researchers, and that it would be advantageous for others to 
be able to provide supporting evidence for their individual choice of 
methodology. 
In order to begin to identify methodologies, and then later to assign quality 
scores to them, a set of key infrastructural services to support this work, had 
to be developed. The world of electronic journal literature is mainly dominated 
by the publishers who disseminate the content of their journals. Even though 
publishers would not exist without the contributions of scientists, it is 
surprisingly difficult for scientists to gain access to the content they produce 
(Cohen and Hersh 2005; Zweigenbaum, Demner-Fushman ef al. 2007; 
Altman, Bergman ef al. 2008). This is further complicated when automatic 
access to large numbers of articles is required (see chapters 1 and 2). 
There is a notable disparity in the literature surrounding text-mining of full-text 
articles: many authors (e.g., Cohen and Hersh 2005; Zweigenbaum, Demner-
Fushman ef al. 2007; Altman, Bergman ef al. 2008) call for easier access to 
full-text, and define the problems currently encountered, yet only a few 
academic projects are attempting to develop solutions to these problems 
(Corney, Buxton et al. 2004; Natarajan, Haines et al. 2006). The first problem 
is that of identifying and downloading your selection of articles. Quosa was 
employed in chapter 4, but it was awkward to use and turned the process of 
downloading articles into a lengthy task. Quosa limits you to the download of 
500 articles per search, so searches had to be split up by date (see chapter 4 
methods). This was the original motivation for the development of software in 
chapter 2. It seemed surprising that such a large information resource as the 
journal literature was relatively unavailable, especially in comparison to some 
information resources in bioinformatics, such as the Ensembl genome 
databases 102 and almost all the NCB I databases 103, which have a human 
interface, as well as one geared toward automated methods 104,105 (Smedley, 
Haider et al. 2009), and one for bulk data downloads 106, 107. 
To address the problem of access, a software agent and associated storage 
and search software elements were created that could be used to automate 
the download of a large number of full-text articles. The main basis of this 
software was to mimic the behaviour of a human attempting to access fUll-text 
articles, but in a way that was far less prone to human error and did not 
require needless repetitive human behaviour. The software was used to 
download some of the articles used in chapter 4, and whole collections of 
articles for other colleagues (Afzal, Stevens et al. 2008; Stevens, Jupp et al. 
2008). The software source code and documentation have been made freely 
available to download from an open-source project-hosting site (see chapter 
2). The software was purposely designed to be tuneable to needs of other 
users. For example, it can be reconfigured to download only JPEG images or 
HTML pages about a specific topic. Users of the software can also improve 
102 http://www.ensembl.org/index.html 
103 http://www.ncbi.nlm.nih.gov/Entrez/ 
104 http://www.ncbi.nlm.nih.gov/entrez/query/static/eutils_help.html 
105 http://www.biomart.orglbiomartlmartview/ 
106 http://www.ncbi.nlm.nih.govlFtp/ 
107 http://www.ensembl.org/info/datalftp/index.html 
on the way in which the agent locates fUll-text articles, by tuning the set of 
rules that it follows. It is hoped that this software can help others to perform 
their text-mining studies, and that it encourages others to ask more complex 
research questions of the literature. 
FUll-text articles can be retrieved in 3 electronic forms: XML, HTML and PDF 
(see chapter 2 and chapter 3 for discussion on availability of these forms). 
PDF is by far the most commonly available form, but it is also the most difficult 
to analyse. The plain-text that can be derived from a PDF is largely 
unstructured, and this removes a layer of detail that can be very useful to the 
analysis of full-text, that of article sections. Given that the aim of this work 
was to analyse experimental methods, and in most research articles there is a 
section devoted to this topic, it was necessary to be able to infer the original 
structure of an article from its plain-text content. Recent work (Pettifer, 
Thorne et al. 2009) on the semantic enhancement of PDF research articles 
has made it possible to analyse the text, article sections, figures, tables and 
other article elements without having to convert to an intermediate file format. 
Given the full development and release of this software, plain-text conversion 
and restructuring may no longer be a necessary process. 
The need for article section structure was further supported by preliminary 
work, where it was found to be common to identify matches to methods 
outside the Methods section, simply because some method names are similar 
to words in common usage (e.g., "MUSCLE", "muscle"). In chapter 4, this 
problem was countered by making our phylogenetic method-matching 
process very specific, and by removing some methods that had ambiguous 
names (e.g., the software tool "network"). This was the motivation behind 
developing the article-section classifier (chapter 3); it also became apparent 
that certain other research projects could benefit from the software as well 
(Muller, Kenny et al. 2004; Hearst, Divoli et al. 2007). 
The article-section classifier was developed in several different forms, after 
interest from other research groups. These include a Web service version, a 
high-performance memory-based version, and a version that made use of a 
database to store the classifier training data. The main version referred to in 
chapter 3 is the high-performance form; this is the version most suited to local 
installation by other researchers, because it takes a short while to start up but 
then is able to classify text very quickly. The database version is more suited 
to moderate intensity usage; it uses minimal compute resources, but also 
does not classify text as fast as the memory-based version. The Web service 
version uses the database version with an added simple object access . 
protocol (SOAP) interface. The SOAP interface allows it to be used in 
Taverna and other Web service client software that follow the SOAP standard. 
Again, the software source code and documentation, as well as the classifier 
training data, have been made available to download for others to use in their 
research. 
The classifier relies on the large number of open-access full-text articles 
currently deposited at PubMed Central (PMC108). These highly-structured 
articles were used to train the classifier automatically, and to a high level of 
accuracy. This is a further justification of the importance of open-access to 
scientific literature. It can enable the development of new kinds of technology 
that were previously impossible or very difficult to create. It also provides the 
raw material to text-mining studies (i.e., text) in a highly structured and easily 
used format. PMC content is now being used in an increasing number of 
research projects. Notable examples of these are the BioText search engine 
(Hearst, Divoli et a/. 2007), the Yale Image Finder (Xu, McCusker et a/. 2008) 
and the BioLit project (Fink, Kushch et a/. 2008), all of which make full use of 
this important resource. 
108 http://www.pubmedcentral.nih.gov/ 
The main aim of the two software development projects (chapters 2 and 3) 
was to enable better access to the literature, and to make text-mining of full-
text journal articles simpler and less restrictive. This has been achieved by 
making it possible to collect PDF articles accurately, and in large numbers, 
and by providing software to reconstruct the structure of an article from plain-
text. It is hoped that the expansion of full-text mining will be enabled by 
making this software freely available to others. 
There were two main research questions posed in chapter 4: how do we 
choose which methods to use in a 'best' practice phylogenetic analysis? And, 
to address this first question, can we capture descriptions of protocols 
automatically from research articles? We wanted to be able to canvas opinion 
on phylogenetiC practice from a wide set of people. So rather than asking 
colleagues, or reading a collection of papers, we analysed the literature. 
The protocol-capture methods we used were manually created, and therefore 
time-consuming to develop, but they were also accurate, giving similar 
. precision and recall scores and an F-measure of 87.7% (see chapter 4 for 
definitions and data). This compares favourably with results from a similar 
text-mining task in the Biocreative " protein-protein interaction extraction 
challenge (Krallinger, Leitner et al. 2008). The Biocreative task required 
participants to determine the experimental method that had been used to 
generate a reported protein-protein interaction. A controlled vocabulary of 
method terms was provided, and only one had to be chosen, and yet the 
highest reported F-measure was 45%. Clearly, these results are not directly 
comparable (the tasks are different), but no other comparative studies are 
available. It does at least provide some evidence that concerted manual effort 
in the construction of a structured controlled vocabulary brings good results in 
the capture of phylogenetic protocol descriptions. Our approach to the 
capture of phylogenetic protocols will also be useful in other fields of research 
as long as the number of different methods is not too great to prohibit manual 
construction of a controlled vocabulary. Other difficulties could include 
methods that are referred to ambiguously (e.g., the phylogenetic tool known 
as 'network' (Sandelt, Forster ef al. 1999)) or those that are referred to using a 
very large range of names. 
We found an interesting mixture of methodological practice in our study of 
phylogenetic protocols. Roughly two thirds (62%) of the articles used one of 
the ten most popular protocols, but this left another 851 unique protocols that 
had, on average, only been used 7 times. A lot of the variability is likely to 
result from methodological reporting standards differing between journals and 
also between authors. There are also temporal and motivational 
considerations, such that method preferences change through time (we also 
found evidence for this), and that a large group of researchers are likely to be 
investigating a large group of different research problems, and this will lead to 
a large range of different methodologies being used. 
Research fields were found to significantly influence practice in chapter 4. 
The most obvious explanation for this is that different fields do different kinds 
of analyses, and therefore use different methods. However, we did find 
evidence for an increasingly significant methodological divergence (inferred 
by network assortativity, see chapter 4, Methods section) between 
evolutionary biology and the other fields in comparison to other pairwise field 
similarities (see chapter 4, Figure 4.3). The divergence between microbiology 
and virology was within the 95% confidence interval (CI) for the majority of 
years, and, when outside the CI, the assortativity coefficient only varied 
between -0.04 and 0.04, indicating very slight disassortativity to very slight 
assortativity (Newman 2003). However, from 2000 to 2005, we found that 
evolutionary biology had positive assortativity values (from 0.08 to 0.35) 
outside of the 95% CI when compared with microbiology and virology; this 
indicates moderate assortativity between evolutionary biology and the other 
two fields, and that this assortativity has increased through time (Figure 4.3). 
More work, analysing why this was the case, is necessary before we can fully 
understand how often methodologies are communicated between fields. We 
certainly found evidence for some protocols never being used outside one 
field, and that in their first year of usage, protocols are not often used outside 
a single field. Whether this observation is a product of poor communication of 
protocols or differences in research aims, or is actually what is to be expected 
in all research disciplines, remains an open question. 
Our final recommendation for 'best' practice was based on the practice of the 
evolutionary biology field. We chose this field for several reasons: first, new 
protocols commonly arose in that field; second, it had departed 
methodologically from the other fields thorough time; and finally, the newest 
methods were more common in that field than the others. Clearly, there are 
caveats to this approach, but without further information, it was the most 
reasonable choice. 
The investigation of 'expert' practice did not lead to any greater insight into 
what could be considered an exemplar of 'best' practice. Clearly, doing 
'expert' research does not involve performing a uniquely 'expert' analysis. 
However, it may be that well-published authors are defining the 
methodological practice of their field indirectly (hence the similarity between 
'expert' and other practice). This could be owing to their status as prominent 
members of the research community and therefore attracting a kind of 
'herding' behaviour. We cannot conclude this definitively from our current 
results, but it could be investigated with a more fine-grained analysis of the 
literature. When exploring the influences of field and authority on practice, it 
became clear that choices of methods are based on a lot of different pieces of 
information. This is what led to our further work defining metrics for methods. 
In chapter 5, the work on developing quality metrics to score experimental 
methodologies and their component elements is described. The metrics allow 
scientists to make choices based on more than just citations. They allow us 
to specify what we want in our methods: if we want something that is well 
known and trusted, then we choose a method with high popularity or high 
citation scores; if we want to perform a cutting-edge analysis, then we can 
choose a high expertness or recentness method. The metric calculations also 
improve on the current article-centric approach of citation-based metrics, and 
allow us to choose methods independent of article boundaries, by analysing 
the use of methods across the literature as a whole. This should help to 
counter the effects of the Matthew effect109 (Merton 1968; Merton 1988; 
Havemann, Heinz et al. 2005). It is important to stress that metric scores 
should only be used to inform methodology design decisions, rather than 
making them directly. This is because the specifics of the users' needs are 
unknown. 
6.1. Future work 
Given time for future development of the article download software (chapter 
2), it could be improved with a better user interface. Ideally, the new user 
interface should be simpler to use and should allow better control of where 
articles are to be stored. Also the link-scoring process could be improved with 
machine-learning methods, where the software would repeatedly prompt the 
user to choose the correct link from a list (a simple task), and would then train 
a data model that could later be used in automatic classification of links. This 
training mode could be turned on and off by a user based on whether the 
software was performing according to their needs or not. 
Further development of the article section classification software (chapter 3) 
would include the testing of a wider-range of machine-learning methods to 
classify the text. The naive Bayesian approach was used because it is 
simple, fast and accurate, and has been proven to work well in many 
situations. However, greater accuracy is likely with other methods, in 
particular, Support Vector Machines (SVMs). 
109 http://blogs. bbsrc. ac. uk/index. php/2009 /03/the-matthew-effect -in-science/ 
The work in chapter 3 has also spawned a second research project that also 
makes use of the PMC article colle~tion. The new project is attempting to 
extract all parts of an article (not just the text sections), including Tables of 
data, images and citations. The proposal is then to extract all citations 
between all articles in the PMC collection and link them with the part of the 
article (e.g., Figure 1 or the Methods section) they are citing. Recent work 
has shown that citations can be classified into groups (Teufel, Siddharthan et 
al. 2006): some are citations of data, others refer to methodologies, and some 
are criticising the cited work. The Citation Typing Ontology (CiTO) also 
provides an ontological framework for describing negative, positive and 
neutral citations between articles (Shotton 2010). This information could be 
used to build a semantic network of citations, whereby, a citation not only links 
two articles, but also the parts of the articles (e.g., figures, tables, results 
section) that are linked and the kind of the citation link (e.g., reference to 
result, criticism of approach) is defined. This newly derived information could 
then be disseminated as an independent data layer that could either be 
overlaid onto a publication when it is viewed on the Web or used in citation 
network analyses. Similar overlaid data layers are being used by the PLoS to 
display reader comments110 on its articles (e.g., Langille and Eisen 2010). 
The proposed data layer could then be used to tell readers about how many 
and what kind of citations (from other articles) refer to the article they are 
reading. Again, it is the high quality, structured nature of the PMC collection 
that makes this possible. However, recent technical improvements have 
made automated citation identification from free text also possible (Councill, 
Giles et al. 2008), so the proposed project could potentially be extended to 
analysis of plain-text articles derived from PDF files. 
The work from chapter 4 would certainly benefit from being able to capture 
more on the context of a given analysis (e.g., species studied, what is the aim 
110 http://www.plosone.org/static/commentGuidelines.action 
of the analysis). Currently this kind of information is hard to capture, because 
it requires a detailed description of what information we are seeking to capture 
as well as the development of new methods to capture this information. 
However, the ability to know the full context behind an analysis would be 
invaluable for exploring the protocol usage data in chapter 4, because it would 
then be possible to separate genuine methodological adaptation in a field 
from changes in experimental aims. 
A more fine-grained analysis of where new phylogenetics protocols first 
appear could help to resolve how innovation in phylogenetic research practice 
occurs. The work in chapter 4 began this process by exploring how protocols 
were used in their first and later years in the three different fields of research. 
However, this does not explore which fields or sub-fields new protocols first 
appear in. Future work should also explore the influence of a field-specific 
community on dissemination of protocols within that community. For 
example, given a known co-authorship network, does the spread of a new 
protocol follow the network structure or is it independent of publication 
practices. 
Many possibilities for improvement have been raised from the work of chapter 
5. First, the application of the metrics to another domain: phylogenetics is a 
good test case, but it is necessary to work on another domain to see if the 
metric scores still give results that differentiate between the different choices. 
Second, the expansion of the range of different phylogenetic methods and 
software that can be compared would be methodologically beneficial. For 
example, a comparison of tree-visualisation tools would be informative for the 
community. The methods studied in chapter 5 did represent all the major 
choices in the field, but some methods were not included, either because they 
were not used frequently enough to calculate the scores, or because it was 
difficult to identify their use in the article text, or because their use was 
ambiguous. Third, ideally the analysis should be re-run on a larger number of 
articles: this is for statistical reasons that would improve the stability of the 
metric scores, and also to get a more comprehensive picture of the use of the 
methods in the literature. 
6.2. Conclusions 
This work has generated a set of major results; some of these have wider 
implications for the research community. Each result and its significance for 
the wider community is listed below. 
1. It is possible to automatically capture 88% of a large collection of 
scientific articles, in PDF form, from the Web. 
Access to full-text literature (Cohen and Hersh 2005; Zweigenbaum, Demner-
Fushman et al. 2007; Altman, Bergman et al. 2008) is commonly described as 
a limitation on text-mining studies (see chapters 2,3,4). This result provides 
evidence that it is possible to collect large numbers of full-text articles from the 
Web. The software that enables this is potentially useful to researchers in the 
field of text-mining who want to capture articles about a particular subject. For 
example, if task- or domain-specific software is required (e.g., software to 
identify reagents used in molecular biology experiments), then this would 
require an equally specific corpus of documents for annotation and training 
purposes. Furthermore, this software would also be useful to 
bioinformaticians looking to collect literature on a specific subject: for example 
FlyBase 111 has an extensive literature component; the collection of this 
literature could be automated by the software. The Textpresso software can 
also be downloaded for individuals or research groups to create their own 
custom installations; this could then be populated with literature collected by 
the software from chapter 2. The PDF retrieval rate of 88% is also 
encouraging, and may, in some cases, be all that is required for the creation 
of a literature resource (as opposed to manual download of the remaining 
12%). 
III http://flybase.org/ 
2. The text of open-access scientific articles can be split and classified 
into sections (Introduction, Methods, Results, Discussion) with an 
accuracy of 85.2%. 
This result links in with the current rise in popularity of semantic-Web 
technologies in the life sciences; it could potentially improve the level of 
granularity at which statements about a given publication can be determined. 
Currently, resources such as Bio2RDF (Belleau, Nolin et al. 2008) are able to 
link statements to individual PubMed or PubMed Central citations, but not 
beyond that. The ability to determine article-section membership could add a 
further layer of detail to the declaration of relationships. 
3. The occurrence of some very generic words (e.g., 'here') in an article 
section can provide more information on what kind of section it is, than 
words that are rare. 
This is an interesting result for the Natural Language Processing (NLP) 
community. Traditionally, stop-word lists (which contain lists of very common 
words) have been used to reduce the computational complexity of indexing 
documents that contain the same very common words in all documents. All 
words on the list are ignored in the processed documents, because they are 
assumed to convey little useful information. The results from chapter 3 
suggests that many words that are common in English text as a whole (and 
are therefore included in stop-word lists) are actually far less or more common 
than average in individual article sections, thus making them useful words in 
the classification of text into article sections. 
4. A large sample of the phylogenetics literature contains three main 
communities accounting for 50% of all articles published. These are 
evolutionary biology, microbiology and virology. 
This result provides an interesting insight into the phylogenetics community, 
but its utility to others is not immediately obvious. The result only covers 50% 
of the phylogenetics community but it does still define three clear communities 
of researchers. 
5. Each of the three communities use different analysis protocols and 
these differences have increased since 2000. 
In theory, this should be of use to researchers working in these three fields; it 
highlights the need for better sharing of methods and software between the 
field. It also suggests that, if software (to perform a certain task) is not 
available in one field, then it may be present in another. However, an 
alternative hypothesis also present itself: that divergence between the fields is 
explained by divergence in experimental aims, and therefore there would be 
no reason to expect that software in one field would be of use to those in 
another field. 
6. Phylogenetic protocols, containing a set of 258 important terms, can be 
identified in articles from the phylogenetics literature with an F-measure 
of 87.7%. 
This is an important result for those who work on text-mining and NLP 
problems related to experimental methods. It provides evidence that methods 
can be identified in text, and more reliably than was previously possible in the 
protein interaction method subtask of BioCreative " (Krallinger, Leitner ef al. 
2008). However, the manually created controlled vocabulary that was used in 
chapter 4 may not be a practicable choice of approach when the number of 
important terms to be found is large. 
7. Highly published authors do not use phylogenetic protocols that are 
any different (on average) from less highly published authors. 
This result provides an example of how common practice in a field may 
actually be determined by only a few highly published individuals. Although 
this result may not be detrimental for research practice per se - because 
consensus over methods can lead to greater sharing and reuse of results - it 
does again highlight a problem similar to the Matthew effect, whereby a 
disproportionate amount of interest is afforded to one choice over all others. 
The 'experts' in any field are likely to use good methodologies that stand up to 
scrutiny, but for a research community to continue to improve, new 
approaches should be allowed to prosper. 
8. Neighbour-Joining and Bayesian methods of tree-inference achieve the 
highest combined metric score. 
NJ is a simple method of tree-inference that can be used on large data-sets; 
whereas Bayesian methods are some of the most complex used in 
phylogenetics (Felsenstein 2004). This result suggests that both complex and 
simple choices of method can be supported (albeit by different metrics) by 
objective measures of methodological quality. This result is important to the 
phylogenetics community because it suggests that, where appropriate for the 
data, simple methods can be just as valid a choice as complex ones. 
9. ClustalW/X was calculated as having the highest combined metric 
score of all the multiple sequence alignment methods. 
The Clustal series of multiple sequence alignment tools are established 
favourites in the phylogenetics community (see chapters 4 and 5). Clustal 
had the highest score for popularity and an intermediate score for recentness 
and citation; however, it scored poorly on the expertness metric. The metric 
scores therefore suggest that Clustal is an established, well-known choice in 
the field. Muscle (which had the second highest combined score) scored 
highly for expertness and recentness; and therefore could offer a useful 
alternative choice to Clustal for users who are more interested in using the 
latest software, rather than the most widely accepted. This result also 
suggests that more recently derived tools for sequence alignment have not 
been able to supersede Clustal as the favoured choice. The implication of 
this to researchers considering the development of new methods or software 
for sequence alignment is that it will be very difficult to gain wide acceptance 
from the phylogenetics community because of the dominance of Clustal. This 
certainly could have negative implications for innovation in the field, but the 
metric scores do not provide evidence to support or refute this. 
The capture and analysis of the full range of methodologies from a reasonable 
proportion of the literature of a research discipline has been a complex task. 
Our analysis of phylogenetic practice (chapters 4 and 5) involved the analysis 
of a huge amount of data, and inspired the development of two new research 
 software packages (chapters 2 and 3). The extension of-this approach to 
methodology analysis in other fields is now a far simpler task. Additionally, 
the tools we have created now allow wider use of full-text literature for data-
and text-mining, an area that is significantly under-researched, despite the 
potential gains. The availability of a wider range of more specific metric 
scores in the choices of methods will complement the continued development 
of future interdisciplinary research areas (such as systems biology, 
nanotechnology and tissue engineering). The metrics explored in this thesis 
define a first step in attempting to capture a more objective measure of 
methodological quality, something that will be required for those who are new 
or inexperienced in a field. As the amount of literature published and the 
academic specialisation of individual researchers increases, being able to 
understand and navigate the literature will become more difficult; this thesis 
provides novel approaches and new directions to try both to counter these 
problems and to support future biological research practice. 
6.3. References 
Aboelela, S. W., E. Larson, S. Bakken, O. Carrasquillo, A. Formicola, S. A. 
Glied, J. Haas and K. M. Gebbie (2007). "Defining interdisciplinary 
research: conclusions from a critical review of the literature." Health 
Services Research 42(1 Pt 1): 329-46. 
Afzal, H., R. Stevens and G. Nenadic (2008). Towards Semantic Annotation 
of Bioinformatics Services: Building a Controlled Vocabulary. 
Proceedings of the Third International Symposium on Semantic Mining 
in Biomedicine, Turku, Finland. 
Altman, R. 8., C. M. Bergman, J. Blake, C. Blaschke, A. Cohen, F. Gannon, L. 
Grivell, U. Hahn, W. Hersh, L. Hirschman, L. J. Jensen, M. Krallinger, 
B. Mons, S. I. O'Donoghue, M. C. Peitsch, D. Rebholz-Schuhmann, H. 
Shatkay and A. Valencia (2008). "Text mining for biology--the way 
forward: opinions from leading scientists." Genome Biology 9 Suppl 2: 
-Bandelt, H., P. Forster and A Rohl (1999). "Median-joining networks for 
inferring intraspecific phylogenies." Molecular Biology and Evolution 
16(1): 37-48. 
Belleau, F., M.-A Nolin, N. Tourigny, P. Rigault and J. Morissette (2008). 
"Bi02RDF: Towards a mash up to build bioinformatics knowledge 
systems." Journal of Biomedical Informatics 41(5): 706-716. 
Cohen, A M. and W. R. Hersh (2005). "A survey of current work in biomedical 
text mining." Briefings in Bioinformatics 6(1): 57-71. 
Corney, D. P., B. F. Buxton, W. B. Langdon and D. T. Jones (2004). "BioRAT: 
extracting biological information from full-length papers." Bioinformatics 
20(17): 3206-3213. 
Councill, I. G., C. L. Giles and M. Kan (2008). ParsCit: An open-source CRF 
reference string parsing package. Language Resources and Evaluation 
Conference (LREC), Marrakech, Morocco. 
Felsenstein, J. (2004). Inferring phylogenies, Sinauer Associates Sunderland, 
Mass., USA 
Fink, J. L., S. Kushch, P. R. Williams and P. E. Bourne (2008). "BioLit: 
integrating biological literature with databases." Nucleic Acids 
Research 36(Web Server issue): W385-389. 
Havemann, F., M. Heinz and R. Wagner-Dobler (2005). "Firm-like behavior of 
journals? Scaling properties of their output and impact growth 
dynamics: Research Articles." Journal of the American Society for 
Information Science and Technology 56(1): 3-12. 
Hearst, M. A, A Divoli, H. Guturu, A Ksikes, P. Nakov, M. A Wooldridge and 
J. Ye (2007). "BioText Search Engine: beyond abstract search." 
Bioinformatics 15(23): 2196-2197. 
Hull, D., S. R. Pettifer and D. B. Kell (2008). "Defrosting the digital library: 
bibliographic tools for the next generation web." PLoS Computational 
Biology 4(10): e1000204. 
Kostoff, R. N. (2002). "Overcoming specialization." Bioscience 52(10): 937-
Krallinger, M., F. Leitner, C. Rodriguez-Penagos and A Valencia (2008). 
"Overview of the protein-protein interaction annotation extraction task 
of BioCreative II." Genome Biology 9 Suppl 2: S4. 
) Langille, M. G. I. and J. A. Eisen (2010). "BioTorrents: A File Sharing Service 
for Scientific Data." PLoS ONE 5(4): e10071. 
Merton, R. K. (1968). "The Matthew effect in science. The reward and 
communication systems of science are considered." Science 159(810): 
56-63. 
Merton, R. K. (1988). "The Matthew Effect in Science, II: Cumulative 
Advantage and the Symbolism of Intellectual Property." Isis 79(4): 606-
Muller, H.-M., E. E. Kenny and P. W. Sternberg (2004). "Textpresso: An 
Ontology-Based Information Retrieval and Extraction System for 
Biological Literature." PLoS Biology 2( 11): e309. 
Natarajan, J., C. Haines, B. Berglund, C. DeSesa, C. Hack, W. Dubitzkyand 
E. G. Bremer (2006). GetltFull - A Tool for Downloading and Pre-
processing Full-Text Journal Articles. Lecture Notes in Computer 
Science. Heidelberg, Springer Berlin. 3886: 139-145. 
Newman, M. E. J. (2003). "Mixing patterns in networks." Physical Physical 
review. E, Statistical, nonlinear, and soft matter physics E 67(2). 
Pettifer, S. R., D. Thorne, P. McDermott, J. Marsh and T. Attwood (2009). 
Utopia Documents: The Digital Meta-Library. Proceedings of 
technology track, ISMB. Stockholm. 
Shotton, D. (2010). "CiTO, the Citation Typing Ontology." Journal of 
Biomedical Semantics 1 (Suppl 1): S6. 
Smedley, D., S. Haider, B. Ballester, R. Holland, D. London, G. Thorisson and 
A. Kasprzyk (2009). "BioMart--biological queries made easy." BMC 
Genomics 10: 22. 
Stevens, R., S. Jupp, J. Bibby, J. Volker and D. Shotton (2008). Exploring 
Semi-Automated Ontology Learning for Biological Ontologies. 
Teufel, S., A. Siddharthan and D. Tidhar (2006). Automatic classification of 
citation function. In proceedings of EMNLP 2006. Sydney, Australia. 
Xu, S., J. McCusker and M. Krauthammer (2008). "Yale Image Finder (YIF): a 
new search engine for retrieving biomedical images." Bioinformatics 
24(17): 1968-1970. 
Zweigenbaum, P., D. Demner-Fushman, H. Yu and K. B. Cohen (2007). 
"Frontiers of biomedical text mining: current progress." Briefings in 
Bioinformatics 8(5): 358-75. 
-...j 
Appendix 1 
Proportional usage of top 30 protocols across all fields. 
all fields (17,732 articles with protocol) 
Protocol 
Neighbour-joining 
Neighbour-joining, Maximum-likelihood 
Neighbour-joining, Parsimony, Maximum-likelihood 
Maximum-likelihood 
Neighbour-joining, Parsimony 
Parsimony 
ParSimony, Maximum-likelihood 
Neighbour-joining, Kimura 2-parameter model 
UPGMA 
Neighbour-joining, UPGMA 
Neighbour-joining, Parsimony, Maximum-likelihood, Kimura 2-parameter model 
Neighbour-joining; Maximum-likelihood, Kimura 2-parameter model 
Neighbour-joining, Parsimony, Kimura 2-parameter model 
Maximum-likelihood, Bayesian 
Neighbour-joining, PAM model 
Neighbour-joining, P-distance 
Neighbour-joining, Jukes-Cantor model 
Neighbour-joining, Parsimony, Maximum-likelihood, HKY model 
Weighted-parsimony 
Neighbour-joining, Parsimony, Maximum-likelihood, Bayesian 
Parsimony, Maximum-likelihood, Bayesian, General time-reversible model 
Neighbour-joining, Maximum-likelihood, Bayesian 
PAM model 
Bayesian , 
Neighbour-joining, Parsimony, Maximum-likelihood, Bayesian, General time-reversible model 
Neighbour-joining, Maximum-likelihood, JTT model 
Neighbour-joining, Parsimony, Maximum-likelihood, JTT model 
Parsimony, Maximum-likelihood, Bayesian 
Neighbour-joining, Parsimony, Weighted-parsimony, Maximum-likelihood 
Weighted-parsimony, Maximum-likelihood 
proportion of articles using protocol 
0.23878866 
0.074742268 
0.063724227 
0.059664948 
0.052641753 
0.047551546 
0.026868557 
0.021778351 
0.017976804 
0.014561856 
0.013208763 
0.011597938 
0.010953608 
0.007731959 
0.007731959 
0.007731959 
0.007087629 
0.006056701 
0.005798969 
0.005734536 
0.00560567 
0.005347938 
0.005219072 
0.005025773 
0.00496134 
0.004832474 
0.004639175 
0.004381443 
0.004252577 
0.004123711 
Appendix 1 
Proportional usage of top 30 protocols in evolutionary biology 
Evolutionary biology (3,340 articles with protocol) 
Protocol 
Neighbour-joining, Parsimony, Maximum-likelihood 
Neighbour-joining 
Neighbour-joining, Maximum-likelihood 
Neighbour-join ing, Parsimony 
Maximum-likelihood 
Parsimony 
Parsimony, Maximum-likelihood 
Parsimony, Maximum-likelihood, Bayesian, General time-reversible model 
Neighbour-joining, Parsimony, Maximum-likelihood, Bayesian, General time-reversible model 
Neighbour-joining, Parsimony, Maximum-likelihood, HKY model 
Neighbour-joining, Parsimony, Maximum-likelihood, Kimura 2-parameter model 
Neighbour-joining, Parsimony, Kimura 2-parameter model 
Neighbour-joining, Parsimony, Maximum-likelihood, General time-reversible model 
Parsimony, Maximum-likelihood, General time-reversible model 
Parsimony, Maximum-likelihood, Bayesian 
Maximum-likelihood, Bayesian 
Neighbour-joining, Parsimony, Maximum-likelihood, lIT model 
Neighbour-joining, Parsimony, Maximum-likelihood, Bayesian 
Parsimony, Maximum-likelihood, HKY model 
Neighbour-joining, Parsimony, Weighted-parsimony, Maximum-likelihood 
Weighted-parsimony 
Neighbour-joining, Parsimony, Maximum-likelihood, PAM model 
Weighted-parsimony, Maximum-likelihood 
Maximum-likelihood, Bayesian, General time-reversible model 
Neighbour-joining, UPGMA 
Neighbour-joining, Maximum-likelihood, lIT model 
Neighbour-joining, Maximum-likelihood, Bayesian 
Neighbour-joining, Parsimony, Weighted-parsimony 
UPGMA 
Maximum-likelihood, HKY model 
~ortion of articles usinQ i1rotocol 
0.069161677 
0.067365269 
0.043413174 
0.04011976 
0.036526946 
0.032934132 
0.02754491 
0.018263473 
0.015868263 
0.015269461 
0.01497006 
0.01257485 
0.011676647 
0.011077844 
0.011077844 
0.010479042 
0.010479042 
0.010179641 
0.008682635 
0.008682635 
0.007784431 
0.00748503 
0.00748503 
0.00748503 
0.007185629 
0.006287425 
0.006287425 
0.005688623 
0.005688623 
0.005389222 
-...J 
Appendix 1 
Proportional usage of top 30 protocols in microbiology 
Microblolo 
Protocol 
Neighbour-joining 
Neighbour-joining, Maximum-likelihood 
Neighbour-joining, Parsimony, Maximum-likelihood 
Neighbour-joining, Parsimony 
Maximum-likelihood 
Neighbour-jOining, Kimura 2-parameter model 
UPGMA 
Parsimony 
Parsimony, Maximum-likelihood 
Neighbour-joining, UPGMA 
Neighbour-joining, Jukes-Cantor model 
Neighbour-joining, Parsimony, Maximum-likelihood, Kimura 2-parameter model 
Neighbour-joining, Parsimony, Kimura 2-parameter model 
Neighbour-jOining, Maximum-likelihood, Kimura 2-parameter model 
Kimura 2-parameter model 
Neighbour-joining, Parsimony, Jukes-Cantor model 
Neighbour-jOining, Parsimony, Maximum-likelihood, Jukes-Cantor model 
Neighbour-jOining, P-distance 
Neighbour-joining, Parsimony, UPGMA 
PAM model 
Jukes-Cantor model 
Neighbour-jOining, Maximum-likelihood, UPGMA 
Neighbour-joining, PAM model 
Neighbour-jOining, Parsimony, Maximum-likelihood, UPGMA 
Neighbour-joining, Parsimony, Maximum-likelihood, PAM model 
Neighbour-joining, Parsimony, Maximum-likelihood, HKY model 
Parsimony, UPGMA 
Maximum-likelihood, UPGMA 
Neighbour-jOining, Maximum-likelihood, PAM model 
Neighbour-jOining, Maximum-likelihood, Jukes-Cantor model 
rODortlon of articles usln 
0.348480545 
0.089747231 
0.086339108 
0.054245953 
0.047429707 
0.03777336 
0.032377166 
0.027833002 
0.02470889 
0.023856859 
0.018176654 
0.015336552 
0.01221244 
0.010224368 
0.005680204 
0.004828174 
0.004828174 
0.004828174 
0.004828174 
0.004544164 
0.003976143 
0.003692133 
0.003408123 
0.003124112 
0.003124112 
0.003124112 
0.003124112 
0.002840102 
0.002840102 
0.002556092 
rotoco 
Appendix 1 
Proportional usage of top 30 protocols in virology 
Protocol 
Neighbour-joining 
Neighbour-joining, Maximum-likelihood 
Parsimony 
Neighbour-joining, Parsimony 
Vlroloav (1.877 articles with 
Neighbour-joining, Kimura 2-parameter model 
Neighbour-joining, Parsimony, Maximum-likelihood 
Maximum-likelihood 
Neighbour-joining, Maximum-likelihood, Kimura 2-parameter model 
Neighbour-joining, Parsimony, Maximum-likelihood, Kimura 2-parameter model 
Neighbour-joining, Parsimony, Kimura 2-parameter model 
UPGMA 
Parsimony, Maximum-likelihood 
Neighbour-joining, UPGMA 
Neighbour-joining, Maximum-likelihood, HKY model 
Neighbour-joining, PAM model 
PAM model 
Neighbour-joining, P-distance 
Neighbour-joining, Maximum-likelihood, General time-reversible model 
Neighbour-joining, Maximum-likelihood, Bayesian 
Neighbour-joining, Parsimony, P-distance 
Kimura 2-parameter model 
Maximum-likelihood, Kimura 2-parameter model 
Neighbour-joining, Maximum-likelihood, UPGMA 
Neighbour-joining, Parsimony, Maximum-likelihood, UPGMA 
Neighbour-joining, Maximum-likelihood, PAM model 
Neighbour-joining, Maximum-likelihood, P-distance, Kimura 2-parameter model 
Neighbour-joining, Jukes-Cantor model 
Neighbour-joining, Maximum-likelihood, Kimura 2-parameter model, HKY model 
Neighbour-joining, Parsimony, UPGMA 
Neighbour-joining, P-distance, Kimura 2-parameter model 
ortion of articles usina orotoco 
0.233883857 
0.10229089 
0.062866276 
0.059669686 
0.055407565 
0.05167821 
0.04741609 
0.040490144 
0.023974427 
0.022908897 
0.014384656 
0.013319126 
0.013319126 
0.009057006 
0.008524241 
0.007991476 
0.007991476 
0.006925946 
0.006393181 
0.005860416 
0.005327651 
0.00426212 
0.00426212 
0.00426212 
0.00426212 
0.003729355 
0.003729355 
0.003729355 
0.003729355 
0.00319659 
