Microsoft Word - Georgios_Michalakidis_PhD_Thesis_UniS_v3.docx
Appreciation of structured and unstructured content to 
aid decision making - from Web scraping to ontologies 
and data dictionaries in healthcare 
Georgios Michalakidis MSc AHEA 
PhD Thesis 
Supervisor:     Prof. Paul Krause 
Co-supervisor:           Dr. Sotiris Moschoyiannis 
Department of Computer Science 
Faculty of Engineering and Physical Sciences 
University of Surrey 
September 2016 
 Georgios Michalakidis 2016, g.michalakidis@surrey.ac.uk 
Abstract 
A systematic approach to the extraction of data from disparate data sources is proposed. The 
World Wide Web is a most diverse dataset; identifying ways in which this large database 
provides means for data quality verification with concepts such as data lineage and provenance 
allows to follow the same approach as a means to aid decision-making in sensitive domains 
such as healthcare. 
Through lessons learned from research in the UK and internationally, we conclude that 
emphasis on interoperable and model-based support of the data syndication can enhance data 
quality, an issue still current (American Hospital Association, 2015) and with data barriers in 
healthcare due to governance concerns. To improve on the above, we start by proposing a 
system for solution-orientated reporting of errors associated with the extraction of routinely 
collected clinical data. We then explore key concepts to assess the readiness of data for 
research and define an ontology-driven approach to create data dictionaries for quality 
improvement in healthcare. Finally, we apply this research to facilitate the enablement of 
consistent data recording across a health system to allow for service quality comparisons. 
Work deriving from this research and built by the author commissioned and aided by the UK 
NHS, University of Surrey, Green Cross Medical, particularly in creating and testing software 
systems in real-world scenarios, has facilitated: quality improvement in healthcare data 
extraction from GP practices in the UK, a state-of-art system for Web-enabling Hospital 
Episode Statistics (HES) data for dermatology and, finally, an online system designed to 
enable cancer Multi-Disciplinary Teams (MDTs) to self-assess and receive feedback on how 
their team performs against the standards set out in The Characteristics of an Effective 
MDT provided by NHS IQ, formerly part of National Cancer Action Team (NCAT), which in 
2016 won the Quality in Care Programmes Digital Innovation in the Treatment of Cancer 
award. 
Further experimentation shows there is potential for the methods proposed to be applicable in 
other sectors such as the investment sector (initial investigation has happened through the 
early stages of this research) but it is suggested that this potential be explored further. 
Acknowledgements 
My deepest gratitude to my mum, dad and my brother for the persisted enthusiasm about my 
research, and to everyone else without whom it wouldnt have been possible to complete this 
thesis. 
I am grateful to Dr. Bogdan Vrusias for his guidance through my first years of research, and 
the opportunity to work with the Department for International Trade (formerly UK Trade & 
Investment or UKTI) on research and development. 
I am also grateful to Prof. Simon de Lusignan for his immense support from the day I joined 
the Clinical Informatics and Health Outcomes Research Group at St. Georges and later at 
University of Surrey, as well as his continuous help in building a publication record from the 
very beginning. I acknowledge my colleagues and co-authors internal and external to the above 
groups for key contributions to my published work, especially through the Quality 
Improvement in Chronic Kidney Disease (QICKD) and Translational Medicine and Patient 
Safety in Europe (TRANSFoRM) research projects. 
I am ever indebted to my supervisor Prof. Paul Krause for his continuous guidance through 
teams, projects and system revisions. Enormous thanks to Dr. Sotiris Moschoyiannis, my co-
supervisor for always pointing out opportunities to amplify the impact of this research. 
Thanks to my business partner Aaron for coping with me during the latter stages of my PhD 
and Nick for his sensible advice. 
Last but not least, thank you David for understanding me and providing a critical eye 
throughout this journey. 
List of authors relevant publications 
Below is a list of academic publications authored and contributed to, related to this research. 
Publication Citations Year 
Key concepts to assess the readiness of data for International research: 
Data quality, lineage and provenance, extraction and processing errors, 
traceability, and curation 
27 2011 
S De Lusignan, ST Liaw, P Krause, V Curcin, MT Vicente, G 
Michalakidis, et al 
Yearbook Medical Informatics 6 (1), 112-120 
A system for solution-orientated reporting of errors associated with the 
extraction of routinely collected clinical data for research and quality 
improvement. 
19 2009 
G Michalakidis, P Kumarapeli, A Ring, J van Vlymen, P Krause, et al 
Studies in health technology and informatics 160 (Pt 1), 724-728 
Defining datasets and creating data dictionaries for quality improvement 
and research in chronic disease using routinely collected data: an 
ontology-driven approach 
18 2011 
S de Lusignan, ST Liaw, G Michalakidis, S Jones 
Journal of Innovation in Health Informatics 19 (3), 127-134 
The provision and impact of online patient access to their electronic 
health records (EHR) and transactional services on the quality and 
safety of health care: systematic review protocol 
11 2013 F Mold, B Ellis, S de Lusignan, A Sheikh, JC Wyatt, M Cavill, G 
Michalakidis et al 
Journal of Innovation in Health Informatics 20 (4), 271-282 
Business process modelling is an essential part of a requirements analysis 
11 2012 
S De Lusignan, P Krause, G Michalakidis, MT Vicente, S Thompson, et 
Contribution of EFMI Primary Care Working Group Yearbook Medical 
Informatics 7 (1), 34-43 
Tiger nation: Empowering citizen scientists 
9 2012 
AD Mason, G Michalakidis, PJ Krause 
Digital Ecosystems Technologies (DEST), 2012 6th IEEE 
What are the barriers to conducting international research using 
routinely collected primary care data? 
8 2010 
S de LUSIGNAN, C Pearce, NT Shaw, ST Liaw, G Michalakidis, et al 
Studies in health technology and informatics 165, 135-140 
International and European Medical Informatics Association and 
Federation Primary Care Informatics Working Groups: What are the 
barriers to conducting international research using routinely collected 
primary care data 7 2011 
S De Lusignan, C Pearce, NT Shaw, ST Liaw, G Michalakidis, MT 
Vicente, et al 
Studies in health technology and informatics 165, 135-140 
Conducting requirements analyses for research using routinely collected 
health data: a model driven approach. 
5 2011 S de Lusignan, J Cashman, N Poh, G Michalakidis, A Mason, T 
Desombre, et al 
Studies in health technology and informatics 180, 1105-1107 
Developing a survey instrument to assess the readiness of primary care 
data, genetic and disease registries to conduct linked research: 
TRANSFoRm International Research Readiness (TIRRE) survey 
instrument 
4 2013 
E Leppenwell, S De Lusignan, MT Vicente, G Michalakidis, P Krause, 
et al 
Journal of Innovation in Health Informatics 20 (3), 207-216 
Consistent data recording across a health system and Web-enablement 
allow service quality comparisons: online data for commissioning 
dermatology services 
4 2012 
O Dmitrieva, G Michalakidis, A Mason, S Jones, T Chan, S de Lusignan 
Studies in health technology and informatics 174, 84-8 
Table of Contents 
CHAPTER 5 Method for solution-orientated reporting of extraction errors of clinical data for 
CHAPTER 6 Key concepts to Assess the Readiness of Data for International Research: Data 
Quality, Lineage and Provenance, Extraction and Processing Errors, Traceability and 
CHAPTER 7 Defining datasets and creating data dictionaries for quality improvement and 
7.2.2 Practical approaches to case finding in chronic disease: Ontologies and 
7.3.3 Defining the relevant concepts and relationships within the reference 
CHAPTER 8 Consistent Data Recording across a Health System and enabling wider access 
through the Web allows for Service Quality Comparisons  use-case in Online Data for 
CHAPTER 1 Introduction 
1.1 Introduction 
In this thesis, we explore the process from case finding to data extraction to data dictionary 
generation, and that automated methods can prove more efficient than manual approaches 
(people extracting routing data to identify cases, with chronic disease used as an example). 
The key research hypothesis is that more consistent data recording across systems, 
particularly in health, can drive improvements and facilitate research and quality 
improvement. The diverse issues with regards to datasets in general, as well as automated 
content aggregation and extraction from the internet, through the Web, the largest database 
in the world1, are identified. We then attempt to classify the key errors associated with the 
data extraction of routinely collected data using an ontology-driven approach, both on the 
Web and in data repositories about health. The readiness of these data of international 
research is then assessed, with key concepts such as lineage, traceability and curation. The 
definition of datasets and creation of data dictionaries is shown to make case finding more 
transparent; hypertension is presented as an exemplar. 
The UK National Health Systems first experimental statistics report in 2012 argues that the 
quality of health and social care data is fundamental to delivering high quality, patient led 
care (Health and Social Care Information Centre, Information Governance and Data Quality, 
2012). In simple words, data needs to be fit for purpose for its intended use. This thesis is 
distinctive from other algorithm-based works mainly as it does not propose a theoretical 
framework to correlation between unstructured Web content extraction and analysis, and data 
extraction from unstructured and structured data repositories for health. The argument is 
1 Broadly speaking, as a database is defined as a comprehensive collection of related data organised for 
convenient access, generally in a computer (Flexner, 1987) and the Web is a network of computers; 
but then again so are most databases nowadays. 
evidence-based; initially with the identification of the same types of errors associated with the 
the initial quality of the source data and the extraction process itself and the improvement 
upon resolution of these errors (Michalakidis, et al., 2010), as well as further on with the 
identification of quality improvements, particularly of disease registers, through the 
introduction of reference terminology and data dictionaries (de Lusignan, et al., 2011). 
While this thesis concentrates on health care data and the impact of an ontology-driven 
approach in the medical domain, applications were identified and explored in domains such as 
business. UK Trade & Investment (UKTI), a UK Government department, contributed input 
data to this research, providing the ability to analyse how the hypothesis applies to business, 
by creating summary knowledge-bases for company entities, as part of an effort to aid humans 
in their decision-making process, for classification of companies as investment-worthy. It is, 
however, argued that further experimentation need to be performed to identify diversities and 
idiosyncrasies in data, particularly from different domains and sectors. 
1.2 Structure of Thesis 
The structure of this thesis is as follows: 
Chapter 2 explores the history and analogies in human communication, the creation of 
networks where the conversion of data to information can be sped up and the inevitable effect 
of the above to other data sciences, such as health. 
Chapter 3 examines technologies and innovations such as the Semantic Web as means to 
conversion of information to knowledge and begins to address data linkage concepts in existing 
literature. 
Chapter 4 starts to address an experimental method to data aggregation for aiding decision-
making, initially as a concept, and then with a prototype system for Web scraping together 
with its early evaluation. 
In Chapter 5, we explore health data repositories from a large study and apply lessons learnt 
from the aforementioned method in data for quality improvement purposes. 
Chapter 6 assesses the readiness of data for international research with concepts such as 
provenance and traceability and discusses linked quality concepts from data recording to 
extraction. 
Chapter 7 describes an ontology-driven approach to case finding in chronic disease and how 
it can be used to create data dictionaries and make case finding transparent. 
Chapter 8 presents a novel Web-framework for making consistently collected health data 
across providers readily available for benchmarking to inform health commissioning decisions. 
Chapter 9 concludes that new models of medical data access and simplified comparisons are 
required. The systems proposed appear to enable rapid and efficient analysis of data, albeit the 
limitations of our studies. We provide suggestions for future research. 
1.3 Thesis Contribution 
This thesis has the following contributions: 
1. The hypothesis an ontology-driven, interoperable model in the tightly-governed health 
domain, can facilitate research and quality improvements is confirmed. 
2. An algorithm is presented to validate Web-based entities based on their metadata and 
content, for decision-making purposes. 
3. A solution-oriented method for the reporting of extraction errors from United Kingdom 
(UK)-based diverse Electronic Patient Record (EPR) systems is developed as part of 
the QICKD study. 
4. A software system is created for aiding case-finding through generating data 
dictionaries from different health data sources and reference coding systems, and the 
respective data dictionaries for public access. 
5. An example application system is created which delivers Hospital Episode Statistics 
(HES) data as the basis for simplified access more transparent comparisons from 
experts. 
CHAPTER 2 Background 
Humans communicate through different signals. Due to an ability to perceive things, 
our species can perform this communication accurately and efficiently even when noise 
is introduced or the signal is presented in different formats. Computer technologies aid 
in cognitive processing and can, to some degree, support intellectual performance and 
enrich individuals minds. We operate (and design systems that operate) using 
analogies, such as reasoning, comparisons, and synonymity. In the end: is analogy a 
shared abstraction? Does it derive from mathematics? Is it high-level perception in 
shared structure theory? 
2.1 Review of Web content evolution 
In 1997, early stages for the World Wide Web, a team from the department of instructional 
technology at the University of Georgia argued that the medium had become, despite its 
usefulness and popularity, an uncontrollable data dump space (Oliver & Bennett, 1997). A few 
years later, in 2002, researchers would still argue against the Webs usefulness, especially for 
domain-specific information; domains such as health (Purcell, et al., 2002). This was despite 
advances in technology over its initial decade  and perhaps even applies today, even with the 
introduction of Web Mining as a topic of particular research interest (Kushmerick, 2003). 
Every second there is vast amounts of textual and other content uploaded on the Web2 - with 
the rate ever increasing (Coffman & Odlyzko, 1998). For the end user, attempting to find the 
right/most accurate information in a most convenient and timely manner is still not the 
simplest of tasks. This is especially the case when a users request (query) cannot easily be 
expressed or described with simple keywords or language syntax. Of course, search engines 
remove that burden to an extend  but can occasionally be fooled (Leskovec, et al., 2014). 
More importantly, even once a target Web resource is identified, the information within isnt 
completely obvious; careful content creation is important and as the U.S. Department of 
Health & Human Services discusses in an online article: Matching users mental models to 
your design will result in a more useful site (Gee, 2009). 
Over the years, the changes to World Wide Web were not only with regards to how content is 
created and where it is placed on a Web-page; there is advancements in the way a machine is 
able to semantically appreciate said content, or even process it. The Semantic Web as its 
called, is a concept where one can define the semantics of a Web page with the extraction of 
semi-structured data from the Webs standard language, Hypertext (Chakrabarti, 2002) or a 
service, making it possible to identify pages that are more relevant to the meaning of the input 
2 Coffman and Odlyzko had predicted in an AT&T research study as early as 1998, that Web 
information transfer would not take long to exceed the de facto communication standard: telephony and 
voice. 
query. This can have different uses, for example in ambiguities in similarly-sounding place 
names, an ever-increasing issue, based on the popularity of Web/mobile services providing 
access to location information (Buscaldi, 2010). Just like in Dorai and Yacoobs approach, the 
embedded semantics describe the contents of a Web page or service and can be interpreted by 
the machine to allow the identification, filtering and retrieval of a given query (Dorai & 
Yacoob, 2001). 
The Semantic Web is an evolving development of the World Wide Web in which the meaning 
(semantics) of information and services on the Web is defined, making it possible for the Web 
to understand and satisfy the requests of people and machines to use the Web content. 
Tim Berners-Lee defines the Semantic Web as a Web of data that can be processed directly 
and indirectly by machines (Berners-Lee, et al., 2001). It is without doubt gaining momentum 
in both industry and the academia. Research has lately been trying to identify market sectors 
that can be used as examples for Semantic Web and Semantic Web Service applicability 
(Daconta, et al., 2003). This is because of the industrys information heterogeneity, market 
fragmentation and rather complex discovery and matchmaking tasks, including substitution 
and composition  all of which are limitations that Semantic Web technologies promise to 
overcome. 
In todays Web, business applications are moving away from static, fixed Web pages to those 
that are dynamically generated at the time of user requests. This kind of Websites is also 
known as data-intensive (Fraternali, 1999) and newer Websites are typically realised using 
relational databases. 
However, Data-intensive Websites form an invisible Web. Search engine crawlers usually do 
not read dynamically generated URLs, thus pages are not included in search engine indexes. 
Consequently, such pages are invisible (Stojanovic, et al., 2002). We believe that, amongst 
others, the Semantic Web can form part of the answer to the invisibility of a large part of the 
Web as it is today [Figure	1]. 
A Semantic approach to the Web allows for logical assertions, classification, formal class 
models, setup of rules and gaining of Trust. The different Semantic levels consist of: things, 
then knowledge about things, and finally, worlds. The above, allow for Search and Retrieval 
mechanisms with: 
 Discovery of Knowledge via Taxonomies 
 Web-service-based data searches 
 Search by association 
 Pattern-based searches 
 Manual and agent-based searches 
 Rule-based orchestration queries 
 Automated inference support 
Figure 1  Demonstration of the ability of human and machine to perceive 
and understand content from the various states of the Web (WebKnox, 
n.d.) 
The World Wide Web Consortium (W3C) suggests that the Semantic Web is formed of a 
Stack (World Wide Web Consortium (W3C), 2010) with distinct core components [Figure	2]. 
 WSML is a language for the Semantic Web 
 The most basic of the Semantic Web languages is the eXtensible Markup Language 
(XML). XML provides an elemental syntax for content structure within documents, 
yet associates no semantics with the meaning of the content contained within 
 RDF is the standard language for exchanging structured and semi-structured data over 
the Semantic Web. It allows for expressing data models, which refer to objects 
("resources") and their relationships. A Resource Description Framework (RDF)-based 
model can be represented in XML syntax 
 The Web Ontology Language (OWL) is the standard ontology language for the 
Semantic Web, extending the RDF schema. It provides more vocabulary for describing 
properties and classes: among others, relations between classes (e.g. disjointness), 
cardinality (e.g. exactly one), equality, richer typing of properties, characteristics of 
properties (e.g. symmetry), and enumerated classes 
 SPARQL (Protocol and RDF Query Language) is a protocol and query language for 
semantic Web data sources 
Figure 2 - The Semantic Web Stack (World Wide Web 
Consortium (W3C), 2010) 
There are known issues to RDF (Daconta, et al., 2003): it is not fully compatible with XML 
documents, parts of it are seriously complex and early RDF examples are weak. For the above 
reasons, bibliography suggests that taxonomies and topic maps should be used as well as 
object classification and finally, use of codes and subcodes. 
2.1.1 Semantic Web scepticism 
There are quite a few sceptical reactions with critics questioning the basic feasibility of a 
complete or even partial fulfilment of the Semantic Web (Marshall & Shipman, 2003). 
Another example of the Semantic Webs bad reputation is the metacrap portmanteau (from 
metadata and crap) publication (Doctorow, 2001), where a less substantiated criticism is 
expressed by describing metadata as a utopia. Its also a pipe-dream, founded on self-
delusion, nerd hubris and hysterically inflated market opportunities. In summary, causes of 
the movement against the Semantic Web are: 
 Bad precedents - especially when it comes to comparing the Semantic Web with 
Artificial Intelligence 
 The concept of machine understandable documents does not imply some magical 
artificial intelligence, which allows machines to comprehend human mumblings. It only 
indicates a machine's ability to solve a well-defined problem by performing well-defined 
operations on existing well-defined data. Tim Berners-Lee states that: Instead of 
asking machines to understand peoples' language, it involves asking people to make the 
extra effort (Berners-Lee, 1998) 
 Fear, Uncertainty and Doubt (FUD) of the advanced and new (Daconta, et al., 2003) 
 Current Status Quo: the claim that we do not need a Semantic Web 
Are the those opposed to RDF wrong? Solutions cannot easily be identified without targeted 
research on the individual topics; however, we have made an effort to introduce suggestions on 
why some solutions are applicable. It is a matter of how data processing has changed over the 
last few years. 
There exists computing power able to perform resource exhaustive computations, power that is 
currently being under-utilised. Current methods are using computational power to transform 
data into information. Data are symbols while information occurs when symbols are used to 
refer to something (Beynon-Davies, 2002). Future methods should incorporate an extra 
processing layer to allow the transformation of information into knowledge (Thomas, 2001) 
with enough intelligence (or governance) on the extraction layer, to allow for strict error 
checking and provenance procedures (Michalakidis, et al., 2010). Furthermore, both consumers 
and businesses are keen on applying the network effects and seamless wisdom generation to 
their information and extracted knowledge. 
The industrial branch of this effort and the breakthrough can be rationalised by quoting work 
from the article Integrating Applications on the Semantic Web, where James Hendler, Tim 
Berners-Lee and Eric Miller clearly state that any players should act fast and that the 
business market for this integration of data and programs is huge. The companies who choose 
to start exploiting Semantic Web technologies will be the first to reap the rewards (Hendler, et 
al., 2002). 
Finally, there is another reason for the creation and use of the Semantic Web: Progress 
through combinatorial experimentation demands it. 
To summarise, the Semantic Web can offer delicate, but accurate and efficient mechanisms for: 
 Decision support 
 Business development 
 Information sharing and knowledge discovery 
 Administration and automation 
Still, however, the promising benefits of the Semantic Web, for a five-year period since the 
term was coined, it had not fully been implemented or adopted by the community and 
businesses at a large scale. This is also despite evidence for the value that semantic resources 
add value to organisations, from studies in the business domain (Silva, et al., 2006). A key 
reason behind this appears to be the fact that identifying and attaching the semantics to a 
page is a laborious task, time consuming and expensive, as it is in its majority manual 
(McIlraith & Martin, 2003).   
It is evident that many of the lessons of the Semantic Web are still limited in their knowledge 
generation. Although there is no argument against one looking forward to a case where the 
Web of Data will have more semantic content, it is acceptable to think about metadata, a 
broader term, for Database content which has been a valid trend for years since the early ear 
of the Web (Gill, 1998). 
Since the Semantic Web concept has a slower than anticipated adoption rate, information 
within HTML pages remains almost as plain and unstructured text (we refer to HTML 
content, static and dynamic, as opposed to content served by engines such as microblogging, 
e.g. Facebook, where some level of annotation of content  or indeed full semantic translation 
 may exist). Today, this accounts for the majority of Web content (Chakrabarti, 2002)  
rendering sentiment analysis methods and opinion mining (alternatives that mostly apply to 
structured content linguistic processing) less usable (Madria, et al., 2002). For that reason 
"Web mining" techniques have been introduced and used to retrieve relevant content (Cooley, 
et al., 1997). 
Web mining is a specification of text mining or data mining in the broader sense, where useful 
facts are discovered from a raw or unstructured dataset. Web mining could be used to 
facilitate businesses discovering patterns for personalised marketing, improving customer 
relationships, and forecasting future activities; but also allows search engines to identify and 
categorise resources better and faster.  
Traditional Web mining alone though does not always succeed (Cooley, et al., 1997), especially 
when the relevant information is "hidden" within complex Web pages or presented in 
ambiguous ways. To structurally shape the content in an understandable way, the same rule 
applies as outside the Information Technology domain: Individual perspective is a necessary 
starting point for overcoming content ambiguity and fuzziness, aiming at collaborative use 
between and among people for efficient content structuring principles (Germonprez & Zigurs, 
2008). Because of very limited application of the above, data mining as a knowledge discovery 
process step, requires the data be already stored, partly cleaned (e.g. through feature weighting 
(Yi & Liu, 2003) and fixed in such ways to make the mining process possible. In the case of 
Web pages, this pre-processing does not exist, since all Web pages are published in a non-
standard manner (Herrera-Viedma, et al., 2006). For that reason researchers have tried, with 
considerable success, processing the contents of Web pages to first identify entities, such as 
places, people and organisations (through Named Entity Recognition  NER which is often 
applied as part of the linguistic analysis of Web content (Hotho & Stumme, 2007)) before 
trying to identify any associations and patterns in the data. 
The problem currently, is that information is dynamic and therefore changes constantly. 
Information extraction is generally being considered a challenge in informatics and Web 
mining (Kosala & Blockeel, 2000), especially when new associations may appear; old 
information may change; new appearing evidence may change existing associations, which 
really implies a need for an adaptable and time dependent approach (Chakrabarti, 2002). 
The above applies to business environments as well (Raisinghani, 2004)  and any mission 
critical paradigm and market. Companies need to remain competitive in order to survive and 
this becomes a real challenge due to the constantly changing environment. Competitor analysis 
can also be (and should be) performed on the Web (Amor, 2000)  there is an ever-increasing 
interest for this in the business and health sectors. The ability to understand the markets and 
consumer needs, and at the same time follow on the competitor trends is one of the vital 
processes, but it consumes many resources and time, especially processes such as blindspot 
analysis and management profiling (Fleisher & Bensoussan, 2002). UK Trade and Investment 
(UKTI) for example, assist companies trade internationally, or overseas companies bring 
investment to the UK, by searching the Web for high quality investors and approach/advise 
them accordingly. The problem is that the Web does not allow for meaningful queries such as 
SME in the steel industry with potential for growth to be executed and content retrieved. 
Therefore, UKTI spends weeks of searching the Web for candidates, before ranking them 
intuitively. This process is time consuming, inefficient, expensive and inaccurate, especially 
when it relies solely on search engines. Search engines like Google, are poor concordances and 
can barely be used for linguistically complex queries and the relevance perception they use it 
linguistically questionable (Sharoff, 2006). Various companies provide services to help with 
intelligent mining of Web data, but the analysis is manual and therefore unaffordable to most 
potential clients; others outsource this process to developing countries where hundreds of 
workers analyse the Web constantly for information in order to generate the necessary reports, 
but again, despite the low labour costs, the results are not as accurate as needed because of 
the quantitative rather than the qualitative approach. In addition to this, most of the time the 
information is already out-dated because of the aforementioned reasons with regards to 
dynamic content. 
2.2 Creating an analogy in the health domain 
Computer technologies aid in cognitive processing and can support intellectual performance 
and enrich individuals minds (Solomon, et al., 1991). Correlations between linguistic 
constructs and conceptual constructs can be built, essentially enabling the enrichment of 
health data by combining data sources (do Amaral, et al., 2000). Arguably, however, 
technological advancements in computing are finding resistance in the health domain, 
particularly on application of innovations at the point of source. In their comment in Nature 
International Weekly Journal of Science, Julian H. Elliott (Infectious Diseases Unit, Alfred 
Hospital and Burnet Institute) and Jeremy Grimshaw (Professor and holder of the Canada 
Research Chair in Health Knowledge Transfer and Uptake) argue that in order to make sense 
of health data, we need to develop the science of data synthesis to join up the myriad varieties 
of health information (Elliott & Grimshaw, 2015).  
Formal methods for evidence synthesis  in which multiple sources of data are combined to 
obtain new insights  were first developed in the social sciences in the 1970s (Elliott & 
Grimshaw, 2015). The techniques have since been adapted in many branches of science, and 
they underpin high-impact decision-making, for example in drug licensing (Institute of 
Medicine, 2011). 
Yet, the problem of availability of data for en masse processing apparently existed in 2000 
and still exists today. Any delay in better and more consistently recording health data, in 
analysing these data more rapidly and in helping with their decision-making process has a 
human cost. In an interview, the Secretary of State for Health suggests that we need a very 
big cultural change around the use of data, data can be a great ally to improve patient safety 
the use of data is something I am absolutely determined to embrace in the NHS We need to 
make peer review and data transparency the main engine for change in the NHS (Hunt, 
2014). There is also a recent drive towards ethical, legal and societal considerations, given the 
governance requirements behind health data also internationally (Lamas, et al., 2015); 
alongside recent initiatives demonstrating the importance of combining electronic healthcare 
databases, for example to allow for large-scale drug safety monitoring (Coloma, et al., 2011). 
2.3 The network of networks  Digging the Web 
It goes without saying, that the World Wide Web (WWW) is evolving; from a simple 
representation of text and images to an interactive multimedia format without geographical or 
cultural frontiers and, more recently, to dynamic content of all sorts and formats. The format 
of the Web is influencing each and every aspect of our lives wherever or whenever we are. 
Tim Berners-Lee, the inventor of the World Wide Web as an internet-based hypermedia 
initiative for global information sharing on his first Mesh-related article (Berners-Lee, 1989) 
(WWWs name at that time) refers to a practical project that is extendible to new data 
formats.  
2.3.1 Research concentrates on a collaborative Web 
Currently, research is concentrated on the Web becoming more collaborative; there is a 
necessity for a more structured backend for the Web, as well as an understandable format for 
the frontend and any content (Daconta, et al., 2003). The same reference, suggests a maturing 
process that allows for both static content (text) and dynamic content (database information) 
to be parsed into forming XML documents for a single domain, to taxonomies and documents 
with mixed vocabulary, and finally, ontologies and rules. 
Inevitably, a process that introduced more, and more interpretable data, as per the above, 
introduces some concerns: 
 Overload of information  also known as infobesity (Rogers, et al., 2013) or 
infoxication  where the presence of too much information may cause difficult not 
only in understanding issues, but also in decision-making; 
 Stovepipe systems  where a process or system has the potential to share data and 
functionality with other system, but does not; 
 Poor content aggregation  where the lack of simple indexing strategies to more 
advanced taxonomies delivers ambiguous results, with additional rights issues. 
CHAPTER 3 Linking Data: From Information to Knowledge 
3.1 Annotating/Tagging content 
Linked Data is a sub-topic of the Semantic Web. The term Linked Data is used to describe a 
method of exposing, sharing, and connecting data via de-referenceable URIs on the Web. Tim 
Berners-Lee outlined four principles of Linked Data in a note of his (Bizer, et al., 2009), 
paraphrased along the following lines: 
1. Use URIs to identify things 
2. Use HTTP URIs so that these things can be referred to and looked up ("dereferenced") 
by people and user agents 
3. Provide useful information about the thing when its URI is dereferenced, using 
standard formats such as RDF/XML 
4. Include links to other, related URIs in the exposed data to improve discovery of other 
related information on the Web 
In 2006, Rob McCool proposed a very lightweight approach to making the Semantic Web a 
reality  mainly by adding some extra tags to existing Web content. Content annotation, or 
"Tagging", is a method that allows assigning freely chosen text labels to objects (regardless of 
them being exclusively descriptive or not) (McCool, 2006). When applied in a socially 
translucent manner (Erickson, et al., 1999), the method can result in stable patterns. 
There are cases against data annotation practices, from a practical perspective. If tools such as 
Human Language Technology (HLT) can perform the above task automatically, the question 
arises whether we should add annotations to data at all, given that we could apply the same 
HLT at data-consumption time. Manual annotation on the other hand, is slow, costly and can 
become inaccurate if an annotator fails to update it when human-readable content changes. In 
this case, annotation violates the one fact in one place paradigm, which had contributed so 
much to data consistency since E.F. Codd introduced it (Codd, 1970). 
For some time, part-of-speech tagging was considered an inseparable part of Natural Language 
Processing (NLP), because there are certain cases where the correct part of speech cannot be 
decided without understanding the semantics or even the pragmatics of the context. 
Examples (Santorini, 1990) show that this is extremely expensive, especially because a higher-
level analysis is much harder when multiple part-of-speech possibilities must be considered for 
each word. 
3.2 Metadata: Data about data 
Tagging can be used in interconnected schemata, with multiple tags assigned in one item or 
multiple items assigned to a certain tag. This, however, does not give the same semantic or 
other meaning to all original items with the same tags. There are other important attributes 
which count as metadata on top of the entities' metadata (i.e. the tags) thus generating a 
multi-level view which can both be machine-processed and also act as a presentation layer to 
the end user as it simulates a human's way of processing combined knowledge over a certain 
topic not only based on facts but also on beliefs about facts, time, importance, relevancy to 
subject and personal views and experiences. 
Metadata also has certain downsides. In the case of Web Feeds, for example, the metadata 
consist of simple nominal values, often just a standard date format (although anything can be 
represented, nothing is imposed). There is no agreed-upon mechanism to identify item authors 
or refer to items in more complex information architectures such as domain ontologies 
(Stefaner & Muller, 2007). For this reason, this research uses both content tagging and 
metadata generation for knowledge mining purposes. 
3.3 Information Extraction and Mining Techniques 
Information extraction (IE) is a type of information retrieval whose goal is to automatically 
extract structured information from unstructured machine-readable documents, generally 
human language texts by means of Natural Language Processing (NLP). 
A simple example of IE would be: 
"Yesterday, New-York based Foo Inc. announced their acquisition of Bar Corp." 
In the above statement, and from the frequency analysis of keywords, the substitution with 
synonyms (using word databases) et cetera, we can identify the topic and locate a predefined 
function in our Knowledge Base: 
MergerBetween(company1,company2,date) 
The entities can then be identified the same way, through either a prepared registry or 
through further mining processes; whereas the date/timestamp is ideally a reference to time in 
the mined set, or alternatively, the date/time of the recording of this relation: 
MergerBetween(Foo Inc.,Bar Corp,2010-12-31) 
Note the order of the entities; it is important that enough processing of the language is 
performed, so as to identify who the taken over is. 
Generally, five types of information can be identified with NLP: 
1. Entities: places, organisations, people, reference to time and currency 
2. Mentions: positions where entities are mentioned within the text 
3. Descriptions: of the entities present 
4. Relations: between entities 
5. Events: that involve the entities 
Finally, there is also a means of adding arbitrary logical expressions to ontologies, as well as 
combining the above entities, through axioms. 
3.3.1 Stream Mining 
A sub-field of knowledge discovery during the information extraction process, Stream Mining, 
addresses the issue of rapidly changing data; this it of outmost importance for our research, 
where the time variable plays a crucial role. Stream Mining defines the ability to update a 
system's models and ontologies simultaneously based on the stream of incoming data, often too 
vast an amount of data to be stored. Evidence from this input is incorporated in the business 
logic (model) even without storing the data itself. Machine learning and online learning 
methods are applied in the background; the model is initially built from available data and 
then updated regularly. 
3.3.2 Web Mining 
Web Mining another knowledge discovery sub-field, addresses Web data, and includes three 
interleaved threads of research: Web content, Web structure and Web usage mining (Cooley, 
et al., 1997). Web pages are themselves ontologies, and in Web usage mining (Chakrabarti, 
2002) one could analyse information on, for example, page views or frequencies and reconstruct 
the Website navigation pattern or model the users' behaviour. The same way, ontologies that 
fail or are hardly used can also be identified and then replaced or removed.  
CHAPTER 4 Exploring methods for knowledge discovery 
4.1 Introduction 
In the previous chapter, concepts such as Metadata and Web mining were discussed. This 
chapter expands on some of the issues behind data extraction from unstructured (e.g. Web 
crawling) and structured datasets. We explore the main attributes of an extraction system and 
experiment with company profiling as an exemplar through a collaborative project between the 
author and UK Trade & Investment. 
4.2 Entity Profiling 
Entity Profiling is a process that uses available and relevant information to describe the 
characteristics of entities and identify discriminators from other entities. The Web approach of 
this kind of profiling should be based on comprehensive and precise knowledge, which in turn, 
is generated from public, proprietary and limited-access databases and datasets, as well as the 
World Wide Web itself through Data Mining. It needs to be made clear at this point, that 
Web Entity Profiling does not refer to entities that only exist in the Web, but instead, entities 
that may exist anywhere but can be profiled using the above techniques. Key to the above 
process is to identify the differentiation of the major (descriptive and directly influential) and 
minor (with a cumulative influence to the entity's attributes) values of the entities. Entity 
profiling allows for a complete and accurate approach to setting and evaluating target 
strategies for handling the output datasets, from either human or machine defined process 
inputs.  
4.3 Data Extraction/Mining and issues 
Data Extraction from Web pages is an active research topic since 1997 (Kushmerick, 2002). 
Fayyad has proposed the below main steps for Data Mining (Fayyad, et al., 1996): 
 Retrieval of data from large sources (e.g. Web, Databases) 
 Selection of relevant subsets to work with 
 Decision on the appropriate sampling system, cleaning of data and dealing with missing 
fields and records 
 Application of the appropriate transformations, dimensionality reduction and 
projections 
 Fitting models to the pre-processed data 
Acquiring and annotating the baseline dataset of the proposed system includes (adapted from 
(Noy, et al., 2001)): 
 Enabling direct annotation of HTML pages with semantic elements 
 Providing connection to external reasoning and inference resources 
 Acquiring the semantic data automatically from text (or other sources) and 
 Presenting a graphical view of a set of interrelated resources. 
The bibliography also suggests there are issues to resolve in the above process since, in 
Webpages, there is no obvious way of differentiating text between valuable data and template 
text. Any word could be part of a template, or part of data, or both (Song, et al., 2004). We 
perform markup curation, where certain content extracted from pages is transformed info 
Document Object Model (DOM) trees, and with NLP and frequency analysis is marked as 
template text (Song & Zhang, 2002) with any important ill-formed HTML being revised into 
well-formed HTML at the same time before the master schema is populated and a cache copy 
is saved (Song, et al., 2004). 
4.3.1 Web Crawling 
We are implementing our in-house version of a Web Crawler3 that targets the extraction of 
knowledge from any Web source. The use of Web Crawlers is quite common, but when 
intelligence is introduced in the process, the processing power (and bandwidth required) 
increases dramatically. Distributed System designs are often proposed, in order to effectively 
use computing resources and thus, we are aiming at a parallel process ourselves (Thelwall, 
2001). Crawlers are normally multi-threaded, so that hundreds of Web pages may be requested 
simultaneously by one process (Jackson & Burden, 1999). If, however, the crawling task 
requires more complex processing, then clearly the maximum number of threads that can be 
efficiently handled will be reduced, and consequently the speed of the crawler too. 
4.3.2 Looping Issues 
The importance of optimisation (as suggested above) can be presented with an example. We 
have come across issues with infinite loops whilst crawling data. We utilise a process that 
identifies the loops, even in distributed systems, and blacklists the cause (Thelwall, 2001): 
1. An instance of the crawler identifies unexpectedly long jobs 
2. Verifies whether the reason is a genuine looping problem 
3. Stops the running thread (or pauses the process) 
4. Identifies the cause of the looping (logs, page content etc.) when required 
5. Adds the page(s) to a blacklist kept in a centralised location which is automatically 
read prior to the initiating of new jobs 
6. The process resumes from the previous position  there is no need to inform the other 
threads 
3 A Web crawler is a computer program that browses the World Wide Web in a methodical, 
automated manner or in an orderly fashion (Wikipedia, n.d.) 
4.3.3 Information Overload 
A key point is to avoid overloading with unnecessary data; a point we are also addressing and 
trying to cover from the early stages of our method.  In an effort to be as efficient as possible 
during populating our Knowledge Base, weak HTML page checking will be utilised (Thelwall, 
2001) in order to reject any new page (except a frameset page), which has a certain identical 
percentage of similar content to a previously retrieved page  this results in even more stressful 
processing from the crawlers part, but less cleaning of the mined output. 
The crawling algorithm will use intelligent methods to dynamically make decisions on issues, 
whilst providing the necessary audit trails for tweaks and optimisation. However, for our 
purposes, data repetition (once we verify that data is coming from unrelated data sources) is 
counted as an important factor; although the data is thrown away to prevent repetition and 
duplicates, the fact itself that we have an identical copy of some piece of information between 
two, otherwise unrelated, sources allows us to boost the confidence level in the systems 
metadata. 
4.3.4 Microdata  or how the Web delivers more concise information 
A current trend is for textual information data get shorter and shorter (e.g. success of Twitter 
et al microblogging systems). This is both an effect of the technologies used to publish content 
and communicate information also of the consumption behaviour of the users and the 
according social practices (Beale, n.d.). In terms of information management developments, 
information delivered in microcontent, can be structured by using microformats (Microformats 
Initiative, n.d.). These allow easy, machine-readable markup of content with XHTML being an 
example (Stefaner & Muller, 2007). 
4.4 Possible attributes of a data extraction system 
4.4.1 Conceptual Modelling and the EKR model 
A conceptual model of a system is a structure used to represent the requirements or 
architecture of the system. It is used for specifying and maintaining requirements for the 
system, in which a number of techniques and heuristics for problem analysis, function 
refinement, behaviour specification, and architecture specification are used. Generic diagrams, 
trees and tables can be used as well as conceptual components such as (Wieringa, 1996): 
 Structured Analysis (SA) for entity-relationship diagrams, data and event flow 
diagrams, state transition diagrams, function refinement trees, transaction-use tables 
and function-entity type tables. 
 Unified Modelling Language (UML) for static structure diagrams, use-case diagrams, 
activity diagrams, state charts, message sequence diagrams, collaboration diagrams, 
component diagrams and deployment diagrams 
 In extreme scenarios: JSD (process structure and network diagrams), recursive process 
graphs and transaction decomposition tables. 
Entity-relationship modelling (as a part of the Structured Analysis) is primarily a database 
modelling method used to produce a type of conceptual schema or semantic data model of a 
system, often a relational database and its requirements, in a top-down fashion. The 
definitive reference for entity-relationship modelling is Peter Chen's paper in 1976 (Chen & 
Pin-Shan, 1976). 
We have extended this model in an effort to include an additional layer (that mainly covers 
annotation), the Keywords, as well as a direct connection of these keywords and the entity 
relationships, with a Metadata Schema [Figure	3]. 
4.4.2 Relational Model for the Data Repository 
The underlying model of relational databases is the relational model. By extending the usual 
formal definition (Spielmann, 1995) (Lang & Lockermann, 1995) of that model with some 
constructs typically found in SQL-DDLs, we can apply a model that constitutes of: finite sets 
of Relations, finite sets of Keywords/Attributes, connections between these keywords and 
relations and primary attributes for the relations, together with keyword types and, optionally, 
atomic data types (Stojanovic, et al., 2002). 
If we compare ontologies with Relational Database (RDB) schemas, the former allow for 
representations of richer models of the world (Davies, et al., 2006). It is also, however, 
noticeable that any form of Markup Language used for Knowledge Representation is 
computationally expensive and thus, a model based on relational algebra allows for cheaper 
interpretation of the data that lies underneath it. 
Figure 3 - A proposed model for Entities, Keywords and their Relationships 
An RDB example allows for the management of huge amounts of data in a predictable and 
verifiable manner (Davies, et al., 2006). In the case of the Semantic Web, vastness is one of 
the major challenges of tomorrows data. In 2016, the indexed World Wide Web contains at 
least 45 billion Web pages (Tilburg University, 2016). Similarly, the SNOMED CT medical 
terminology ontology contains 370,000 class names, and existing technology has not yet been 
able to eliminate all semantically duplicated terms; there is a need for automated reasoning 
systems that can deal with truly huge inputs, thus covering vastness as a challenge, RDB 
approaches are best for handling sets of this size. 
However, the semantic Web is not only about common formats for interchange of data, but 
also about relationships among data, which sharply distinguishes the former from the current 
generation of the Web. Many current methods for accessing tagged data/metadata falsely base 
themselves only on SQL-like syntax, to retrieve exactly matching results in disorder. 
Therefore, on the presentation layer of an RDB approach, specialised search mechanisms to 
support inference and properly order search results are important for the success of an entity 
profiling system (Ning, et al., 2008). 
Our process does not incorporate the full SQL-like standard, as many parts are still vendor 
specific (e.g. use of BINARY or BLOB for binary data types) and not all databases implement 
the full standard. Generally, it can be said that all behavioural parts of SQL (like triggers, 
stored procedures, referential actions) cannot be mapped. This is also true for SQL queries. 
Built-in functions (such as count, min, max) and ordering cannot be mapped either. For this 
reason, we are proposing our own metadata schema format. We also reinstate that the goal of 
this mapping is to preserve a maximum of information under the ontology framework. It is 
important to state that the process of using a pre-generated schema cannot be lossless, but 
needless to say, almost all machine-understandable and machine-interpretable information is 
preserved. A side effect of this, making the semantics of a database more explicit, will also be 
addressed with our approach leading to more formal semantics. Using this approach, we can 
maintain data-driven applications in an easier, slightly simplified fashion. 
4.3.3 Pseudo-Intelligence and Ontologies 
Newton's 3rd Law of Motion states that every action has an equal and opposite reaction. We 
can apply this theory within our application by saying that every input will provide us with an 
output formed according to the input. This is called AI (Maloney, 1984). However, in our 
research we use pseudo-Intelligence. We use the term pseudo, not to claim the use of 
complex and absolute AI, but rather, to show that the system produces output according to 
predefined actions. Artificial Intelligence (AI) researchers have used ontologies for a long time 
to express formally a shared understanding of information (Noy, et al., 2001). 
4.5 Semantic Mappings with Elastic Lists 
The presentation of Semantic Mappings it an entire field of investigation in itself and certain 
techniques have been referenced in a 2001 survey on schema mappings (Rahm & Berstein, 
2001).  
Halevys team, indicates that given the peer and stored relations, their mappings and a query 
over some schema, in order to answer the given query, data from the stored relations must be 
used. To formally specify the problem of query answering, we need to define the semantics of 
queries (Harvey, et al., 2003). The same reference indicates that Query Reformulation 
Algorithms may be applied for this purpose, mainly utilising rule-goal-tree optimised methods. 
Furthermore, mined sets frequently need to be updated and thus, a dynamic approach to 
resource creation, through the use of Elastic Lists4, allows the ad-hoc creation of documents 
based on dynamic and nested content blocks (Stefaner, et al., 2008). These blocks constitute a 
human readable and machine processable representation of an identifiable piece of business 
knowledge. It is granularly structured for reuse in single sourcing environments (Websites, 
Web Apps, documents) and appropriate for content management processes and automation. 
4 Elastic Lists are a principle for browsing multi-facetted data structures (Stefaner, n.d.) 
Visualisation in elastic lists uses weighted proportions and emphasises mainly on the 
characteristic values of local metadata profiles and animated filtering (Stefaner, et al., 2008). 
Intelligent systems allow to facilitate content access (and browsing) and provide the end users 
with the ability to more easily understand content distribution. The literature, however, 
suggests that there are always ways to provide a more complete view of data, especially data 
that involve background statistical analysis (Heer & Robertson, 2007). 
4.6 Experimentation  Company Profiling 
Our aim during experimentation is to address each of the above items in an effort to construct 
lists of data attributes and provide spherical views of a topic or multiple topics at once, in a 
fraction of time compared to generating knowledge manually. 
For this reason, we ran a first round of experiments with company profiling experts and IT 
experts. The team had expert knowledge in identifying the key data about companies and an 
expert knowledge in data mining so as to allow for the replication of the profiling process and 
migration to a machine understandable one. The team was asked to produce 10 company 
profiles from a set list of international corporations that operate in the UK market amongst 
others and then present the generated knowledge: 
 We recorded the Business Data Sources the experts used as part of the company 
profiling process [Figure	4] 
 We generated a detailed but more generic list of the Data Sources that need to be used 
for Entity Profiling in general, divided in type of content (e.g. unstructured/semantic 
Web) and format of data (e.g. markup/feeds/corpora) [Figure	5] 
 We used the experts advice on the important assets of a company, to generate: 
o (a) an ordered list of screening criteria, sorted by importance [Figure	6] 
o (b) a scoring system that uses this criteria amongst others [Figure	7] 
Figure 4 - Recording Business Data Sources in need of Mining	
 We generated a table of specific attributes in an organisation [Table	1] as well as 
calculation formulas for assessing important financial variables [Table	2] 
Figure 2: WebSchema is able to intelligently parse data from a vast amount of different sources.
As the system uses Natural Language Processing (NLP) it mines data and generates knowledge.
WebSchema
Repository
Unstructured and 
semi-structured Web
HTML/PHP 
(code)
(annotated sets)
ATOM/JSON
(rss feeds)
(vector graphics)
Web Pages/
Content
Feeds/Sets
Semantic Web and 
other sources
Structured Data 
sources (e.g.:)
Semantic 
Databases (e.g.:)
WikiPedia
RDF-based 
Schemata
(e.g. DBpedia, 
BBC, massive set 
exploitation)Social Media
(e.g. Facebook, 
Twitter, Blogger)
Company Data
(e.g. Google Maps, 
Finance, News)
Corpora
Figure 5 - Tree diagram of the Data Sources (by data type and semantic layer)	
Figure 6 - Identifying screening criteria by relevance to the 
required output	
Figure 7 - An experimental Scoring System for company assessment	
Table 1 - A breakdown of the Financial and Operational Risk Variables being considered (Koyuncugil & 
Ozgulbas, 2012) 
 Based on the assessment of sectors and companies, we generated a checklist of 
opportunities and threats for sectors [Table	4] and organisations [Table	3] 
 We mapped the profiling process to a machine generated one, and created a flow 
diagram out of that process [Figure	3] 
 The manual/human analysis of individual documents on the Web, resulted in a flow 
diagram that, once again, automates the above [Figure	9] 
A summarised version of the entity profiling process is described here: 
1. Begin with a set list of entities 
2. For each entity, scan the Web. This is done via a Java-based component which 
utilises search engine APIs. We combine visible-Web engines (Google) and semantic-
Web knowledge engines (Wolfram|Alpha) 
3. Compare the end data items, as well as their content, with existing content in the 
systems Database (using checksums)  this way, duplication and un-necessary effort in 
analysing content can be avoided 
a. If the same source has newer content (whereby the checksums dont match), 
both data items are stored, with their relevant access data and parsed content 
Table 2 - Calculation and assessment of the most important Financial 
Ratios (Johnson, n.d.) 
creation data (where it exists)  maintaining references to both the older the 
newer content allows for comparisons, for example time-series analyses. Where 
the frequency of changes allows consecutive measurements of changes in 
content, patterns can be identified  
b. If the content is the same, ignore the retrieved data item and process with the 
next entity 
Table 3 - Positive, Neutral and Negative Indicators by company 
4. Identify the nature of the entity on the Web by executing queries in databases that 
contain categorised or semantic content (for example Google Finance, Google 
Places/Maps/Local, Google News and sources other than Google) to tag the entity 
with data such as Name, Location, Area of Interest 
5. Identify Websites and individual Web pages based on the results above via an 
enrichment of the search terms; crawl all relevant information (for example look for 
about, product, people, news, annual report/stock/turnover or other keywords 
 keywords which vary depending on the type of entity from the results of the previous 
step above) 
6. Locate references to time, otherwise record the current time. This is two ways: 
a. In simple terms, via the comparison of dates where an online document was 
retrieved, with the data of a revised document which simply provides an 
indication of the recently-retrieved content being the current one 
Table 4 - Positive, Neutral and Negative Indicators by sector 
b. In more advanced, linguistics terms, with temporal relations in the content 
itself, such as beginning-postspan (e.g. phrases with the use of since) and 
prespan-end (e.g. with the use of until) relations 
7. Semantically annotate the obtained dataset 
a. Concept extraction is applied as a means to comparing data points within an 
entity. [Figure	11	-	Conversion	of	Knowledge	to	Wisdom	(Colour-code	specifies	
type	of	impact)] shows some of the key terms defined and used in an example 
extraction, such as Positive, Negative or Neutral terms 
b. Identify connections between entities themselves, and between people. The 
relationships between the extracted concepts are identified and interlinked with 
related external or internal domain knowledge 
8. Apply a simple means of evaluating content: 
a. Validity is assessed by looking up Web domains and content sources on 
blacklists (e.g. MXtoolbox Health) 
b. Confidence level is a metric derived from the amount of times connection 
between entities appears in the retrieved results. 
9. Export and visualise the results in different formats, for example with the use of 
Elastic Lists (Stefaner & Muller, 2007). Elastic Lists particularly aid the appreciation 
of content by humans, due to the ability to provide two-way filtering, metadata 
selection that filters end-content, and content filtering that presents metadata. We 
have experimented with ProtoVis (http://mbostock.github.io/protovis/) as the 
visualisation library. 
In the above process, summarisation is key, with an effort to present data as much accurate as 
possible. [Figure	10] shows the above process in a flow diagram and finally, [Figure	11] shows 
an example of the output with Positive, Neutral and Negative generated knowledge for an 
example company. 
display decision 
making (confidence 
alterations)
Database, RDF 
Schema, disk et al
start
input: One company 
(Name, Keywords)
identify 
(WebCandY)
crawl website
display cleaned page 
content, progress 
estimate
check Google 
Place data
verified 
correct?
crawl news items 
paralelly w/ 
dynamically defined 
sliding window
output AI decisions 
and news/financial info 
traced
semantically 
annotate data
display the conversion 
of data to information
identify key 
entities w/ 
gravity on 
Abouts and 
display key entities, 
connections, gravity produce 
summary of 
results
display results in 
readable format
allow alteration 
of data, as well 
as reproduction 
from schemata
Figure 8 - Flow diagram (Data Input and Output level) of the prototype Entity Profiling system	
Document Format
(XML. HTML, TXT etc)
Cleaner (e.g. 
Markup removal)
Markup Fixed List
Tokeniser (POS, 
concordances, 
collocations etc)
Multilingual Corpora
"Weirdness" analysis
Dynamic Word Frequency 
Tagger / Annotator
Schema Population
Metadata Populator 
(for Data 
Provenance)
Outcome Protocol, 
Loosely defined lists
Figure 9  Data processing layers	
Start
Analyse entities, apply WebCandY
Data Input
(set of entities)
Output
Finish
Information lookup
(website, market, location etc.)
Crawl websites
(entity pages, products, referring 
websites)
Apply NLP algorithms
Webmining, confidence algorithm, 
fuzzy logic
Process collected set, prepare 
visualised result
Figure 10 - Data transformation sub-system	
4.7 Experimentation  Domain name identification 
The purpose of this experiment is to enable Web crawling and scraping, through the use of key 
search engines as well as through HTML and metadata parsing. After semantically tagging the 
content, the system will build a confidence metric on the best representative set of domain 
names for a particular type of input query. For example, we expect a query such as large 
CPU vendor to return results such as Intel, ARM and Qualcomm amongst others, 
together with their key domains (company domains or representative articles of the notion 
large vendor) such as www.intel.com, www.arm.com and www.qualcomm.com. 
The above process aids human decision-making by speeding up the identification of related 
content for any given set of entities. In a collaborative project with UK Trade & Investment 
(UKTI), this prototype system was able to accurately propose domain names and URLs, as 
well as extract metadata, for hundreds of input queries that regarded company profiling. 
Before the rollout of this prototype system, the process would involve manually looking for 
News Sources
bbc.co.uk news item
Keyword Processing: POSITIVE
americanchronicle.com article
Keyword Processing: POSITIVE
dotmet.com news story
Keyword Processing: NEGATIVE
Company Website
Data mined from Company Profile
NL Processing: NEUTRAL
Data mined from Homepage
NL Processing: NEUTRAL
Exclusion Criterion due: NO INFO
Data mined from About Us
NL Processing: POSITIVE
International Trade Shows
bio-equip.cn Bio Equip, Japan
Semantical Analysis: HIGHRANK
Confidence Criterion: RECENT
Sector Industries
rsna.org Radiological Society of 
North America
Word DB Analysis: NEUTRAL
Exclusion Criterion: UNIMPORTANT
Export/Trade Directories
allbusiness.com on Health-care and 
Medical-practice subcategories
Keyword Processing: POSITIVE
Exclusion Criterion due: OLD ITEM
Example Company XYZ
(Trading in UK as Ultrasound Therapeutics)
Result: GREEN-Investment Recommended
Confidence level: 8 out of 10
Alternative company names, UK trading sector, Origin, 
Product Name(s), Product Area(s), Web sources, 
Company Website(s), Media News Sources, Int'l Trade 
Shows, Sector Industries, Export/Trade Directories
Identified Information
Figure 11 - Conversion of Knowledge to Wisdom (Colour-code specifies type of impact)	
information relevant to companies together with positive, negative or neutral keywords via the 
use of online search engines manually triggered by analysts. 
The prototype system includes a Graphical User Interface (GUI) which allows experts to flag 
any machine-introduced errors during the process; the content is validated as it is being 
accessed. This way any erroneous results can be either discarded or excluded from further use 
by the analysts. 
By building a system that explores a dataset as diverse as the Web to provide faster and more 
accurate information to UK Trade & Investment (UKTI) analysts, we will then attempt to use 
the system in a different context, this time in the medical domain. We will attempt to 
assessing the success rate, the errors and general applicability of the same level of analysis in 
that domain. 
4.7.1 Lessons learnt from the Web scraping system implementation 
During the experimentation with entity profiling as part of the exploratory research, a key 
problem is identified; without a good understanding, from the researcher, of the structure and 
idiosyncrasies (and later on the format, whether structured or unstructured) of Web content 
the setup of a system that analyses said content results in a variety of error cases. This 
research would amalgamate algorithms and apply tools that can create an end to end approach 
to help analysts with what, so far, would have been manual Web research. The more rules for 
the machine, the better the understanding of Web content that is relevant to a given entity 
input prior to delivery to the analyst.  
In terms of input, we are looking at key phrases such as ASIC manufacturer UK. This may 
seem like a spot-on request for a search engine, but a search engine will, other than employing 
sophisticated and/or proprietary algorithms for page-ranking or content analysis, will merely 
be based on metadata from the HTML file header. Usually with a bit of bias, the result 
returned from a traditional search engine will be a domain name with good-enough SEO, 
references to the keywords, and a lot of gravity on the metadata title and description. For our 
system, there will be a combination of core domain names and other URLs (including, for 
example, social media shared content) after parsing each result and applying Natural Language 
Processing (NLP) and Data Mining individually. 
4.7.2 Method breakdown 
We decided to name the system that would aid UKTIs analyst work, WebCandY (short for 
Web Candidate Yielding). WebCandY comprises sub-systems that analyse entities by: 
 Looking up for domain names and URLs that consist of the entitys name(s) in many 
variations (e.g. simply combining the keywords and variations of the keywords), then 
applying NLP to any registered domain names returned. 
 Running distributed searches on major engines, as per the above technique and also by 
trying several variations of the name and keywords, including exact match 
approaches, inclusion and removal of low-gravity words (e.g. ABC Limited UK to 
ABC Limited) and substitutions (e.g. Limited to Ltd and vice versa) 
 Filtering the result set with a function that orders by top-level domains first, removes 
domains that belong to ignore lists (e.g. amazon except if amazon is being searched 
for, eBay except if eBay is being searched for, although this is dependent on the use-
case) and then as a second iteration, calculating the domains lengths and hit accuracy 
 A equivalent method to Weirdness analysis (Ahmad, et al., 1999) is applied next, 
using word frequency from language corpora metrics, to change the level of confidence 
for all hits and identify which of the search engine results are simply because of 
synonyms or common words and phrases 
 Storing all the above results together with URL metadata that can help fast processing 
and error detection by the analysts, the most precious of which is the Confidence flag. 
Confidence is a variable without a set limit that shows how close to a positive result 
the system is. 
The above functions contribute to the final output, being the presentation of results based on 
the Confidence level  including all intermediate data (but excluding any obvious mishits per 
the above methods). 
The system uses standard techniques for filtering/cleaning both of the initial and of the 
resulting datasets (for example for the removal of unwanted tags using the BeautifulSoup 
python library). In an effort to limit the classes of data that may be processed by the system, 
we apply tokenisation, a critical step in the entity analysis process (Buettcher, et al., 2010). 
Converting the input data items to raw text as plain character strings allows applying the sub-
methods presented above. 
4.7.3 Experimenting with a real-world UKTI dataset 
We tested the algorithm using a list of 764 names of companies that are currently active in the 
worldwide market. The sizes vary, and most have at least some activity in the UK. Generally, 
the list is as normalised as possible prior to initiating the experiment. For example, all the 
entities in the list operate in a small range of sectors. This is to prevent biased results or 
domains very easy (or difficult) to track the presence of. We run several iterations, beginning 
with the search engine lookups and are now implementing the complete filtering mechanism, 
the confidence assignment and failsafe mechanisms (Fox & Brewer, 1999) (e.g. use of different 
channels through distributed services or various search engines).  
We have achieved a high level of accuracy on the tested dataset and the evaluation of our 
proposed system shows we can export usable results without any human interference at all. On 
the first iteration, WebCandY correctly and accurately identified an average of 72% second 
level domains (SLD) for the input company title list. The sample [Table	5] shows the response 
query for evaluating the result-set. The identified domain appears on the first column, followed 
by the Web pages metadata and the initial entity name requested. An important note is that 
any apparent mishits only appear in an average of 6% of the individual cases  a percentage 
that we will be minimising during the next iterations. Also, the remaining 22% that forms the 
remaining wrong domains, points to the correct resource, but on third-party Websites (for 
example news agencies or trade shows), this is a positive outcome as we will be keeping these 
results to then process them linguistically as part of the larger entity profiling system. 
Table 5 - Identified domain, Domain metadata and the Original entity name and Reference - we achieved a 
72% average success rate during experimentation 
4.7.4 Preliminary evaluation of the method 
After assessment of the output results by analysts, WebCandY demonstrated that it can 
enhance decision-making with its ability to facilitate entity-profiling. Specifically, it introduced 
a control theory concept and feedforward mechanism utilisation that provided stability to 
systems (or processes in the case of this analysis), keeping state errors within uniform bounds 
especially during further Natural Language Processing of the results (Kuc, et al., 1992). The 
confidence of the results can save time in otherwise exhausting data extraction processes. 
Although the claim that an automated, online system, can facilitate the decision-making 
process for human analysts may appear trivial, it is in fact the human input requirements in 
optimising said system and its performance that usually have the opposite effect. 
Traditionally, all levels of natural language processing require a substantial amount of human 
knowledge and interaction with physical entities; we are trying to minimise that necessity but 
still provide the same quality of output. 
4.7.5 Next experimentation steps 
It would be important that the same mechanism be applied to other types of datasets than 
just the Web, such as proprietary databases or closed-source systems with API/RESTful 
integration with free search or even with direct extraction tools. 
Also, Buettchers team in Information Retrieval, suggests that a ranking mechanism can 
play an important role to the accuracy of the results (Buettcher, et al., 2010). Buettcher uses 
an ad-hoc retrieval model, associating informational search queries on a document collection 
similar to the Web (government repositories). Ranking is often reduced to the computation of 
numeric scores on query/document pairs; a baseline score function for this purpose is 
the cosine similarity between TFIDF vectors representing the query and the document in 
a vector space model, best matching scores (BM25  essentially a bag-of-words retrieval 
function), or probabilities in a probabilistic Information Retrieval model. The use of processes 
to rank information (e.g. from Google and their PageRank algorithm or similar 
implementations e.g. Alexa) as part of WebCandY is a proposed future step. Another item of 
future work would be to test the method with as many popular search engines as possible 
taken into account. In a best-case scenario, different types of output from crawlers and search 
engines can be combined together with the use of Collective Intelligence (Segaran, 2007). In a 
similar approach, the metasearching process is usually taken to mean the combination of 
results from autonomous search engines, as surveyed by Mengs team (Meng, et al., 2002). 
Due to the aforementioned involvement of the researchers in a healthcare project (Quality 
Improvement in Chronic Kidney Disease) at St. Georges University in London and the 
University of Surrey as a Research Associate, which would regard extraction of patient 
information, the next natural step for this research was to apply the lessons learned in health 
care datasets. 
CHAPTER 5 Method for solution-orientated reporting of 
extraction errors of clinical data for research and quality 
improvement 
5.1 Introduction 
Following experimentation in unstructured Web data in the previous chapter, we attempted 
routine data extraction from health data repositories and Electronic Patient Record (EPR) 
systems as part of studies for research and quality improvement. The initial effort 
concentrating on identifying the diversity of these systems and the different errors likely to be 
encountered on data extraction  even when involving structured data. The findings were 
presented by the author of this thesis at the 13th World Congress on Medical and Health 
Informative (MedInfo) in 2010 with various attributions since. 
5.2 The need for a structured approach  
Internationally, routinely collected clinical data from primary care electronic patient record 
systems (EPR) is used for research and quality improvement (de Lusignan & van Weel, 2006).  
However, many of the research databases only draw their data from a single vendor; thereby 
circumventing many of the difficulties due to variation in the way that national and 
international standards are implemented in different brands of EPR system.  By way of 
contrast the Primary Care Data Quality (PCDQ) programme has worked with large datasets 
drawn from different vendors some using different classification systems and generating their 
own local codes to supplement standard taxonomies: truly heterogeneous distributed databases 
(de Lusignan, et al., 2006).  The largest PCDQ study drew data from 2.4 million patient 
records (de Lusignan, 2007); current studies work with databases of just under 1 million 
records drawn from six different brands of EPR system but extracting several hundred 
variables (de Lusignan, et al., 2009). 
The difficulties in extracting data from heterogeneous distributed sources are well known 
(Elmagarmid, et al., 1998) and standard methods and toolkits for measuring the validity and 
utility of electronic patient record systems have been proposed (Hassey, et al., 2001). The 
difficulties arise because of the different architectures of the heterogeneous distributed systems, 
the local autonomy of these systems, problems in representational diversity of the same clinical 
concept, and the potential lack of precise semantic meaning.  None of the classification systems 
within the UK have definitions, making it possible for meaning to vary between professional 
groups and over time (de Lusignan, 2005).  There may be a trade-off for vendors between 
achieving functionality and strict adherence to guidance on requirements of EPR systems. 
Difficulties can then arise because standards are not strictly implemented. 
We frequently encountered data extraction problems, which we historically reported in terms 
of what data were missing from our response files rather linking our problem to the underlying 
cause.  We carried out this study to see if we could develop a system of solution-orientated 
error reporting, which improved our problem solving and predicted likely time to resolution. 
5.3 Method 
We carried out a literature review on the standard bibliographic databases to identify 
structured approaches to data extraction techniques from heterogeneous distributed databases. 
We narrowed our search to clinical data and databases. We tried to identify any current 
approaches to identifying and resolving data extraction errors apply prospective and 
retrospective statistics with dynamic validation as data are being extracted (Tatum, et al., 
2000).  
Our literature review initially focussed on the UK National Health Service methods of data 
extraction.  We also looked for proprietary tools for data extraction provided by EPR vendors.  
We found two generic approaches to enable enquirers to execute queries and extract data from 
different types of general practice computer systems using a common query language. The two 
alternatives are the MIQUEST (Morbidity Information and Export Syntax) HQL (Health 
Query Language) data extraction tool and the proprietary Apollo SQL interface. PCDQ uses 
the MIQUEST data extraction tool.  
We then collected information about the experiences of five data collectors during 2008 and 
2009. Interviews with the collectors, observations, documentation reviews and test data 
extraction queries were used as information gathering techniques. The study exports included 
detailed descriptions of errors, frequencies of occurrence and comments on specific issues, 
system installations and system versions. A post-hoc exploration process was carried out. 
We constructed an initial list of the errors encountered and classified the errors according to 
their effect on the data collection process. This included; information about whether the query 
would execute at; the stage each query would run to, and whether it produced partial or no 
results at all. We then reviewed and categorised the observed data collection issues based on 
their impact. We subsequently redefined the data collection issues to create a briefer but 
nevertheless descriptive functional IT component approach to facilitate problem solving. 
Finally, we reclassified the improved list of errors based on our need for a system which 
enabled understanding of whether certain groups of errors could be resolved by the collection 
team or not; and finally whether they were vendor specific. This led to the creation of a 
taxonomy of errors for our data collection problems. We then created an online resource for 
the PCDQ data collectors.  The online problem reporting form was designed to capture the 
key information needed to diagnose the IT component which was responsible for any errors. 
The studies carried out during the development of our taxonomy were ethically approved, and 
only used pseudonymised data. 
5.4 Results 
5.4.1 Errors during the Data Collection Process 
We identified 94 problems with the data extraction from the four major UK GP electronic 
patient record (EPR) suppliers: EMIS PCS, EMIS LV, INPS Vision and iSOFT Synergy. 
These four EPR systems account for over 90% of GP EPR market for England (Moulene, et 
al., 2007). 
Problems of different levels of severity and impact were identified and initially mapped to a set 
of groups in a purely clinical use driven approach. The frequency of the type of problems 
encountered is summarised in [Table 6].  
In summary, errors were reported in the following broad categories: 
(A) MIQUEST  the data extraction tool did not work, or the query code failed at some point, 
(B) The MIQUEST specification was differently implemented on one of the brands of EPR 
systems. For example, the word CHOSEN returns different response files, 
(C) Clinical System and database would not return information, 
(D) Supporting Software and operating system, 
(E) Hardware and Infrastructure, 
(F) Generic/Uncategorised and/or Human Errors. 
We documented the cumulative frequencies for each issue based on each individual collector's 
recordings and assigned them to any category they applied to. A more thorough analysis of the 
above categories, together with examples of errors collected during the process, is presented in 
section 5.5 below. 
5.4.2 Multiple mappings 
This type of direct mapping did not allow for an optimised approach to error solving. We 
found that 56 of the 94 errors (60%) could be assigned to multiple categories as shown in 
[Figure 12]; a limitation of our initial categorised reporting. 
Table 6 - Stage Process Categorisation Frequencies 
5.4.3 Post processing of the error list 
We initiated another round of result interpretation to identify the generic functional IT 
standings of each specific issue and generate a list with strictly targeted errors that could be 
classified to one of our newly defined categories. 
5.4.4 Taxonomy of errors 
We then investigated whether the errors identified could be mapped to a single root cause  
recognising that this may involve careful analysis as to which IT component was responsible 
for the error encountered. We connected each of the redefined issues to the respective group 
described in [Table 7]. The new definition allowed for identifying the point of contact as well 
as an estimate on the delay for solving individual cases. 
The list was structured from the data collectors' experiences in the field, response time average 
and contact points. 
Figure 12 - Frequencies for Multiple Categories 
We completed our direct mapping of each individual case to one category through several 
iterations; based on comments and feedback from collectors, IT resources and documented 
processes.  This enabled us to summarise all the errors into the categories and export metrics 
as in [Table 8]. 
This process, involved merging a total of 15 error descriptions with closely related ones. This 
created an accurate assignment to the various categories of secondary level processing. 
Of the 79 final individual error types, category C (Clinical EPR System) was most frequent 
and found in 30 of these 79 problem categories  requiring input from the vendor.  This was 
followed by D (Network and Operating System) and B (Data Extractor Specification) with 16 
and 14 occurrences respectively. Categories A (Query Content), E (Hardware) and F (Other 
Human Errors) had 6 and 7 error types recorded for each. Around 40% (30/79) of the 
problems could be solved by phoning software support for the particular brand of EPR system, 
20% by actions from the data collector, and 10% by senior team member. 30% could not be 
resolved by the team in the expected timeframe and required some external input. The 
majority involved external resources and required changes to the underlying hardware or 
Table 7 - Component Oriented Approach 
technical intervention with the on-site systems; albeit usually delivered remotely by the 
vendor. 
An online structured system for the initial reporting of errors was implemented. We used a 
project management platform and divided the tracking mechanism into several sub-systems for 
different projects (QICKD (de Lusignan, et al., 2009), IAPT (an evaluation of Improved 
Access to Psychological Therapies), Osteoporosis, and Diabetes studies) but with the same 
underlying format for cross reporting. A set of optional and required fields, allowed for 
immediate connection to a predefined taxonomy. 
Table 8 - Taxonomy of Errors and Frequent Cases 
5.5 Practical examples of errors and solution orientated reporting 
We report exemplar errors in each of the seven areas and the action taken and time taken to 
solve them: 
(a) Data extraction queries and process problems: One brand of EPR vendor creates its own 
local codes to plug what it perceives as gaps in the standard (Read code) hierarchy. Unless 
queries collect these local codes this data area is deficient. Smoking data provides a good 
example - where sometimes local codes have accounted for >10% of the data.  The solution 
took under two weeks, involving query re-writing and exploring with clinic doctors how these 
patients were represented in the EPR system.  
(b) Extraction system errors: The INPS Vision and EMIS PCS systems can take several 
minutes to extract large response files during which the session cannot be interrupted. We 
prevented this by logging in another session on the same workstation on the proviso that login 
details are held. We asked that these details be provided so that restarting the extraction 
would require less time. Interference from support staff would often take hours or require a 
revisit. 
(c) Top level system and database errors: Systems are not designed to handle large-size result 
files. EMIS LV in particular, is negligent for not having sufficient free database and disk space 
which is an essential prerequisite for data extraction. This often resulted in system crashes if it 
was utilised for clinical purposes at the same time. We made sure at least 100MBytes are free, 
to avoid interruptions and system restarts that would take 30 minutes or more. 
(d) System and communication errors: Due to insufficient user privileges, execution of queries 
is sometimes impossible. We used accounts with >Level 4 access to MIQUEST. Also, the 
interface would sometimes incorrectly indicate the current load. We were proactively ensuring 
that we had enough resources than the stated, to prevent having the support staff sort it for 
us with a 15-minute to 1-hour addition to the collection. 
(e) Hardware and infrastructure errors: iSOFT Synergy and Premiere are both vulnerable to 
complex query sets when an external reporting server is not being used. We found that even in 
cases where a reporting server was installed, we had to use the live clinical system because of 
poor maintenance and data that were not up-to-date or incorrectly linked. 
(f) Human errors: Experience has shown that human errors were also commonplace even in 
some instances where communication had been successful. On occasion, queries had been 
removed from the system by practice staff unknowingly, where researchers had scheduled set 
execution times or where practice staff needed to execute internal queries or maintenance tasks 
(the data extraction tasks get lower priority than the system maintenance processes). 
5.6 Discussion 
5.6.1 Principal findings 
Data extraction techniques are widely used to answer research questions from routinely 
collected clinical data. Problems are faced during data extraction. Most appear to be 
associated with the way specific data extraction engines have been implemented by the 
different EPR system vendors. The adoption of this error taxonomy would enable consistent 
reporting to system manufacturers and potentially improved efficiency in data collecting. 
5.6.2 Implications of findings 
Our system imposed a much more analytical approach to error reporting and handling from 
data collectors. Errors were assigned to all categories. By initially assessing and evaluating 
errors from previous but similar data collections, we defined a set of categories that are fairly 
normalised both in terms of severity of issues and in terms of frequency of errors (prevalence). 
We ensured that the negative influence of errors in each individual category would be equal, 
before apply the categorisation/groupings. The process allowed us to be more accurate, fast 
and proactive. For example, the common error of queries being randomly interrupted was 
originally handled as a query writing issue. Statistically, and based on our classification and 
findings, there are less chances of this error feedback because of a syntax or vocabulary error 
and the failure output is almost directly connected to automated backup processes on the 
reporting server or other maintenance issues. On another example, the traditional way of 
handling long-running queries was to generate subsets even in cases where the practice systems 
had a specific (in most cases easy to resolve) issue. With our process, we would classify the 
error, check the most probable causation and proceed with the collection without rescheduling 
a visit which involved writing vendor-specific subset queries in-between, using valuable study 
time and resources. Also, the taxonomy helped us provide feedback to the EPR vendors (and 
MIQUEST) as in a recent example, new data together with the inclusion of post-collection 
validation error information pointed to a vendor bug with a false return of text data type ACR 
values instead of numeric ones, resulting in gaps inside the collected data. This was flagged, 
the developers were notified and a fix was introduced prior to our next collection. We found 
that the duration of this process was less than our estimate for rewriting the queries, deploying 
and executing them as well as any changes to our analysed flat-file generation mechanisms for 
converting the inconsistent data type. 
We followed a set of principles for research projects we have been involved in, for example the 
London Life Sciences Prospective Population Study (LOLIPOP), where we would provide a 
follow-up of the cohort. This was done by collecting routine health care data (primary and 
secondary) for all of the patients that have agreed to participate in the LOLIPOP study - all of 
which was held in an anonymised format at the University of Surreys secure servers. While 
running the data collections we found that early steps and precautions also allowed for minimised 
error frequency. For example, the use of mechanical processes for query writing, code execution 
and system maintenance minimised errors in categories A and F (we followed the commonly 
used reusable code principle via component based engineering for our processes), whereas 
documented solutions to usual problems on software-level (for categories B, C and D) allowed 
for error solving by the collectors themselves. Finally, articulating the least acceptable hardware 
features and specifications before a data provider joined a study minimised the impact of any 
issues with the infrastructure (category E). 
The error classification through the taxonomy we implemented makes error reporting a 
solution-orientated approach. By solution-oriented, we mean that we adopt an active (or 
proactive) approach to solving problems, as opposed to dwelling on dilemmas, especially in 
tightly-governed and regulated environments. It allows for the flagging of the nature of any 
obstacles combining precautions as well as immediate action thenceforth.  Problems which 
were frequent 12 months ago no longer feature on our error list, these include but are not 
limited to: Some human errors, disk space issues where feedback would be inaccurate, crashes 
on the server because of lengthy or problematic queries and shared folder issues where the 
mapping of local drives needs to be set up by staff with sufficient privileges. This process 
provided team members the confidence to approach vendors or explain the limitations of 
hardware or software to healthcare providers participating in research. 
5.6.3 Comparison with literature 
We identified a dearth of literature on error reporting.  Though much is written about how 
data from EPR systems are expected to have a central role within healthcare commissioning, 
and quality improvement (Thiru, et al., 2003).  
There are generic IT approaches to problems with data extraction: namely the resolution of 
possible data conflicts occurring in the database integration process; incompatibilities between 
databases, differences in data types; and copies of the same information stored in different 
databases (Madhavaram, et al., 1996). There are examples of logging mechanisms able to 
identify errors either in extraction itself or the underlying EPR system data (for example, the 
miscoding of family history as heart disease resulting in apparently 25% of practice population 
as having this diagnosis) (Oxfordshire MAAG, 2000). The above reveal the need for a 
structured error handling process. The literature recognises the problems with heterogeneous 
distributed databases as well as the cost and effort for overcoming them without a standard 
well-defined and designed process (Elmagarmid, et al., 1998) (Madhavaram, et al., 1996). 
The UK national data quality programme PRIMIS+ discussion board illustrates how data 
extraction problems extend widely; but does not incorporate any sort of error taxonomy 
(University of Nottingham, n.d.).  
5.6.4 Limitations of method 
We found that on rare occasions the translation of an issue that can be connected to multiple 
categories has a slight change on its meaning (not on its effect, however) when redefined for 
our functional approach. Also, for a number of problems we had to analyse feedback from the 
data collectors several times, in order to define the most appropriate category definition and 
the course of action that required the least effort (in terms of man-hours or external resources 
involved). Based on our need for immediate logging of the problems and comments about 
them, we had to update the list as the extractions progressed and collections were rescheduled. 
The design of the online system for error reporting allowed us to propagate knowledge on the 
effect and effort in resolving issues across several projects by setting principles and user-access 
rights for different teams in several locations connected to the same central repository. An 
approach that can widely be used for cross-platform access with direct assignees, minimising 
the time spend for both administering and providing solutions in timely manner. 
5.6.5 Call for further research 
Based on our findings, the implementation of a shared reporting system adopted by the 
individual EPR system vendors might help the secondary use of routinely collected data. As is 
evident from a review of the existing EPR solutions and their data extraction processes, a new 
culture of embracing solutions that facilitate change is needed, particularly by way of enabling 
fast-track channels to the researchers desk. 
5.6.6 Conclusions 
Particularly in the LOLIPOP study and the assessment of data extraction errors, the duration 
of the extraction, and the route of the patient data to the researchers table, the solution-
oriented approach has enabled us to achieve higher levels of successful problem reporting and 
solving; the effect of this was direct on the delays that were being initially imposed after 
encountering each type of error (as presented in [Table 7 - Component Oriented Approach]). Its 
method could be replicated in other projects. We recommend the use of this taxonomy for 
error reporting for any type of study whether using primary or secondary care data. System 
vendors should be more aware of the potential impact of non-standard interfaces.  Better 
structured systems, which more strictly implemented standards, would reduce the time spent 
in crisis managing problems when extracting data.  A national or international system of error 
reporting would allow sharing of workarounds is urgently needed.  Adoption of a standardised 
method of solution-orientated error reporting that would help EPR vendors identify and 
address errors in data extraction from their systems. 
CHAPTER 6 Key concepts to Assess the Readiness of Data for 
International Research: Data Quality, Lineage and Provenance, 
Extraction and Processing Errors, Traceability and Curation 
This work is part of the authors participation in the TRANSFoRm FP7 project 
with collective contributions from the working group as references 
in the publication list. Published in the Yearbook of medical informatics, 
International Medical Informatics Association. 
6.1 Introduction 
The previous chapter presented the need for a taxonomy from the early stages of data analysis 
(from the point of entry and extraction) which could aid more consistent data extraction 
module implementation, especially when information is extracted from disparate datasets. In 
this chapter, we explore the whole research workflow in the health domain, from the 
collection and extraction of data, for example from disease registries, genetic databases or 
practice systems, through to the creation of metadata for research output. There is an 
extensive review of terminology by way of introducing the concepts discussed. 
6.2 Consistency in terminology  lessons from an FP7 Project 
Findings from International studies are more likely to be generalizable, as the intervention will 
be tested across a range of cultures, ethnic groups, and health systems. The computerization of 
primary care should facilitate that process (Peterson, 2006). However, the type of data 
collected, the way data are structured and coded, and language used may vary between health 
systems (de Lusignan & van Weel, 2006); creating challenges for the primary care researcher 
to overcome (Hummers-Pradier, et al., 2008). 
Currently, there is no international consensus of how to describe data quality and its usability 
for research. The Translational Medicine and Patient Safety in Europe (TRANSFoRm) project 
aims to remove some of the barriers to conducting International research by developing a 
process which facilitates the conduct of research across European states (TRANSFoRm 
Working Group, n.d.). The programme has developed two use-cases - simulated data 
requirements for research studies - to provide a specification against which to test the potential 
of existing databases for research. However, their utility is limited by a lack of consistency in 
many of the terms used to define the origin and quality of the data. 
We carried out this review to try to define the terms which should be used to define the origin 
and quality of data. This is needed if primary care research is to grow from studies principally 
carried out in single countries in networks drawn from a single vendor to international studies 
which link primary care data to other data sources. These additional sources may be other 
health providers (e.g. hospitals and clinics); or other health data (e.g. genetic data from 
biobanks, or disease registry data); or social data. 
6.3 Method 
Our method was based on a literature review, workshop discussions, and developing an expert 
consensus. We carried out a literature review using the ISI Web of Knowledge and Pubmed 
Medline. We searched using the following key words: provenance, lineage, pedigree and 
traceability. We combined the key word in the following ways: 
"Data provenance OR "Provenance of data" 
Provenance AND Database 
Provenance AND "Service Oriented Architecture" 
We repeated these searches for the other key terms. For pedigree we added "NOT genetic*" as 
searches for pedigree were swamped by descriptions of genetic pedigree. 
We also searched for papers about data quality using: 
"Data Quality" AND "Medical Records systems, computerized" 
or using as a second search term: 
"Computers", Classification", or "Family practice". 
We took a linear view of the overall research process: from the point of data recording to the 
creation of the final tables for analysis by the researcher per [Figure	13] and [Figure	14]. We 
determined that data quality was the overarching concept and that each step of the process 
needs to have an unambiguous quality descriptor.  The role and place of each descriptor was 
determined by consensus. 
We harnessed international informatics expertise from the IMIA (International Medical 
Informatics Working Group) (International Medical Informatics Association (IMIA), n.d.) and 
EFMI (European Federation for Medical Informatics) (European Federation for Medical 
Informatics (EFMI), n.d.) Primary care Informatics working groups (PCI WG); and discussed 
this theme at the WG workshops at the 2010 EFMI conference in Reykjavik, the 2010 IMIA 
MEDINFO conference in Cape Town, in subsequent email discussions, and within the 
TRANSFoRm working group. 
We excluded broader issues relating to the quality of research data: social and cultural 
context; health service organization and study specific issues as they formed part of a previous 
study (de Lusignan, et al., 2011). Our investigation was orientated toward family practice, and 
we only included studies relevant to research which might be relevant to primary care 
Figure 13 - Research, using routine data, as a linear process 
research; albeit that such research often needs to link to other data sources to identify high 
risk groups or provide health outcomes data. 
Similarly, we did not consider consent, governance, privacy or data security issues; including 
obfuscation or other methods for masking patient identities in aggregated records (de 
Lusignan, et al., 2007). Finally, we excluded any data migration process between the electronic 
patient record system (EPR) where the data were recorded and research data repository. 
There are a range of methods used at this step; most are proprietary and often collect data 
from a single brand of computer system. There is a dearth of literature about this step, and no 
ready mechanism to investigate further. 
Figure 14  The process from data recording to data analysis 
We also tested our data quality model using the TRANSFoRm use-cases: One is a study of the 
genetics of type 2 diabetes requiring linked primary care and genetic data; the second a study 
to explore the relationship between gastro-oesophageal reflux disease (GORD) and its 
treatment in primary care with oesophageal cancer. We decided to simulate the likely very 
different ontologies (concepts and their relationships), informational models and semantic 
issues around these use-cases; and use them to test the face validity of our definitions. 
6.4 Results 
6.4.1 Data Quality 
The International Standards Organization (ISO) defines quality as: 
The totality of features and characteristics of an entity that bears on its ability to satisfy 
stated and implied needs (The International Standards Organization (ISO), n.d.). This is 
echoed by the EFMI PCI WG who defined data quality as fitness for purpose (de Lusignan, 
2006). 
Data quality was initially defined in quantitative terms, using measureable components that 
give some indication of the validity of the data; data quality was initially defined in terms of 
completeness and accuracy (Pringle, et al., 1995). Later definitions added the concept of 
currency (Williams, 2003): Subsequently, it was defined as the positive predictive value and 
sensitivity (Thiru, et al., 2003) or by its specificity (Roten, et al., 2010). 
More recently, definitions have tried to be more analytical. Aqil et al., suggest that 
comparisons should be made at each step: (1) Comparing what data are collected with 
information needs; (2) Are all data fields filled and how do they compare with expected levels 
of completeness; (3) Are data entry timely compared with the norm; and (4) Accuracy should 
be tested by comparing between records and with other data sources (Aqil, et al., 2009). Arts 
et al., propose analysis of the whole process, using planned and systematic procedures before, 
during, and after data collection (Arts, et al., 2002).  
6.4.2 Improving the Quality of Data Entry 
The quality of data entry for the same clinical scenario will differ between clinicians. It can 
reflect: the pattern of computer use of the clinician (Kushniruk, et al., 2006); aspects of the 
computer interface: for example where picking lists are used for clinical coding the lists can 
vary between brands apparently using the same coding system (Tai, et al., 2007) and 
cardiovascular risk scores can vary between brands (Debar, et al., 2010); or external influences, 
e.g. Direct transmission of pathology results into the computer system. Moves towards a more 
service oriented architecture where industry standard tools are used may help standardize 
processes. For example, semantic lookup services might standardize the way coding systems 
are accessed and reduce variation in coding between different brands of EPR (Zdun, 2004), 
and an interoperable cardiovascular risk calculator could standardize risk calculation between 
different EPR systems (Pan, et al., 2008). 
There are three principal ways that the quality of data entry can be improved: 
(1) Feedback ideally through regular meetings and education (Turbelin & Boelle, 
2010), with or without the provision of token (de Lusignan, et al., 2002) or substantial 
financial incentives (de Lusignan & Mimnagh, 2006). 
(2) Use of data entry forms which either facilitate or mandate the collection of a 
partial or complete dataset (de Lusignan, et al., 2010), or structuring the record in a way that 
forces linkage between problem and therapy, often referred to as problem orientation (Carey, 
et al., 2003). 
(3) Decision support which either prompts for missing data or which suggest diagnoses 
(Kostopoulou, et al., 2008). 
6.4.3 Data Lineage and Provenance 
The terms provenance, pedigree, lineage and traceability have all been used to describe the 
origins of data within the informatics literature. The first references to these were: data lineage 
in 1991 (Lanter, 1991), traceability in 1995 (Yamamoto, 1995), pedigree in 1998 (Moellman & 
Cain, 1998), and provenance in 2000 (Buneman, et al., 2000). 
Data lineage is an output record of all the contributory inputs [162] (Cheney, et al., 2007). 
Traceability links the outputs to their originating inputs across a system. Data pedigree 
predominantly refers to the authority of the source; implying that data of good pedigree can be 
trusted (Chief Information Officer (CIO), 2003). 
Data provenance first appeared in Medline in 2004 (Beresford, et al., 2004). However, in the 
broader scientific literature pedigree and data lineage are used as near synonyms (Simmhan, et 
al., 2005). Provenance is a type of metadata, concerned with the history of data, its origin and 
changes made to it, often including versioning information (Lee, et al., 2009). Definitions of 
data provenance include: 
The history, lineage or provenance of a given piece of data provides understanding of 
how it was that the data came to be as it is. This understanding enables users to validate data 
by providing the means to examine the processes that produced it, for fitness for purpose, 
compliance to regulations, replication, validation and examination (Groth, et al., 2008). 
The provenance of a data item includes information about the processes and source 
data items that lead to its creation and current representation (Glavic & Dittrich, n.d.). 
In medical software systems, the data (EPR and instrument data), the workflow (procedures 
carried out to perform extraction and analysis) and the histories (recording meaningful events 
in those procedures) may be distributed among several heterogeneous and autonomous 
Information systems. Capturing this knowledge requires a provenance framework that is 
separate from the individual systems, enabling the traceability of the origins of decisions and 
processes, the information that was available at each step, and where that information came 
from. In turn, this provides an integrated view of treatment processes, and enables 
performance analysis and procedure audit of distributed healthcare services. 
Data provenance has five potential functions: 
(1) Understanding and ultimately Improving data quality; 
(2) Providing an audit trail of the data (this encompasses lineage); 
(3) Generating replication recipes to allow the process to be reproduced; 
(4) Providing attribution and ownership of the data (an important feature of pedigree), 
(5) Enabling the discovery of new information about a process (Goble, 2002). 
A provenance standard has been developed to facilitate collaboration. The Open Provenance 
Model (Moreau, et al., 2010) is designed to meet the following requirements: 
(1) Allow provenance information to be exchanged between systems, by means of a 
compatibility layer; 
(2) Build and share tools; 
(3) To define the model in a precise, technology-agnostic manner; 
(4) Support a digital representation of provenance for any thing, whether produced 
by computer systems or not; 
(5) To define a core set of rules that identify the valid inferences that can be made on 
provenance graphs. 
A provenance store can also be defined as part of a workflow based service oriented 
architecture. There can be single or multiple provenance stores depending on the complexity or 
requirements of the workflow (Groth, et al., 2005). 
We recommend the use of primary data provenance as the overarching term between the point 
of data entry and the point that the researcher extracts their data; and that that lineage and 
pedigree are used in data quality as subordinate terms. Lineage is primarily a study of where 
data comes from and pedigree a term with more emphasis on the quality and trustworthiness 
of data, though this is not the way pedigree is used within genetics. If the data remains 
unaltered 
Through subsequent processing then this primary data provenance will apply throughout; 
however if in the process of extraction, analysis or curation new variables are created then this 
secondary data will have its own provenance. These different parts of the workflow could have 
a single or their own provenance store (Groth, et al., 2008). 
6.4.4 Data Extraction Errors 
Taking data from one system to another inevitably involves data loss. The migration process 
often includes pre-processing to anonymize or completely remove data items that identify 
individuals. In data extraction difficulties and errors arise because of the different architectures 
of the heterogeneous distributed systems, the local autonomy of these systems, problems in 
representational diversity of the same clinical concept, and the potential lack of precise 
semantic meaning (Pringle, et al., 1995). We suggest the use of this error taxonomy 
(Michalakidis, et al., 2010) to share errors and facilitate the identification of underlying 
causation and enable them to be rectified [Table 10]. 
Table 9 - Applications of provenance of information and overlap with pedigree and lineage
6.4.5 Data Aggregation, Linkage and Processing Errors 
More than one source of data may be required to conduct a research study; for example, the 
TRANSFoRm use-cases require primary care and genetic data to conduct one study and 
primary care and cancer registry data for the other. 
Data aggregation is an eight step process: 
(1) Design 
(2) Data entry, 
(3) Extraction, 
(4) Migration, 
(5) Integration, 
(6) Cleaning, 
(7) Processing, and 
(8) Analysis (van Vlymen, et al., 2005). 
All these steps involve making assumptions and are prone to error. Steps two and three are 
dealt with earlier in this chapter; and not discussed further. The data processing design has to 
take into account whether the analysis is using complete data, just coded data, or whether 
Table 10 - Types of extraction errors and consequences 
there is access to free text and taking into account missing data (Marston, et al., 2010) (Davis, 
et al., 2004). And, the conversion of extracted code into information, namely the process of 
creating deriving new variables. 
The process of creating derived variables involves, cleaning data to remove non-credible values; 
grouping the data into categories; using cut points or combining variables to produce a new 
variable (e.g. calculating cardiovascular risk) (Debar, et al., 2010), relevant to the intended 
analysis. Any new data created must have its provenance defined; and its own metadata (van 
Vlymen & de Lusignan, 2005). 
Data linkage between data sources is becoming more and more important in research based on 
routine data. There are a number of dimensions to linkage and how it can be facilitated. 
Registration based health systems where one individual only registers with one primary care 
provider; and health systems with a unique identifier can more readily link data than those 
without. 
More recently systems of private record linkage are being developed which can link data, 
where needed in the absence of strong identifiers (Durham, et al., 2010). 
6.4.6 Traceability 
Traceability is defined as the ability to retain the identity of a product and its origin 
(Khabbazi, et al., 2010) and can be achieved in large datasets. Traceability of each data item 
requires tracing the individual who contributed the data and the variable that describes that 
individual. Within most clinical databases a patients name will identify them, but within 
research databases usually pseudonyms are used which can only be decoded by the clinicians 
who provided the original data. The variables describing that individual will usually be in a 
tuple of code-date-value (e.g. 44p is the Read code for cholesterol; 26-Sep-2009 is the date the 
test was conducted; and 5.5 mmol/l is the result). However, to interpret this datum we need to 
be able to link the data to the extraction query and data about when these data were 
extracted; for example did the query request the latest, the minimum or the maximum 
cholesterol value? The Primary Care Data Quality (PCDQ) programme developed its own 
metadata (van Vlymen, et al., 2005) to avoid data misinterpretation and ensure traceability. 
Table 11 - Linking data processing activities, traceability and curation 
6.4.7 Metadata 
Metadata is data that describes data, the study of metadata is part of the Semantic Web 
which sets out to allow data to be shared and reused across applications; the first set of work 
in this area was the programme of Resource Description Frameworks, the process of 
developing tagged information (World Wide Web Consortium (W3C), n.d.). The PCDQ 
metadata just relates to the data source, the content and format of the data. Typically core 
metadata will contain: 
(1) Resource; 
(2) Summary content; 
(3) Format; and 
(4) Security descriptors; with additional extensible layers added as needed. 
The sophistication of the metadata will define what links can be made from a datum to its 
source. Metadata should help flag related concepts, elements of the data model and help ensure 
semantic meaning. Provenance information is also often placed into the metadata, for example 
as XML representations of directed acyclic graphs tracing the origin of data. Metadata is a key 
enabler to the emergence of quality measures that are socially constructed from within the 
community of users of medical data. 
Few databases publish their metadata (van Vlymen & de Lusignan, 2005); and if we fail to do 
this, data risks being misinterpreted when analysed remote from people who understand the 
context in which it is recorded. 
6.4.8 Curation of Data 
The term curation was defined by Lord et al (Lord, et al., 2004) as an activity that manages 
and promotes the use of data from its point of creation, ensuring that it is available for 
discovery and reuse, and fit for purpose. Curation is essential to provide a substrate to access, 
share and reuse data collections successfully (Karasti, et al., 2006). Interpretation of data may 
require the simultaneous archiving of the metadata schema; look up tables for clinical codes 
and drug dictionaries (as without these it is impossible to know the extent of coding choices 
available to clinicians at the time); data extraction queries; and syntax or code which describe 
how data were cleaned and processed to create the final analysis variables. 
In provenance-enabled systems, the full trace of curation is stored for future querying and 
analysis, thus enabling full reproducibility and verifiability of the data transformations 
performed. There may also be lessons for archiving health data from the metadata standards 
developed for archiving reference works: Reference model for an Open Archival System (OAIS) 
(Lavoi, 2004). 
6.4.9 Simulation using the TRANSFoRm use-cases 
We used the two TRANSFoRm use-cases to simulate ontological issues, scope of the data 
model and semantic meaning per [Table 12] and [Table 13]. The key ontological issues 
shared across both the use-cases are their ontological richness or complexity. In both use-cases 
risk factors are common and complex; co-morbidities are common but not readily predictive; 
and involve multi-disciplinary care and records. The issues with the data model were the 
complexity of relevant data and scepticism about the reliability of summary data from the 
specialist repositories. The semantic challenges were largely about type and severity of disease. 
This simulation suggested that omission of data in a study either through not having access to 
a data source or through invisible exclusion criteria may be as important as knowing the 
quality of the data that is included (e.g. Over the counter OTC pharmacy data). Metadata 
constructed to support the provenance model, should contain links to the comprehensiveness of 
the data source as well as what data fields and data types (e.g. free-text) are included. 
Table 12 - Overview of challenges in the genetic study of type 2 diabetes use-case 
Table 13 - Overview of challenges in gastro-oesophageal reflux disease (GORD) and oesophageal 
cancer use-case. Implications on how issues can be solved are provided in the section 6.5.2 
6.4.10 Use-Case testing 
We analysed the use-cases to define the data requirements and the integrity of the use-cases, 
constructing a DFD and UML models for each. The use-cases were in two clinical domains, 
type-2 diabetes and acid-reflux related oesophageal disease. 
The type-2 diabetes cohort study explored the risk of complications and response to oral 
medication, a cohort study linking primary care and genetic data. The second domain, gastro-
oesophageal reflux disease (GORD), included two use-cases linking primary care and cancer 
registry data. One was a randomised controlled trial of on-demand compared with continuous 
use of proton pump inhibitors (PPIs) a class of anti-indigestion medicine. The other was a 
nested case-control study to explore the association of GORD symptoms and the use of PPIs 
with developing Barretts disease and cancer of the oesophagus. 
6.5 Discussion 
6.5.1 Principal findings 
Black box processing and reporting of findings based on routine data should no longer be 
acceptable. More explicit and structured descriptions of the origins of data, and wherever 
possible the use of open standards should be mandated for studies based on routine data. 
Auditing the whole process, from data recording to curation, is critical to ensure data quality 
[Figure 15] in any final published output. Validation of data is impossible unless the 
provenance of the data, extraction and processing errors are recorded in a structured way and 
each cell in the final analysis table is traceable. 
We have separated the primary data provenance, capturing the origin of data records, from 
the secondary data provenance, tracing the operations performed by the researcher, as the 
underlying causation is likely to be different. 
A shared understanding of the relevant ontology, data model and semantic issues are essential, 
and should be conducted on a study-by-study basis. This output feeds into the metadata 
schema, which should be published to demonstrate that these processes handle data in a 
consistent and reliable way; and also what data are not included within a study. 
6.5.2 Implications of the findings 
A common set of descriptors of the process from data recording through to the curation of 
data should be adopted and published as an appendix to studies using routine data as it will 
improve the ability of researchers to compare data processing methods and understand where 
data losses may occur. This will be increasingly important for studies involving linked data. 
We propose that provenance, data extraction and processing errors and curation are used to 
describe the issues related to the processing of data to produce research outputs and its 
subsequent archiving respectively. The term traceability should be reserved for the 
retrospective audit of the data within the final research output. We have made this term the 
functional aspect of provenance: one of its purposes is to enable traceability. 
Exploring for each study its ontology, data model and semantic issues will help ensure that the 
metadata schema meets the needs of the project as well as describing the data quality. 
6.5.3 Comparison with the literature 
Central to our study is a belief in an open systems approach to defining and assessing data 
quality, in keeping with the Toyota Production System or TPS (Seddon, 2008). An approach 
which is essentially socio-technical, is efficient, does not over-burden the research process, nor 
does it allow inconsistency. Of course optimal variables would be almost impossible in any 
system, let alone in emerging technological areas, as is health data extraction for research 
using a carefully executed streamlined business process. Inconsistencies will always exist, 
especially when imposed by human (and with constrains such as time or level of experience 
and expertise). It is also clear that inherent inefficiencies or problems in any procedure will 
always be most apparent to those closest to the process. The improvement process in TPS is 
known as kaizen and indicates improvement in every sphere  in our case that could be from 
the medical practitioners office to the researchers table, effectively enhancing delivery of care 
to the patient and the wider community beyond. 
Figure 15 - An overview of the stages and linked quality concepts from data recording to extraction 
Much pharmaceutical research uses the methods set out within the Clinical Data Interchange 
Standards Consortium (CDISC) including the Biomedical Research Integrated Domain Group 
(BRIDG) (Biomedical Research Integrated Domain Group (BRIDG), n.d.) - an internationally 
recognized standards body (Clinical Data Interchange Standards Consortium (CDISC), n.d.). 
CDISC has also developed a relationship with health level seven (HL-7) which has adopted the 
BRIDG data analysis model (DAM). In clinical trials the data set is generally complete and 
there are less challenges in linking records and managing incomplete datasets (Bohensky, et al., 
2010) (Nur, et al., 2010). 
Statistical process control techniques may provide better mechanisms for the direct exploration 
of clinical data, with much less collection and processing overheads (Abdel Wahab, et al., 
2004) (Aylin, et al., 2003). 
6.5.4 Limitations of the method 
This schema is based on experiential learning from those involved in data processing and a 
literature review; expert consensus of this sort forms the lowest grade of evidence (University 
of Oxford, 2009). Prospective studies have not yet tested whether these elements are essential. 
6.5.5 Call for further research 
We need to test different approaches and strategies for a complete, applicable model to 
enhance datasets and their readiness for further analysis. For example, methods adopted in the 
fast-paced Internet/tech domain can be used. The recent emergence of tagging in various 
social computing sites provides the opportunity for enriching metadata. In the broad context 
of use, we see this as being used to help users provide metadata on relevance and quality. In 
social computing, many of the terms are weak in information content. Allowing the medical 
professional to tag data, or sources of data, may enable the emergence of a socially constructed 
quality model. This could apply not only to the initial dataset (for example the information 
recorded about a patient on a practice level) but also in the intermediate steps to research 
output (and in that case by the researcher), by way of secondary or associated metadata that 
might refer to anything from extraction queries to subsequent analysis queries (statistical or 
other) used to summarise or visualise data. It should be noted that our analysis refers to the 
use-cases presented, and further work to establish whether the same linear business process 
model can apply in cases where the research activities are diverse, is encouraged. 
6.6 Conclusions 
If we accept fitness for purpose as the central feature of data quality, then it is essential that 
our model of data quality be constructed in a way that represents consensus as to best practice 
amongst its community of users. Consistent description of the process will improve 
understanding of the validity of research findings based on routinely collected data, and this 
description should be formalized in the metadata schema. 
The process from data recording to research output is complex but can be represented as a 
simple model, particularly in the studied use-cases and as evidenced in [Figure 15	-	An	
overview	of	the	stages	and	linked	quality	concepts	from	data	recording	to	extraction]. Prior to 
embarking on research using routine data investigators should carefully map the process from 
data recording to curation. A schema of the research process including provenance of the data 
and the details of data extraction and processing should be developed as a check list and be 
available for all publications based on routine data. 
CHAPTER 7 Defining datasets and creating data dictionaries 
for quality improvement and research in chronic disease using 
routinely collected data: an ontology-driven approach 
7.1 The need for an engine that facilitates inclusive querying of data 
The previous chapter explored concepts involved in making data fit for research, with a model 
that involves steps from creating and extracting data, to curating it and allowing traceability. 
In this chapter, a method for generating data dictionaries is introduced with two key aims: 
bridging data from disparate datasets, as well as facilitating querying of research output in an 
inclusive rather than limited manner. 
7.2 Specific needs for ontologies and data repositories in health 
7.2.1 Growing burden of chronic disease 
Internationally, there is a growing burden of chronic disease, and a need to re-orientate health 
services towards the provision of chronic care (Boult, et al., 2008). Computerized medical 
records systems may have a role in improving management by enabling the ready 
identification of cases and in monitoring quality (Muttitt & Alvarez, 2007) (Samoutis, et al., 
2008). These computerized disease registers are likely to be important in areas where there are 
quantitative measures that define whether you have a particular disease and for measuring the 
quality of care. Diabetes (O'Mullane, et al., 2010) and the secondary prevention of 
cardiovascular disease including the management of hypertension (Belsey, et al., 2008) 
(Saxena, et al., 2007) provide examples of where computerized medical records enable quality 
improvement even though there remains scope for refinement (Debar, et al., 2010) (de 
Lusignan, et al., 2010). 
7.2.2 Practical approaches to case finding in chronic disease: Ontologies 
and data dictionaries  
Two practical approaches are given to ensure that we identify cases and systematically list the 
codes required to conduct research or quality improvement. Ontologies provide insight into 
what might be extracted from a clinical system to provide the data we require and data 
dictionaries provide an accessible list of the extracted variables. 
7.2.3 Ontologies to define cases with a chronic disease 
Ontologies provide a method for describing concepts and relationships within a domain. The 
principal use of ontologies in informatics is to enable human and machine communication, by 
defining the terms used to describe an area of knowledge. Ontologies usually have the following 
components: 
 classes  general types of entities in the domain 
 relationships that can exist among and between the things within the domain 
 the properties (or attributes) those things may have (Gruber, 1995). 
Another recognized use of ontologies is for the retrieval of data (Smith & Ceusters, 2010). 
However, this approach has not been widely used in quality improvement or research into the 
management of chronic disease. There is the potential to use ontologies to define datasets that 
might be used to identify people with a chronic condition for quality improvement or research. 
One of the best known definitions of ontologies in informatics emphasizes both their machine-
processable and human interpretability: 
Ontologies are collections of formal, machine-processable and human-interpretable 
representations of the entities, and the relations among those entities, within a defined 
application domainare helping researchers manage the information explosion by providing 
explicit descriptions of biomedical entities and an approach to annotating, analysing the 
results. 
Ontologies are useful because they provide regimentations of terminology that can support the 
reusability and integration of data and thereby support the development of useful systems for 
purposes such as decision support, data annotation, information retrieval, and natural 
language processing (Rubin, et al., 2006). 
7.2.4 Data Dictionary - a centralised repository of the data set that defines a case 
A data dictionary is a centralized repository of information about data such as the meaning 
relationships to other data, origin, usage and format (McDaniel, 1994). A data dictionary 
could capture the classes of information, some relationships and properties of the data. Data 
dictionaries are a potential mechanism for ensuring the transfer of meaning into clinical 
information systems and ultimately improve care efficiency (Anderson, 1986). Data dictionaries 
can also play an important role in modelling and in the specification and requirements analysis 
with the use of metadata (Navathe & Kerschber, 1986). 
7.2.5 Objective of the method 
This chapter proposes that ontologically rich approaches should be used to define datasets to 
identify cases in quality improvement and research projects. If this were done, it would 
substantially improve the identification of cases within routinely collected data. We propose 
how a dataset might be constructed and how variable lists for all studies using substantial 
datasets might be displayed in an accompanying data dictionary. 
7.3 Method for generating Data Dictionaries 
7.3.1 Overview 
We propose a five-step process: 
(1) identifying a reference coding system or terminology; 
(2) using an ontology-driven approach to identify relevant concepts and relationships 
that might define a case; 
(3) Developing metadata that can be used to identify the source and nature of 
extracted data; 
(4) mapping the extracted data to the reference terminology; and 
(5) creating the data dictionary. 
7.3.2 Identifying a reference coding system or terminology  
We recommend selecting a comprehensive coding system for use as the reference coding system 
for a project. The coding system selected should be the most commonly used for that 
particular study and be capable of having the core relevant concepts mapped to it.  
Increasingly, data for a study are recorded using more than one coding system. For example, 
in the UK, the coding systems used are (de Lusignan, 2005): 
 Read version 2 (hierarchical) 
 Read Clinical Terms version 3 (CTv3) 
 Systematized Nomenclature of Medicine Clinical Terms (SNOMED CT) 
 International Classification of Disease version 10 (ICD-10) 
 Office of Population Census and Surveys version 4 (OPCS4) 
By way of contrast in Australia the following are used: 
 International Classification of Primary Care (ICPC) 
 Doctors Command Language (DOCLE) (de Lusignan & Teasdale, 2007) 
 ICD-10-AM (Australian Modification) 
Historically, much research was based on data collected from single brands of computer system 
and such single EPR supplier research networks have been extremely successful (de Lusignan 
& van Weel, 2006). However, more and more research involved linking between databases, so 
that the effect of an intervention in one part of the health system can be seen in another. We, 
for example, have demonstrated how improving access to psychological therapies (IAPT) has a 
positive impact on accident and emergency uptake (de Lusignan, et al., 2012). 
7.3.3 Defining the relevant concepts and relationships within the 
reference terminology 
A person having a chronic disease may be identified from a range of codes. A disease code will 
usually signify that a person has a chronic condition; e.g. the disease code for Essential 
Hypertension in Read codes version 2 is G20z. However, codes from other parts of the 
classification may also signify that someone has hypertension and codes are sometimes inserted 
in error. 
We suggest using a tabular approach in which each chapter of a coding system is explored to 
see if codes that might represent a person with hypertension indicate that the person has the 
condition. 
7.3.4 Metadata to control extraction and uploading 
Data extraction is not consistent between different brands of electronic patient record (EPR) 
systems or coding systems. It is necessary to create a metadata system that links and labels 
the coding system used in the site from which data is extracted; the brand of EPR and 
version, for example, can affect the drug dictionary used and the data extraction query. 
Metadata are data that describe other data and therefore can be used to control and manage 
processes (Park, 2009). 
7.3.5 Mapping data from different sources 
The uploaded data from different sources, labelled by system metadata, then needs to be 
mapped using validated processes wherever possible. If not available, this needs to be done 
involving clinicians in the field who understand its ontological significance. 
7.3.6 Creating the data dictionary 
The data dictionary should be readily searchable and display the code and term, the domain 
and a link to the relevant data extraction query. The metadata drives the creation of the data 
dictionary for all the terms returned by the data extraction queries. It links extracted data 
from different coding systems to a common list of subdomains and domains, as well as to the 
data extraction query. 
For example: 
 G20z is an example code 
 The term is Essential Hypertension NOS 
 It belongs to the subdomain called G2 Hypertensive Disease 
 The related domain is G: Circulatory System Disease 
 It was extracted by a query called Cardiovascular co-morbidities.... 
7.4 Results 
7.4.1 Identifying a reference coding system or terminology 
We generally use the most commonly used in a particular study. We currently use Read 
version 2, 5-Byte for UK primary care studies; and ICD-10 for hospital studies; using OPCS4 
where operations or procedures are the primary focus. However, this choice can vary according 
to the usual practice in the areas under investigation. Where a single brand of computer 
system is used we may have to include local codes. These remain much used in the EMIS 
system, and for other brands we may use CTv3. 
7.4.2 Ontologically rich approach to identifying cases 
We look for codes that might enable us to identify cases by systematically searching across 
coding hierarchies, or identifying alternative codes in other non-hierarchical systems which 
may indicate that the patient has the condition. We look for cases by searching for History of 
codes (e.g. 14A2, History of Hypertension), Diagnosis codes (e.g. G20z, Essential 
Hypertension), Procedure codes (e.g. 6628, Poor hypertensive control), Administration codes 
(e.g. 901% Hypertension monitoring administration) and Therapy codes which imply the 
condition (e.g. b2% Thiazide diuretics). Our method for identifying cases of hypertension is 
shown in [Figure 17]. Further refinements include the use of % as the end of a code when all 
child codes are included and a . (Full stop) when just the code listed is required. We also list 
codes within a hierarchy we wish to exclude. 
We also indicate the likelihood of a code to truly map to a condition. We develop rules on a 
study by study basis. The most complex we have developed were to enable the machine 
processing of a diagnosis of diabetes into definite, probable, possible or not having the 
condition (Sadek, et al., 2011). 
7.4.3 Metadata to control the process 
We initially developed metadata to make our data processing more efficient and consistent 
(van Vlymen & de Lusignan, 2005). However, this was developed when we were principally 
working with just primary care data and complex methods are needed to cope with linking 
heterogeneous datasets. 
We have subsequently developed and added a solution-orientated taxonomy to report data 
extraction errors, so we can understand any gaps in our data (Michalakidis, et al., 2010). We 
use Java and Another Tool for Language Recognition (ANTLR) for the parsing of data (Parr 
& Quong, 1995) to ensure their consistency with the reference terminology. 
7.4.4 Mapping data from different sources 
Our system collects all individual de-duplicated (via parallel processing) clinical codes from 
different extraction samples and stores them in a memory-efficient data store for further 
processing (Moreira, et al., 1999). The results carry heavy metadata (e.g. coding system used, 
original extracted set for traceability) (de Lusignan, et al., 2011). 
We carry out our mapping and translate codes to definitions within our reference coding 
systems. Wherever possible we use validated translation schemas provided by Technology 
Reference Data Update Distribution (TRUD), NHS  for mapping Read Clinical Terms version 
3 (CTv3) to Read 2. We also use translations provided by EPR vendors, for example, Egton 
Medical Information Systems (EMIS) mapping to convert EMIS drug codes to standard Read 
2 codes. Only exceptionally will we devise manual schemas for mapping. Where we do, we 
classify our mapping into Direct, Partial and No clear mapping (Rollason, et al., 2009). 
Where mapped codes appear in the data dictionary they appear with the code to which they 
are mapped, e.g. the CTv3 code XE0Uc appears with the comment Essential hypertension 
(Read 2 equivalent: G20). The mapped codes do not display a domain or subdomain, but do 
display the name of the data extraction query as this will be different to the query used to 
extract data (Collection request) from practices using Read 2 codes. 
7.4.5 Creating the data dictionary 
We have created a method whereby code hierarchy is dynamically generated, with the 
identification and translation of the code domain (e.g. 1: History/Symptoms) and subdomain 
(e.g. 12: Family History) for each individual code as well as the extraction metadata. 
A Web-enabled JavaScript-based system for Web and mobile representation of the dictionary 
data is placed online, with various dictionaries freely available (e.g. Osteoporosis data 
dictionary is available at: www.clininf.eu/osteoporosis-data-dictionary.html). This allows for 
dynamic searches in sets with thousands of codes and a view of the complete dataset each 
study holds [Figure 16]. It also means that project investigators can readily identify the data 
available for a particular project. The purpose of this process is to enable stakeholders 
(primarily researchers, and regardless of them being internal or external to a project or study) 
to assess the readiness of data repositories, in terms of wealth and completeness, for care 
quality improvement. The benefit of the process is presented in the next section. 
7.5 Discussion 
This chapter describes a way of using ontologies to ensure the high chance of identifying 
people who have chronic diseases from routinely collected clinical data; and data dictionaries 
can provide navigable lists of variables extracted. Data in primary care may not be complete 
or accurate, or current (Williams, 2003) and there may be measurable gaps in data quality 
(Thiru, et al., 2003) (Brown & Warmington, 2003). Therefore, we need to extract and process 
data in a way that takes account of its limitations (de Lusignan, et al., 2004) and this should 
include taking account of the presence or absence of ontological relationships. 
Many researchers and others involved in extracting routinely collected data who understand 
issues about data quality may already be addressing the principles set out in this chapter.  
However, others come to work on routinely collected data without contextual insight as to the 
range of codes that might be used to represent a case (Stevens, et al., 2011). Data dictionaries 
make explicit the link between code and term, to subdomain and domain, and data extraction 
query. They can be generated dynamically from systems that have developed metadata to link 
these items and to flag mapping between coding systems. 
However good the ontologically rich process of defining cases or of setting out of the terms 
used in our data dictionary there will be limitations. Concepts often evolve and relationships 
change. Not all relationships will be perfect. For example, it appears impossible to avoid 
Figure 16 - Example online data dictionary, searched using the term 
Hyperten 
extracting family history of hypertension codes when looking for the codes for hypertension. 
This can be adjusted for in the final analysis of data, but illustrates that it is not always 
possible to make perfect ontological links. The Chocolate teapot not otherwise specified 
discussion paper illustrates this point well and provides good counsel against ontological 
obsessionalism (Duncan, n.d.) (HL-7 Connection, n.d.). Not all concepts have direct mapping 
to a single diagnosis, and sometimes an operation, procedure or other process of care code may 
be the only indication that a person might have the condition. Others have recognized that 
there may be mandatory, multiple or numeric criteria for formalizing description logic 
ontologies (Bertaud-Gounot, et al., 2012). A similar approach to identify patients with 
diabetes, using a combination of diagnostic terms as well as medications and laboratory tests 
has been used in Australian primary care (Liaw, et al., 2011). 
A final advantage of the ontology-driven approach to defining cases is that it will be inclusive 
rather than limited. Hayes, in his principles, decries the dataset mentality (de Lusignan & 
Krause, 2010). This is effectively an arbitrary list of codes which signifies that an individual 
has a condition. The downside of the limited dataset approach is that it will inevitably miss 
cases represented elsewhere within the clinical record. Whether for research or as part of a 
disease register for quality improvement, adopting an ontology-driven approach is likely to 
create a list of variables that are inclusive of patients with a particular condition; albeit that 
some of the mappings will be partial. 
Figure 17 - An ontologically rich approach to identifying cases of essential hypertension 
7.6 Conclusion 
The process from case finding to data extraction to creating a data dictionary should be seen 
as a continuum. Our findings, as well as data from access to the online Data Dictionaries 
supplied, show how in the two studies presented in Chapter 5 (QICKD and LOLIPOP), data 
dictionaries can link extracted codes and terms to clinical domains and data extraction queries. 
Data Dictionaries offer the potential to automate methods for case finding and often prove 
more efficient than manual approaches. For example a previous study hypothesized that an 
automated computer system could use dictionaries to process unstructured, free-text content 
and accurately perform time and labour-intensive tasks such as tumor case identification  it 
aided registry personnel in ensuring that cancer cases are not overlooked (Hanauer, et al., 
2007) with considerable increase in cases identified compares to manual means. We propose 
that this be extended with the use of an ontologically rich approach. 
CHAPTER 8 Consistent Data Recording across a Health System 
and enabling wider access through the Web allows for Service 
Quality Comparisons  use-case in Online Data for 
Commissioning Dermatology Services 
8.1 Introduction 
From the creation of data dictionaries as presented in the previous chapter, we attempt to 
create an online system for rapid access to information, employing the same principles behind 
the data dictionary creation process. The resulting system demonstrated in this chapter, allows 
for the same inclusive queries to be executed, essentially providing fast access to information 
with filters and data aggregation that spans localities, disease types or any other type of 
composite filter. 
8.2 Deciding on a standardised dataset 
In England, health service reforms mandate that family physicians must group themselves 
into Clinical Commissioning Groups (CCGs). CCGs will commission all NHS services. These 
services must be clinically and cost effective, include continuous improvement in the quality 
of services; with the focus on clinical outcomes (UK Parliament, 2011). 
The UK collects a standard dataset, hospital episode statistics (HES), from state and 
private providers of health service care across England. We hypothesized that the 
adaptation of this national collection of HES for small-scale neighbourhood based 
commissioning consortia could enable benchmarking of local services. This comparison could 
be used facilitate smarter commissioning of services. This study Web-enabled data in one 
specialty: dermatology, to enable rapid access and comparison of locality services. 
8.3 The online system 
Data for dermatology diseases were extracted from HES data. This included NHS locality 
and individual provider and family practice codes. For each provider HES lists accident and 
emergency, outpatients and in-patient data. These data include codes for specialty, disease 
using the International Classification of Disease (ICD-10) and procedure codes using Office 
of Population Census and Surveys version 4 (OPCS4) codes. These data can be linked at a 
locality level to socio-demographic data: population, deprivation and ethnicity data. 
The patient record tables alone consist of a total of 62 distinct data columns. The Web-
enablement system was developed using open-source and community software including 
the Ruby on Rails Web framework (Meenakshi, 2015) and MySQL relational database 
management system (RDBMS) (MySQL AB, 2002). 
The system has a Web front-end, which uses HTML/CSS and JavaScript (through the 
jQuery JavaScript library) to deliver and present data provided by the database. 
The database itself is mapped onto the Web framework so that underlying data can be 
managed effectively  through the so-called scaffolding process. 
The dataset is stored in a community edition MySQL database which was optimized to 
ensure that data are retrieved in an efficient and timely manner the original HES 
database and schemas used have been extensively reconstructed. To aid further fast 
delivery of data (especially in online environments) our system employs an advanced 
caching mechanism. Every query executed is recorded in an efficient (and fast) storage 
for retrieval of the results without further table-joining requirements, which would 
otherwise introduce additional overheads. 
Our system learns the users commonly used searches and any intermediate query executed at 
the backend level. The solution depends in a method that, without exposing other system 
users identifies and data, combines intelligence from all searches in the system for any 
current or new users. 
The system employs role-based access. This way, we only allow access to summary data 
(without sensitive dates or data that allow to identify individuals) for specific groups of 
users. The system developers or other user groups that are governed by the HES policy, have 
unrestricted access to the output data. We also recorded the server response time. 
We report by way of an exemplar population rates for outpatients, inpatients receiving 
dermatology treatments and the proportion of activity which is represented by skin cancer. 
We quote rates per 1,000 population and standard deviation (SD) to give an indication of 
variation between localities. 
8.4 Results 
Skin diseases are prevalent in the UK and affect 54% of population (Mashayekhi & 
Hajhosseiny, 2013). 2000 skin disorders are now recognized, of which 100 are common with 
the skin cancer among the commonest cancer in young adults. A screen shot from the 
program is presented on [Figure 18A]. The top bar (arrow 1) gives links to the front page 
dashboard with summary information about the set, the data filter of the WebEnable HES 
system, and the Attribute drop-down menu which allows the users to navigate into (and 
edit) collections of information and linked data, individually (e.g. practices, trusts, code 
groups).  
 Figure 18 - Web-enablement HES: A demonstration project 
An overview of the dataset information is displayed (arrow 2), showing a count of the items 
in each dataset. Basic system performance information is displayed (arrow 3), showing 
response time at database and application level. [Figure 18B] shows the information 
processing page. There are five information views (arrow 1), each with a count of the total 
data items. These scopes are hyperlinks that filter data accordingly (for example A&E or 
outpatients). Results (arrow 2) are paginated to 30 rows per page, which can be navigated 
using links at the page footer. The large number in bold, displays the total number of results 
in the current view. Data (arrow 3) is displayed in a table; each column is sortable using the 
column header. Diagnosis codes are processed (arrow 4) and are transformed into a valid 
URL in the World Health Organisation online dictionary for immediate translation. 
The filter menu enables the dataset to be queried with a number of parameters. A filter can 
contain multiple locality and provider codes, which are selected using a user-friendly design. 
The information scopes are updated when a filter is applied. This enables the user to 
instantly see and compare the data sources. 
Analysis of variations between localities revealed that there were 68.9 (SD 25.0) outpatient 
consultations per 1,000 population per year; 31/1,000 people were inpatients, of whom 26% 
were treated as day cases; more males were treated than females. Skin cancer represented 
around 10% of dermatology inpatient activity (2.4/1000, SD 1.32). 
We found that female population is prevalent by six per cent over male population in the UK 
[Table 14]. However, there are more man than women in some areas on the North, West 
and South of the UK. The population of patients older than 75 years old is distributed 
unevenly in England. We also found wide ethnic diversity across different localities. 
Table 14 - Variation between localities may explain difference in service requirements 
Although the exact geographical areas for consortia yet to be defined, nearly half of primary 
care trusts are expecting a single commissioning consortium to span the same geographical 
area that they currently cover (West, 2011). The Web-enabled system allows one or more 
practices to be selected and the activity for their practice to be benchmarked with national 
or regional data. 
8.5 Discussion 
The simple searches enable groups of practices to compare their dermatology activity with 
national or regional rates. This can be a starting point for exploring the effectiveness and 
appropriateness of services provided. 
Local CCGs will be expected to commission health services to meet the needs of local 
communities, there is a need to develop models for decision-making expertise. Tools such as 
this can be starting point. They also enable comparisons between health service hospitals, 
private healthcare providers and family practices that opt to include specialty dermatology 
services (Erler, et al., 2011) (English, 2010). 
Another priority of the clinical commissioning groups, aimed for the reduction of 
hospitalization rate, will be identification patients with the highest risk of re-admission to 
hospital and organization of home-based services in their communities. New systems of 
Locality Male Female >75 years old %non white 
London 49.2% 50.8% 19.3% 38.3% 
Bristol 51.6% 48.4% 23.2% 8.1% 
Brighton 51.1% 48.9% 26.7% 5.0% 
North Yorkshire 51.9% 48.1% 29.4% 1.4% 
Norfolk 49.6% 50.4% 34.2% 1.3% 
UK 47.0% 53.0% 20.3% 9.5% 
integration and processing of medical data such as this may allow rapid and efficient exchange 
and analysis of medical information. 
Hospital Episode Statistics (HES) is large scale dataset including details of all admissions 
to National Health System (NHS) hospitals in England and private providers commissioned 
by the NHS. More than 12 million new records are added each year and cover a range of 
topics, including operations and diagnoses. However, limited research methods are available 
for the processing and analysis of large scale datasets (de Lusignan & van Weel, 2006). We 
have created an application to enable non-experts to answer questions about health service 
utilization and make comparisons by simply searching a database system HES. Our 
application allows rapid and efficient analysis of individual differences in the patients 
population between NHS localities and may be used for the development of the efficient 
quality improvement strategies. 
Our study has several limitations including a relatively small selected sample of NHS 
localities and the fact that we used only medical data containing socio-demographic records 
while the patients without these data were not included in the analysis. Further work is 
also required to include the disease-specific parameters search options. 
8.6 Conclusions 
The development of locality-based primary care led healthcare systems focused on the 
community needs to be underpinned by access to data. Those responsible for providing 
commissioning require new models of medical data access and simplified comparisons. 
Nationally defined datasets, although imperfect, provide a basis for making comparisons. 
Local commissioners needs can then be at least partially met by the Web-enablement of 
these national datasets. 
It is, however, important to note that we propose a means for facilitating change  an 
effective way of enabling rapid, or fast-track routes to data access for research purposes. 
Even with its imperfections, and in some cases blockers to innovation (for example due to 
patient confidentiality issues or ethical considerations), Web-enablement could add value and 
help solve data interoperability issues. 
CHAPTER 9 Conclusions 
9.1 Revisiting the hypothesis 
This thesis was motivated by the need for access to ever-growing datasets and data. This is an 
issue, and an opportunity, across domains and principles. We hypothesised that key to high 
quality research output is consistent data recording, and an approach from data entry to 
research output which facilitates traceability, provenance and curation. 
We proposed a method that generates metadata from entities identified through, and filtered 
using, input queries both on the Web, as well as in data repositories for health with the 
creation of Data Dictionaries and the supply of summary data online; processes that can aid 
human decision-making. We are a long way away from replacing humans that can shed a 
critical eye and make decisions  especially in sensitive domains such as health. We can, 
however, provide them with more information, and more summarised information. A process 
that gathers metadata to confidently provide these summary data is shown to make a positive 
impact to research. We tested two use-cases: Business, through company profiling with a 
collaborative project with UKTI and Health, through research projects at St. Georges 
University and University of Surrey. In both cases, we identified the importance of metadata 
for case finding and presented the merits of an ontologically rich approach. 
The hypothesis and the methods of experimentation, posed the question: would a system that 
provides access to the same underlying database, curated and with the inclusion of 
automatically-generated data dictionaries to accompany the dataset as metadata, enable 
rapid case finding and facilitate quality improvement in care? 
We attempted to identify answers in the two domains assessed: 
Business/Company data: Analysts from the Department for International Trade (formerly 
UK Trade & Investment or UKTI) examined the results of the automated domain-name 
identification algorithm, as well as the identified news sources and Webpages that were 
classified as positive, negative or neutral articles for a given set of entities as presented in 
chapter 4. The system reduced the amount of time analysis required, compared to experts 
manually applying search engine queries, in other words without the use of an automated 
engine. 
Health data: In a system which was the same as the above, we had medical-trained 
researchers assess the output of the Web-enabled access system to both instantly see, as well 
as compare results with national or regional data. A simple process, allowed rapid and efficient 
exchange and analysis of medical information. 
9.2 Innovation driven by growth  future research direction 
The Web is currently the largest information source, but literature indicates that it is also the 
most valuable (Bar-Ilan, 2001). There exists increasing evidence that meaningful patterns can 
be extracted from Web data (Thelwall, 2002), whether that data be directly the content of 
underlying Web pages, or knowledge generated from the analysis of visits5, statistics and 
popularity of Websites. 
The explosive growth in data and databases has resulted in the need to develop new 
technologies and tools to process data into useful information and knowledge intelligently and 
automatically. Data mining is yet another area with increasing importance and popularity 
since the academic community recognised the importance of this science, especially in the 
business and press communities (Fayyad, et al., 1996). This was a particularly important move 
from the 1960s misconceptions, where data fishing or data dredging (the then name for 
5 Any data can be used for research, with quantifiable contribution. For example, Google uses 
mathematical calculations on a huge matrix in order to extract meaning from the link structure of the 
Web (Brin & Page, 1998) 
Data Mining) were considered bad practices tied to data analysis without an a-priory 
hypothesis. In recent years, there is interest in the Data Mining science particularly in the 
healthcare and medical insurance industry for example as a means for predicting health 
outcomes and providing policy information for conditions such as hypertension (Young, et al., 
2001), through to the use of mining techniques to extrapolate data from Apps for healthy 
lifestyle recommendations such as successful weight loss (Serrano, et al., 2016). 
9.2.1 Data meet Business, data meet Health  exploration beyond these domains 
Data are important. Rapid analysis and decisions, can unlock better health systems worldwide; 
but also in business, for example for measuring and detecting operational risk  currently a 
complicated task and a fundamental element of business success (Koyuncugil & Ozgulbas, 
2012). Data mining tools can answer business questions that traditionally were too time 
consuming to resolve. They scour databases for hidden patterns, finding predictive information 
that experts may miss because it lies outside their expectations (Thearling, n.d.). Availability 
of data about data, or metadata, is important for the above tools to operate, and human 
analysts to perform their tasks for efficiently. 
For continuation of this research, we propose the introduction of temporal profiles in the data 
analysis process. So far, the company profiling method demonstrated ability to identify 
positive, negative or neutral indicators associated with entities. Together these will provide an 
initial, basic outlook barometer enabling third-party systems to make decisions about entities 
for decision support mechanisms. 
As a further contribution, a more intelligent approach to Entity Profiling should be considered, 
by generating historical outputs (temporal snapshots) important for intelligent systems in the 
new Web era  this is due to the rapidly changing information. Comparisons between the 
snapshots can demonstrate meaningful patterns (Hu, et al., 2009). 
As a direct translation of this to the industry that was studied, the key contribution could be a 
flow of Web-generated information, business performance, activity and commentary on existing 
foreign-investor target companies to monitor their growth and diversification profiles but also 
to set more effective relationships in place, based on company-specific business intelligence 
triggers. We see no reason why the same principle cannot be applied to health data, and 
beyond (different domains, outside the medical one  for example the insurance sector), with 
potentially useful outcomes, expanding on the commonly-used time series analysis. 
Bibliography 
1. Abdel Wahab, M. M., Nofal, L. M., Guirguis, W. W. & Mahdy, N. H., 2004. Statistical 
process control for referrals by general practitioner at Health Insurance Organization 
clinics in Alexandria. Journal of Egyptian Public Health Association, 79(5-6), pp. 415-
2. Ahmad, K., Gillam, L. & Tostevin, L., 1999. Weirdness Indexing for Logical Document 
Extrapolation and Retrieval (WILDER). Washington, National Institute of Standards 
and Technology, pp. 717-724. 
3. American Hospital Association, 2015. Why Interoperability Matters, s.l.: American 
Hospital Association. 
4. Amor, D., 2000. The e-business (r)evolution: living and working in an interconnected 
world. Upper Saddle River: Prentice Hall PTR. 
5. Anderson, J., 1986. Data dictionaries - a way forward to write meaning and 
terminology into medical information systems. Methods of Information in Medicine, 
Jul, 25(3), pp. 137-8. 
6. Apache Software Foundation, n.d. Apache Lucene. [Online]  
Available at: http://lucene.apache.org 
[Accessed 15 06 2010]. 
7. Apache Software Foundation, n.d. Elastic Search. [Online]  
Available at: http://www.elasticsearch.com 
[Accessed 30 09 2010]. 
8. Aqil, A., Lippeveld, T. & Hozumi, D., 2009. PRISM framework: a paradigm shift for 
designing, strengthening and evaluating routine health information systems. Health 
Policy and Planning, Volume 24, pp. 217-228. 
9. Arts, D. G. T., Keizer, N. F. & Scheffer, G. J., 2002. Defining and Improving Data 
Quality in Medical Registries: A Literature Review, Case Study, and Generic 
Framework. J Am Med Inform Assoc, Volume 9, pp. 600-11. 
10. Aylin, P., Best, N., Bottle, A. & Marshall, C., 2003. Following Shipman: a pilot system 
for monitoring mortality rates in primary care. Lancet, 362(9382), pp. 485-91. 
11. Bar-Ilan, J., 2001. Data collection on the web for informetric purposes - a review and 
analsis. Scientometrics, 50(1), pp. 7-32. 
12. Beale, R., n.d. Information fragments for a pervasive world. New York, NY, USA, 
ACM, pp. 48-53. 
13. Belsey, J. et al., 2008. Abnormal lipids in high-risk patients achieving cholesterol 
targets: a cross-sectional study of routinely collected UK general practice data. Current 
Medical Research and Opinion, 24(9), pp. 2551-60. 
14. Beresford, N. A. et al., 2004. Estimating radionuclide transfer to wild species - data 
requirements and availability for terrestrial ecosystems. Journal Radiol Prot, Dec, 
24(4A), pp. A89-103. 
15. Berners-Lee, T., 1989. Information Management: A Proposal.  
16. Berners-Lee, T., 1998. What the Semantic Web can represent. [Online]  
Available at: http://www.w3.org/DesignIssues/RDFnot.html 
[Accessed 28 11 2009]. 
17. Berners-Lee, T., Hendler, J. & Lassila, O., 2001. The Semantic Web. Scientific 
American Magazine. 
18. Bertaud-Gounot, V., Duvauferrier, R. & Burgun, A., 2012. Ontology and medical 
diagnosis. Informatics for Health and Social Care, 37(1), pp. 22-32. 
19. Beynon-Davies, P., 2002. Information Systems: An introduction to informatics in 
organisations. s.l.:Palgrave Macmillan. 
20. Biomedical Research Integrated Domain Group (BRIDG), n.d. Biomedical Research 
Integrated Domain Group (BRIDG). [Online]  
Available at: http://www.cdisc.org/bridg 
21. Bizer, C., Heath, T. & Berners-Lee, T., 2009. Linked Data - The Story So Far. 
International Journal on Semantic Web and Information Systems, 5(3), pp. 1-22. 
22. Bohensky, M. A. et al., 2010. Data Linkage: A powerful research tool with potential 
problems. BMC Health Serv Res, Volume 10, p. 346. 
23. Boult, C., Karm, L. & Groves, C., 2008. Improving chronic care: the 'guided care' 
model. The Permanente Journal, 12(1), pp. 50-4. 
24. Brin, S. & Page, L., 1998. The anatomy of a large scale hyper-textual web search 
engine. Computer Networks and ISDN Systems, 30(1-7), pp. 107-117. 
25. Brown, P. J. & Warmington, V., 2003. Info-tsunami: surviving the srotm with data 
quality probes. Informatics in Primary Care, 11(4), pp. 229-33; discussion 234-7. 
26. Buettcher, S., Clarke, C. L. A. & Cormack, G. V., 2010. Information Retrieval: 
Implementing and Evaluating Search Engines. s.l.:The MIT Press. 
27. Buneman, P., Khanna, S. & Tan, W. C., 2000. Data provenance: Some basic issues. 
FST TCS 2000: Proceedings, Volume 1974, pp. 87-93. 
28. Buscaldi, D., 2010. Ambiguous Place Names on the Web, Puebla: Universidad 
Politecnica de Valencia. 
29. Carey, I. M. et al., 2003. Implications of the problem orientated medical record 
(POMR) for research using electronic GP databases: a comparison of the Doctors 
Independent Network Database (DIN) and the General Practice Research Database 
(GPRD). BMC Fam Pract, 4(14). 
30. Chakrabarti, S., 2002. Mining the Web - Discovering Knowledge from Hypertext Data. 
1st Edition ed. San Francisco: Morgan Kaufmann Publishers. 
31. Chakrabarti, S., 2002. Mining the Web: Analysis of Hypertext and Semi Structured 
Data. s.l.:Morgan Kaufmann. 
32. Chalmers, D., French, R. & Hofstadter, D., 1992. High-Level Perception, 
Representation and Analogy. Journal of Experimental and Theoretical Artificial 
Intelligence, Volume 4, pp. 185-211. 
33. Cheney, J., Chiticariu, L. & Tan, W. C., 2007. Provenance in Databases: Why, How 
and Where. Foundations and Trends in Databases, 1(4), pp. 379-474. 
34. Chen, P. & Pin-Shan, P., 1976. The Entity-Relationship Model - Toward a Unified 
View of Data. ACM Transactions on Database Systems, 1(1), pp. 9-36. 
35. Chief Information Officer (CIO), 2003. Net-centric data strategy. [Online]  
Available at: http://cio-nii.defense.gov/docs/net-centric-data-strategy-2003-05-092.pdf 
36. Clinical Data Interchange Standards Consortium (CDISC), n.d. Clinical Data 
Interchange Standards Consortium (CDISC). [Online]  
Available at: http://www.cdisc.org 
37. Codd, E. F., 1970. A Relational Model of Data for Large Shared Data Banks. 
Communications of the ACM, 13(6), pp. 377-387. 
38. Coffman, K. G. & Odlyzko, A. M., 1998. Growth of the Internet, s.l.: AT&T Labs - 
Research. 
39. Coloma, P. M. et al., 2011. Combining electronic healthcare databases in Europe to 
allow for large-scale drug safety monitoring: the EU-ADR Project. s.l., John Wiley & 
Sons. 
40. Cooley, R., Mobasher, B. & Srivastava, J., 1997. Web Mining: Information and Pattern 
Discovery on the World Wide Web. s.l., s.n., p. 558. 
41. Daconta, M. C., Obrst, L. J. & Smith, K. T., 2003. The Semantic Web: A Guide to the 
Future of XML, Web Services and Knowledge Management. s.l.:John Wiley & Sons. 
42. Davies, J., Studer, R. & Warren, P., 2006. Semantic Web Technologies: Trends and 
Research in Ontology-based Systems. s.l.:Wiley. 
43. Davis, P. et al., 2004. The New Zealand Socioeconomic Index of Occupational Status: 
methodological revision and imputation for missing data. Aust N Z J Public Health, 
28(2), pp. 113-9. 
44. de Lusignan, et al., 2009. The QICKD study protocol: a cluster randomised trial to 
compare quality improvement interventions to lower systolic BP in chronic kidney 
disease (CKD) in primary care. Implementation Science. 
45. de Lusignan, S., 2005. Codes, classifications, terminologies, and nomenclatures: 
definition, development and application in practice: a theme of the European 
Federation for Medical Informatics. Informatics in Primary Care, 13(1), pp. 65-69. 
46. de Lusignan, S., 2006. The optimum granularity for coding diagnostic data in primary 
care: report of a workshop of the EFMI. Primary Care, Volume 14, pp. 133-7. 
47. de Lusignan, S., 2007. An educational intervention, involving feedback of routinely 
collected computer data, to improve cardiovascular disease management in UK primary 
care. Methods Inf Med, 46(1), pp. 57-62. 
48. de Lusignan, S. et al., 2012. Referral to a new psychological therapy service is 
associated with reduced utilisation of healthcare and sickness absence by people with 
common mental health problems: a before and after comparison. Journal of 
Epidemiology and Community Health, 66(6). 
49. de Lusignan, S., Chan, T., Theadom, A. & Dhoul, N., 2007. The roles of policy and 
professionalism in the protection of processed clinical data: a literature review. 
International Journal of Medical Informatics, 76(4), pp. 261-8. 
50. de Lusignan, S., Hague, N., van Vlymen, J. & Kumarapeli, P., 2006. Routinely-
collected general practice data are complex, but with systematic processing can be used 
for quality improvement and research. Informatics in Primary Care, 14(1), pp. 59-66. 
51. de Lusignan, S. et al., 2010. A method of identifying and correcting miscoding, 
misclassification and misdiagnosis in diabetes: a pilot and validation study of routinely 
collected data. Diabetic Medicine, Feb, 27(2), pp. 203-9. 
52. de Lusignan, S. & Krause, P., 2010. The Hayes principles: learning from the national 
pilot of information technology and core generalisable theory in informatics. 
Informatics in Primary Care, 18(2), pp. 73-7. 
53. de Lusignan, S. et al., 2011. Key concepts to assess the readiness of data for 
international research: data quality, lineage and provenance, extraction and processing 
errors, traceability, and curation. Yearbook of Medical Informatics, 6(1), pp. 112-20. 
54. de Lusignan, S. & Mimnagh, C., 2006. Breaking the first law of informatics: the 
Quality and Outcomes Framework (QOF) in the dock. Informatics in Primary Care, 
14(3), pp. 153-6. 
55. de Lusignan, S. et al., 2011. What are the barriers to conducting international research 
using routinely collected primary care data. Stud Health Technol Inform, Volume 165, 
pp. 135-40. 
56. de Lusignan, S. et al., 2010. Miscoding, misclassification and misdiagnosis of diabetes in 
primary care. Diabetes Medicine, Feb, 29(2), pp. 181-9. 
57. de Lusignan, S., Siaw-Teng, L., Michalakidis, G. & Jones, S., 2011. Defining datasets 
and creating data dictionaries for quality improvement and research in chronic disease 
using routinely collected data: an ontology-driven approach. Informatics in Primary 
Care, Issue 19, pp. 127-134. 
58. de Lusignan, S., Stephens, P. N., Adal, N. & Majeed, A., 2002. Does feedback improve 
the quality of computerized medical records in primary care?. Journal of the American 
Medical Informatics Association, 9(4), pp. 395-401. 
59. de Lusignan, S. & Teasdale, S., 2007. Achieving benefit for patients in primary care 
informatics: the report of an international consensus workshop at Medinfo 2007. 
Informatics in Primary Care, 15(4), pp. 255-61. 
60. de Lusignan, S. et al., 2004. Problems with primary care data quality: osteoporosis as 
an exemplar. Informatics in Primary Care, 12(3), pp. 147-56. 
61. de Lusignan, S. & van Weel, C., 2006. The use of routinely collected computer data for 
research in primary care: opportunities and challenges. Fam Pract., 23(2), pp. 253-63. 
62. Debar, S., Kumarapeli, P., Kaski, J. C. & de Lusignan, S., 2010. Addressing modifiable 
risk factors for coronary heart disease in primary care: an evidence-base lost in 
translation. Family Practice, August, 27(4), pp. 370-8. 
63. do Amaral, M., Roberts, A. & Rector, A., 2000. NLP techniques associated with the 
OpenGALEN ontology for semi-automatic textual extraction of medical knowledge: 
abstracting and mapping equivalent linguistic and logical constructs.. Los Angeles, 
AMIA. 
64. Doctorow, C., 2001. Metacrap: Putting the torch to seven straw-men of the meta-
utopia. s.l., s.n. 
65. Dorai, G. K. & Yacoob, Y., 2001. Facilitating Semantic Web Search with Embedded 
Grammar Tags. s.l., s.n. 
66. Dourra, H. & Siy, P., 2002. Investing using technical analysis and fuzzy logic. Fuzzy 
sets and systems. 
67. Duncan, M., n.d. Medical terminology version control discussion paper: The chocolate 
teapot (Version 2.3). [Online]  
Available at: http://www.mrtablet.demon.co.uk/chocolate_teapot_lite.htm 
68. Durham, E., Xue, Y., Kantarcioglu, M. & Malin, B., 2010. Private medical record 
linkage with approximate matching. AMIA Annual Symposium Proceedings, 13 
Nov.pp. 182-6. 
69. Elliott, J. H. & Grimshaw, J., 2015. Make sense of health data. Nature, November, pp. 
31-32. 
70. Elmagarmid, A., Rusinkiewicz, M. & Sheth, A., 1998. Management of Heterogeneous 
and Autonomous Database Systems.  
71. English, P. M., 2010. Commissioning in English NHS. The market delusion. BMJ, 11 
May.340(c2522). 
72. Erickson, T. et al., 1999. Socially translucent systems: social proxies, persistent 
conversation and the design of "babble". New York, NY, USA, ACM Press, pp. 72-79. 
73. Erler, A. et al., 2011. Preparing primary care for the future - perspectives from the 
Netherlands, England, and USA. Z Evid Fortbild Qual Gesundhwes, 105(8), pp. 571-80. 
74. European Federation for Medical Informatics (EFMI), n.d. Primary Care Informatics 
Working Group (PCI WG). [Online]  
Available at: http://www.efmi.org 
75. Fayyad, U. M., Piatesky-Shapiro, G. & Smyth, P., 1996. From data mining to 
knowledge discovery: an overview. Advances in Knowledge Discovery and Data Mining 
ed. Menlo Park(CA): AAAI Press / MIT Press. 
76. Fleisher, C. S. & Bensoussan, B., 2002. Strategic and Competitive Analysis: Methods 
and Techniques for Analyzing Business Competition. 1st Edition ed. s.l.:Pearson. 
77. Flexner, S. B., 1987. The Random House dictionary of the English language. New York: 
Random House. 
78. Fox, A. & Brewer, A. E., 1999. Harvest, yield and scalable tolerant systems. s.l., s.n., 
pp. 174-178. 
79. Fraternali, P., 1999. Tools and approaches for developing data-intensive (Web) 
applications: a survey. ACM Computing Surveys, 31(3), pp. 227-263. 
80. Gee, S., 2009. Organizing Content on Web Sites. [Online]  
Available at: http://www.usability.gov/get-involved/blog/2009/08/organizing-web-
content.html 
81. Germonprez, M. & Zigurs, I., 2008. Causal Factors for Web Site Complexity. All 
Sprouts Content. 
82. Gill, T., 1998. Metadata and the World Wide Web. Pathways to digital information, 
Volume 9. 
83. Glavic, B. & Dittrich, K., n.d. Data Provenance: A Categorization of Existing 
Approaches. [Online]  
Available at: http://subs.emis.de/LNI/Proceedings/Proceedings103/giproc-103-014.pdf 
84. Goble, C., 2002. Position Statement: Musings on Provenance, Workflow and (Semantic 
Web) Annotations for Bioinformatics. Chicago, s.n. 
85. Groth, P., Luck, M. & Moreau, L., 2005. A protocol for recording provenance in 
service-oriented grids. Grenoble, Springer-Verlag, pp. 124-39. 
86. Groth, P., Munroe, S., Miles, S. & Moreau, L., 2008. Applying the Provenance Data 
Model to a Bioinformatics Case. [Online]  
Available at: http://www.mendeley.com/pro-files/paul-
groth/document/861379562/#highlighted 
87. Gruber, T., 1995. Toward principles for the design of ontologies used for knowledge 
sharing. International Journal of Human-Computer Studies, 43(4-5), pp. 907-28. 
88. Hanauer, D. A. et al., 2007. The registry case finding engine: an automated tool to 
identify cancer cases from unstructured, free-text pathology reports and clinical notes. 
Journal of the American College of Surgeons, November, 205(5), pp. 690-7. 
89. Harvey, A. Y., Ives, Z. G., Suciu, D. & Tatarinov, I., 2003. Schema Mediation in Peer 
Data Management Systems. ICDE. 
90. Hassey, A., Gerrett, D. & Wilson, A., 2001. A survey of validity and utility of 
electronic patient records in a general practice. The BMJ, 322(7299), pp. 1401-5. 
91. Health and Social Care Information Centre, Information Governance and Data Quality, 
2012. The quality of nationally submitted health and social care data, s.l.: NHS. 
92. Heer, J. & Robertson, G., 2007. Animated transactions in statistical data graphics. 
IEEE Transactions on Visualisation and Computer Graphics, 13(6), pp. 1240-1247. 
93. Hendler, J., Berners-Lee, T. & Miller, E., 2002. Integrating Applications on the 
Semantic Web. Journal of the Institute of Electrical Engineers of Japan, October, 
122(10), pp. 676-680. 
94. Herrera-Viedma, E., Pasi, G., Lopez-Herrera, A. G. & Porcel, C., 2006. Evaluating the 
information quality of Web sites: A methodology based on fuzzy computing with 
words. Journal of the Association for Information Science and Technology, Volume 57, 
pp. 538-549. 
95. HL-7 Connection, n.d. Chocolate Teapot Not Otherwise Classified. [Online]  
Available at: http://www.hl7connection.com/2011/05/chocolate-teapot-not-otherwise-
classified/ 
96. Hotho, A. & Stumme, G., 2007. Mining the World Wide Web - Methods, Applications, 
and Perspectives. Volume 3, pp. 5-8. 
97. Hu, C., Xu, L., Shen, G. & Fukushima, T., 2009. Temporal Company Relation Mining 
from the Web. Advances in Data and Web Management, Volume 5446, pp. 392-403. 
98. Hummers-Pradier, E. et al., 2008. Simply no time? Barriers to GPs' participation in 
primary health care research. Family Practice, 25(2), pp. 105-12. 
99. Hunt, J., 2014. Data Quality and Clinical Coding for Improvement [Interview] (2 July 
2014). 
100. Institute of Medicine, 2011. Finding what works in Health Care: Standards for 
systematic reviews. s.l.:The National Academies Press. 
101. International Medical Informatics Association (IMIA), n.d. Primary Health 
Care Informatics Working Group. [Online]  
Available at: http://www.imia-medinfo.org/new2 
102. Investopedia, n.d. Efficiency Ratio - Investopedia. [Online]  
Available at: http://www.investopedia.com/terms/e/efficiencyratio.asp 
[Accessed 05 08 2010]. 
103. Jackson, M. S. & Burden, J. P. H., 1999. WWLib-TNG - new directions in 
search engine technology. s.l., s.n. 
104. Johnson, M., n.d. Competitive profiling with financial ratio analysis.  
105. Karasti, H., Baker, K. S. & Halkola, E., 2006. Enriching the Notion of Data 
Curation in E-Science: Data Managing and Information Infrastructuring in the Long 
Term Ecological Research (LTER) Network. Computer Supported Cooperative Work, 
Volume 15, pp. 321-58. 
106. Kaufman, P. J., 2005. New trading systems and methods. 4th Edition ed. 
s.l.:Wiley. 
107. Khabbazi, M. R., Yusof Ismail, M. D., Ismail, N. & Mousavi, A. S., 2010. 
Modeling of Traceability Information System for Material Flow Control Data. 
Australian Journal of Basic and Applied Sciences, 4(2), pp. 208-16. 
108. Klain, D. A. & Rota, G. C., 1997. Introduction to Geometric Probability.  
109. Kosala, R. & Blockeel, H., 2000. Web mining research: a survey. ACM 
SIGKDD Explorations Newsletter, June, 2(1), pp. 1-15. 
110. Kostopoulou, O., Delaney, B. C. & Munro, C. W., 2008. Diagnostic difficulty 
and error in primary care - a systematic review. Family Practice, 25(6), pp. 400-13. 
111. Koyuncugil, A. S. & Ozgulbas, N., 2012. Financial Profiling for Detecting 
Operational Risk by Data Mining. Expert Systems with Applications: An International 
Journal archive, May, 39(6), pp. 6238-6253. 
112. Kuc, T. Y., Lee, J. & Nam, K., 1992. An iterative learning control theory for a 
class of nonlinear dynamic systems. Automatica, 28(6), pp. 1215-1221. 
113. Kushmerick, N., 2002. Finite-state approaches to Web information extraction. 
Rome, s.n. 
114. Kushmerick, N., 2003. Finite-State Approaches to Web Information Extraction. 
Berlin: Springer-Verlag Berlin Heidelberg. 
115. Kushniruk, A., Borycki, E., Kuwata, S. & Kannry, J., 2006. Predicting changes 
in workflow resulting from healthcare information systems: ensuring the safety of 
healthcare. Healthc Q, Oct. 
116. Lamas, E., Barh, A., Brown, D. & Jaulent, M.-C., 2015. Ethical, Legal and 
Social Issues related to the health data in the research and public health research. s.l., 
European Federation for Medical Informatics. 
117. Lang, S. M. & Lockermann, P. C., 1995. Datenbankeinsatz. Berlin: Springer-
Verlag Berlin Heidelberg. 
118. Lanter, D., 1991. Design of a Lineage-Based Metadata Base for GIS. 
Cartography and Geographic Information Systems, 18(4), pp. 255-61. 
119. Lavoi, F., 2004. The Open Archival Information System Reference Model: 
Introductory Guide Microform and Imaging Review. Microform & Digitization Review, 
June, 33(2), pp. 68-81. 
120. Lee, E. S., McDonald, D. W., Anderson, N. & Tarczy-Hornoch, P., 2009. 
Incorporating collaboratory concepts into informatics in support of translational 
interdisciplinary biomedical research. International Journal of Medical Informatics, 
January, 78(1), pp. 10-21. 
121. Leskovec, J., Rajaraman, A. & Ullman, J. D., 2014. Mining of Massive 
Datasets. Second Edition ed. California: s.n. 
122. Liaw, S. T. et al., 2011. Health reform: is current electronic information fit for 
purpose?. Emergency Medicine Australasia, Sep. 
123. Lord, P., Macdonald, A., Lyon, L. & Giarretta, D., 2004. From Data Deluge to 
Data Curation. Proceedings of the UK e-Seience All Hands meeting, pp. 371-5. 
124. Madhavaram, M., Ali, D. L. & Zhou, M., 1996. Integrating Heterogeneous 
Distributed Database System. Computers & Industrial Engineering, 31(1-2), pp. 315-
125. Madria, S. K., Bhowmick, S. S. & Lim, E. P., 2002. Research Issues in Web 
Data Mining. Lecture Notes in Computer Science, 1 March, pp. 303-312. 
126. Maloney, D., 1984. Rule-governed approaches to physics - Newton's third law. 
Phys. Educ., 19(37). 
127. Marshall, C. C. & Shipman, F. M., 2003. Which semantic web?. Nottingham, 
128. Marston, L. et al., 2010. Issues in multiple imputation of missing data for large 
general practice clinical databases. Pharmacoepidemiol Drug Saf, 19(6), pp. 618-26. 
129. Mashayekhi, S. & Hajhosseiny, R., 2013. Dermatology, an interdisciplinary 
approach between community and hospital care. Journal of the Royal Society of 
Medicine Short Reports, 5 Jun, 4(7), pp. 1-4. 
130. McCool, R., 2006. Rethinking the Semantic Web, Part 2. IEEE Internet 
Computing, Volume 10, pp. 93-96. 
131. McDaniel, G., 1994. IBM Dictionary of Computing. 10th Edition ed. New Yord: 
McGraw-Hill. 
132. McIlraith, S. A. & Martin, D. L., 2003. Bringing semantics to Web services. 
IEEE Intelligent Systems, 18(1), pp. 90-93. 
133. Meenakshi, S., 2015. Ruby on Rails: An Agile Developer's Framework. 
International Journal of Computer Applications, 112(1). 
134. Meng, W., Yu, C. & Liu, K. L., 2002. Building efficient and effective 
metasearch engines. ACM Computing Surveys, 34(1), pp. 48-89. 
135. Michalakidis, G. et al., 2010. A system for solution-orientated reporting of 
errors associated with the extraction of routinely collected clinical data for research and 
quality improvement. Stud Health Technol Inform, 160(1), pp. 724-728. 
136. Microformats Initiative, n.d. Microformats Initiative. [Online]  
Available at: http://www.microformats.org 
[Accessed 23 10 2010]. 
137. Moellman, D. & Cain, J., 1998. Intelligence, mapping and geospatial 
exploitation system (IMAGES). Proceedings of Digitization of the Battlespace III, 
Volume 3393, pp. 86-95. 
138. Moreau, L. et al., 2010. The open provenance model core specification (v1.1). 
Future Generation Computer Systems, July. 
139. Moreira, J., Midkiff, S., Gupta, M. & Lawrence, R., 1999. High Performance 
Computing with the Array Package for Java: A Case Study using Data Mining. s.l., 
ACM/IEEE. 
140. Moulene, M. V. et al., 2007. Assessing the Impact of Recording Quality Target 
Data on the GP Consultation Using Multi-Channel Video. s.l., s.n. 
141. Muttitt, S. C. & Alvarez, R. C., 2007. Chronic disease management: it's time 
for transformational change. Healthcare Papers, 7(4), pp. 43-7; discussion 68-70. 
142. MySQL AB, 2002. MySQL Bible. New York: Wiley Publishing, Inc.. 
143. Navathe, S. & Kerschber, L., 1986. Role of data dictionaries in information 
resource management. Information and Management, 10(1), pp. 21-46. 
144. Ning, X., Jin, H. & Wu, H., 2008. RSS: A framework enabling ranked search on 
the semantic web. Information Processing Management, 44(2), pp. 893-909. 
145. Noy, N. F. et al., 2001. Creating Semantic Web Contents with Protege-2000. 
IEEE Intelligent Systems, 16(2), pp. 60-71. 
146. Nur, U. et al., 2010. Modelling relative survival in the presence of incomplete 
data: a tutorial. International Journal of Epidemiology, 39(1), pp. 118-28. 
147. Oliver, K. M. & Bennett, L. T., 1997. Evaluating the Quality of Internet 
Information Sources. Calgary, Department of Instructional Technology The University 
of Georgia. 
148. O'Mullane, M., McHugh, S. & Bradley, C. P., 2010. Informing the development 
of a national diabetes register in Ireland: a literature review of the impact of patient 
registration on diabetes care. Informatics in Primary Care, 18(3), pp. 157-68. 
149. Oxfordshire MAAG, 2000. Case study: a review of the Oxfordshire scheme. 
Collection of health data from general practice, s.l.: Oxford: Primary Care Information 
Services (PRIMIS). 
150. Pan, J., Chen, K. & Hsu, W., 2008. Self Risk Assessment and Monitoring for 
Cardiovascular Disease Patients Based on Service-Oriented Architecture. Computers in 
Cardiology, Volume 35, pp. 637-40. 
151. Park, J.-R., 2009. Metadata quality in digital repositories: a survey of the 
current state of the art. Cataloging & Classification Quarterly, 47(3-4), pp. 213-28. 
152. Parr, T. J. & Quong, R. W., 1995. ANTLR: a predicated-LL(k) parser 
generator. Software - Practice & Experience, 25(7), pp. 789-810. 
153. Peterson, K., 2006. Practice-based primary care research - translating research 
into practice through advanced technology. Family Practice, Volume 23, pp. 149-50. 
154. Pringle, M., Ward, P. & Chilvers, C., 1995. Assessment of the completeness 
and accuracy of computer medical records in four practices committed to recording 
data on computer. Br J Gen Pract, 45(399), pp. 537-541. 
155. Purcell, G. P., Wilson, P. & Delamothe, T., 2002. The quality of health 
information on the internet. The BMJ, 324(557). 
156. Rahm, E. & Berstein, P., 2001. A survey of approaches to automatic schema 
matching. VLDB Journal, 10(4), pp. 334-350. 
157. Raisinghani, M., 2004. Business Intelligence in the Digital Economy: 
Opportunities, Limitations and Risks. Hershey: IGI Publishing. 
158. Rogers, P., Puryear, R. & Root, J., 2013. Infobesity: The enemy of good 
decisions. [Online]  
Available at: http://www.bain.com/publications/articles/infobesity-the-enemy-of-good-
decisions.aspx 
159. Rollason, W., Khunti, K. & de Lusignan, S., 2009. Variation in the recording of 
diabetes diagnostic data in primary care computer systems: implications for the quality 
of care. Informatics in Primary Care, 17(2), pp. 113-19. 
160. Roten, I., Marty, S. & Beney, J., 2010. Electronic screening of medical records 
to detect inpatients at risk of drug-related problems. Pharmaceutical World Science, 
32(1), pp. 103-7. 
161. Rubin, D. L. et al., 2006. National Center for Biomedical Ontology: advancing 
biomedicine through structured organization of scientific knowledge. OMICS, 10(2), pp. 
185-98. 
162. Sadek, A. R., van Vlymen, J., Khunti, K. & de Lusignan, S., 2011. Automated 
identification of miscoded and misclassified cases of diabetes from computer records. 
Diabetes Medicine, 14 Sep. 
163. Samoutis, G. A. et al., 2008. Designing a multifaceted quality improvement 
intervention in primary care in a country where general practice is seeking recognition: 
the case of Cyprus. BMC Health Service Research, 8(181). 
164. Santorini, B., 1990. Part-of-speech Tagging Guidelines from the Penn Treebank 
Project. ScholarlyCommons. 
165. Saxena, S. et al., 2007. Practice size, caseload, deprivation and quality of care 
of patients with coronary heart disease, hypertension and stroke in primary care: 
national cross-sectional study. BMC Health Service Research, 27 Jun.7(96). 
166. Seddon, J., 2008. Systems thinking in the Public Sector. s.l.:Triarchy Press. 
167. Segaran, T., 2007. Programming Collective Intelligence - Building Smart Web 
2.0 Applications. s.l.:O'Reilly Media. 
168. Serrano, K. J. et al., 2016. Mining Health App Data to Find More and Less 
Successful Weight Loss Subgroups. Journal of Medical Internet Research, June, 18(6), 
p. 154. 
169. Sharoff, S., 2006. Creating General-Purpose Corpora Using Automated Search 
Engine Queries. WaCky Working papers on the Web as Corpus.. 
170. Shelley, C., 2003. Multiple analogies in Science and Philosophy.  
171. Silva, M. M., Soares, A. L. & Simoes, D. M., 2006. Integrating Semantic 
Resources to Support SME Knowledge Communities. Saint-Etienne, s.n. 
172. Simmhan, Y. L., Plale, B. & Gannon, D., 2005. A Survey of Data Provenance 
in e-Science. SIGMOD Record, 34(3), pp. 31-6. 
173. Smith, B. & Ceusters, W., 2010. Ontological realism: a methodology for 
coordinated evolution of scientific ontologies. Applied Ontology, Volume 5, pp. 139-88. 
174. Solomon, G., Perkins, D. N. & Globerson, T., 1991. Partners in Cognition: 
Extending Human Intelligence with Intelligent Technologies. Volume 20, pp. 2-9. 
175. Song, H., Giri, S. & Ma, F., 2004. Data extraction and annotation for dynamic 
Web pages. s.l., s.n., pp. 499-502. 
176. Song, H. & Zhang, L., 2002. Object-Extraction-based Hidden web Information 
Retrieval. s.l., s.n. 
177. Spielmann, M., 1995. Foundations of Databases. Reading(MA): Addison-Wesley 
Publishing Company. 
178. Spivack, N., 2007. Radar Networks. [Online]  
Available at: www.radarnetworks.com 
179. Stefaner, M. & Muller, B., 2007. Elastic lists for facet browsers. s.l., s.n., pp. 
217-221. 
180. Stefaner, M., n.d. Well Formed Data. [Online]  
Available at: http://well-formed-data.net/ 
[Accessed 01 10 2009]. 
181. Stefaner, M., Urban, T. & Seefelder, M., 2008. Elastic lists for facet browsing 
and resource analysis in the enterprise. s.l., IEEE. 
182. Stevens, P. E., Farmer, C. K. & de Lusignan, S., 2011. Effect of pay for 
performance on hypertension in the United Kingdom. American Journal of Kidney 
Disease, Oct, 58(4), pp. 508-11. 
183. Stoilos, G. et al., 2005. Fuzzy OWL: Uncertainty and the Semantic Web. 
Proceedings of the International Workshop on OWL: Experience and Directions. 
184. Stojanovic, L., Stojanovic, N. & Volz, R., 2002. Migrating data-intensive web 
sites into the Semantic Web. Madrid, Spain, s.n. 
185. Tai, T. W., Anandarijah, S., Dhoul, N. & de Lusignan, S., 2007. Variation in 
clinical coding lists in UK general practice: a barrier to consistent data entry?. 
Informatics in Primary Care, 15(3), pp. 143-50. 
186. Tatum, J. T., Wilkinson, C. W. & Jannarone, R. J., 2000. Automatic data 
extraction, error correction and forecasting system. US. 
187. The International Standards Organization (ISO), n.d. 8402-1986 Quality 
Vocabulary. [Online]  
Available at: 
http://www.iso.org/iso/iso_catalogue/catalogue_ics/catalogue_detail_ics.htm?csnum
ber=15570 
188. Thearling, K., n.d. An Introduction to Data Mining - Discovering hidden value 
in your data warehouse. [Online]  
Available at: http://www.thearling.com/text/dmwhite/dmwhite.htm 
[Accessed 6 12 2009]. 
189. Thelwall, M., 2001. A web crawler design for data mining. Journal of 
Information Science, 27(5), pp. 319-325. 
190. Thelwall, M., 2002. Extracting macroscopic information from web links. Journal 
of the American Society for Information Science and Technology. 
191. Thiru, K., Hassey, A. & Sullivan, F., 2003. Systematic review of scope and 
quality of electronic patient record data in primary care. The BMJ, 326(7398), p. 1070. 
192. Thomas, S., 2001. Wealth of Knowledge. New York(NY): Doubleday. 
193. Tilburg University, 2016. The size of Web - Daily. [Online]  
Available at: http://www.worldwidewebsize.com 
194. TRANSFoRm Working Group, n.d. Translational Medicine and Patient Safety 
in Europe (TRANSFoRm). [Online]  
Available at: http://www.transformproject.eu 
195. Turban, E., Sharda, R. & Delen, D., 2013. Decision Support and Business 
Intelligence Systems. 9th Edition ed. s.l.:Pearson New International Edition. 
196. Turbelin, C. & Boelle, P. Y., 2010. Improving general practice based 
epidemiologic surveillance using desktop clients: the French Sentinel Network 
experience. Studies in Health Technology and Informatics, 160(Pt 1), pp. 442-6. 
197. UK Parliament, 2011. Health and Social Care Bill, s.l.: House of Lords and 
House of Commons. 
198. University of Nottingham, n.d. PRIMIS+ Discussion Board. [Online]  
Available at: http://forum.primis.nottingham.ac.uk 
199. University of Oxford, 2009. Centre for Evidence Based Medicine - Levels of 
Evidence. [Online]  
Available at: http://www.cebm.net/index.aspx?o=1025 
200. van Vlymen, J. & de Lusignan, S., 2005. A system of metadata to control the 
process of query, aggregating, cleaning and analysing large datasets of primary care 
data. Informatics in Primary Care, Volume 13, pp. 281-91. 
201. van Vlymen, J. et al., 2005. Ensuring the quality of aggregated general practice 
data: lessons from the Primary Care Data Quality Programme (PCDQ). Studies in 
Health Technology Informatics, Volume 116, pp. 1010-5. 
202. WebKnox, n.d. Visible vs Semantic vs Deep Web. [Online]  
Available at: http://www.webknox.com/blog/2009/12/visible-web-vs-semantic-web-vs-
deep-web 
[Accessed 06 01 2010]. 
203. West, D., 2011. Dozens of consortia match PCT area. [Online]  
Available at: http://www.hsj.co.uk/topics/pbc-practice-based-commissioning/dozens-
of-consortia-match-pct-area/5027223.article 
204. Wieringa, R. J., 1996. Requirements Engineering: Frameworks for 
Understanding. s.l.:Wiley. 
205. Wikipedia, n.d. Web Crawler. [Online]  
Available at: http://en.wikipedia.org/wiki/Web_crawler 
[Accessed 01 03 2010]. 
206. Williams, J. G., 2003. Measuring the completeness and currency of codified 
clinical information. Methods Inf Med, 42(4), pp. 482-8. 
207. World Wide Web Consortium (W3C), 2009. eXtensible Markup Language. 
[Online]  
Available at: http://www.w3.org/XML/ 
208. World Wide Web Consortium (W3C), 2010. OWL Web Ontology Language 
Overview. [Online]  
Available at: https://www.w3.org/TR/owl-features/ 
209. World Wide Web Consortium (W3C), n.d. Technology and Science Domain: 
Metadata and Resource Description. [Online]  
Available at: http://www.w3.org/Metadata 
210. World Wide Web Consortium (W3C), n.d. W3C - Linking Open Data. [Online]  
Available at: 
http://esw.w3.org/TaskForces/CommunityProjects/LinkingOpenData/DataSets/Statis
[Accessed 21 06 2010]. 
211. Yamamoto, S., 1995. Reconstructing data-flow diagrams from structure charts 
based on the input and output relationship. IEICE Transactions on Information and 
Systems, e78d(9), pp. 1118-26. 
212. Yi, L. & Liu, B., 2003. Web page cleaning for web mining through feature 
weighting. s.l., IJCAI. 
213. Young, M. C. et al., 2001. Data mining approach to policy analysis in a health 
insurance domain. International Journal of Medical Informatics, July, 62(2-3), pp. 103-
214. Zdun, U., 2004. Semantic Lookup in Service-Oriented Architectures. 
Proceedings of Fourth International Workshop on Web-Oriented Software 
Technologies, pp. 101-10. 
Table of Figures 
Figure 1  Demonstration of the ability of human and machine to perceive and understand 
Figure 8 - Flow diagram (Data Input and Output level) of the prototype Entity Profiling 
Figure 15 - An overview of the stages and linked quality concepts from data recording to 
Table of Tables 
Table 5 - Identified domain, Domain metadata and the Original entity name and Reference - 
Table 9 - Applications of provenance of information and overlap with pedigree and lineage ... 79	
Table 13 - Overview of challenges in gastro-oesophageal reflux disease (GORD) and 
