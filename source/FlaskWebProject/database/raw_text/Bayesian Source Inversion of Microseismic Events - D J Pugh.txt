Rupture Properties from Microseismic Events
Bayesian Source Inversion of
Microseismic Events
David James Pugh
Emmanuel College
University of Cambridge
This dissertation is submitted for the degree of
Doctor of Philosophy
Examined August 2015
Acknowledgements
This work was undertaken through a UK Natural Environment Research Council CASE
studentship (NERC Reference: NE/I018263/1) in partnership with Schlumberger. I wish
to thank my colleagues at Schlumberger Gould Research and at the Cambridge University
Bullard Laboratories for their help and advice, especially Colin Thomson andChris Chapman.
Special thanks tomy supervisors, Prof. Robert (Bob)White andDr. Philip (Phil) Christie,
for their advice and direction, along withMikeWilliams for his help and advice and providing
a sounding board for many of my ideas.
Much of the real data were acquired in Iceland between 2009 and 2014 as part of an
ongoing research project involving a large seismometer deployment, run by Bob White. A
few of the contributors to this project include Heidi Soosalu, Janet Key, Jon Tarasewicz, Juerg
Schuler, Tim Greenfield and Rob Green.
Finally, a special thanks to Victoria and my parents for their support and encouragement
during my research.
I hereby declare that this thesis entitled Bayesian Source Inversion of Microseismic
Events is the result of my own research except as cited in the references. This thesis is
not substantially the same as any that I have submitted, or is being concurrently submitted,
for a degree or diploma or other qualification at the University of Cambridge or any other
University or similar institution. I further state that no substantial part of my dissertation has
already been submitted, or is being concurrently submitted, for any such degree, diploma
or other qualification at the University of Cambridge or any other University or similar
institution. This thesis does not exceed the extension of the prescribed page limit to 250
pages agreed with the Degree Committee for the Faculty of Earth Sciences and Geography
and the Board of Graduate Studies.
Signature:
Student : David J. Pugh
Date :
Supervisors: Prof. R. S. White & Dr P. A. F. Christie
Summary
Rapid stress release at the source of an earthquake produces seismic waves. Observations of
the particle motions from such waves are used in source inversion to characterise the dynamic
behaviour of the source and to help in understanding the driving processes. Earthquakes either
occur naturally, such as in volcanic eruptions and natural geothermal fields, or are linked to
anthropogenic activities including hydrofracture of gas and oil reservoirs, mining events and
extraction of geothermal fluids.
Source inversion is very sensitive to uncertainties in both themodel and the data, especially
for low magnitude, namely microseismic, events. Many of the uncertainties can be poorly
quantified, and are often not included in source inversion.
This thesis proposes a Bayesian framework enabling a complete inclusion of uncertainties
in the resultant probability distribution using Bayesian marginalisation. This approach is
developed for polarity and amplitude ratio data, although it is possible to use any data type,
provided the noise model can be estimated. The resultant posterior probability distributions
are easily visualised on different plots for orientation and source-type. Several different
algorithms can be used to search the source space, including Monte Carlo random sampling
and Markov chain Monte Carlo sampling. Relative information between co-located events
may be used as an extension to the framework, improving the constraint on the source.
The double-couple source is the commonly assumed source model for many earthquakes,
corresponding to slip on a fault plane. Two methods for estimating the posterior model
probability of the double-couple source type are explored, one using the Bayesian evidence,
the other using trans-dimensional Markov chain Monte Carlo sampling. Results from both
methods are consistent with each other, producing good estimates of the probability given
sufficient samples. These provide estimates of the probability of the source being a double-
couple source or not, which is very useful when trying to understand the processes causing
the earthquake.
Uncertainty on the polarity estimation is often hard to characterise, so an alternative
approach for determining the polarity and its associated uncertainty is proposed. This uses
a Bayesian estimate of the polarity probability and includes both the background noise and
the arrival time pick uncertainty, resulting in a more quantitative estimate of the polarity
uncertainty. Moreover, this automated approach can easily be included in automatic event
detection and location workflows.
The inversion approach is discussed in detail and then applied to both synthetic events
generated using a finite-difference code, and to real events acquired from a temporary seis-
mometer network deployed around the Askja and Krafla Volcanoes, Iceland.
Contents
Summary iii
Nomenclature ix
1 Introduction 1
1.1 Source Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.2 Probabilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.3 Source Inversion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
1.4 Thesis Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
2 Seismic Sources 9
2.1 Source Representations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
2.2 The Moment Tensor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
2.3 Source Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
2.4 Moment Tensor Decomposition . . . . . . . . . . . . . . . . . . . . . . . . 22
2.5 Source Plots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
2.6 Volume Change of the Source . . . . . . . . . . . . . . . . . . . . . . . . 37
2.7 Moment Tensor Eigenvalue Distribution . . . . . . . . . . . . . . . . . . . 39
2.8 Non Double-Couple Sources . . . . . . . . . . . . . . . . . . . . . . . . . 43
2.9 Summary and Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
3 Source Inversion: Common Data Types and Measurement Uncertainties 50
3.1 Seismogram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
3.2 Observations: First Arrival Polarity . . . . . . . . . . . . . . . . . . . . . 55
3.3 Observations: Arrival Amplitudes . . . . . . . . . . . . . . . . . . . . . . 57
3.4 Observations: Arrival Amplitude Ratios . . . . . . . . . . . . . . . . . . . 61
3.5 Observations: Full Waveform . . . . . . . . . . . . . . . . . . . . . . . . . 64
3.6 Existing Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
3.7 Summary and Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
4 Bayesian Source Inversion 68
4.1 Bayes Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
4.2 Probability Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
4.3 Data Independence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
4.4 Source PDF Representations . . . . . . . . . . . . . . . . . . . . . . . . . 83
4.5 Summary and Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
5 Bayesian Automated Polarity Estimation 90
5.1 Probabilistic Auto-Picking . . . . . . . . . . . . . . . . . . . . . . . . . . 91
5.2 Integration With Automated Monitoring . . . . . . . . . . . . . . . . . . . 95
5.3 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97
5.4 Time benchmarking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104
5.5 Source Inversion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
5.6 Summary and Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
6 The Effects of Uncertainties on Source Inversion 111
6.1 Event Detection and Arrival Picking . . . . . . . . . . . . . . . . . . . . . 111
6.2 Hypocentre Determination . . . . . . . . . . . . . . . . . . . . . . . . . . 112
6.3 Source Inversion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
6.4 Summary and Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
7 Relative Moment Tensor Inversion 136
7.1 Exploring the Effects of Event Co-location . . . . . . . . . . . . . . . . . . 136
7.2 Observations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140
7.3 Relative Seismic Moment: The Scale Factor . . . . . . . . . . . . . . . . . 142
7.4 Posterior PDF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150
7.5 Summary and Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . 152
8 Moment Tensor Sampling Algorithms 154
8.1 Monte Carlo Random Sampling . . . . . . . . . . . . . . . . . . . . . . . 155
8.2 Markov Chain Monte Carlo Sampling . . . . . . . . . . . . . . . . . . . . 156
8.3 Algorithm Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165
8.4 Estimating the Probability of Double-Couple Source . . . . . . . . . . . . 171
8.5 Relative Moment Tensor Inversion Search Algorithms . . . . . . . . . . . . 172
8.6 Event Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176
8.7 Summary and Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . 177
9 MTINV - Source Inversion from Surface Data 180
9.1 MTINV . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180
9.2 Inversion of Surface Data from Iceland . . . . . . . . . . . . . . . . . . . . 182
9.3 Relative Amplitude Inversion . . . . . . . . . . . . . . . . . . . . . . . . . 187
9.4 Summary and Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . 189
10 Conclusions 192
References 199
A Distribution of the n-dimensional inner product 218
B Random Moment Tensors 221
B.1 Coordinate Independence . . . . . . . . . . . . . . . . . . . . . . . . . . . 221
B.2 Generating a Random Moment Tensor . . . . . . . . . . . . . . . . . . . . 223
B.3 Generating a random example of a specific type of moment tensor . . . . . 225
B.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228
C Effects of Rock Anisotropies on the Moment Tensor 230
D Homogeneous Isotropic Response functions 240
D.1 Radiation Components . . . . . . . . . . . . . . . . . . . . . . . . . . . . 240
E Maximum Entropy Distribution for Mean and Variance 242
F Truncated Gaussian Distribution 243
F.1 Moments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243
F.2 Combining Truncated Gaussian PDFs . . . . . . . . . . . . . . . . . . . . 244
G Distribution of the Ratio of Random Exponential Variables 245
H Distribution of the Ratio of Independent Log-Normally Distributed Variables 248
Nomenclature
Akaike Information Criterion (AIC). Introduced by Akaike (1974), this provides a compar-
ison between models of different parameter sizes, based on the maximum likelihood
and the model dimensions.
Anisotropy. An anisotropic medium has directionally dependent elastic parameters.
Bayesian Information Criterion (BIC). Introduced by Schwarz (1978), this provides a com-
parison betweenmodels of different parameter sizes, based on themaximum likelihood
and the number of observations.
Bijection. A bijection is a one-to-one mapping between two sets or spaces, where each
element in one set maps to only one element in the other set.
Coalesence Microseismic Mapping (CMM). An event detection algorithm introduced by
Drew et al. (2013). It combines STA/LTA detection with a migration to determine if
the arrival coalesces at a source.
Compensated Linear Vector Dipole (CLVD). A source model introduced by Knopoff and
Randall (1970) corresponding to a sudden change in shear modulus in the presence
of an axial strain with no net volume change.
Crack + Double-Couple (CDC). A source model combining both opening and slip on a fault
plane.
Cumulative Distribution Function (CDF). Gives the probability that the random variable
X has a value less than or equal to x. It is equal to the integral of the PDF from the
lower bound to x.
Double-Couple (DC). A source model corresponding to shear faulting, which is slip along
a fault plane with no opening or closing components. This is the commonly assumed
fault mechanism (e.g. Shearer, 2009) for most earthquakes.
Focal sphere. A projection of the source onto a sphere to show the orientation of the fault
surfaces and the receiver locations.
Force Balanced Accelerometer (FBA). Makes an electro-mechanical measurement of the
acceleration from the force required to maintain a test mass at a given position.
Green functions. The response of a medium to an impulsive point force.
Hydrofracture. Hydraulic fracturing of rocks using pumped fluids and proppant to improve
extraction of fluids (usually oil and gas) from host rock.
Interferometric Synthetic Aperture Radar (INSAR). Ameasurement technique using radar
interferometery to determine changes in deformation and elevation to high accuracy.
Long term average (LTA). Used in STA/LTA measurements as the baseline to compare the
short time average with.
LQT. Right handed ray coordinate system. L is the longitudinal component, T the transverse
and Q corresponds to the remaining orthogonal component.
Lune. A segment of a sphere, bounded by half of two great circles through the same pole.
Markov Chain Monte Carlo (McMC). A type of algorithm for sampling a probability dis-
tribution by constructing a Markov chain where the equilibrium distribution is a
satisfactory sample of the target probability distribution.
Message Passing Interface (MPI). A standardised interface for passingmessages on parallel
computers (Official standard documentation http://www.mpi-forum.org/docs/docs.html)
Microseismic Earthquakes. Small earthquakes, generally with moment magnitude less than
2. These events can rarely be felt at the surface, and the low energy involved makes
them difficult to measure.
Moment tensor (MT). Asource description using nine force couples in different orientations.
The moment tensor is the second order tensor in a Taylor expansion of the equivalent
force description of displacement (Julian et al., 1998a; Aki and Richards, 2002),
meaning that there are higher order moments, which can be linked to more complex
attributes of the source including source finiteness and rupture propagation (Stump
and Johnson, 1982; Julian et al., 1998a).
Monte Carlo (MC). A sampling approach which produces numerical estimates of a distri-
bution from random samples of possible parameters.
Multiplets. Multiple events occuring in the same location, with similar waveforms.
North-East-Down, NED. A right handed coordinate system with the x axis in the North
direction, y to the East, and z positive down.
Orthogonal Distance Regression (ODR). An approach for regression incorporating uncer-
tainties in both the independent and dependent variables, introduced by Boggs and
Rogers (1990a); Boggs et al. (1992).
Posterior Probability Distribution. The resultant probability distribution after using Bayes
theorem to include the model prior information, likelihood, and evidence.
Probability Density Function (PDF). A distribution of the probability that a random vari-
able takes a given value. The probability of it taking valuex is calculated by integrating
between x and x+ dx.
Root-Mean-Square (RMS). Root-mean-squared value for a series, also known as the quad-
ratic mean, defined as vRMS =
v2i .
Short Term Average (STA). Measurement of the short term variation for use in STA/LTA.
Signal-to-noise ratio (SNR). The SNR is an indicator of the noise level on a seismogram. It
can be defined in terms of power or amplitude, usually as the ratio of the noisy signal
and the noise.
Tensile Crack. Tensile opening or closing of a fault, sometimes linked to fluid flow.
Vertical Transverse Isotropy (VTI). A form of anisotropy with a vertical axis of symmetry.
ZRT. ARadial coordinate system, where Z is the vertical component, R the radial component
and T the orthogonal transverse component.
Mathematical Notation
In this thesis, the mathematical notation used represents a scalar in italicised roman (a), a
matrix in bolded italicised roman (M), and a tensor in bold italicised sans-serif (c). Ele-
ments of a matrix are shown in italicised roman (Mij) and of a tensor in italicised sans-serif
(cijkl).
[ ] is used to symbolise a discontinuity in a continuous function.
Summation notation is used throughout so aii =
i=1 aii = tr (a), although where it is
necessary to clarify the summation is written in full.
1 Introduction
Scientists and philosophers have long been interested in understanding the structure of the
Earth and the processes driving the planets dynamic behaviour. While the ancient Chinese
constructed a rudimentary method for measuring the motion of the Earth and estimating the
location of an earthquake1, modern seismometers mostly derive from instruments which use
the motion of a mass to measure the seismic waves, such as those developed by James Forbes;
Filippo Cecchi; and John Milne, James Ewing and Thomas Grey, three British professors
teaching in Japan in the late nineteenth century (Dewey and Byerly, 1969). The development
of modern seismometers have enabled seismologists to investigate the structure of the Earth,
including deducing the existence of a solid inner core inside the molten outer core (Lehman,
1936), and finding evidence to support the theory of plate tectonics, such as the observation
of Wadati-Benioff zones of deep seismicity linked to subducting slabs (Wadati, 1928, 1935).
There are several different types of source which generate seismic waves, including
earthquakes and landslides. These seismic waves travel through the earth and include several
different phases, related to the type of motion of the wave. The main classification of these
phases is into P-waves (primary), which are compressional waves that behave like sound
waves in air or water, and S-waves (secondary or shear), which are shear waves with motion
transverse to the direction of propagation and are similar to waves on a string. These two
types of wave propagate at different velocities related to the elastic properties of the medium,
although there can be additional complications due to anisotropy of the medium. The shear
waves can be split into horizontal (SH) and vertical (SV) components. The arrival phases can
also contain phases generated by the interactions of the seismic waves at boundaries between
media with different physical properties. Aki and Richards (2002) and Chapman (2004),
among many others, provide a much more in-depth description of seismic wave propagation
and the interactions between different phases.
The observed seismic waves radiated from a source depend on both the position, mag-
nitude, and orientation of the source, and the properties of the medium the waves propagate
through. Consequently, for natural earthquakes it is necessary to try and estimate both the
source parameters and the structure the waves have passed through. If these source paramet-
ers can be constrained by prior knowledge, as is the case for active sources used in reflection
and refraction seismic surveys, then the only unknowns are the structure the wave-field has
1The Houfeng Didong Yi , translated as the instrument for measuring the seasonal winds and the movements
of the Earth, presented by Zhang Heng to the Han court in 132 AD.
propagated through. Alternatively, there may be some prior constraint on the structure, gen-
erated using various approaches. On a global scale there are simple one-dimensional models,
such as the preliminary Earth model (Dziewonski and Anderson, 1981), and more complex
three dimensional models (e.g. Ritsema et al., 1999; Kustowski et al., 2008) and on smaller
scales, there are many regional tomographic models (e.g. Aldersons, 2004; Di Stefano et al.,
2011). If neither the source or the structure can be constrained from prior information, then
both must be estimated jointly.
While interest in the structure of the earth has driven many advances in seismology,
understanding the earthquake source processes is also of great interest. An earthquake occurs
eitherwhen a rupture generates and propagates through the host rock, or bymotion along a pre-
existing fault. The rapid stress release produces seismic waves, which are usually observed
in the far-field. Source inversion is the method of characterising the behaviour of the source
from such observations, while accounting for measurement uncertainties. Understanding the
behaviour of the source provides an insight into the processes causing an earthquake.
Microseismic earthquakes are small-scale earthquakes, which are usually detected on
seismometer deployments close to the source (10100 km), and are too small to be detected at
the great distances teleseismic events can be observed at (these can reach 20 000 km andmore).
Microseismic earthquakes are frequently observed in volcanic regions, where they have been
linked to melt movement and other volcanic processes (e.g. White et al., 2011; Tarasewicz
et al., 2012, 2014), consequently, understanding the mechanisms of these earthquakes can
provide insight on the volcanic system. Another source of micro-seismic earthquakes is in
geothermal systems, where the motion of superheated water and steam can lead to seismic
activity (e.g. Julian and Foulger, 1998; Lippitsch et al., 2004). Microseismicity can often
be induced, with some earthquakes linked to hydraulic fracturing, a process used to crack
rocks by the injection of high pressure water, used in a variety of areas, including geothermal
fields (e.g. Sasaki, 1998; Julian and Foulger, 1998), and shale gas reservoirs (e.g. Rutledge
and Phillips, 2003; Rutledge et al., 2004). Understanding the processes causing this induced
seismicity can provide information on the fracture propagation and opening, and therefore
increase understanding of the effects of hydraulic fracturing as a technique. Mining also
can produce microseismic earthquakes, both from collapses (e.g len et al., 2002), and
hydraulic fracturing techniques used in mining (e.g. Gibowicz et al., 1990; Dahm et al.,
1999).
The large variation in types of microseismicity means that different techniques can be
applicable to estimating the source, and there are many different possible models of the
underlying source processes, nevertheless there are many consistent challenges in all of these
scenarios.
1.1 Source Processes
The dynamic source processes that occur during rupture propagation and earthquakes can be
investigated using rock physics experiments, such as those by Benson et al. (2008), using tri-
axial presses to investigate the behaviour of rock samples on small scales, and the associated
acoustic emission sources. These allow an examination of the resultant cracks after the
experiment has been conducted, and can be extended to hydraulic fracturing experiments
(e.g. Blanton, 1982; Damani et al., 2012). Alternatively, the fracture behaviour can be
investigated using experimental analogues, such as starch (Mller and Dahm, 2000), to give
more information on the crack propagation. Field observations, such as those by Reid (1910)
and others (e.g. Jackson and McKenzie, 1999) have shown that faults seem to have slip along
the fault plane in some direction, as evidenced by the fault corrugation, with often narrow
fault zones. These fault corrugations and associated slip directions have been shown in many
cases to be consistent with the solutions derived from source inversion for the simple model
of slip along a fault plane (e.g. Roberts and Ganas, 2000).
Combining these experimental analogues and observationswithmodelling of the dynamic
behaviour of the source (e.g. Freund, 1979; Aki and Richards, 2002), can produce simple
models of possible seismic sources associatedwith crack propagation. Rice andCocco (2007)
provide a good overview of the mechanics of these sources, linking the observations of fault
structure from field geology to laboratory experiments and the associated mathematical
models.
In many cases, the dynamic behaviour of the source is ignored during source inversion,
especially for microseismic events, due to the small scale of the events. If the wavelength
of the signal of interest is large enough that the source can be approximated as a point (e.g.
OToole, 2013; Udias et al., 2014), then this dynamic or kinematic modelling of a finite fault
source can be ignored, and the source approximated by the moment tensor, as is assumed in
this thesis. The moment tensor is given by a 3 3 matrix, corresponding to the force couples
required to define the source. The moment tensor can be described by several different
possible kinematic source models (Chapter 2).
The source processes, either dynamic or kinematic, point or finite fault sources, can be
converted into the expected seismic observations, including the amplitudes and first-motion
polarities of different phase arrivals, as well as the full waveforms, so can be used for source
inversion.
1.2 Probabilities
A simple definition of probability is a measure of how likely an event is to occur, taking values
between 0 and 1, where 0 indicates no possibility and 1 certainty. Rolling a six-sided fair dice
is a commonly used example, with the probability of obtaining the number 3 from any roll
corresponding to 1/6. Probabilities can be interpreted in two main ways, either objectively
or subjectively. The objectivist interpretation, of which frequentist probability is the most
common approach, relate the probability to the number of times that result would occur if the
experiment or measurement were repeated a large number of times, equivalent to the relative
frequency of the result (e.g. Friedman, 1999). Alternatively, the subjectivist interpretation,
such as Bayesian approaches, instead assigns the probability based on some degree of belief
in the outcome of the result, and commonly include prior information to help constrain this
degree of belief.
Although both interpretations are valid, this thesis will focus on the Bayesian interpret-
ation, without discussion of the alternate. The Bayesian approach was introduced by Bayes
and Price (1763) and Laplace (1812), and is the most common subjectivist interpretation.
Sivia (2000) and Jaynes (2003) provide a good overview of probabilistic approaches and
Bayesian theory, but a basic introduction to probability theory is provided here, and some
additional aspects of Bayesian probability are introduced in Chapter 4.
Assigning probabilities to discrete events requires that the sum of probabilities for all
events add up to one, which is equivalent to stating that all possible outcomes have been
accounted for. If the probability of an event A occurring is P (A), the probability of it not
occurring is
P (not A) = 1 P (A) . (1.1)
If two events occur and are independent, then the probability of both events occurring is
simply given by the product of the independent probabilities:
P (A and B) = P (A  B) = P (A) P (B) . (1.2)
The probability of either one of two events occurring is related to the sum of the probabilities
P (A or B) = P (A  B) = P (A) + P (B) P (A and B) , (1.3)
where P (A and B) is 0 if the two events are mutually exclusive. Additionally probabilities
can be conditional, so the probability that event A happens given event B having already
happened is defined as
P (A|B) =
P (A  B)
P (B)
. (1.4)
These definitions provide the basic building blocks of probability theory. However,
measurements are often not discrete sets, but are instead continuous distributions, with a
probability density function (PDF), p (x). The PDF reflects the relative probability of the
parameter taking a specific value, and is no longer constrained to values between 0 and 1,
instead the PDF is normalised by integrating the continuous distribution over all possible
values of the parameter:  
p (x) dx = 1. (1.5)
As the variable is continuous, the cumulative distribution function (CDF) is the probability
that the parameter takes a value less than a and is defined as
P (x 6 a) =
p (x) dx. (1.6)
Similarly the probability of the parameter taking a value between two values, a and b, is
P (a 6 x 6 b) =
p (x) dx. (1.7)
Consequently, the probability that the parameter takes an exact value is always zero and there
are only non-zero probabilities for the parameter taking values in some range, except when
the PDF is a delta function.
1.3 Source Inversion
Inversion approaches fit model parameters to observed data. This requires a mathematical
model to relate the parameters to the data, and a method for evaluating the fit of the model
parameters to the data. The models can be either empirical or constructed from an often
simplified theoretical understanding of the behaviour. The construction of these models
often ignores the limitations and frailties of the observable data and the uncertainties in the
knowledge of different model variables. If the relationship between the model parameters and
the data is linear, then the model parameters can occasionally be estimated using the inverse
of the forward operation. However, this is often impossible, as the solution is not unique.
Instead, the solution which minimises some objective function is determined. More complex
non-linear relationships can either be linearised and inverted or a search can be conducted
over the model space to evaluate the fit for possible parameters, known as a forward model
approach.
Evaluating the forward model over the range of possible parameters to compare the results
to the observed data is a common approach, especially for non-linear cases. A simple forward
model approach evaluates the likelihood of the data, which is the probability of obtaining the
data given the model, although there are more complex approaches, such as that of Tarantola
and Valette (1982), who use the forward model to investigate the marginalised posterior
model parameter distribution. Forward model approaches use a search algorithm of which
there are many possible types. These approaches can estimate only the maximum likelihood
value, often using iterative methods such as gradient descent, or the full distribution of the
PDF can be estimated, either analytically if possible, or using approaches such as Monte
Carlo sampling or Markov chains.
There are several approaches to source inversion, often depending on the available data.
For regional and global earthquakes, it is often feasible to pre-calculate databases of Green
functions to efficiently perform full waveform inversions (Dziewonski et al., 1981; Duputel
et al., 2012b;Heimann, 2011). Inmanymicroseismic cases it is difficult to efficiently compute
this database, since both the velocity structure may be complex and the locations variable.
Furthermore, the uncertainty in the Green functions is difficult to include in such approaches.
Alternatively, measurements from the observed arrival waveforms, such as P- and S-wave
polarities and amplitude ratios, can be used to constrain the source inversion (e.g. Reasenberg
and Oppenheimer, 1985; Hardebeck and Shearer, 2002, 2003; Snoke, 2003). These require
a good estimate of the associated uncertainty distributions, which can be more complex than
the Gaussian. Using these measurements still depends on an accurate estimate of the velocity
model in order to calculate the azimuths and take-off angles of the rays from the source to
the instrument.
In most cases, especially for teleseismic events, the moment tensor appears to be a double-
couple or close to it, signifying slip along a fault plane. Some non-double-couplemechanisms
have been observed, including nuclear explosions (e.g. Mller, 1973; Ford et al., 2008) and
potentially events in volcanic and geothermal regions and other areas associated with induced
seismicity (e.g. Vasco, 1990; Foulger et al., 2004; Templeton and Dreger, 2006; Vavryuk
et al., 2008). These non double-couple mechanisms could arise from mechanisms such as
conduit collapse, or fracture opening, perhaps associated with fluid inflow. It is also possible
that the non-double-couple characteristics arise from uncertainties in the inversion, such as
noise in the data and velocity model uncertainties, finite fault effects (Kuge and Lay, 1994),
as well as the improved fit due to the extra parameters in the full moment tensor, compared
to double-couple (Panza and Sarao, 2000). The proposed method enables a calculation of
the posterior model probabilities for different source models, providing an estimate of which
is the correct model.
This thesis introduces a new kinematic Bayesian approach to source inversion, described
in Chapter 4, which incorporates estimates of the measurement uncertainties in the resultant
PDF, along with sampling the full source PDF to provide more information about the source
and the confidence in the result.
1.4 Thesis Outline
Chapter 2 presents a review of the kinematic seismic source, including source representations
and decompositions, and compares different source models. Several possible source plots
are compared, and the distribution of moment tensor eigenvalues is explored, including
distributions from Hudson et al. (1989) and Tape and Tape (2012b), as well as a new
distribution of randommoment tensors, discussed inAppendixB.Additionally, threemethods
of evaluating the fit of different possible source models are reviewed.
The different observations used in source inversion are examined in Chapter 3, and an in-
vestigation into the uncertainty distributions for amplitude and amplitude ratio measurements
are presented, illustrating the possible difficulties in accurately estimating these parameters.
Several extant approaches that use these different types of observations are also reviewed.
The Bayesian approach used in this thesis is presented in Chapter 4, with the resultant
posterior PDF for different observation types. This chapter also presents the Bayesian
model evidence, adapted from astrophysical approaches (e.g. Liddle, 2004). This approach
estimates which model provides the best fit while accounting for the changing numbers of
model parameters, using the full source PDF evaluated for different model constraints. The
source PDF can provide a lot of information about the confidence in the result, but this
requires some method of visualisation, so different approaches based on the commonly used
source plots are shown.
First motion polarities are commonly used for source inversion, but it is difficult to
quantitatively estimate the uncertainties on these observations. Chapter 5 introduces a novel
automated approach which quantitatively estimates the uncertainties on the measurements,
and can be used to include phases the orientations of which are dependent on the source to
receiver ray path, and consequently the hypocentre, such as the SH and SV phases. This
approach is used on data obtained from a local seismic network around the Askja and Krafla
volcanoes in Iceland, as well as synthetic waveforms produced by a finite difference approach
(Bernth and Chapman, 2011), and integrated into the Bayesian approach described in Chapter
Chapter 6 contains the results of source inversions on synthetic data constrained to
specific uncertainties: the background noise, the receiver network distribution, the location
uncertainty, and the one-dimensional velocity model uncertainty and its associated location
uncertainty. These tests provide information on the effect of these uncertainties on solutions
using different data-types, and the impact on the Bayesian model evidence estimates.
Dahm (1993, 1996) introduced an approach for relative moment tensor inversion for co-
located events. This approach is examined in Chapter 7, and integrated into the Bayesian
approach of Chapter 4, including allowing double-couple only solutions. It is dependent on
the two events being co-located, so an estimate of the probability of this is derived. This
relative source inversion approach requires a new method of estimating the relative seismic
moment between the two events, which is presented in this chapter, along with derivations
of the prior distributions for the relative seismic moment based on two common estimates of
magnitude distributions, those of Gutenberg and Richter (1949) and Eaton et al. (2014).
Chapter 8 evaluates three different sampling approaches, for the forward model approach,
a Monte Carlo random sampling approach, and two Markov chain Monte Carlo methods.
Additionally, an alternate method for estimating the posterior probability of the source model
is investigated and compared with the Bayesian model evidence.
These methods are applied to surface data from a local seismic network around the Askja
and Krafla volcanoes, in Iceland, in Chapter 9. The results from the relative source inversion
approach used on both synthetic data for a known source, and real data from the Upptyppingar
region of Iceland are examined.
2 Seismic Sources
Seismic waves have many different sources, including explosions, earthquakes, volcanic
eruptions and landslides. These can broadly be classified into two main types: internal and
external sources. The former, corresponding to events such as earthquakes and underground
explosions, are modelled as closed systems with no external forces, and therefore momentum,
energy and mass must be conserved. External sources are open systems, modelled with an
external force acting on the system. A meteorite impact is a simple example of an open
system, where the source can be treated as a point force at the surface, rather than modelling
the orbit to remove the external forces. Open systems no longer conserve momentum and
energy, due to the external forces.
Although several seismic sources, both natural (landslides) and man-made (seismic re-
flection and refraction surveys), are modelled as open systems, earthquakes are modelled as
closed systems. Consequently, momentum, mass and energy are conserved. External sources
can be easily represented by point force sources, but earthquake sources are more complex,
and can be represented in several different ways, including the stress-glut model Backus and
Mulcahy (1976a,b) and the slip-discontinuity model Burridge and Knopoff (1967).
This chapter reviews the different approaches to source description and several possible
source models, presenting them using the same nomenclature, and outlines the potency tensor
and the effects of elastic parameters on the source. The different methods of source decom-
position are examined, along with the resulting methods of representing the source. Several
different distribution of moment tensor eigenvalues are shown, including those proposed by
Hudson et al. (1989) and Tape and Tape (2012b), as well as a distribution arising from
the sampling approach discussed in Appendix B. A distance measure for moment tensors,
introduced by Willemann (1993), is used to derive the closest double-couple to any moment
tensor, and three different methods of estimating the correct model are reviewed.
2.1 Source Representations
Two main approaches are used for a kinematic representation of the source: modelling the
fault as a discontinuity (Section 2.1.1), and treating the source as a body force equivalent
(Section 2.1.2). Both of these effectively match the displacement outside of the fault region
to that in an un-faulted medium, creating an equivalent force description, as described in
Julian et al. (1998a). Different physical sources can have the same equivalent force descrip-
tion. Consequently, the physical source is not unique and requires additional information to
determine which is correct.
2.1.1 Stress-Glut Source
Two relationships govern internal sources, the equations of motion and the constitutive
relations. In a closed system with no unbalanced forces, the equations of motion are
, (2.1)
where tj is the component of the traction in the jth direction,  is the density, and v is the
medium velocity. The constitutive relation (Hookes law) can be written in two ways:
= cjk
, (2.2)
ij = cijklekl, (2.3)
where c is the elastic stiffness tensor, so cjk are 3 3 matrices such that (cjk)il = cijlk,  is
the stress tensor, and e the strain tensor (Chapman, 2004, Chapter 4).
However the only solution to both of these relationships in a closed system with no initial
motion is identically zero, so for internal sources to occur, one of them must not hold. The
equations of motion are well tested and follow from Newtons second law of motion, but the
constitutive relationship is an empirical relationship, merely extended from Hookes law.
Backus and Mulcahy (1976a,b) suggest that an internal source is a transient failure of the
constitutive relationship, and therefore can be represented in terms of the stress-glut
glut = model  true. (2.4)
This is the difference between the modelled stress from the constitutive relationship and the
true stress satisfying the equations of motion.
In the idealised case, the fault is infinitesimally thin, which creates a discontinuity in the
displacement across the fault. Therefore, the modelled stress must have a singularity across
the fault, while the real stress must be finite and the stress-glut is singular on the fault surface.
In reality, the volume of the stress-glut is still small, and close to the fault surface.
2.1.2 Burridge-Knopoff Slip Discontinuity Source
Burridge and Knopoff (1967) propose removing the region where the constitutive relations
fail from the model, representing the source by the boundary conditions between the surface
of this region and the rest of the model (Fig. 2.1). It is known that this formulation is
equivalent to the stress-glut formulation above (e.g. Chapman, 2004, Section 4.6.1) .
Figure 2.1: Burridge-Knopoff slip
discontinuity source. The fault region () is
embedded in the volume (V ). The fault is
described by a normal (n) and tangential slip
discontinuity (u). The fault region, where
the constitutive relations break down, is
removed from the volume and the boundary
conditions between the region and the
volume are matched.
Figure 2.2: Moment Tensor components
shown as force couple. The black arrows
represent the force vectors, and the blue line
signifies the axis of the couple.
2.2 The Moment Tensor
A point source in a closed system can be completely described by the symmetric moment
tensor, conserving linear and angular momenta. But all real sources are finite, so can be
modelled either as a superposition of point sources (Heimann, 2011) or using the moment
tensor density function, which varies accross the source volume. Alternatively, a finite source
can be approximated by a point source if the wavelength, , multiplied by the distance of
the source from the receiver is much larger than the source dimensions squared, 2, and the
seismic period, T , is much longer than the source duration,  (Udias et al., 2014):
point source iff
r 2
T  
This approximation means that the source is described by the moment tensor rather than the
volume-dependent moment density, and is usually assumed to be valid for seismic events,
especially microseismic events, as long as the receivers are sufficiently distant from the
source.
If the point source approximation holds, then the stress-glut arising from the the modelled
and real stresses (Section 2.1.1) can be averaged across the source volume, allowing the
stress-glut source to be described in terms of the second order symmetrical moment tensor
glut
dt dV =
[glut] dV, (2.5)
where [glut] corresponds to the stress-glut discontinuity across the fault
The moment tensor is a symmetric tensor, which can be thought of as the force-couples
required to fully describe the source (Fig. 2.2). There are possible source descriptions that
include an asymmetrical component to the source description. Takei and Kumazawa (1994)
extended theBackus andMulcahy stress glut source description, demonstrating the possibility
of a single-force and torque in the source. These theoretical single-force sources arise from
different density structures, density redistribution due to mass advection, and the possibility
of momentum exchange between the source region and the rest of the Earth. Observation
of single-force sources have also been linked to mine collapses (Taylor, 1994; Bowers and
Walter, 2002) and other anthropogenic sources. For this thesis, the source is assumed to
conserve linear and angular momenta and is thus given by the symmetric moment tensor.
Fig. 2.2 shows the different moment tensor components as the corresponding couple. If
the tensor is symmetric, linear and angular momenta are conserved so that no net torque is
added. This reduces the number of independent components to six. However, the source
model is often constrained further, such as requiring no isotropic component, or only a
single double-couple component. Consequently, the number of independent moment tensor
components are further reduced, depending on the system. Constraining the source to a
physical model is important, because increasing the number of free parameters will usually
produce a better fit (Panza and Sarao, 2000), so it is important not to have spurious free
parameters in the moment tensor.
The scalar moment magnitude of an event,M0, can be determined from the norm of the
moment tensor:
MF =
 3
M2ij, (2.6)
M  , (2.7)
where MF is the Frobenius norm (Golub and Van Loan, 1996, p. 55) of the moment
tensor, and
M  is the Euclidean norm of the scaled moment tensor six vector


, (2.8)
a form similar to that shown in Voigt (1910) and Chapman and Leaney (2011). The mul-
tiplication by 1
keeps these definitions consistent with the original definition for a shear
faulting source:
M0 = A [d], (2.9)
defined in terms of the fault area, A, the discontinuous displacement, [d], and the Lam
parameter , corresponding to the shear strength (e.g. Aki and Richards, 2002).
The moment tensor is a second rank tensor, and can be derived using a Taylor expansion
of the equivalent force description of displacement (Doornbos, 1982; Julian et al., 1998a;
Aki and Richards, 2002; Adamov and len, 2010), meaning that there are higher order
moments, which can be linked to more complex attributes of the source including source-
finiteness and rupture propagation (Stump and Johnson, 1982; Julian et al., 1998a). However,
these are commonly assumed to have negligible effects for microseismic events, because the
point source approximation is assumed to be valid, even at much larger scales (e.g. Jost
and Herrmann, 1989; Dahm, 1996; Walsh et al., 2009; Song and Toksz, 2011), although
Adamov and len (2010) suggest that these effects can account for some non-double-
couple components in moderate and strong earthquakes, especially when using full-waveform
inversion approaches. However many of the other data-types used in source inversions
(Chapter 3) are based on the first arrival, so further support the point source approximation.
2.3 Source Models
While the source can be represented in different ways, there are often additional constraints
introduced, based on some physical model of the source. These source models allow inter-
pretation of the moment tensor in terms of the fault properties and other parameters.
2.3.1 Double-Couple Source Model
Since Reid (1910) first observed that earthquakes are caused by shear faulting, most earth-
quakes have been successfully modelled as slip on a fault plane. A shear-fault source in an
elastic isotropic medium is equivalent to a double-couple source (Maruyama, 1963; Burridge
and Knopoff , 1964).
The double-couple source conserves both linear and angularmomenta, with no net volume
change. It produces the P-wave radiation pattern shown in Fig. 2.3, and is commonly used
as a source model for earthquakes on a variety of scales (Zahradnik et al., 2001; Bai et al.,
2013; Stank and Eisner, 2013).
2.3.2 Classical Moment Tensor Model
The double-couple source model can be extended to a more general case. Aki and Richards
(2002) show that the moment tensor density,
mpq = cijpq [ui]nj, (2.10)
is non-zero in the fault volume. It is not just dependent on the slip discontinuity, [u], and the
fault normal, n, but also on the elastic parameters, c . Integrating the moment tensor density
over the source volume, V , gives the moment tensor
Mpq =
cijpq [ui]njdV. (2.11)
In the idealised case, the source is localised to the fault surface, and the slip discontinuity,
[u], can be considered as a single vector, u, so the integral becomes an integral over a delta
function. Consequently, the moment tensor can be written in terms of the slip discontinuity,
normal vectors, and the elastic parameters:
Mpq = cijpquinj. (2.12)
Aki and Richards (2002, eq. 2.33) show that the elastic parameters for an isotropic
medium are dependent only on the Lam parameters,  and :
cijkl = ijkl +  (ikjl + iljk) . (2.13)
This means the moment tensor is
Mpq = uinipq +  (upnq + npuq) , (2.14)
with scalar moment given by Eq. 2.6.
The moment tensor is dependent on the fault parameters and the elastic properties at the
source, so for a medium that is not isotropic the elastic parameters can introduce additional
complexity. Eq. 2.12 can be written in terms of an inner product between the elasticity tensor
and the outer products of the normal vector and the slip discontinuity vector:
unT + nuT
, (2.15)
where M is dependent symmetrically on u and n because the elastic parameters have a
symmetry, cijkl = cijlk = cjilk = cklij .
In all of these cases, the moment magnitude dependence is ignored in the definitions of the
moment tensor. FollowingDufumier and Rivera (1997) and Tape and Tape (2013) this model
is referred to as the classical moment tensor model, and provides an interpretation of the
moment tensor source description in terms of the fault properties and the elastic parameters.
2.3.3 Crack Model
Shimizu et al. (1987) proposed the tensile crack model, with moment tensor
MTC = mTC

1 0 0
0 1 0
0 0 3
 . (2.16)
This describes the opening of a crack plane (Fig. 2.4). However, following Eq. 2.14, the
crack model can be defined as a source with angle between the slip and normal vectors of 0
or . Therefore, the diagonal moment tensor can be written in terms of the Poissons ratio
2 (+ )
, (2.17)
and normalised (cf. Tape and Tape, 2012a, eq. 47)
MTC =
32  2 + 1

1 0 0
0 1 0
0 0 1
 . (2.18)
Consequently, the tensile crack model from Shimizu et al. (1987) (Eq. 2.16) is an opening
crack for  = 0.25, which is a Poissons ratio consistent with rock up to depths of the deepest
subduction zone earthquakes (Christensen, 1996; Dziewonski and Anderson, 1981).
Figure 2.3: 2D projection of the P-wave
radiation pattern for a double-couple source.
Red corresponds to positive amplitudes, and
blue negative amplitudes. The black lines
indicate the two nodal planes, corresponding
to the possible fault planes, and the arrows
show the orientation of the two couples.
Figure 2.4: Tensile Crack source (Eq. 2.18)
P-wave radiation pattern. Red corresponds to
positive amplitudes. The black line indicates
the crack plane, and the arrows show the
orientation of the opening. The solution
corresponds to a Poissons ratio  = 0.25.
Figure 2.5: Crack + Double-Couple P-wave radiation pattern for four different opening angles ().
The double-couple plane and the crack plane are aligned. Red corresponds to positive amplitudes
and blue negative. The black lines indicate the possible fault planes, and the arrows indicate the
possible slip vectors. The Poissons ratio was  = 0.25.
2.3.4 Crack + Double-Couple Model
The crack + double-couple (CDC) source model (Vavryuk, 2001; Minson et al., 2007;
Vavryuk, 2011; Tape and Tape, 2013), also called the tensile model by Vavryuk, is another
way of describing the classical moment tensor model. It comprises a shear component added
to the opening or closing crack (Fig. 2.5). This allows the slip vector to take any arbitrary
orientation compared to the fault normal, and therefore the source geometry can be described
by a slip vector and the fault normal, as with the classical moment tensor model. There are
two variants of the model: the basic CDC in which the crack plane and the double-couple
fault plane are aligned, and the more general case in which they are not.
2.3.4.1 Basic CDC Model
The opening angle, , is defined as the angle between the fault normal and the slip vector.
Considering a vertical fault normal and slip vector at angle  to the normal, the normalised
moment tensor for an isotropic medium (Eq. 2.14) is
MCDC =
 22
(1 2)2 + cos2  (1 + 22)

cos 0 122 sin
0 cos 0
2 sin 0
 , (2.19)
with eigenvalues
( cos
1 2
2 cos
1 2
1 2
, (2.20)
|| =
2 (1 2)2 + 2 cos2  (1 + 22)
1 2
. (2.21)
The basic isotropic CDC model with 0 6  6  and  6  6  can describe most
source types, except for the small number of sources consisting of a double-couple sourcewith
some added isotropic component ( = 0,  6= 0, 2 ,
2 in the Tape source parameterisation
described in Section 2.4.2) (Tape and Tape, 2013). Consequently, almost any source-type
can be described in terms of a basic CDC type solution for an arbitrary Poissons ratio.
The CDC interpretation of the source is ultimately identical to the classical moment tensor
model described above, since describing the classical moment tensor in terms of the opening
angle,  = sin1 (u  n), gives the same form as Eq. 2.19. If the material is anisotropic, the
distribution of the CDC solutions are no longer linear with respect to the elastic parameters
(Section 2.3.6 and Appendix C).
The opening angle is not a good indicator of the proportion of the shear component in the
CDC source, but this can be determined from the CDC moment tensor by considering the
proportions of the normalised double-couple and tensile crack moment tensors:
MCDC = NCDC
1 2
2 sinMCDC +NCDC
32  2 + 1
cosMTC, (2.22)
NCDC =
 22
(1 2)2 + cos2  (1 + 22)
. (2.23)
Tape and Tape (2013) introduce the parameter  , which describes the amount of shear
in the CDC source. It is defined (Tape and Tape, 2013, eq. 46) from the eigenvalues of the
source (Eq. 2.20) as
cos  =
2 (1  2) (2  3). (2.24)
This gives the CDC moment tensor:
MCDC = cos MDC + sin MTC. (2.25)
Eqs 2.22 and 2.25 are identical, since cos  is equal to 1||
2 sin and the sum of the
squares of the two fractions are equal to 1. Consequently, Eqs 2.22 and 2.23 provide a
relationship between  and the elastic parameters.  corresponds to the angle between the
CDC moment tensor and its double-couple component in the moment tensor 6-space.
Eq. 2.22 shows that the fraction of shear in the source is equal to the fraction of the
tensile crack component in the source for 0 = tan1
(624+212 ) and therefore it is less
for  < 0 and  >   0. For a Poissons ratio of  = 0.25, 0 = tan1
, leaving
only approximately twenty degrees either side of  = 90 where the shear component is
larger than the tensile crack component.
2.3.4.2 General CDC Model
Figure 2.6: General Crack + Double-Couple model P-wave radiation pattern. The double-couple
source (first radiation pattern) has a different fault plane orientation to the crack orientation (second
radiation pattern), which when combined together (with arbitrary amounts) produce the resultant
general CDC source (third radiation pattern). Red corresponds to positive amplitudes and blue
negative. The black lines indicate the possible fault planes/cracks, and the arrows indicate the
possible motion vectors.
The more general CDC model allows the double-couple orientation to vary from the crack
plane, introducing two more free parameters to the solution: the pair of rotation angles
between the crack plane and the double-couple. This allows a larger range of possible
solutions for a given Poissons ratio. But this model is not as physically plausible, at least in
the microseismic case where there is unlikely to be multiple fault plane orientations due to
the small scale of the events.
Further complications arise, since any single moment tensor has no unique interpretation
in terms of the crack and double-couple orientations and proportion, because there are six
parameters to describe the five parameter normalised moment tensor (Tape and Tape, 2013).
Consequently, the model provides no useful method of interpreting the fault properties.
2.3.5 Other Source Models
There are other possible source models for internal seismic sources. including explosions,
landslides and volcanic sources.
Explosions may not be well characterised by any of these source models. Mller (1973;
2001) proposed an interpretation of an isotropic explosive source in a homogeneous infinite
medium, which has a volume change component. This theoretical source would give rise
to P-waves only with no S-waves generated at the source, and is a potential model of an
explosion. However, Persson et al. (1993) describe explosives as materials which produce
a rapid expansion of volume, often using a phase change to gas. The seismic waves are
generated by the mechanical work of the expansion on the surrounding material, but this
depends on the confinement of the explosive. The gases will expand most rapidly in the
direction of least constraint, therefore, since the Earth is self-gravitating and has a free
surface, there will usually be at least one direction with a different stress state. Consequently
an explosion will not produce a completely isotropic expansion on large scales1, although
it may be well approximated by such a model (Richards, 2005). This is complicated by the
concept of shaped charges, and other complexities. However, this asymmetrical expansion
suggests that the classical moment tensor model would be satisfactory, especially if the point
source approximation is valid.
While explosions are a possible source, they are linked to specific conditions, and not
likely to be linked to natural sources. Even so, in many cases, an explosion may well lead to a
seismic source that can be described by the classical model, such as moment tensor solutions
for underground nuclear explosions, including the 2013North KoreaNuclear Explosion (Ford
et al., 2010), and older explosions (Ford et al., 2009). These are not fully isotropic since
shear waves have been observed (Hong and Rhie, 2009). While these may have a different
physical explanation, they can be described by a classical moment tensor source.
1Unless of course it is at the centre of the self-gravitating object (an interesting idea for a research proposal)
Some natural sources are not internal sources. Landslides are a good example of an
external source, and are modelled by a distribution of forces on the surface of a slope, which
is a very different source process to the moment tensor model.
Volcanic eruptions have several different seismic sources related to them, the eruption
can produce seismic waves due to the force of the material ejection, as well as fluid flow and
magma reservoir pressure changes generating seismic waves.
Long-period sources at volcanoes are commonly linked to pressure fluctuations in the
fluid system consisting of both the magma system and hydrothermal systems in the volcano
(Chouet, 1996). Similar sources have been linked to hydraulic fracturing (Bame and Fehler,
1986). While this source is very different from the point source used for the moment tensor
description, Zecevic et al. (2013) use a time-dependent moment tensor approach (De Barros
et al., 2011) to describe the source, and produce a model that is linked to the opening and
closing of a horizontal tensile crack. The authors argue that the approach produces a more
reliable moment tensor solution if single forces are included in the inversion, but this may be
due to uncertainties in source location and the velocity model.
Julian et al. (1998a) suggest a further source model linked to deep earthquakes, where
rapid phase changes in materials radiate seismic waves, generating a strongly non-double-
couple source.
2.3.6 Potency Tensors
The potency tensor, D, has been introduced by Vavryuk (2005) among others (Ben-Zion,
2008; Bailey et al., 2009; Chapman and Leaney, 2011), although it is known by other names,
including the seismic source tensor (Vavryuk, 2005). Combining the potency tensor with
the elasticity tensor, c , using the inner product gives the moment tensor
M = c .D (2.26)
The potency tensor can be calculated from the inner product of the compliance, s , the inverse
of the elasticity tensor, and the moment tensor:
D = s.M (2.27)
If the classical source model is used, then, from Eq. 2.15, the potency is given by the
fault normal and slip directions (with the ambiguity between vectors):
unT + nuT. (2.28)
The vectors from the potency tensor combined with the scalar magnitude gives the fault
parameters.
Ben-Menahem and Singh (1981) introduced the scalar potency as the geometric compon-
ent of the seismic moment,
D0 = A [d] . (2.29)
If the medium is isotropic and the source a shear source, the scalar moment is
. (2.30)
The scalar potency can also be calculated from the Frobenius norm of the potency tensor:
DF , (2.31)
or if a normalised moment tensor is used and the scalar moment magnitude is known, it is
the product of the norm of the potency tensor calculated from the normalised moment tensor
with the scalar moment magnitude:
s.M
. (2.32)
2.3.7 Elastic Parameters
The elastic parameters can have an effect on the interpretation of the source. For an isotropic
material, the Poissons ratio only affects the magnitude and not the fault orientation or source-
type. Gercek (2007) showed that the typical range of Poissons ratio for rocks are between
0.05 6  6 0.4. Tape and Tape (2013) propose that the Poissons ratio can be constrained
to the range for real physical materials, between 1 6  6 0.5, instead of taking any real
value. This constrains the range of possible classical moment tensor solutions to half of the
full moment tensor space.
In an anisotropic medium, the effects are more complicated, as both the orientation and
source type are affected by the parameters. len and Vavryuk (2002) observed that the
ignoring anisotropy when carrying out a full-waveform based source inversion can lead to
large errors in the moment tensor.Vavryuk (2005) analysed the effect of anisotropy on the
source, showing that for specific anisotropic symmetries and source orientations, a double-
couple potency can give a double-couple moment tensor, although the orientation can differ.
In other cases, a double-couple source will give a non-double-couple moment tensor.
Figs 2.7 and 2.8 show several examples of how anisotropy can affect the distribution of
moment tensors for a range of potency tensors given by Eq. 2.28. The elastic parameters are
taken fromWang (2002), showing several examples of differentmaterial types and behaviours.
While some examples are well constrained, resembling an isotropic medium with a specific
Poissons ratio, others have a wider distribution of source types.
Fig. 2.7 shows the distribution of the minimum angle of rotation between the eigenvector
triples (Kagan, 1982, 2005; Tape and Tape, 2012c) for these elastic parameters, and while the
deviation in orientation can be large, but most are peaked within the first few tens of degrees..
0 50 0 50 0 50 0 50 0 50 0 50 0 50 0 50
Figure 2.7: Distribution of 0, the minimum angle of rotation between the moment tensor and
potency eigenvectors (Kagan, 1982, 2005; Tape and Tape, 2012c), for random moment tensors
generated from the classical fault model (Eq. 2.28) and the elastic tensors fromWang (2002).
Vavryuk (2005) also investigated the variation in source orientation between the moment
tensor and a double-couple potency tensor for 21 different anisotropic media. He found the
maximum deviation in orientation was less than 10, except for shale, slate and gneiss, which
reached deviations of up to 62.
50 0 50 50 0 50 50 0 50 50 0 50 50 0 50 50 0 50 50 0 50 50 0 50
 (o)
Figure 2.8: Distribution of the change in fault opening angle  for random moment tensors
generated from the classical fault model (Eq. 2.28) and the elastic tensors used in Fig. 2.7, from
Wang (2002). The red line corresponds to zero change in opening angle.
The variation in opening angle can also be investigated in a similar way (Fig. 2.8), and
is well constrained for most of the anisotropies, showing little variation between the true
opening angle and the opening angle for the isotropic approximation of the moment tensor.
Appendix C contains similar distributions for all of the anisotropy measurements from
Wang (2002). These suggest that if there are enough varied source mechanisms, the effect of
anisotropymay be negligible for the ensemble of sources. However, if the source mechanisms
are not varied, it is possible that the deviation and error in opening angle could be quite large.
Vavryuk (2005) shows that the deviation is dependent on the difference in orientation between
the source and the anisotropy.
These figures all assume that the anisotropy at the source is related to that of the bulk
medium of the source, although it is not clear that the parameters in Eqs 2.26 and 2.12
are in any way related to the elastic parameters of the bulk medium. Eq. 2.11 contains an
integration over the fault volume, but the introduction of a fault surface will change the elastic
parameters from the bulk medium. This makes it hard to constrain the elastic parameters for
the fault volume. It is likely that the value for these parameters is not that of the bulk medium
but is affected by the presence of the fault plane itself, thus changing the elastic properties at
the fault plane.
Al-Harrasi et al. (2011) examine the anisotropy in a petroleum field, and show that the
presence of faults introduces anisotropy into the medium, due to the presence of fractures,
so the fault may provide a change in the elastic parameters from the bulk medium. It is not
clear how large this change will be, but estimates can be made from measurements of the
anisotropy caused by micro-cracks (Crampin et al., 1986; Sayers and van Munster, 1991;
Al-Harrasi et al., 2011).
Chapman and Leaney (2011) introduced an arbitrary isotropic component in the bi-axial
decomposition (Section 2.4.3) to obtain a displacement discontinuity potency tensor for any
moment tensor. This assumes the knowledge of the medium is perfect, introducing the
isotropic component to account for the difference between the displacement discontinuity
potency and the observed moment tensor. However, Vavryuk (2004, 2011) suggests that the
moment tensor can instead be used to invert for the elastic parameters of the source region
by constraining the source type to be a displacement discontinuity. In an isotropic medium,
it is possible to estimate the Poissons ratio for a classical model source (Eq. 2.28). In an
anisotropic medium, it is not possible to estimate both the fault properties and the elastic
parameters from a single event. Vavryuk (2004) shows that if a double-couple fault model
can be applied, it is possible to invert for 19 of the 21 elastic parameters with 10 earthquakes.
This approach relies on multiple double-couple earthquakes with different orientations in a
sufficiently small region such that the elastic parameters are constant andwell-sampled, but, as
Al-Harrasi et al. (2011) showed, the anisotropy varies with proximity to faults. Consequently,
such an ensemble of events is unlikely.
2.4 Moment Tensor Decomposition
Another approach for understanding the source more easily, is to split the moment tensor into
different source-types, based on the eigenvalues. The moment tensor decomposition is often
linked to different methods for parameterising the source, and can depend to some extent on
the underlying source model.
The moment tensor is symmetric and therefore can be diagonalised into the eigenvalue
matrix

1 0 0
0 2 0
0 0 3
 . (2.33)
If the eigenvalues are ordered as
1 > 2 > 3, (2.34)
the principal axes (e1, e2, e3) are known as the T (tensile), N (neutral) and P (pressure)
axes, and these can be used to characterise the mechanism as three dipoles aligned with the
principal axes, so the moment tensor can be written in terms of the outer products of the
eigenvectors and the eigenvalues:
M = 1e1eT1 + 2e2e
2 + 3e3e
3 . (2.35)
M can be uniquely decomposed into an isotropic and deviatoric component. The isotropic
part of the moment tensor,
M ISO =
tr (M ) I
= ISOI, (2.36)
M ISO =
1 + 2 + 3

1 0 0
0 1 0
0 0 1
 , (2.37)
is invariant under rotation. The remainder is the deviatoric component,
MDEV = M M ISO, (2.38)
MDEV =

1 0 0
0 2 0
0 0 3
 , (2.39)
where each element is
i = i 
1 + 2 + 3
(2.40)
= i  ISO. (2.41)
This deviatoric component has zero trace:
tr (MDEV) = 1 + 
2 + 
3 = 0. (2.42)
Although the decomposition of the moment tensor into isotropic and deviatoric com-
ponents is unique, any further decomposition of the deviatoric component is not. There are
several equivalent decompositions, which can affect the interpretation of the source.
The deviatoric component can be decomposed into a mixture of double-couples and
compensated linear vector dipole (CLVD), introduced by Knopoff and Randall (1970). The
CLVD is a dipole component with no volume change (e.g. an extensional dipole with
compression on two orthogonal axes). These different types of moment tensor can be seen
in Fig. 2.9.
(a) Isotropic Explosive:
 = (1, 1, 1)
(b) Double-Couple:  = (1, 0,1) (c) Compensated Linear Vector
Dipole:  = (2,1,1)
Figure 2.9: P-wave radiation patterns of different source-types with eigenvalues. Red corresponds
to positive amplitudes and blue negative.
Jost and Herrmann (1989) showed that the deviatoric component can be decomposed
into three double-couples, or three CLVDs, as well as a major and minor double-couple, and
a double-couple and CLVD component:
MDEV = MDC +MCLVD (2.43)
MDEV = (3  2
0 0 0
0 1 0
0 0 1
 3
1 0 0
0 1 0
0 0 2
 ; (2.44)
MDEV = M1DC +M
DC +M
DC (2.45)
MDEV =
1  2
1 0 0
0 1 0
0 0 0
+ 
2  3
0 0 0
0 1 0
0 0 1
3  1
1 0 0
0 0 0
0 0 1
 ; (2.46)
MDEV = M1CLVD +M
CLVD +M
CLVD, (2.47)
MDEV =
2 0 0
0 1 0
0 0 1
+ 
1 0 0
0 2 0
0 0 1
+ 
1 0 0
0 1 0
0 0 2
 ; (2.48)
MDEV = M
DC +M
DC , (2.49)
MDEV =



1 0 0
0 0 0
0 0 1
+ 2

0 0 0
0 1 0
0 0 1
 |1| > |3|

1 0 0
0 0 0
0 0 1
+ 2

1 0 0
0 1 0
0 0 0
 |3| > |1|
. (2.50)
This variation in decomposition can lead to different interpretations of the same moment
tensor. For the construction of the major and minor double-couple decomposition, Jost
and Herrmann (1989), following Kanamori and Given (1981) andWallace (1985), assumed
that|3| > |2| > |1|. This condition amounts to that seen in Eq. 2.50, given the ordering
of Eq. 2.34, since the sum of the deviatoric eigenvalues is zero (Eq. 2.42), so 2 will always
have the smallest absolute magnitude.
It is also possible to decompose the deviatoric component into a further pair of double-
couples:
MDEV = 1

1 0 0
0 1 0
0 0 0
+ 

0 0 0
0 1 0
0 0 1
 . (2.51)
Kuge and Lay (1994) define the non-double-couple component
|min||max| (2.52)
as the amount of CLVD in the deviatoric source.  takes values of 0 for a double-couple
source and 0.5 for CLVD sources. The global centroid moment tensor (CMT) catalogue
(Dziewonski et al., 1981; Ekstrm et al., 2012), and many others (e.g. Adamov and len,
2013; Horlek et al., 2010) use the percentage double-couple, percentage CLVD and the
percentage of the isotropic component in the source as an indicator of source type. These are
given by (Kuge and Lay, 1994; Julian et al., 1998a; Vavryuk, 2001):
%ISO =
ISO|max|100%, (2.53)
%CLVD = 2 (100 |%ISO|) , (2.54)
%DC = 100 |%ISO|  |%CLVD| , (2.55)
where|min|and
|max| are the minimum andmaximum absolute eigenvalues of the deviatoric
moment tensor respectively.
These source decompositions and the percentages of the different source types do not
improve the physical interpretation of the source, since the actual rupture properties are
unlikely to be linked to a combination of non co-planar double-couples or other source-
types, especially given the point source approximation. Describing the source as having
some percentage of the different end members shown in Fig. 2.9 adds complication to
understanding the source. A possibly more useful parameter to examine is cos  (Eq. 2.24),
corresponding to the fraction of shear in the CDC source model, although this can obscure
the elastic parameter dependence, much like  and the percentages of the different source
types.
2.4.1 Hudson Decomposition
Hudson et al. (1989) proposed a parameterisation of the source eigenvalues which is com-
monly used in the literature. For a diagonal moment tensor (Eq. 2.33), Hudson et al. order
the eigenvalues as 1 > 3 > 2 in their paper, but here the same ordering as Eq. 2.34 is
used. Depending on the sign of the middle deviatioric component 2, the parameters take
different values,
(k, T ) =


|ISO|3
22
2 > 0(
|ISO|+1
2 = 0(
|ISO|+1
2 < 0
. (2.56)
Hudson et al. then transform T into  :
 = T (1 |k|) . (2.57)
Given the assumption of a uniform prior distribution for the eigenvalues, a plot of T and k
does not have a uniform probability density, so to generate an equal area plot Hudson et al.
further transform the coordinates into u and v:
(u, v) =


1/2 ,
1/2
, k > 0 &  < 4k(
12k ,
, k > 0 &  > 4k
(, k) , k = 0(
1+/2 ,
1+/2
, k < 0 &  > 4k(
1+2k ,
, k < 0 &  6 4k
. (2.58)
2.4.2 Tape and Tape Decomposition
Tape and Tape (2012a) proposed an alternative method for parameterising the moment tensor.
This method plots the eigenvalues in eigenvalue space, normalising the eigenvalues onto the
surface of a unit sphere. The eigenvalues are then parameterised by latitude  (or colatitude
) and (deviatoric) longitude  on this eigenvalue sphere. A typical eigenvalue triple can be
permuted to give six different combinations, corresponding to six positions on the sphere.
Ordering the eigenvalues according to Eq. 2.34, selects one of these permutations, defining
the fundamental eigenvalue lune, L, shown in Fig. 2.10.
Figure 2.10: Fundamental eigenvalue lune, L, plotting the normalised eigenvalues as a vector point.
The eigenvalues are ordered according to Eq. 2.34. The axes are offset, with 3 on the reverse side
of the sphere. There are several source types indicated: DC (green), CLVD (purple), tensile crack for
a Poissons ratio of  = 0.25 (red) and isotropic (blue). The purple arc shows the range of deviatoric
sources. The red arc shows the CDC sources for  = 0.25. The corresponding P-amplitude focal
sphere distributions are shown for each example source type. The fault plane orientations for the
sources are shown on the P-amplitude beachballs as black lines. The three eigenvalue basis vectors
are shown (1, 2, 3).
The longitude
 = tan1
1 + 22  3
3 (1  3)
(2.59)
and co-latitude
 = cos1
1 + 2 + 3
3 ||
(2.60)
on L are constrained to 6 6  6
6 and 0 6  6  to completely describe the ordered
eigenvalues. The latitude is simply related to the co-latitude:
 = /2 , (2.61)
with 2 6  6
2 . The bearing of the great circle through the eigenvalue point and the , 
origin measured anti-clockwise from the great circle  = 0 is
 = tan1
1 + 22  3
2 (1 + 2 + 3)
. (2.62)
 can be related to  and  using eqs 19 and 24 of Tape and Tape (2013):
tan =  tan  sin . (2.63)
The spatial orientation of the source can be described by the strike, , dip, , and slip
(also called the rake), , angles. This parameterisation is commonly used with double-couple
events and has simple meanings for the source. The strike is the angle from north of the fault
plane intersection with a horizontal plane. The dip is the steepest angle of the fault plane,
which is in the direction perpendicular to the strike. The slip is the angle of the slip vector in
the fault plane. The interpretation of these is not as clear for a full moment tensor, but in the
classical moment tensor model, the angles correspond to the projection of the slip vector on
the fault plane.
To uniquely describe the source, one permutation of the fault and auxiliary plane normals,
n and u, must be selected. Choosing the fault plane normal to point downwards from the
surface reduces it to two. Tape and Tape (2012a) show that for this permutation of normals,
only one set of angles satisfy || 6 /2, allowing a source to be uniquely defined in strike,
dip and slip space where  6  6 , 0 6  6 /2 and /2 6  6 /2.
In a North-East-Down coordinate system, a normal pointing down from the surface has a
positive z component. The strike, dip and slip angles, and the strike and dip vectors, k and d
respectively, are given by:
k = n e3, (2.64)
 = tan1
(n  e1
n  e2
, (2.65)
d = k  n, (2.66)
 = tan1
|n e3|
|n e3 (n  e3)|
, (2.67)
h = cos , (2.68)
 = cos1
k  u
. (2.69)
TheCDC sourcemodel (Section 2.3.4) can be related to these source parameters following
Tape and Tape (2012a), giving:
tan  =
cos, (2.70)
cos  =
cos (1 + )
(1 2)2 + (1 + 22) cos2 
, (2.71)
tan =
1 2
2 ( + 1)
, (2.72)
cos = 
3 tan , (2.73)
2 tan  sin 
2 tan  sin 
. (2.74)
Figure 2.11: Fundamental eigenvalue lune, L, showing the distribution of sources for different
opening angles at a Poissons ratio,  = 0.25. Sources at cos = 1, 0.5, 0, 0.5, 1 are shown as
black dots, with the corresponding P-amplitude focal sphere distributions shown for each source. The
fault plane orientations for the CDC sources are shown on the P-amplitude beachballs as black lines.
Figure 2.12: Fundamental eigenvalue lune, L,
showing the distribution of sources for different
possible Poissons ratio. Specific Poissons ratio
are shown in cyan lines, corresponding to
 = 1,0.5, 0, 0.1, 0.25, 0.4 (solid lines) and
 = 0.5 (dashed line). The region of possible
CDC sources for Poissons ratios between
 = 1 and  = 0.5 is shown in red, and
sources with Poissons ratio between  = 0.1
and  = 0.4 in blue.
Figure 2.13: Fundamental eigenvalue lune, L,
showing the distribution of moment tensors for
an elastic tensor using the classical fault model
(Eq. 2.28), with the vertical transverse isotropic
(VTI) elastic tensor selected from Wang (2002).
The cyan lines correspond to the isotropic
approximation to the anisotropic medium
(Chapman and Leaney, 2011, eqs 81a and 81b).
Fig. 2.11 shows example source positions on the fundamental eigenvalue lune and the
corresponding P-amplitude beachballs, for a specific Poissons ratio, and its corresponding
radial great circle line for constant . The CDC source model in an isotropic medium
corresponds to radial lines from the double-couple point (constant ) corresponding to
possible sources for a constant Poissons ratio, as shown in Figs 2.11 and 2.12. The range
of possible sources on the lune for physical Poissons ratio values between 1 6  < 0.5 is
shown in red, and the typical range of Poissons ratio for rocks (Gercek, 2007), 0.05 6  6
0.4, is shown in blue in Fig. 2.12. For the CDC source model, these regions are the regions of
possible physical sources, however when anisotropy is included, the range of possible sources
can deviate from these regions (Fig. 2.13 and Appendix C).
2.4.3 Bi-Axes Decomposition
Chapman and Leaney (2011) propose using a biaxial representation from Fedorov (1968) to
describe the moment tensor. This decomposition arises because
1 + e2e
2 + e3e
3 = I, (2.75)
which means that Eq. 2.35 can be written as
M = 2I + (1  2) e1eT1 + (3  2) e3e
3 . (2.76)
Figure 2.14: Bi-Axes Diagram, adapted from Chapman and Leaney (2011, fig. 10).  are the
bi-axes in the e1 - e3 plane (Eq. 2.77).  is the angle of the bi-axes from the e1 direction (Eqs 2.78
and 2.79). The dashed lines correspond to the fault and auxiliary plane normals for a double-couple
type source.
The bi-axes
 = cose1  sine3 (2.77)
are defined by the angle  (Fig. 2.14),
sin =

2  3
1  3
 , (2.78)
cos =

1  2
1  3
 . (2.79)
The moment tensor can be written in terms of the bi-axes:
M = 2I +
1  3
+
 + 
. (2.80)
Chapman and Leaney (2011) show that the bi-axes decomposition remains valid in degenerate
cases for all eigenvalues identical, as well as pairs of identical eigenvalues. The isotropic
component of the biaxial decomposition is contained in all parts of Eq. 2.80, unlike many
of the other decompositions (Eqs 2.44 - 2.50). Despite this, Chapman and Leaney (2011)
propose a physical interpretation for the different components in Eq. 2.80: the isotropic
part relates to a pressure change; and the dyadic parts represent a displacement discontinuity
combined with the elastic stiffnesses.
Chapman and Leaney (2011) subtract an isotropic part from the moment tensor to make
the potency tensor a displacement discontinuity:
M = V [P ] I +
A [d] c :
unT + nuT
, (2.81)
V [P ] = MEXP, (2.82)
A [d] = 1 3, (2.83)
u = , (2.84)
n = . (2.85)
In Eq. 2.81, this isotropic component has been replaced by the parameters for a pressure
change, [P ] , in a spherical cavity of volume V (following Chapman and Leaney (2011)),
and the remaining parameters are written in terms of a displacement discontinuity potency
tensor, with eigenvalues 1, 2 and 3.
The spherical isotropic part can be calculated as
MEXP =
(1 + 3) , (2.86)
if the medium is isotropic, but in an anisotropic medium it is necessary to solve a cubic
equation.
Chapman andLeaney stated that this interpretation is possible for all possible anisotropies,
but Tape and Tape (2014) showed that this is only true for weakly anisotropic materials, as
some anisotropies can have no bi-axial decomposition, and it can be non-unique for certain
anisotropies and moment tensors. Consequently, the introduction of the spherical isotropic
value may not improve the source interpretation, and instead it may reflect uncertainties in
the model.
The bi-axial moment tensor (Eq. 2.76) is very similar in form to the classical moment
tensor model (Eq. 2.14). This leads to an alternate interpretation of this decomposition,
consistent with the tensile model of Vavryuk (2011). Vavryuk proposed the tensile source
model which is equivalent to the bi-axial model with zero arbitrary isotropic part. The
classical model source is a displacement discontinuity, so Vavryuk (2004, 2011) suggests
that any isotropic term arises as a condition of the elastic parameters. Consequently, any
non-zero isotropic term in the bi-axial model could be an indicator of uncertainties in the
elastic parameters or the source. As mentioned in section 2.3.6, Vavryuk suggests using this
as a constraint to estimate the elastic parameters.
2.5 Source Plots
Both the visualisation of the moment tensor and the method of decomposition, can have a
large impact on how a result is interpreted. There are two commonmethods for displaying the
moment tensor source type: Riedesel-Jordan plots (Riedesel and Jordan, 1989) and Hudson
plots (Hudson et al., 1989).
The Riedesel-Jordan and Hudson type plots both pose difficulties in interpretation, as
discussed by Chapman and Leaney (2011), who also proposed a plot of the biaxial decom-
position. Tape and Tape (2012b,a) compare some of the different types of plots and show
a method of plotting the source on the fundamental eigenvalue lune, L. In this section an
example moment tensor is used:
M ex =

6 1 0
1 5 2
0 2 3
 . (2.87)
2.5.1 Two Dimensional Projection Functions
There are two simple projections used to project from a sphere to a circle: the equal angle
projection and the equal area projection. These are both commonly used and have similar
results despite being constructed very differently.
2.5.1.1 Stereographic Equal Angle Projection
The stereographic projection was known in the classical era, and was used by Hipparchus,
Ptolemy and possibly even the ancient Egyptians. The projection preserves angles, and is
defined for the entire sphere except for the projection point (Fig. 2.15).
In seismology the lower hemisphere projection is commonly used for fault planes. The
choice of projection point depends on the desired projection direction and the coordinate
system used. Here, a North-East-Down (NED) coordinate system is used, therefore a "lower"
hemisphere projection (in the geographic system) has projection point (0, 0,1).
The projection converts theCartesian coordinates on the sphere, x, y, z, to the coordinates
on the plane,
1 + z
, (2.88)
1 + z
. (2.89)
Area is not preserved, so the ratio of areas on the circle does not correspond to the ratio of
those areas on the sphere, but angular distances are preserved. The projection is used in the
Wulff net (Wulff , 1902).
2.5.1.2 Lambert Azimuthal Equal Area Projection
Named after Johann Heinrich Lambert, this projection preserves area but not angles. There-
fore, the ratio of areas on the projection match the ratio of those areas on the sphere. Again for
a NED coordinate system a lower hemisphere projection point is (0, 0,1). The Cartesian
coordinates on the sphere, x, y, z, are converted to the coordinates on the plane,
X = x
1 + z
, (2.90)
Y = y
1 + z
. (2.91)
Often called the Schmidt net, the equal area projection is commonly used in fault plane
and other structural geology plots in preference the stereographic equal angle projection
(Fossen, 2010).
Figure 2.15: Equal Angle Projection of two
points, a and b, on to the x-y plane. The pole
is at p, and the points are projected along rays
from the pole through to the projected points
on the planes.
Figure 2.16: Equal Area Projection of two
points, a and b. The pole is at p, with the
projection onto the plane tangential to the
point q, the antipode to p. The points are
projected along circles centred on q given by
the blue lines to the projected points on the
planes.
2.5.2 Amplitude Plots
The theoretical amplitude variation can be plotted on the focal sphere (Calculated inAppendix
D), and represents the moment tensor (Fig. 2.17). However, the amplitude plot does not
provide much discrimination between non-double-couple events.
Figure 2.17: Amplitude plots of an example moment tensorM ex (Eq. 2.87), using two different
lower hemisphere projections: (a) equal angle projection and (b) equal area projection. The white
hexagonal stars correspond to the T-axis (e1) (red region) and P-axis (e3) (blue region). The central
black hexagonal star is the N - axis (e2), and the two grey stars correspond to the fault normals.
2.5.3 Riedesel and Jordan Type Plot
Riedesel and Jordan (1989) plotted the eigenvalue vector, , on an eigenvalue lune rotated
into the eigenvectors basis. The different types of source mechanisms can be plotted on the
focal sphere, along with the source orientation information. The eigenvector forms for the
different source-types, including the two possible compensated linear-vector dipole vector
forms (Knopoff and Randall, 1970) are:
(e1  e3) , (2.92)
clvd1 =
(2e1  e2  e3) , (2.93)
clvd2 =
(e1 + e2  2e3) , (2.94)
iso =
(e1 + e2 + e3) . (2.95)
The isotropic vector, iso, is orthogonal to the double-couple and compensated linear
vector dipole vectors, along with an other deviatoric mechanisms. There is an implicit
ambiguity to the vector forms due to the ambiguity in the eigenvector direction.
Plotting the different source mechanisms in the eigenvector basis, conflates the orientation
and source-type information onto one plot. Consequently, different orientations make it
difficult to compare between events. Moreover, the separation of source-type and orientation
means that the example source mechanisms are not necessarily the best fitting mechanisms
for the data. (Section 2.8.1).
Figure 2.18: Riedesel and Jordan type plot
of an example moment tensorM ex (Eq.
2.87), using a lower hemisphere equal area
projection. The green area indicates the valid
region of the source eigenvalue vector (),
and the different source-types are plotted and
labelled, along with the T,N and P axes.
Figure 2.19: Bi-Axes plot of an example
moment tensorM ex (Eq. 2.87), viewed
along the vertical axis from above. The
sphere shows size of the spherical isotropic
component. Red corresponds to a positive
change and blue a negative change. The blue
arrow is the slip vector for the corresponding
fault normal (grey line). There is an
ambiguity between the two, with the disk
indicating one possible fault plane.
2.5.4 Bi-Axes Plot
Chapman and Leaney (2011) introduce the bi-axes plot, which is based on the bi-axes de-
composition (Section 2.4.3). It represents the pertinent information from the decomposition,
including both the source-type and orientation information.
An example of this plot can be seen in Fig. 2.19. The sphere represents the volumetric
change equivalent to a pressure change in a spherical isotropic cavity. It is red if the
volumetric change is positive and blue if it is negative. However, there is additional volume
change contained in the displacement discontinuity component. The grey disk identifies a
possible fault plane orientation, along with a scale based on the scalar seismic moment. The
grey line shows a fault normal orientation, with the arrow signifying the possible slip vector.
There is an ambiguity between the fault normal orientation and the slip vector with either
being possible.
2.5.5 Hudson Type Plot
The source representation proposed by Hudson et al. (1989) plots either , k or u, v. It only
shows the source-type information with no orientation information. The transformation to the
u, v coordinates is only logical under certain assumptions about the eigenvector distribution
(Section 2.7).
CLVD  CLVD
Explosion
Implosion
CLVD  CLVD
Explosion
Implosion
Figure 2.20: Hudson source plots of an example moment tensorM ex (Eq. 2.87), red dot, using two
different projections: (a) u, v projection and (b) , k projection
2.5.6 Lune Plot
Tape and Tape (2012b) plot the eigenvalues in a similar manner to Riedesel and Jordan, but
rather than plotting the lune in the eigenvector basis, the source orientation is not included
CLVD  CLVD
Explosion
Implosion
Figure 2.21: Lune source
plot of an example moment
tensorM ex (Eq. 2.87). The
source is shown as a red dot
on an equal area projection of
the fundamental eigenvalue
lune shown in Fig. 2.10 onto
the 2D plane, with the
projection pole through the
,  origin.
(Fig. 2.21), although the eigenvectors can be plotted on the
same focal sphere. However, the plot is primarily useful for
representing source-type information. It has the property that
radial great circles from the double-couple point correspond
to constant Poissons ratio for the CDC source model. Tape
and Tape also show that the lune and the Hudson type plot are
different approaches to parameterising the same source-type
space, depending on the assumed eigenvalue distribution, and
the two can be mapped to each other.
2.5.7 Comparison of Source Plots
Fig. 2.22 shows a comparison of the source plots described
above, for random orientations of the seven main source types.
Figure 2.22: Comparison of source plots described in section 2.5 for random orientations of the 7
main source types, from top to bottom: explosion,  = (1, 1, 1); opening crack,  = (3, 1, 1);
CLVD,  = (2,1,1); double-couple,  = (1, 0,1); CLVD,  = (1, 1,2); closing crack,
 = (1,1,3); and implosion,  = (1,1,1).
2.6 Volume Change of the Source
The isotropic component of the moment tensor (Eq. 2.36) is often interpreted as the volume
change of the source because the end members correspond to a spherical explosion or an
implosion. Tape and Tape (2013) show that the volume change, V , for a source can be
determined by considering the dot product of the slip vector, u, and the fault normal, n, (cf.
Mller, 2001, eq. 5):
V = |u| |n| cos = u  n. (2.96)
This interpretation means that the volume change is not solely given by the isotropic
component. The classical moment tensor model only has isotropic solutions if the shear
modulus, , is 0, which is impossible in solids. Fig. 2.23 shows the distribution of the
volume change with source-type and the difference between the V from Eq. 2.96 and the
isotropic component of the moment tensor (Eq. 2.36). V can be related to the trace of the
moment tensor using Eq. 2.14 to give
tr (M) = (+ 2) V = 3V, (2.97)
which is dependent on the elastic parameters via the bulk modulus, . Eq. 2.97 satisfies the
conclusion that deviatoric sources have zero trace and consequently no volume change, since
the bulk modulus is zero for deviatoric sources (Tape and Tape, 2013).
The isotropic spherical volume change derived from the bi-axes decomposition (Eq. 2.80)
is different to the other descriptions (Fig. 2.23), as the biaxial spherical isotropic component
does not fully contain the isotropic component of the moment tensor. Moreover, it appears to
describe a misfit from the displacement discontinuity source given those elastic parameters.
Figure 2.23: Distribution of different volume change measures over the fundamental Eigenvalue
lune, L. The first plot is the isotropic component (Eq. 2.36). The second is the distribution of cos,
which is proportional to the volume change for a CDC type source (Eq. 2.96), and the third and
fourth plots are the isotropic spherical volume change due to pressure from the bi-axes decomposition
(Eq. 2.80) for an isotropic medium and an anisotropic medium respectively. For each of these red
corresponds to a positive value and blue negative, with the darker colours signifying larger values.
If the material is anisotropic, the lune longitude of the moment tensor does not correspond
to cos, instead it is the longitude of the potency tensor that gives the opening angle.
2.7 Moment Tensor Eigenvalue Distribution
The distribution of randommoment tensors eigenvectors underpins some of the assumptions
for source decompositions such as the Hudson decomposition. Hudson et al. (1989) assume
a uniform prior distribution of eigenvalues.
However, there are several possible methods for generating the random moment tensors
with different eigenvalue distributions, discussed in detail in Appendix B.
Fig. 2.24 shows that randomly generated normalised three dimensional matrices sampled
using algorithm B.1 have an unordered eigenvalue distribution that is not uniform, and the
ordered eigenvalues (Eq. 2.34) are strongly peaked distributions. Although the normalisation
of the eigenvalues does affect the distributions, it does not affect the transformation to the
Hudson or lune source parameters.
1 0.8 0.6 0.4 0.2 0 0.2 0.4 0.6 0.8 1
1 0 1
1 0 1
1 0 1
Figure 2.24: Distribution of unordered and ordered eigenvalues for a normalised random
distribution of moment tensors (Sampled using algorithm B.1).
1 0.8 0.6 0.4 0.2 0 0.2 0.4 0.6 0.8 1
1 0 1
1 0 1
1 0 1
Figure 2.25: Distribution of unordered and ordered eigenvalues, following the assumptions of
Hudson et al. (1989). The red lines correspond to the normalised probability distributions which are
given by Eqs 2.98 - 2.100 for the ordered eigenvalues(Hudson et al., 1989).
Hudson et al. show that for uniformly distributed eigenvalues (Fig. 2.25), the distributions
of the ordered eigenvalues are:
p (1 = ) =
(1 + )2 , (2.98)
p (2 = ) =
1 2
, (2.99)
p (3 = ) =
(1 )2 . (2.100)
There is a factor of two difference between Eq. 2.99 and the PDF given in Hudson et al. Eq.
42, accounting for normalisation of the PDFs.
The assumptions2 to arrive at Hudson et al. Eqs 41 and 42 are valid for random moment
tensors sampled using algorithm B.3 from a uniform eigenvalue distribution. Consequently, a
plot of u, v has a uniform prior probability density for this sampling. This is easily illustrated
by Fig. 2.26, which show the , k and u, v distribution for three different approaches to
generating random moment tensors. Those generated from algorithm B.1 have a higher
density just above and below the origin, and moment tensors sampled from a uniform
distribution on the fundamental eigenvalue lune again have a non-uniform distribution in
the , k and u, v plots. The distribution of moment tensors distributed according to Eqs.
2.98-2.100 is, as expected, uniform on the corresponding u, v plot, but not the , k plot.
Figure 2.26: , k and u, v distribution of moment tensors. (a) and (d) are random moment tensors
sampled from the multi-dimensional normal distribution (Section B.1 and Fig. 2.24), (b) and (e) are
moment tensors with uniform random eigenvalues (Fig. 2.25) and (c) and (f) are moment tensors
with eigenvalues distributed uniformly on the fundamental eigenvalue lune (Fig. 2.27).
2The description of Hudson et al. Eq. 30 as the combined probability distribution for Mx, My , and Mz
taking values X , Y , and Z respectively, (X > Z > Y ) reflects the sampling from the uniform box described
by the uniform a priori distribution assumed in Hudson et al. Eq. 41. However, it is important to note that
the triple can always be ordered appropriately, and that the probability reflects the probability of obtaining an
eigenvalue triple (a, b, c) which is then ordered largest to smallest, and not that a always corresponds to X .
Eigenvalues generated from uniform sampling on the lune (Fig. 2.27) are uniform when
unordered, due to the dimensionality of the inner product (Appendix A), but do not match
the ordered Hudson et al. eigenvalue distribution (Fig. 2.25).
The distribution of these sources on the fundamental eigenvalue lune (Fig. 2.28) shows
that the different approaches again have different patterns. The Hudson et al. eigenvalue
distribution has a maximum at two points and strong sampling at the isotropic limits, but
poor sampling at the double-couple points.
1 0.8 0.6 0.4 0.2 0 0.2 0.4 0.6 0.8 1
1 0 1
1 0 1
1 0 1
Figure 2.27: Distribution of unordered and ordered eigenvalues, following the assumptions of Tape
and Tape (2012b) to generate a uniform distribution of eigenvalues on the fundamental eigenvalue
lune.
The distribution of the moment tensors sampled using algorithm B.1 has a maximum at
the double-couple source and reduces outwards. It is also important to note that the moment
tensors distributed according to 2.24 have very few if any samples at the implosive and
explosive extrema, unlike the other case when the eigenvalues are uniformly distributed.
Figure 2.28: Lune type plot of the distribution of moment tensors. (a) is the distribution of random
moment tensors sampled from the six-dimensional normal distribution (Section B.1 and Fig. 2.24),
(b) is the distribution of moment tensors with uniform random eigenvalues (Fig. 2.25) and (c) is the
distribution of moment tensors with eigenvalues distributed uniformly on the fundamental eigenvalue
lune (Fig. 2.27).
Tape and Tape (2012b) observe that when sampling the eigenvalues according to Hudson
et al. from the point of view of the sphere dweller - and of us - the distribution of s would
retain some very peculiar directional dependencies, which concurs strongly with Fig. 2.28.
This also shows that the fundamental eigenvalue lune is not an equal area plot with respect
to random moment tensors sampled using algorithm B.1.
Using the probabilities derived in Appendix B (Eqs 8.15 and 8.16), it is possible to
construct a parameterisation for which the source-type distribution is uniform. The trans-
formation can be derived from the cumulative distribution functions (CDFs) for the parameter
probability distribution functions (PDFs) (Eqs B.10 and B.11) to give
(sin 3 + 1) , (2.101)
B (x; 5.7479, 5.7479)
B (5.7479, 5.7479)
, (2.102)
where B (x;, ) =
1 (1 t)1 dt is the incomplete Beta function, and
CLVD  CLVD
Explosion
Implosion
Figure 2.29:    type plot. The red lines
show constant Poissons ratios, clockwise
from the deviatoric axis
 = 1, 0.5, 0, 0.1, 0.25, 0.4, 0.5, with
the dashed line corresponding to  = 0.5.
B (, ) =
(1+t)+
dt is the complete Beta
function, or Euler Integral of the first kind.
The source-type endmembers are at the edges
of the plot (Fig. 2.29), and the distribution of the
random moment tensors from algorithm B.1 is
now uniform in these parameters, as shown in
Fig. 2.30. Unfortunately, while this plot has the
desirable equal area property for this distribution,
it does not have the useful properties of the elastic
parameters of the fundamental eigenvalue lune.
Fig. 2.29 shows the contours of constant Pois-
sons ratio, which are great circles on the fundamental eigenvalue lune, and correspond to
curved lines in this plot.
Figure 2.30: ,  plot of the distribution of moment tensors. (a) is the distribution of random
moment tensors sampled from the six-dimensional normal distribution (Section B.1 and Fig. 2.24),
(b) is the distribution of moment tensors with uniform random eigenvalues (Fig. 2.25) and (c) is the
distribution of moment tensors with eigenvalues distributed uniformly on the fundamental eigenvalue
lune (Fig. 2.27). The white lines correspond to grid lines for the  and  coordinates.
2.8 Non Double-Couple Sources
Source inversion for the complete moment tensor introduces additional free parameters
compared to the double-couple solution. This often improves the fit to the data. Consequently,
care does need to be taken when interpreting a full moment tensor solution.
Non-double-couple sources have been associated with nuclear explosions (e.g., Mller,
1973; Ford et al., 2008), and earthquakes in volcanic and geothermal regions, as well as other
areas associated with induced seismicity, such as hydraulic fracturing (e.g., Miller et al.,
1998b; Dreger et al., 2000; Minson et al., 2007; Vavryuk et al., 2008; Taira et al., 2010)
Several possible sources of spurious non-double-couple sources exist (Frohlich, 1994),
including anisotropy (Vavryuk, 2005, 2004; len, 2009), curved fault surfaces (Kuge and
Lay, 1994; Tkali et al., 2009), and finite faults (Adamov and len, 2010; Adamov and
len, 2013). Moreover, noise and other uncertainties can also lead to non-double-couple
components, as discussed in Chapter 6. All of these can make interpreting a non-double-
couple source difficult, especially when the source is close to double-couple.
2.8.1 Best fitting double-couple
In the literature, the double-couple component of a source decomposition is sometimes used
as a suitable double-couple description of the source (Miller et al., 1998a;Minson andDreger,
2008). This rarely corresponds to the best fitting double-couple or even the closest double-
couple to the source. Strelitz (1989) shows that under some measurements of the moment
tensor, the best fitting double-couple is given by removing a CLVD component, although this
does not account for the resultant fit to the data, which greatly complicates the problem.
Jost and Herrmann (1989) suggest that the "best" double-couple is obtained replacing
the largest absolute eigenvalue in the major double-couple (Eq. 2.48) with an average of the
largest two eigenvalues in the absolute sense, which scales the major double-couple. Using
the ordering of Eq. 2.34, the average value is
m =
3ISO  |min|
, (2.103)
where |min| is the smallest absolute eigenvalue, depending on the isotropic content of the
moment tensor. Consequently, according to Jost and Herrmann, the "best" double-couple is
MDC =


3ISO|min|

1 0 0
0 0 0
0 0 1

|1| > |3|
3ISO|min|

1 0 0
0 0 0
0 0 1

|3| > |1|
. (2.104)
In order to comment on the best fitting solutions and comparemoment tensors for different
sources, it is necessary to define a suitable metric so that the "distance" between different
sources can be determined. Willemann (1993) introduced the distance measure based on the
inner product of the normalised moment tensors. This distance measure has been used in
several approaches especially when looking at clustering and event constraint (Frohlich and
Davis, 1999; Hjrleifsdttir and Ekstrm, 2010; Silwal and Tape, 2013). Since the moment
tensor has six independent components, the inner product of the moment tensor matrix is
equal to the inner product of the six-vector (Eq. 2.8). This gives a distance measure
 = cos1
1M 2
(2.105)
betweenmoment tensors, which allows comparison between events, such as finding the closest
double-couple to a given moment tensor. The choice of this metric is perhaps arbitrary, as
the different components in the moment tensor could be weighted differently.
The closest double-couple to a given moment tensor can be determined from considering
the variation of  with the orientation of the source. Evaluating the inner product in the
eigenvector basis of the closest double-couple by writing the moment tensor in terms of its
eigenvectors and eigenvalues:
cos  1
e211  e
e212  e
e213  e
. (2.106)
For spherical angles  and  in the double-couple eigenvector basis and an angle , corres-
ponding to the azimuth of the e1 direction in the e1  e2 plane, the eigenvectors are

cos cos  + (sin2  cos sin sin cos) (1 cos )
sin cos  + (cos2  sin cos sin cos) (1 cos )
 cos cos sin   sin sin sin 
 , (2.107)

sin  cos
sin  sin
cos 
 , (2.108)
where e1 is calculated from rotating the x-y plane to the plane orthogonal to e3.
Rewriting cos in terms of these parameters and searching for the stationary points shows
that , ,  = 0 is a stationary point, and differentiating again shows that it is a maximum. In
fact it is the only maximum in the permitted range of values for , , , which are given by
0 6  6 , 0 6  6  and 0 6  6  due to the ambiguity in eigenvector orientation. This
means that cos is maximised and therefore  minimised for the double-couple coinciding
with the full moment tensor eigenvectors, which is therefore the closest double-couple to
the full moment tensor using this metric. This corresponds to the orientation of the major
double-couple in the decomposition of Eq. 2.50.
The closest double-couple may not be the best-fitting double-couple solution due to the
imperfect sampling of the seismic radiation pattern. The best fitting double-couple depends
on the function used to define the fit of the observations and their uncertainties. Consequently,
to estimate the best fitting double-couple solution, it is necessary to perform a moment tensor
inversion constrained only to the double-couple space.
2.8.2 Distinguishing between source types
The variation between full moment tensor and double-couple solutions is important when
sampling the radiation pattern. Fig. 2.5 shows some of the variation for different opening
angles. The negative polarity region disappears completely within 30 degrees of opening
angle from the double-couple (Section 2.3.4). Fig. 2.31 shows the percentage of the dominant
Figure 2.31: Fraction of the
dominant polarity on the focal
sphere(positive or negative)
for different source types on
the fundamental lune.
polarity on the fundamental lune. The value at which the po-
larity is the same across the focal sphere varies with the elastic
parameters, but it can be difficult to differentiate between the
double-couple model and a full-moment tensor source for lim-
ited sampling. The different geometries of the regions of posit-
ive and negative amplitude can sometimes provide a constraint,
although this can be poorly constrained by the receiver geo-
metry.
As discussed in sections 3.3 and 3.4, amplitude informa-
tion can theoretically improve the constraint on the source. Al-
though, there are several corrections that may be non-negligible
alongwithmeasurement uncertainties. Furthermore, the biased
sampling arising from the receiver distribution can lead to biased sampling away from the
true double-couple source (Section 6.3.2).
There are several approaches to distinguishing between the two models. One approach is
to use an information criterion. Akaike (1974) defined the Akaike Information Criterion in
terms of the log-likelihood,
IA = 2 lnLmax + 2k, (2.109)
where where Lmax is the maximum likelihood for the model, k is the model dimension.
This provides a method for comparing between different models, with the larger value
corresponding to the better fit. However, this has been shown to favour models with more
parameters (Kashyap, 1980; Kass and Raftery, 1995; Liddle, 2004), so it is not always an
optimal approach. An extension is the Bayesian Information Criterion (BIC), introduced by
Schwarz (1978), it is defined as
IB = 2 lnLmax  k lnn, (2.110)
where n is the number of data points used. The absolute values are not useful, however the
difference between values for the models is used as an indicator of evidence for the model
with larger BIC. In the literature, differences between 2 and 6 are regarded as evidence for
the larger BIC model, from 6 to 10 is considered strong evidence and differences bigger than
10 very strong evidence (Jeffreys, 1998; Mukherjee et al., 1998).
The Bayesian model evidence,
p (data|model) =
p (data|x) p (x|model) dx, (2.111)
ismore complex, and requires good sampling of the un-normalised likelihoods for comparison
between the models for the different model parameters, x. Higher dimensional models are
penalised by the parameter priors.
2.9 Summary and Discussion
The standard source model for many natural seismic events is that of a displacement discon-
tinuity on a fault surface. Mostly, this discontinuity is assumed to be along the fault plane,
giving a double-couple source, but more complex sources are possible. These are described
by the classical moment tensor model, which is equivalent to the basic CDC model, with
end-members consisting of the tensile crack and double-couple sources. While there are
other plausible sources, such as explosions, many of these may still be well described by the
model of a displacement discontinuity on a fault plane due to the free surface and gravity.
There are many ways of decomposing the moment tensor, but many of the approaches,
including those described in Jost and Herrmann (1989), add little understanding and increase
the apparent complexity. These decompositions are non-unique for the deviatoric compon-
ent, resulting in many possible equivalent decompositions. Consequently, unless there is a
physical reason for the decomposition, these often only add complexity to the source inter-
pretation, with estimates such as the percentage of CVLD or double-couple component not a
goodway of interpreting amoment tensor source. Instead, a decomposition and interpretation
related to the possible source model should be used.
A possible source model is the CDCmodel, which is parameterised in terms of the source
orientation, opening angle and elastic parameters. In this model, the fraction of shear in
the source (Eq. 2.24) does not correspond to the percentage of the double-couple source
(Eq. 2.55). Almost any source can be parameterised in terms of the CDC model parameters,
although this is dependent on the elastic parameters, and whether the medium is isotropic.
Relating the source to the CDC source model means that a better estimate of the volume
change in the source can be estimated, because sources have a volume change that is related
to the cosine of the opening angle, and is linked to the opening or closing of the fault planes,
and not solely dependent on the isotropic component of the source.
Interpreting the source in terms of the possible source model can provide a deeper
understanding of the source, and a more intuitive grasp of the difference between sources,
which is not easy to obtain from the commonly used moment tensor decompositions.
The bi-axes decomposition from Chapman and Leaney (2011, 2014) seems beguilingly
simple, but the easy pitfall of interpreting the explosive component as the sole volume change
can lead to misinterpretations of the source. An alternative interpretation of the explosive
component arises from the classical moment tensor. Vavryuk (2011) suggests the explosive
component arises due to errors in the elastic parameters and the source, since a bi-axial
decomposition of such a source should be purely a displacement discontinuity.
The dependency of the moment tensor on the elastic parameters is overlooked in many
of the source decompositions. In anisotropic mediums, the source-type and orientation are
dependent on the elastic parameters of the source, and these effects can be very large. These
elastic parameters are not given by the properties of the bulk medium, so measurements
from samples or well logs will not give the correct parameters, as these are complicated by
the presence of the fault (Al-Harrasi et al., 2011). Consequently the elastic parameters are
often poorly constrained, even when they are known prior to the inversion. In the bi-axes
decomposition, Chapman and Leaney (2011) remove an isotropic component of the moment
tensor to leave the commonly used CDC potency tensor, assuming that the elastic parameters
are correctly known. However, this component appears to correspond to a misfit from the
CDC source type (Fig. 2.23), consistent with the suggestion of Vavryuk (2005), that it is
oftenmore intuitive to allow the elastic parameters to vary until a suitable CDC potency tensor
is obtained. This is not a well-posed problem, requiring additional constraint on the elastic
parameters, or multiple events with different source-type at the same fault, which is very
unlikely. Therefore, it is unlikely that these parameters can be well constrained, and different
anisotropic elastic parameters can cause large differences between the moment tensor and
potency source interpretations, changing the source type and orientation, so care must always
be taken to account for the elastic parameters, and the corresponding uncertainties, when
estimating the physical fault parameters.
The different source-type plots have different advantages and disadvantages. TheRiedesel-
Jordan plot does not allow for easy comparison between source-types, while the Hudson u, v
plot is equal area only for a certain eigenvalue distribution. However, it does not correspond
easily to physical attributes of the source. Plotting the eigenvalues on the fundamental lune is
a useful plot of the source, since the coordinates are easily related to the eigenvalues. Further-
more, the plot is easily understood and has useful properties such as constant Poissons ratios
corresponding to radial great circles from the double-couple source point. The ,  plot is a
toy example of a plot with the equal area distribution for random moment tensors sampled
from Alg. B.1, but it is also of little use when interpreting the source, and provides little
physical intuition, such as the non-linear contours of constant Poissons ratio, in comparison
to the Hudson and lune plots. Although the equal area probability property is desirable, using
parameters that are complex functions of the eigenvalues with no physical interpretation only
add to the complexity of source interpretation. Instead, the lune source-type representation
of Tape and Tape (2012a) provides a more intuitive interpretation of the source, based on
the eigenvalue components, although the prior distribution is not always uniform (e.g. for
moment tensors sampled from Alg. B.1), so must be kept in mind when interpreting the plot.
To compare between different sources, it is important to have a measure for the moment
tensor space, such as, the angle between themoment tensor six-vectors. The closest double-
couple to any source using this metric is the double-couple source with the same eigenvectors.
However, whether this is the best-fitting double-couple depends on the sampling of the focal
sphere and the uncertainties on the data. Furthermore, with limited sampling of the focal
sphere, it is often difficult to distinguish between different source types.
Although the full moment tensor model provides more free parameters, it is possible to
have non-double-couple sources. The BIC provides an approach for estimating the better
fitting source model, accounting for the extra dimensions of the moment tensor model com-
pared to the double-couple model. However, the values can be hard to interpret. Given a
sufficient sampling of the source likelihood, the Bayesian evidence can be used to estimate
the posterior model probabilities for the different source models. The posterior probabilities
account for the different dimensionalities of the models, and produce easily understandable
values.
3 Source Inversion: Common Data
Types and Measurement Uncertainties
This chapter explores how different observations made from a seismogram can be used to
characterise an earthquake source. These observations are usually made from seismic waves,
recorded using seismometers. A displacement seismogram at a given point and source
location can be modelled as the convolution of several different time series,
ui(t) = Ri(t)  Gijk(t)  Sjk(t). (3.1)
The instrument response for the ith component,Ri(t), can be measured and is usually known,
so can be removed during processing.
The Green functions, G (t), describe the impulse responses of the medium in which
the waves are travelling, and are a tensor function of space and time. They depend on the
receiver component and the source components. Gijk (t) corresponds to the ith seismogram
component and the jth and kth source components, and is a spatial derivative of the Green
functions, called the response function. The Green functions depend on the velocity model,
but they can be measured for a known source and instrument combination. However, they
are usually calculated from the best guess velocity model (Cesca et al., 2006; Jechumtlov
and len, 2005; Pesicek et al., 2012), since the source function is unknown. In regions
with complex 3D structure or heterogeneity or anisotropy, this can be very computationally
intensive. The Green functions can also be very uncertain if the velocity model is poorly
constrained.
The seismic source function, S(t), is both time and space dependent. If the time depend-
ence of the source function is independent of the spatial components, it can be decomposed
into a time component, s (t), and a spatial component, the moment tensor,M . Otherwise, the
moment tensor components have an additional time dependence. Microseismic source time
functions are usually modelled as a step function, because the fault size is small compared
to typical seismic wavelengths, so this separation holds. The displacement seismogram in
the far-field is proportional to the first time derivative of the source time function, a delta
function, so the far-field approximation of the displacement returns to its original position
(Aki and Richards, 2002).
Although further data such as INSAR observations, near-field strongmotion observations,
and GPS can be included in source inversion (e.g. Konca et al., 2010; Weston et al., 2014;
Houli et al., 2014; Yokota et al., 2012; Delouis et al., 2002; Lundgren and Salichon, 2000;
Page et al., 2009), the associated signals from microseismic events are often too small to be
reliably detected, although not always (e.g. OToole, 2013), and so the only information on
the event is in the seismogram (Fig. 3.1). Consequently many source inversion approaches
solely use observations made from the seismogram.
In some cases the seismometers are sufficiently close to the source (within a few
wavelengths of the source (Aki and Richards, 2002) that the far-field approximation, com-
monly used for interpreting seismograms, no longer dominates, so near-field effects need to
be included.
In this chapter, the effect of filters, attenuation, background noise, and surface corrections
on the seismogram are explored, and the common measurements used for source inversion or
examined, including the effect of noise on the amplitude and amplitude ratio measurements.
Finally several existing approaches that use these data-types are reviewed.
3.1 Seismogram
The seismogram is produced from the measurement of the seismic waves at a given point by
a receiver (seismometer). This measurement is usually based on the inertia of a suspended
mass, either by measuring the displacement of a damped mass, which is very frequency
dependent, or by using a Force Balanced Accelerometer (FBA), which gives an electro-
mechanical measurement of the acceleration from the force required to maintain a test mass
at a given position.
8 10 12 14 16 18 20 22 24
trace time (s)
Figure 3.1: Plot of an example seismogram showing the effects of two different filters. The top plot
shows the raw data without any filtering, the middle plot shows the data with a high-pass filter,
filtering above 1.1 Hz. The last plot shows the data with a bandpass filter between 2 Hz and 16 Hz.
Havskov and Alguacil (2004) provide an in-depth examination of how seismometers work
and the different designs and methods to measure seismic waves and convert them to a digital
signal. Typically velocity seismograms are used for easy identification of phases, aided by
filtering the seismogram to suppress some of the unwanted features of the waveform, such
as noise. Fig. 3.1 shows that both high-pass and bandpass filters can have a large effect on
the observed waveform, and when combined with the choice of displacement, velocity, or
acceleration seismograms, care needs to be taken with measurements.
3.1.1 Noise Model
There are several sources of noise on a seismogram, including anthropogenic noise linked to
traffic and machinery, wind noise, ocean noise, and river noise. These have both different
frequency content, and different causes, as discussed in Bormann and Wielandt (2013).
The simplest noise model for background noise is the Gaussian uncertainty noise model,
which has values distributed according to the Gaussian distribution:
p (X = x|, ) =
(x)2
22 . (3.2)
This assumes that the noise is incoherent, and has no directional dependence. However,
the unfiltered noise nearly always has some directional dependency, which can even remain
when filtered. This is not unexpected, as ambient noise tomography approaches would not
work if the noise is completely incoherent and directionally independent. This directional
noise environment could be used to help estimate the relative and absolute orientations of the
receivers following a similar approach to Grigoli et al. (2012).
0 1 2 3 4 5 0 1 2 3 4 5
Time (s)
Figure 3.2: Comparison of real noise from the seismometer DDAL (left column) and a Gaussian
noise model (right column) for different filters. Each row corresponds to a different type of data
processing: the first is the raw data, the second is a high-pass filter above 1.1Hz, and the third row is
a bandpass filter from 2 16Hz. The y-axis scales are independent.
Despite this directional dependence, Gaussian noise is often a suitable model for the
noise on a particular seismogram component (Fig. 3.2). The high-pass and bandpass filtered
real noise resembles the corresponding filtered Gaussian noise model, although the bandpass
filtered signal does not resemble the unfiltered Gaussian noise, as the filter has introduced a
correlation between neighbouring points (Fig. 3.3).
The Kolmogorov-Smirnov test (Chakravarti et al., 1967) calculates the probability that
the observed values can be obtained from a given distribution. For the filtered seismograms
in Fig. 3.2, the p-value is much larger than would be required to discount the Gaussian
distribution as the noise distribution. A Gaussian noise model also has the benefit that, when
the seismogram is differentiated, the noise model remains Gaussian.
5 2.5 0 2.5 55 2.5 0 2.5 5
t (s)
Figure 3.3: Noise autocorrelation for real noise from the seismometer DDAL (left column) and a
Gaussian noise model (right column) for different filters. Each row corresponds to a different type of
data processing: the first is the raw data, the second is a high-pass filter above 1.1 Hz, and the third
row is a bandpass filter from 2 16 Hz. The y-axis scales are independent.
3.1.2 Q Correction
Propagating waves can be attenuated by performing work on the medium at some scale. This
intrinsic attenuation is quantified by the dimensionless number Q. Q is defined in several
ways: it is inversely proportional to the fractional change in energy per cycle:
Q ()
, (3.3)
and, if the medium has a linear stress-strain relationship, it is also inversely proportional to
the fractional change in amplitude per cycle:
Q ()
. (3.4)
Even though the intrinsic attenuation is frequency dependent, it is usually assumed to be
approximately constant (Aki and Richards, 2002).
A (t) = A0e
2Q (3.5)
gives the plane-wave amplitude at time t, while
A (x) = A0e
2cQ (3.6)
gives the amplitude at a distance x. The amplitude at a receiver is therefore dependent on the
frequency, , and either the travel-time,  , or the distance, x, and wave speed, c.
Often the Q values are not well estimated, and are assumed constant for the whole
region, especially when they are measured from earthquake signals, such as the arrival codas
(Wennerberg, 1993;Delpezzo et al., 1995). This allows theQ correction to be calculated for a
given travel-time rather than having to account for differentQ values in different materials. As
well as measuring Q from earthquake data, attenuation can be measured in the lab (Peacock
et al., 1994), and from seismic reflection data (Tullos and Reid, 1969; Dasgupta and Clark,
1998; Schuler et al., 2014).
It is also possible to use a probabilistic approach similar to a grid search method around
the range of Q values, and then marginalise to obtain a correction. If Q has a normal
distribution, the amplitude correction, C, for a given travel-time,  , is
A ()C = A0 (3.7)
 C = e
2Q , (3.8)
which has a complex distribution:
p (C) = p (Q)
C ln2 C
22Q
( 2 lnCQ)
22Q . (3.9)
Following Dasgupta and Clark (1998), it may be an improvement to consider a Gaussian
distribution on q = Q1, which gives a log-normal distribution for C:
p (C) = p (q)
2q2 C
. (3.10)
Consequently an amplitude measurement with Gaussian noise has a complex corrected amp-
litude distribution given by the product of the log-normal and Gaussian distributions, which
is not possible to determine analytically.
Attenuation can change the pulse shape (Aki and Richards, 2002), introducing some
additional uncertainty into the arrival onset time and first motion, although this is usually
negligible in comparison to both the effects of the background noise.
3.1.3 Surface and Near-Surface Corrections
Since many microseismic events are observed from either the surface or shallow vaults and
boreholes, the free surface can have an effect on both the amplitude and shifting the phase
of the waveform, affecting the polarities (Snoke, 2003). Aki and Richards (2002) provide a
simple formula for free surface corrections for a half-space, although the effect is complicated
by the sub-surface structure.
Shearer and Orcutt (1987) introduced a plane-wave model to account for differences
between surface and sub-surface receivers, using matrix methods for plane waves incident
on a layered medium (e.g. Aki and Richards, 2002). This approach includes the effects
of converted phases and resonances, ray angle variation, impedance contrasts and the free
surface effect, and produces a frequency-dependent effect that can be large. Shearer and
Orcutt (1987) show an example for a model of a layer of sediments over bedrock, which
increases the SV-amplitude around sixty times. The effects also differ between seismogram
components. The correction can be important when considering amplitude-based source
inversion, but if the arrival ray at the receiver is approximately vertical, some of the effects
are reduced. Near-surface structure is usually corrected for statistically with a site correction
for individual receivers, although in many cases it is ignored because there is insufficient
knowledge of the sub-surface.
3.2 Observations: First Arrival Polarity
13 13.5 14
Trace Time (s)
Figure 3.4: First arrival polarity
example from high-pass filtered
seismogram in Fig. 3.1. The observed
polarity is indicated by the blue arrow.
The simplest observation for characterising the source
that can be made from a seismogram is the first mo-
tion of the arrival. Although a polarity observation
contains little information, it can help constrain the
moment tensor. Fig. 3.4 shows an example of a po-
larity observation. However, as Fig. 3.1 showed, the
choice of filter can have an affect on both the wave-
form and the observed polarity, especially when using
a bandpass filter.
Measuring the first arrival polarity requires an ac-
curate arrival time pick with a clear peak, rather than
an emergent arrival. For small amplitude arrivals, the polarity can be sensitive to the
background noise obscuring the arrival. The P-wave arrival is usually clear in the vertical
component of a seismogram, with an upwards polarity corresponding to a positive amplitude
on the focal sphere.
The polarities for other phases can be measured from the rotated three-component seis-
mogram, such as the ray (LQT) orientation, which gives the SH-polarity on the transverse
component (T) and the SV-polarity on the Q component. This requires a good understanding
of the ray path from the velocity model and the location. The observed rotated waveform is
also highly dependent on the coupling of the different receiver components to the ground.
Snoke (2003) observed that the effect of the free surface can introduce a phase shift
that reverses the SV-polarity over a few degrees change in inclination angle, suggesting
that these should be used with care in any source inversion. Additionally, both the SH-
and SV-polarities can have ambiguities arising from the decomposition of these individual
components, leading to the same SH-radiation patterns for very different source types (Fig.
3.5). The ambiguity in the SH-polarities arises when the source has a vertical eigenvector, so
the radiation pattern depends equally on the two horizontal components with opposite sign
(Eq. D.5). Consequently rotating the source 90 in the horizontal plane and multiplying by
1 gives the same radiation pattern, so an east-west striking normal fault (Fig. 3.5 top row)
has the same radiation pattern as a north-south striking thrust fault (Fig. 3.5 middle row).
The ambiguity in the SV radiation patterns only affects the polarities and not the amplitudes.
Figure 3.5: Beach-ball plots for P-, SH- and
SV-wave radiation patterns, showing the
polarity ambiguities in the SH and SV
phases, and the amplitude ambiguity in the
SH-phase for three different sources. From
top to bottom these are an east-west striking
normal fault, a north south striking thrust
fault and a north-south striking normal fault.
3.2.1 Polarity Inversion
The sign of the modelled amplitude provides in-
formation on the polarity expected for a given
moment tensor. The fit of the moment tensor can
be determined by comparing the observed po-
larities for different receivers with the modelled
values. If the velocity model is homogeneous
and isotropic, and the hypocentre is known, the
signs of the calculated amplitudes are given by
the sign of the radiation components (Appendix
D, Eqs. D.4 - D.6). For a more complex velo-
city model with a known hypocentre, the take-off
angles and azimuths of the source-receiver rays
can be calculated from the ray paths. The mod-
elled polarities, Y , are given by the signs of the homogeneous isotropic radiation components,
R, for the source-receiver ray take-off angles and azimuths:
YSTA1 = sgn (RSTA1) . (3.11)
If the moment tensor, M , is written in six-vector notation (Eq. 2.8), normalising it is
consistent with the Frobenius norm of the matrix (Eq. 2.6):M  = M211 +M222 +M233 + 2M212 + 2M213 + 2M223. (3.12)
The six-vector normalisation matches the moment tensor normalisation. Consequently, the
radiation components are given by the inner product of the source-receiver ray path angular
coefficients, a, defined in Appendix D, and the moment tensor six vector:
RSTA1 = aSTA1.M . (3.13)
Substituting for the six-vector form of the radiation components into Eq. 3.11 gives an
equation for the polarity in terms of the moment tensor
Y = sgn
, (3.14)
where Y is the vector of observations and a the matrix of station angle coefficients. The
signum function, sgn(x), makes the polarity problem non-linear, so for polarity inversions it
is necessary to evaluate the forward problem and determine the fit.
As Julian et al. (1998a) show, near-field polarity observations can provide additional
constraints on the source. This is due to the different spatial dependencies of these components
and therefore the differing dependencies on the moment tensor components. Furthermore,
if the receivers are sufficiently close to the source and in a low noise environment, such as
in down-hole monitoring, it is possible that the near-field component can dominate regions
close to the nodal plane because the far-field component drops to zero.
3.3 Observations: Arrival Amplitudes
13 13.5 14
Trace Time (s)
Figure 3.6: First arrival amplitude
example from the high-pass filtered
seismogram in Fig. 3.1. The maximum
amplitude is indicated by the blue arrow
and the maximum peak-to-peak
amplitude is indicated by the red arrow.
The lower plot shows the amplitude
squared, along with the windowed mean
in blue, corresponding to the RMS2.
The arrival contains much more information than the
simple polarity observation. The arrival amplitude is
an additional observation that enables more discrim-
ination between sources. It is strongly dependent on
both near-surface effects and the propagation effects
between the source and the receiver. Furthermore, the
arrival amplitude is measured by rotating the seismo-
gram into the correct orientation to avoid projection
of the arrival onto multiple components (Section 3.2).
There are three main methods for making this ob-
servation (Fig. 3.6). The simplest is to construct a
window around the arrival and simply take the max-
imum absolute value. However, the amplitude is very
dependent on the noise environment, which can re-
duce or increase the observed value.
A slightly more robust approach is to take the maximum peak-to-peak amplitude for the
window, which increases the signal-to-noise ratio (SNR) on the measurement provided the
noise is uncorrelated. For a Gaussian noise model, the distribution of the difference between
two values with Gaussian noise  has a Gaussian distribution with standard deviation given
2, but the signal amplitude compared to the maximum amplitude is doubled, increasing
the SNR by
2. The root-mean-squared (RMS) value of the window (with n samples) is a
logical extension of the peak-to-peak approach. It reduces the dependence of the observation
on the noise further, as long as the noise is uncorrelated
RMS 
. This reduction in the
effect of the noise is a benefit of the RMS approach. However, because a seismic arrival is
pulse-like and not periodic, the choice of window length is important. Extending the RMS
window a long way beyond the noise reduces the effect of the signal by a similar factor of
n, and reduces the SNR benefits of this. The choice of window length can also make
it difficult to compare between different arrivals, as the RMS values are dependent on the
window length.
The choice between these different approaches is therefore not straightforward, and can
be sensitive to the noise level.
When using arrival amplitudes for source inversion, it is more important that the obser-
vations are consistent between different receivers. This can be complicated by the coupling
of the receiver components to the ground and local structure, which varies between receivers,
leading to an additional correction to be estimated.
PeaktoPeak
 = 0.01
Maximum RMS
 = 0.1
 = 0.3
2 4 6
 = 0.5
1 3 5 0.4 0.8 1.2
Amplitude
Figure 3.7: Histogram of the distribution of amplitude measurements using different approaches for
different fractional Gaussian noise levels, f , shown in dark blue. The cyan line is the true amplitude
value, and the red line is the theoretical Gaussian distribution for the given uncertainty. The random
Gaussian noise was added to the unfiltered signal.
0 0.5 1
0 0.5 1
Figure 3.8: Fractional (left) and absolute (right) deviation of the mean sample amplitude from the
correct amplitude for different fractional noise levels. The blue line corresponds to the peak-to-peak
measurement, the red line is the maximum absolute value and the green line is the RMS.
The estimates from the different approaches can be compared with the true values using
numerical simulation of unfiltered Gaussian noise on a synthetic arrival. Fig. 3.7 shows that
Figure 3.9: Distribution of amplitude
SNR estimation at different fractional
noise levels, f . The amplitude SNR
is estimated as
SNRA =
RMS(Signal+Noise)
RMS(Noise) . The
red line is the mean value as a
function of f , and the cyan line
corresponds to the expected SNR for
the fractional uncertainty. The short
window for the arrival leads to the
increased uncertainty in the SNR
estimate, especially for large f .
the mean of the simulated amplitude distribution in-
creases with the fractional noise level, f , defined as
the ratio of the noise level to the signal without any
noise (i.e. the true noise level). This deviation occurs
because the approach is measuring the maximum point,
and although the uncertainty on any point is Gaussian,
when only the maxima is being selected, the value will
be affected by the noise level. The effect is similar for
the RMS approach, although it has the smallest abso-
lute deviation in amplitude, it has a small uncertainty
due to the window size. Both the peak-to-peak and
maximum amplitude approaches have similar percent-
age deviations and uncertainties, but the RMS approach
has a larger fractional deviation (Fig. 3.8).
Fig. 3.8 shows that the variation of the distribution
mean is a function of the fractional uncertainty. Con-
sequently, it may be possible to calculate a correction
value for the amplitude to align the distribution means.
However, the exact distribution depends on the specific waveform, preventing any form of
general correction. This variation in the amplitude distribution makes it hard to accurately
measure the amplitudes, which can lead to inconsistencies when used in source inversion.
The fractional noise level, f , is related to the SNR, as can be clearly seen in Fig.
3.9. However, difficulties arise in calculating the SNR arising due to the short length of
the arrival. This reduces the accuracy of the estimate, especially for large uncertainties.
There are different approaches to calculating the SNR, both in power and amplitude, and
clearly the value will differ between the different measurements used in the calculation.
Furthermore, Fig. 3.9 shows that the calculated SNR can correspond to a wide range of
fractional uncertainties, especially for low SNR values.
Anisotropy can further affect the arrivals, as the shear phases are split into fast and slow
arrivals (Crampin, 1985), with amplitudes that are a superposition of the split arrival phases.
Consequently, the propagation corrections also need to account for the anisotropy, which can
be a large source of uncertainty. Without this correction, the amplitudes do not reflect those
calculated using the isotropic homogeneous response functions (Appendix D).
3.3.1 Amplitude Inversion
Source inversion from amplitude observations is possible using the six-vector form for the
amplitude
AP = aPM , (3.15)
whereAP is the P-amplitude vector, and aP is the matrix of P-amplitude station coefficients
(Appendix D).
Unlike inversion using polarities, the amplitude is linear with respect to the moment
tensor, although aP is not necessarily square so a suitable inverse cannot be constructed.
There are several approaches to general matrix inversion that can produce inversions for
non-square matrices, such as calculating the left inverse for Eq. 3.15
, and
inverting to obtain the moment tensor.
1 0.5 0 0.5 1
Figure 3.10: Histogram of the distribution of amplitudes, A = a.M for the source distribution
described in Algorithm B.1, and the red line shows the distribution of the 6-dimensional dot product
(Eq. A.17).
The theoretical amplitude on the focal sphere for a normalised moment tensor can take
values between -1 to +1, and is equivalent to the cosine of the angle between the receiver ray
six-vector and the moment tensor six-vector. For moment tensors sampled from the uniform
6-sphere (algorithm B.1), this is distributed as shown in Fig. 3.10, corresponding to the
six-dimensional inner product (Appendix A). For a single station and all possible moment
tensors, the most likely amplitude is 0.
3.4 Observations: Arrival Amplitude Ratios
Many of the arrival amplitude corrections can be difficult to model accurately. An alternative
approach is to consider the amplitude ratios between different phase arrivals. If both arrivals
experience similar attenuation and propagation effects, the ratio is independent of these
effects, although it is still dependent on component-specific site effects such as the free
surface correction and receiver coupling. If the ray paths and the propagation effects are not
similar, the effects must be calculated and the advantage of the amplitude ratio is reduced.
Consequently, the amplitude ratio is often measured in the same frequency bands for the two
phases concerned.
The propagation of different phases depends on the phase velocity (Eq. D.2), therefore
the amplitude ratios need a correction. For P/S amplitude ratios, the correction in its simplest
form (Section 4.2.3) is proportional to the cube of the V p/V s ratio.
Although the S and P arrivals may have different frequency content and travel times,
the effect of the Q correction is also reduced for their amplitude ratio. This correction has
an exponent proportional to PP
 SS
, which tends to reduce the exponent towards zero
and therefore the correction towards one. Microseismic events usually have 
< 1, further
keeping the difference between the exponents small.
The ASH
ratio corresponds to the S-wave polarisation which is not sensitive to the velocity
effects in an isotropic medium, but like all of the amplitude ratio measurements, it is sensitive
to different coupling between the receiver components and the rotation into other coordinate
systems.
The amplitude ratio can also be strongly affected by the near field, especially for receivers
close to the nodal plane (Section 6.3.3).
Although the amplitude ratios are less dependent on the propagation correction to the
different receivers than the amplitudes, anisotropy can still have a large effect. Shear wave
splitting into fast and slow arrivals (Crampin, 1985) leads to amplitude estimates that are
superpositions of the different arrival phases. Consequently, the amplitude ratios need to
include a correction for this, which can be a large source of uncertainty.
As the noise level increases, the amplitude ratio can exhibit large deviations from the true
value (Fig. 3.11). These deviations can even be large for low noise levels and can lead to a
misfit in amplitude-ratio-based source inversion, because the likelihood of the data given the
correct model is much reduced.
PeaktoPeak
 = 0.1 
 = 0.5 
Maximum RMS
 = 0.4 
 = 0.1 
 = 0.01 
 = 0.2 
 = 0.3 
 = 0.01 
 = 0.5 
 = 0.4 
 = 0.1 
 = 0.1 
0 1 2 3
 = 0.5 
 = 0.5 
0 2 40 2 4
Amplitude Ratio
Figure 3.11: Distribution of amplitude ratios with differing fractional noise levels for the P and S
arrivals. The cyan line corresponds to the correct value and the red line is the expected distribution
of the observed ratios given the correct amplitudes and uncertainties (ratio PDF as defined in section
4.2.3).
If the fractional noise levels are the same, the distribution is slightly more consistent
with the expected distribution for all of the measurement approaches (Fig. 3.11), although
this is dependent on the similarity of the arrivals. This variation and uncertainty in the
approach suggests that the amplitude ratios can be difficult to estimate accurately, potentially
reducing their effectiveness in source inversion approaches. While the RMS measurement
can produce a good approximation of the distribution when the uncertainties on the arrivals
are the same, the uncertainties that arise due to possible window length variation combined
with the narrow uncertainties of some of the distributions in Fig. 3.11 suggest that it may not
be a good measurement technique for amplitude ratios. The maximum absolute value and
the peak-to-peak measurements have very similar distributions.
3.4.1 Amplitude Ratio Inversion.
The calculated ratio for AP
RPSH =
aP.M
aSH.M
. (3.16)
However, this cannot easily be solved directly as for amplitudes, so the options are to carry
out a grid search type method such as that proposed by Hardebeck and Shearer (2002), or to
add in a linear constraint on the source (Dahm, 1996). This linear constraint,
c = 1M , (3.17)
where 1 is a vector with 1 as each element, of the same dimension as M , enables construction
of a non-homogeneous system of equations,
aP RPSHa
M (3.18)
c = 1M (3.19)
which can be rewritten in a matrix form,
 aP RPSHaSH
M , (3.20)
and inverted by using a suitable left inverse for the ratio coefficients matrix. This constraint
is arbitrary, and required to solve the homogeneous system of equations for the amplitude
ratio. The constraint proposed by Dahm (1996) (Eq. 3.17) scales the moment tensor, so the
choice of c is arbitrary.
Figure 3.12: Plot of the different distributions of the amplitudes and absolute amplitude ratios for
several different source types. The first row corresponds to a double-couple source, the second a
tensile crack source and the third is a compensated linear vector dipole source. For the P-wave
amplitudes (first column) red corresponds to positive amplitudes and blue negative. The first three
amplitude ratio columns have no noise level and the second three have a minimum amplitude of 0.1
of the maximum theoretical P and SH amplitudes (SNR=10). Large absolute amplitude ratios are
given by white colours, with lower values corresponding to darker colours. The amplitude ratio plots
all have the same colour scale.
The distribution of the amplitude ratio has little variation apart from at the nodal surfaces,
as is shown in Fig. 3.12. This suggests that the amplitude ratio cannot provide much
constraint, as it only changes rapidly when one of the amplitudes drops to zero. However, the
background noise prevents measurement of very low amplitude arrivals, which often provide
the most discrimination, further reducing the effectiveness of the amplitude ratio (Fig. 3.12).
This is consistent with Hardebeck and Shearer (2003), who found that there is often little
additional information gained by using amplitude ratios for well constrained events, along
with strong noise effects.
The distribution of theoretical amplitude ratios for moment tensors sampled from the
uniform 6-sphere (algorithm B.1) is shown in Fig. 3.13. For a single station and all possible
moment tensors, the most likely amplitude ratio is 0.
30 20 10 0 10 20 30
Figure 3.13: Histogram of the distributionRPSH =
aSHM
for amplitudes distributed
according to Fig. 3.10 for the P amplitude and SH amplitude scaled for an example station
orientation.
3.5 Observations: Full Waveform
The simplest observation to use is just the full waveform from the seismogram. With the
instrument response removed, the waveforms, u (t), are a linear function of the source time
function:
ui (t) = Gijk (t)Sjk (t) . (3.21)
In Eq. 3.21 each component of the seismogram has a corresponding set of response
functions, which can be represented as a three-dimensional matrix. It is possible to calculate
the response functions, however they are heavily dependent on the earth model, as well as the
source and receiver locations.
Several approaches to full-waveform inversions rely on pre-calculating a database of
response functions for the set of receivers and possible locations (e.g. Heimann, 2011),
often taking advantage of reciprocity (see Knopoff and Gangi, 1959; Chapman, 2004). In
most microseismic cases, the events and receivers are within the study area, making the
response functions database very large for appropriate region coverage. Furthermore, the
earth structure is not always well-known or well-constrained, leading to many uncertainties
in both the response functions calculations and the event location. As the frequency of interest
increases, the calculation of the response functions becomes even more complicated due to
the increased sensitivity of seismic waves to smaller-scale heterogeneities.
3.5.1 Full Waveform Source Inversion
The time-dependent moment tensor can be written as a matrix multiplication for a set of
observed seismograms (corrected for the instrument response) and known response functions,
u (t) = G (t)M (t) . (3.22)
This can be solved using standard least squares on each spectral component in the frequency
domain, or by concatenating the time components to give a solution calculated using a least
squares method, with the advantage of using the block Toeplitz form of the matrix to perform
the inversion (Sipkin, 1982).
Full waveform source and location inversion are routinely used for teleseismic earth-
quakes. The Harvard CMT catalogue and USGS, among others, publish moment tensors
very rapidly after an earthquake, subject to revision as more data become available. There
are also fast source inversion approaches using waveform matching of certain phases, such as
the W-phase (Duputel et al., 2012b). These approaches do not work as well in the microseis-
mic case since the response functions are not usually well known and the frequency content
is often a lot higher and more sensitive to heterogeneities.
3.6 Existing Approaches
There are several approaches that utilise first motion polarity for source inversion including
FPFIT (Reasenberg and Oppenheimer, 1985), HASH (Hardebeck and Shearer, 2002, 2003)
and FOCMEC (Snoke, 2003).
FPFIT uses a two-step grid search, finding the double-couple source model which min-
imises a normalised weighted sum of first motion polarity misfits. The weighting consists of
the estimated variance of the data combined with the theoretical P-wave amplitude including
a source to station distance term where available. This reduces the weight of observations
close to the nodal plane as well as at distant receivers.
Rather than solely finding the minimum misfit solution, FPFIT obtains other significant
minima in the misfit, which can correspond to different fault geometries. The uncertainties
estimated by FPFIT are calculated as the range of parameters corresponding to the confidence
interval for the misfit. However, asHardebeck and Shearer (2002) mention, the source mech-
anism changes non-linearly with uncertainties, therefore the range of possible mechanisms
will rarely form an easily parameterised distribution.
FPFIT does account for possible errors in the observed polarities, but it does not include
any uncertainties in the location, or the associated ray path uncertainty.
HASH can include velocity model uncertainties using random sampling from the velocity
models to estimate the ray paths. HASH also uses a grid search approach over strike, dip
and rake, and rather than estimating errors on the parameters, it produces a range of possible
solutions. The misfit minimisation is split between the impulsive and emergent polarities,
first reducing the misfit on the impulsive polarities, and then on the emergent polarities. To
generate the range of possible solutions, those with a misfit less than some threshold are
included.
Different ray paths are tried from various possible one-dimensional velocity models and
source depths, and the acceptable solutions are combined. The RMS difference from the
average solution is used as a crude indicator of the range of possible solutions. Hardebeck
and Shearer (2003) extended this to include S/P amplitude ratios in the misfit calculation.
FOCMEC (Snoke, 2003) uses more data types, such as P, SH and SV polarities and P/SH,
P/SV and SH/SV amplitude ratios, in a grid search approach similar to the others. Again,
this generates samples of possible solutions within allowed observation errors. As with
HASH, the number of polarity errors allowed can be tuned, and the errors can be weighted
in several different ways. However, in using multiple data types it is important to preserve
data independence, as described in section 4.3.
Julian (1986) introduced a linear programming approach to seismic source inversion. This
uses polarities by treating them as inequalities. Amplitudes can be included by constructing
pairs of inequalities equivalent to an equality. Linear programming approaches solve for
the solution by maximising a cost function, subject to the observational constraints (the
inequalities). Julian and Foulger (1996) extended this approach to amplitude ratios, using an
L1 norm of the residuals as the cost function.
Many other approaches use inversions based on Green functions (e.g. Kim et al.,
2000; Bernardi, 2004; Bernardi et al., 2004; Hjrleifsdttir and Ekstrm, 2010; Kim, 2011),
although these are not always full waveform based inversions, but instead use other data such
as amplitude spectra (Cesca et al., 2006). Godano et al. (2009) used amplitudes rather than
the full-waveform to invert for a double-couple source. Source inversions that are a joint
inversion between the source and location are usually Green functions based inversions (e.g.
Wber, 2006; Rodriguez et al., 2012; Kaeufl et al., 2013). OToole (2013) have adapted the
CMT approach (Dziewonski et al., 1981) to micro-earthquakes using near-field and geodetic
observations, resulting in estimates of the centroid and the moment tensor of the event.
3.7 Summary and Discussion
TheGaussian noisemodel is a valid noisemodel for a seismogram provided the low frequency
noise, such as the ocean microseisms, are filtered out. There is some directionality to real
noise, although this does not appear to be inconsistent with the Gaussian noise model.
Both the Q correction and the near-surface effects alter the seismogram, although the
effects are not always strong. Furthermore, site effects and instrument coupling also affect
the seismogram, especially when rotating into different components.
The polarity of the first arrival is the easiest observation to make, although it is non-linear,
requiring a forward model based approach.
The arrival amplitude can be used in a simple matrix formulation to invert for the source.
However, this chapter has shown that since the measurement of the amplitude is related to
the extrema of the arrival, the value is more sensitive to background noise than expected
from a simple noise model. Both the mean value and the uncertainty increase as the noise
level increases, leading to very uncertain measurements, with an arrival specific systematic
uncertainty. This uncertaintyaffects the amplitude ratio in complex ways.
There are several different approaches to estimating the amplitude, and both themaximum
absolute amplitude and peak-to-peak amplitude estimates produce very similar results. The
RMS amplitude often has much lower uncertainties, but has a larger percentage deviation
from the true value. Amplitude ratios can also be used in a linear formulation of source
inversion to within some constant scaling factor on the source.
There are several existing approaches to source inversion, many using just first motion
polarities and limiting the source type to the double-couple model. However, some in-
clude amplitudes and amplitude ratios, and can even include some of the velocity model
uncertainties.
4 Bayesian Source Inversion
Source inversion approaches use the observed data to estimate the source properties. Amp-
litudes and waveforms are linear with respect to the moment tensor, and so the source can be
calculated by using an appropriate left inverse of the station propagation coefficient matrix
(Chapter 3). However, the station propagation coefficients are often uncertain. Furthermore,
the easily observed polarity information is non-linearly related to the source and therefore
cannot be inverted using a matrix form.
A Bayesian approach to solving for the moment tensor and source parameters can sur-
mount some of these problems. Probabilities allow different types of observations to be easily
combined to produce a probability density function (PDF) for the source.
There are several Bayesian approaches, including that of Brillinger et al. (1980) and
Walsh et al. (2009) for polarity inversion, as well as full waveform approaches, such as those
proposed Wber (2006) and Kaeufl et al. (2013).
In this chapter, a new Bayesian approach to source inversion is described using polarity
data, which has been derived separately but is consistent with the methods of Brillinger et al.
(1980) and Walsh et al. (2009), and amplitude ratio data, as well as the full moment tensor
source description. Additionally, two different approaches to representing the resultant source
distribution on source plots are compared. Throughout the chapter, the Bayesian definition of
probability is used, so the probability represents the state of belief in a result, rather than the
frequentist approach where the probability reflects the relative frequency of a result, when
the measurement or experiment is repeated.
4.1 Bayes Theorem
Optimisation processes aim to find the best fitting model parameters for the given data. For
source inversion, the likelihood of the observed data being correct is evaluated for different
possible sources. The resulting estimates of the probability density function (PDF) can be
combined for all the data to approximate the true PDF for the source. This PDF describes
the likelihood of observing the data for a given source, p (data |model). However, the value
of interest in such an inversion is the probability of the model given the observed data, the
posterior PDF. This can be evaluated from the likelihood using Bayes Theorem (Bayes and
Price, 1763; Laplace, 1812; Sivia, 2000)
p (model | data) =
p (data |model) p (model)
p (data)
. (4.1)
Bayes theorem links the two conditional PDFs using prior probabilities. The prior rep-
resents the information known before the experiment, and can be used to include constraints
such as fault geometries or source type.
The choice of prior is not trivial, because it does have an effect on the resultant posterior
PDF (Sivia, 2000, Chapter 2), although the uniform prior is often used.
The normalised full moment tensor has five independent components, compared to three
in the normalised double-couple tensor. These extra free parameters provide an improved
fit, for example to both data and noise. A possible choice of prior could be one that reduces
the tendency for full moment tensor inversions not to fit an exact double-couple solution,
reflecting the commonly held view that most earthquakes are double-couple, without taking
the step of forcing the solution to be double-couple.
4.1.1 Bayesian Marginalisation
Source inversion is particularly sensitive to uncertainties and, because it is usually carried out
after several other required steps, the effect of the uncertainties may be difficult to understand.
The effects of uncertainties on the inversion results are discussed in Chapter 6.
Although interdependencies between the uncertainties can be explored, a quantitative
relationship is not known. For such a treatment to be truly rigorous, the variations in these
errors must be included throughout the inversion.
The Bayesian formulation allows rigorous inclusion of uncertainties in the problem using
marginalisation (Sivia, 2000, Section 1.3). Marginalisation removes the dependence of the
joint PDF, p (A,B), on one of the variables by integrating over the PDF for that variable:
p (A) =
p (A , B) dB =
p (A |B) p (B) dB. (4.2)
Consequently, marginalisation can be used to remove the dependence of the final PDF on
the uncertainties in the inversion. With discrete sampling from an unknown PDF, Eq. 4.2 can
be evaluated as a Monte Carlo integration (Caflisch, 1998), using samples randomly drawn
from the conditional and prior PDFs:
p (A) =
p (A |B) p (B) B, (4.3)
where B is the space of B divided by the number of random samples, N .
4.2 Probability Distributions
The likelihood for a given observation depends on the type of data (Chapter 3). The PDFs for
the different observations are combined together, along with the prior probabilities, allowing
the posterior PDF to be evaluated. The uncertainties, both in the measurements and the
model parameters are included in the source PDF using Bayesian marginalisation (Section
4.1.1), which requires an estimate of the PDFs for the different uncertainties, either from
estimates on the measurements or from direct sampling of the associated PDF if it is known
(e.g. location). Often Gaussian uncertainty is assumed as the uncertainty PDF, but this can
be incorrect, leading to systemic errors in the resultant PDFs.
4.2.1 Polarity
Polarity observations are binary, and the inversionmust test this for each station. Stations with
no observed polarity are ignored, as they provide no information. This lack of information
arises because the absence of an observation is not the observation of an absence, since it is
not possible to determine whether the signal is obscured by the noise, or not present at the
receiver.
For a possible source there are twooutcomes for an observation: matching or notmatching.
Each of these can be assigned a probability such that for a given measurement the probability
of matching and non-matching must sum to one. Assigning a probability of a mistaken
pick can incorporate uncertainties in the observations, but these uncertainties can also be
incorporated in other ways. An amplitude-based weighting on an incorrect pick, or perhaps
some estimate of pick quality, may also be a desirable inclusion.
The PDF for polarity, Yi, given a polarity observation, yi, at an instrument for a given
theoretical amplitude, Ai, is given by a step function such as the Heaviside step function,
H (x) =
 (s) ds:
p (Yi = yi |Ai, Y) = H (yi (Ai +Y)) , (4.4)
where Y is the measurement error arising from the background noise. Eq. 4.4 can be
marginalised (Section 4.1.1) to remove the dependence on Y, which requires a distribution
for the noise. Since both the mean and variance can be measured, the most ambiguous
distribution (maximum entropy) is the Gaussian distribution (Appendix E). De-trended data
has a Gaussian noise distribution with zero mean, and a standard deviation Y:
p (Y |Y) =
22Y
22Y . (4.5)
Marginalising over the measurement error, Y gives
p (Yi = yi |Ai, Y) =
p (Yi = yi |Ai, Y) p (Y |Y) dY
H (yiAi + yiY)
22Y
22Y dY. (4.6)
The integral can be evaluated using the integral identity 
H (x+) f () d =
f () d. (4.7)
The product yiY changes the sign of the noise to reflect the polarity, but since the PDF for
Y is symmetric about zero, this sign change has no effect. Consequently, the marginalised
PDF is
p (Yi = yi |Ai, Y) =
 yiAi
22Y
22Y dY =
1 + erf
yiAi
, (4.8)
with the Gauss error function defined as erf(x) = 2
t2dt.
Eq. 4.8 is dependent on the standard deviation of the measurement uncertainty, which
smooths the PDF as the value increases (Fig. 4.1).
There is also a possibility that a manufacturing error or user error has occurred and the
instrument is incorrectly wired and the trace is inverted. Consequently, the PDF for a given
polarity is also dependent on the probability of this occurring, i:
p (Yi = yi |Ai, Y, i) =
1 + erf
yiAi
(1 i) +
1 + erf
yiAi
(4.9)
The PDF can be rearranged:
p (Yi = yi |Ai, Y, i) = i + (1 2i)
1 + erf
yiAi
, (4.10)
which is equivalent to eq. 7 from Walsh et al. (2009), and eq. 3 of Brillinger et al. (1980).
1 0 1
Figure 4.1: Polarity PDF marginalised over a
Gaussian measurement error (Eq. 4.8). The
uncertainty parameter (Y) has different values
corresponding to different lines. These are: 0
(blue), 0.05 (red), 0.25 (green), 0.5 (light blue)
and 0.85 (black).
Since the moment tensor six vector is
normalised, the modelled amplitude can
take values between -1 and 1. The po-
larity standard deviation (Y) is related to
the amplitude uncertainty compared to the
maximum theoretical amplitude at the sta-
tion (based purely on the event magnitude,
source-to-station propagation and receiver
coupling). However, this is difficult to cal-
culate, especially for events with poor focal
sphere coverage.
Walsh et al. (2009) call Y the noise for the arrival, but this does not scale correctly
in comparison to the modelled amplitude due to the propagation effects. They treat it as
a user-determined value, representing the confidence in the arrival. Y could be estimated
from the fractional amplitude uncertainty, but this will be greater than or equal to the true
value, as the amplitude at a receiver is only less than or equal to the maximum theoretical
amplitude. Consequently, this would most likely overestimate the uncertainty. It is clear that
the uncertainty value should be station-specific as noise environments at different stations
often vary, so the maximum estimate of the event signal-to-noise ratio (SNR) fails to account
for the variation across the receivers.
The difficulty in estimating the uncertainty is increased further when polarity picking is
done manually, so the uncertainty on the trace is perhaps not even known, especially when
dealing with old data. Due to the difficulty in quantifying the uncertainty, it is best left as
a user-defined parameter that reflects the confidence in the arrival polarity pick, which can
be mapped to the pick quality. Although this uncertainty is difficult to quantify, there is
an alternate method for including the noise uncertainty in the source, which is presented in
Chapter 5.
4.2.2 Amplitude Ratio
The amplitude ratio is more complex than the polarity, and there are different approaches to
the measurement, each of which can have different effects (section 3.3). Moreover as the
uncertainty on the amplitude measurement is usually modelled as a Gaussian, the uncertainty
on the amplitude ratio cannot be Gaussian, and is more complex (Section 4.2.3). This results
in a non-Gaussian amplitude ratio likelihood (Section 4.2.3).
The modelled amplitude ratio is dependent on the theoretical amplitudes, APi , A
i , and
ASVi . Each amplitude observation has a measurement uncertainty, and the uncertainty of
the ratio is not simply the ratio of the amplitude uncertainties. The PDF for a given ratio
observation vector Ri =
SHRi,
, at an instrument with given theoretical amplitudes,
APi , A
i , andA
i , is two dimensional. However, since the observations are independent, the
PDFs are also independent, so the PDFs for each ratio can be multiplied together.
Considering a single observed ratio ri = xiyi and theoretical amplitudes A
i and A
i , the
PDF for the ratio is given by the delta function (x), although this is again dependent on the
measurement error for the two observations x and y:
|Axi , A
i , 
x, y
Axi +x
i +y
. (4.11)
4.2.3 Amplitude Ratio Noise Model
The modelled amplitude ratio, RPSH, does not just depend on the model amplitudes, but it
also contains a propagation correction, Z, (c.f. Aki and Richards, 2002, eq. 4.97):
RPSH =
Z ASH
. (4.12)
This correction depends on the velocity ratio between these phases. The simplest form of
3 4 5 6 7 8
) (a)
0 2 4 6 8 10
0 5 10 15
(Vp/Vs)3
10 0 10 20 30
(Vp/Vs)3
Figure 4.2: Histograms of the propagation correction (Eq. 4.13) distributions (in blue) assuming
Gaussian error on VP/VS. The red lines correspond toN (Z, Z, Z) calculated according to Eqs 4.14
and 4.15. The cyan vertical lines correspond to Z, and the histograms show the actual distributions
calculated from 1 000 000 samples from Gaussian distributions for VP/VS with VP/VS = 1.75. The
standard deviation was varied, taking values of (a) VP/VS = 0.04, (b) VP/VS = 0.1, (c) VP/VS = 0.2
and (d) VP/VS = 0.5. The vertical scales are independent between the plots as the amplitude
distributions are normalised so that the area integral is 1.
the propagation correction for P/S ratios, assuming constant VP/VS ratio along the ray path, is
Z = (VP/VS)3 . (4.13)
Assuming a Gaussian uncertainty for the VP/VS ratio, the distribution of the propagation
coefficient is shown in Fig.4.2. The mean and standard deviation are
Z = 3VP/VS , (4.14)
Z = 3 (VP/VS)
2VP/VS
, (4.15)
where Z can be determined from the Taylor expansion of the correction function (Eq. 4.13).
Fig. 4.2 shows that a Gaussian distribution is a good approximation to the true distribution of
values, provided the percentage uncertainty on the VP/VS ratio is small compared to the ratio(
VP/VS & 9VP/VS
. The VP/VS percentage uncertainty is typically small, so the Gaussian
noise model is a valid model to use. Fractional uncertainties within 0.2 often encompass all
the uncertainty in the Wadati plot of S to P delay times versus P arrival time (Wadati, 1933;
Wadati and Oki, 1933).
However, the correction can be applied to the observed P amplitude, instead of the
modelled S amplitude. The corrected P amplitude, AP can be approximated by a Gaussian
distribution, with mean and standard deviation
AP = ZAP , (4.16)
AP =
Z + Z22AP . (4.17)
Fig. 4.3 shows the distribution of the corrected P-wave amplitudes, evaluated for a range
of VP/VS uncertainties and noise levels. For small VP/VS uncertainties, the distribution is ap-
proximately Gaussian. However, as the uncertainty increases it becomes skewed, although the
region where the Gaussian approximation fails is higher than the expected VP/VS uncertainties.
1 2 3 4
) (a)
2 0 2 4 6
0 1 2 3 4
) (c)
2 0 2 4 6
0 2 4 6 8 10
Corrected A
 (arbitrary units)
) (e)
2 0 2 4 6 8 10
Corrected A
 (arbitrary units)
Figure 4.3: Histograms of the propagation corrected model P amplitude AP distributions (in blue).
The red lines correspond to N
aP , AP , AP
calculated according to Eqs 4.16 and 4.17. The
cyan vertical lines correspond to AP , and the histograms show the actual distribution calculated
from 1 000 000 samples from Gaussian distributions for VP/VS with VP/VS = 1.75. The left column
shows the distributions for a P amplitude SNR = 40, the right column SNR = 2. (a) and (b)
VP/VS = 0.04, (c) and (d) VP/VS = 0.1 and (e) and (f)VP/VS = 0.3. The vertical scales are
independent between the plots as the amplitude distributions are normalised so that the area integral
is 1.
The amplitude ratio can be described in terms of two parameters: the mean
ZAP
, (4.18)
and standard deviation
R = RPSH
 2P
, (4.19)
giving a Gaussian distribution as the maximum entropy distribution (Appendix E).
These two parameters describe a possible Gaussian model for the amplitude ratio uncer-
tainty, with a PDF
p (R = |r| |R, R) = N (r, R, R) +N (r, R, R) (4.20)
22R
e (rR)
22R + e
rR)
 , (4.21)
and cumulative distribution function (CDF)
P (R 6 |r| |R, R) =  (r, R, R) +  (r, R, R) 1 (4.22)
r  R
+ erf
r  R
, (4.23)
where the Gaussian PDF is
N (R = r, R, R) =
22R
rR)
22R , (4.24)
and the Gaussian CDF is
 (R = r, R, R) =
1 + erf
r  R
. (4.25)
The inversion approach uses unsigned (absolute) amplitude ratios, so the distribution reflected
in R = 0 must be included.
Figs 4.4 and 4.5 shows a simulated distribution of the ratio given Gaussian uncertainty
on the amplitudes and the VP/VS ratio, and it is clear that when the denominator uncertainty
is significant, the Gaussian is a poor noise model.
While the Gaussian distribution is the maximum entropy distribution given a mean
and a variance, so is consistent with the uncertainty on the amplitude observations, the
modelled amplitude ratio is a ratio of two normally distributed parameters. Consequently the
distribution of the ratio must be used. Assuming that a Gaussian model is sufficient for the
corrected P amplitude, there are, in fact, 5 parameters, P , S, P , S and .
Fieller (1932) derived a distribution for the ratio, R = X
, of two normally distributed
observations with means X and Y, standard deviations X and Y and correlation .  is
the correlation between the parameters, which is assumed to be zero in this case because the
observations are independent. The PDF for R, p (r) is determined from the joint density of
X, Y , g (x, y):
p (R = r) =
|y| g (ry, y) dy. (4.26)
Substituting a bivariate normal density for g (x, y) gives the ratio PDF (Hinkley, 1969)
p (R = r) =
b (r) d (r)
XYa3 (r)
b (r)
a (r)
1 2
b (r)
a (r)
1 2
1 2
XYa2 (r)
2(12)
, (4.27)
with coefficients a (r), b (r), c and d (r):
a (r) =
 r2
Xly
, (4.28)
0.2 0.22 0.24 0.26 0.28 0.3
) (a)
50 55 60 65
0 5 10 15 20
) (c)
0 5 10 15 20
0 10 20 30
) (e)
0 2000 4000 6000
Figure 4.4: Distribution of the model ratio R. The red line corresponds the Gaussian noise model
(Eq. 4.20). The green line corresponds to the ratio distribution (Eq. 4.35), again adding on the part
corresponding to the distribution reflected in R = 0 to account for the fact that the unsigned
(absolute) ratio is used. The cyan line corresponds to R , while the histogram reflects the actual
distribution calculated from 1, 000, 000 samples from a Gaussian distribution for AP, AS and V pV s,
with VP/VS = 1.74 and VP/VS = 0.1. The P and S uncertainties were varied (a) and (b) have low P
and S amplitude uncertainties (SNR > 10), (c) SNRP = 2, SNRS = 20, (d) SNRP = 15,
SNRS = 3.8, (e) SNRP = 200, SNRS = 2 and (f) SNRP = 1000, SNRS = 4
b (r) =
X + Yr
, (4.29)
, (4.30)
d (r) = e
b2(r)ca2(r)
2(12)a2(r)
. (4.31)
The CDF for the ratio distribution is (Hinkley, 1969)
P ( 6 R 6 r) = L
X  Yr
XYa (r)
Yr  X
XYa (r)
Yr  X
XYa (r)
Yr  X
XYa (r)
(4.32)
where L (u, v; )is the standard bivariate integral
L (u, v; ) =
1 2
22xy+y2
2(12) dxdy (4.33)
with zero means and covariance matrix
 1 
 . (4.34)
Figure 4.5: Cumulative distribution of the model ratio R. The red line corresponds to the the
Gaussian noise model CDF (Eq. 4.22). The green line corresponds to the ratio distribution CDF (Eq.
4.36). The solid cyan line corresponds to R and the distribution median, the dotted cyan line to an
estimate of the distribution mode and the dashed line to the distribution mean. The histogram reflects
the actual distribution calculated from 1, 000, 000 samples from a Gaussian distribution for AP, AS
and V pV s, with VP/VS = 1.74 and VP/VS = 0.1. The P and S uncertainties were varied (a) and (b)
have low P and S amplitude uncertainties (SNR > 10), (c) SNRP = 2, SNRS = 20, (d)
SNRP = 15, SNRS = 3.8, (e) SNRP = 200, SNRS = 2 and (f) SNRP = 1000, SNRS = 4
The ratio PDF shows a much better fit to the observed ratio distributions (Fig. 4.4)
than the Gaussian model, and a corresponding better fit to the CDFs (Fig. 4.5). The CDF
plot confirms that the Gaussian is not a good noise model while the uncertainties on the
denominator are large, and furthermore shows that the distribution mean is higher than the
true value of the ratio, and the best estimate is the median value. Unlike the Gaussian PDF,
there is a dependence on the numerical values of the errors and Eq. 4.27 is not symmetrical
in the ratio and the means.
For ease of use throughout the rest of this thesis, the ratio PDF (Eq. 4.27) is referred to
asRN (r, X, Y, X, Y, ).
To account for the fact that the amplitude ratio used is unsigned (absolute), the PDF used
p (R = |r|) = RN (r, X, Y, X, Y, 0) +RN (r, X, Y, X, Y, 0) , (4.35)
and the CDF
P (0 6 R 6 |r| , X, Y, X, Y) = RN (r, X, Y, X, Y, 0)+RN (r, X, Y, X, Y, 0)1
(4.36)
where RN (r, X, Y, X, Y) is the ratio CDF (Eq. 4.32) and X, Y > 0.
4.2.4 Amplitude Ratio PDF
For Gaussian uncertainties on the measurements, the amplitude ratio noise model is given by
the ratio PDF (Eq. 4.35). The ratio PDF is not symmetric in the means and the ratio, but it is
straightforward to show that the distribution depends on only the ratio of the means and the
percentage errors, and so the uncertainties in Eq. 4.35 (X and Y) are estimated from the
percentage error on the measurement,m:
i = Ai
measuredi
. (4.37)
Marginalising Eq. 4.11 for the ratio PDF (Eq. 4.35) gives,
Ri = ri|A1i , A
i , 
i , 
 ri +R
R|A1i , A
i , 
i , 
dR. (4.38)
The integral can be evaluated using the delta function, giving the distribution for the ratio
uncertainty in Eq. 4.39.
R|A1i , A
i , 
i , 
+R, A1i , A
i , 
i , 
i , 0
R, A1i , A
i , 
i , 
i , 0
(4.39)
Consequently, the marginalised PDF is given by
Ri = ri |A1i , A
i , 
i , 
ri, A
i , A
i , 
i , 
i , 0
ri, A1i , A
i , 
i , 
i , 0
(4.40)
4.2.5 Posterior PDF
Using the polarity PDF (Eq. 4.9) and the amplitude ratio PDF (Eq. 4.40), it is possible to
Symbol Parameter
Y Observed polarities
R Observed amplitude ratios
 Pick arrival times
x Event location
A Theoretical amplitudes
 Observed measurement errors
 Pick arrival time uncertainties
 Probability of reversed trace
a Station angle six-vector
M Moment tensor
G Earth model
s Station locations
Table 4.1: Parameters for the posterior PDF
extend the approach of Walsh et al. (2009) to
include amplitude ratio information and the
full moment tensor solution. The parameters
used in this section are described in Table 4.1.
For a probabilistic approach to source in-
version it is necessary to obtain an approxim-
ation to the posterior PDF for the source para-
meters, p (M | d,k). The posterior PDF de-
pends on the observations, d = (d,  ) where
d = (Y ,R); the unknown nuisance paramet-
ers which are marginalised over,  = (,x),
where  = (A,,); and the known para-
meters, k = (s, ,G ). Bayes theorem (Eq.
4.1) allows us to rewrite the posterior PDF
in terms of the likelihood of the data and the
model prior:
p (M | d,k)  p (d |M ,k) p (M ) . (4.41)
The posterior PDF is
p (M | d,  ,k)  p (d |M ,  ,k) p (M) , (4.42)
with the likelihood of the observed source mechanism data given by
p (d |M ,  ,k) = p (Y ,R |M ,  ,k) . (4.43)
Unfortunately the data likelihoods (Eqs 4.9 and 4.40) depend on the nuisance parameters, so
to obtain Eq. 4.43, the nuisance parameters, , must be marginalised over:
p (d |M ,  ,k) =
[p (Yi,Ri |M ,i, i, ki, i)] p ( |M ,  ,k) d. (4.44)
As the observations used are independent, the resultant PDF can be calculated simply from
the product of the individual observation PDFs (Sivia, 2000, Section 3.5):
p (Yi,Ri |M ,i, i, ki, i) = p (Yi |M ,i, i, ki, i) p (Ri |M ,i, i, ki, i) . (4.45)
Substituting this into Eq. 4.44 gives
p (d |M ,  ,k) =
[p (Yi |M ,i, i, ki, i) p (Ri |M ,i, i, ki, i)] p (i |M ,  ,k) d.
(4.46)
Expanding the nuisance parameters into the components , which includes the modelled
amplitudes and data uncertainties, and the location, x, using the product rule for conditional
probabilities,
p (a, b, c) = p (a|b, c) p (b|c) p (c) , (4.47)
gives
p (d |M ,  ,k) =
[p (Yi |M ,i, ,x,k, i) p (Ri |M ,i, ,x,k, i)]
p (x |M ,  , ,k) p ( |M ,  ,k) dxd. (4.48)
However, since Yi and Ri depend only on  and the location depends only on the arrival
times,  , and the known parameters, k, the likelihood can be simplified to
p (d |M ,  ,k) =
[p (Yi | , i) p (Ri | , i)] p (x |  ,k) p ( |M ,  ,k) dxd.
(4.49)
Expanding for the nuisance variables in  using the product rule (Eq. 4.47) and splitting
these variables into the receiver dependent and independent parameters gives
p (d |M ,  ,k) =
p (Yi |Ai,i, i, i) p (Ri |Ai,i, i) p (Ai |x,M ) dAi
p (x |  ,k) p () p (M ) dxdd. (4.50)
The probability of the observations for the source model parameters, and the known data,
including the time picks and associated uncertainties, station locations and the velocity model
is given by Eq. 4.50. This is the product of the polarity and amplitude ratio likelihoods
(Eqs 4.9 and 4.40) marginalised over four nuisance parameters, the theoretical amplitude
probability, p (Ai |x,M), the location PDF, p (x |  ,k, ), the probability of obtaining
the observed measurement errors for the polarities and amplitude ratios, p (), and the
probability of an instrument trace reversal, p ().
The theoretical amplitude probability can be written in terms of the dot product between
the station propagation coefficients,a, and themoment tensor six-vector, M , where the station
propagation coefficients are dependent on the location, x. The PDF for the amplitude for a
given source location, xj , is dependent only on that location so is given by a delta function,
Ai  aj  M
, where aj = a (xj) refers to the station propagation coefficients associated
with the location at xj:
p (d |M ,  ,k) =
p (Yi |Ai,i, i, i) p (Ri |Ai,i, i) 
Ai  aj  M
p (x = xj |  ,k) p () p () dxdd. (4.51)
Integrating over the delta function is equivalent to evaluating the theoretical amplitude asso-
ciated with the location. Consequently, the PDF is given by
p (d |M ,  ,k) =
Yi |Ai = aj  M ,i, i, i
Ri |Ai = aj  M ,i, i
p (x = xj |  ,k) p () p () dxdd. (4.52)
The integral over the location uncertainty is not analytic but can be evaluated using a Monte
Carlo approach by summing over T hypocentre samples drawn from the location PDF,
p (x = xj |  ,k):
p (d |M ,  ,k) =
Yi |Aij = aj  M ,i, i, i
Ri |Aij = aj  M ,i, i
p () p () dd. (4.53)
Eq. 4.53 includes the location uncertainty in the PDF by converting the integral over dx to a
sum over the samples.
Following Walsh et al. (2009), the likelihood for a case with an unknown earth model
differs from the known earth model case (Eq.4.53). It has an additional Monte Carlo type
integration over the velocity models, with the station propagation coefficients now dependent
on the location, x, and the earth model, G , where ajk = a (xj,G k) refers to the station
propagation coefficients associated with the location at xj and earth model G k. For U
earth models and T locations, there is a large U  T space to sum over to carry out the
marginalisation:
p (d |M ,  ,k) =
Yi |Aijk = ajk  M ,i, i, i
Ri |Aijk = ajk  M ,i, i
p () p () dd. (4.54)
The specific PDFs for the polarity and amplitude ratios are given by Eqs 4.9 and 4.40, where
there is an arrival time dependence implicit in the measurement. There is no dependence
on the trace reversal probability, i, in the amplitude ratio likelihood as the observations are
unsigned so are unaffected.
p () andp () are the chosen priors for themeasurement uncertainties and the probability
of instrument trace reversal. It is possible to remove any dependence on the trace reversal
probability, i, by using observations from a source with a known mechanism (e.g. a
teleseismic earthquake) to calibrate the trace orientation. Consequently, the trace reversal
probability in Eqs 4.9 and 4.40 would be either 0 or 1, or more usefully, any incorrect traces
could be inverted so that i = 0 for all stations.
The likelihoods for a known velocity model (Eq. 4.53) and unknown velocity models
(Eq. 4.54) have been defined in some detail. The equations include uncertainties in the
observations, the location, with all of its associated uncertainties, and the probability of
a trace reversal. The source model distribution is fully described by the posterior PDF,
p (M | d,k), which can be evaluated by multiplying the likelihood (Eqs. 4.53 and 4.54) with
the chosen source prior.
4.2.6 Posterior Model Probabilities
Using samples from the likelihood Eqs 4.53 and 4.54, the Bayesian model evidence can be
evaluated using a Monte Carlo integration for the full moment tensor and double-couple
constrained source models:
p (data|model) =
p (data|x) p (x|model) x. (4.55)
Care must be taken with the choice of the prior parameterisation, which must correspond to
the one the samples were drawn in, either directly or by correcting both the prior distribution
and the x values. A Monte Carlo approach can be affected by small sample sizes in the
integration, which is sometimes the case when the source PDF is dominated by a few very
large probability samples.
The Bayesian evidence for the moment tensor and double-couple models can be converted
into the corresponding posterior values (pDC and pMT) using Bayes theorem (Eq. 4.1), for
a suitable prior such as the uniform prior p = 0.5 for each. The posterior probabilities
can be normalised so that the sum is one, because the source is contained within the two
models. This is similar to a test of the hypothesis that an event is non-double-couple.
Consequently, the approaches for determining statistical significance levels can be applied,
similar to Horlek et al. (2010) using the F-test. The resultant probabilities help distinguish
between the different types of events, and, consequently, between different physical processes
and source interpretations (e.g. Baig and Urbancic, 2010).
4.2.7 Priors
Bayes Theorem (Eq. 4.1) shows that the prior PDF for the source is equally important
in determining the resultant posterior PDF. Example priors include a basic uniform prior,
which signifies no previous knowledge of the possible source distribution. A more complex
prior for the moment tensor space could be one with a strong weighting for double-couple
solutions. As mentioned previously, this would counter the improved fit produced by the
increased number of free parameters in a full moment tensor inversion. The full moment
tensor inversion often produces results where the non-double-couple component appears to
be an artefact of the inversion, such as in the distribution of non-double-couple components
found by White et al. (2011), which resemble a normal distribution about zero. It is also
possible to use more complicated prior PDFs to fit known fault geometries or previously
observed results.
The normalisedmoment tensor has five independent components, compared to three in the
normalised double-couple tensor, and these extra free parameters provide an improved fit. A
suitable prior could reflect the commonly held view that most earthquakes are double-couple,
without taking the step of forcing the solution to be double-couple.
Fig. 4.6 provides a comic illustration of the effect that prior observations can have on an
observation, and why they are important.
Figure 4.6: Adapted from Frequentists vs. Bayesians, xkcd  a web-comic by Randall Munroe
(http://www.xkcd.com/1132)
4.3 Data Independence
An important caveat when using the different pieces of information is that the independence
of the data must be preserved. Without this, the inversion is effectively reusing measure-
ments, leading to incorrect results. For amplitude ratios, the ratios of AP/ASH and AP/ASV are
independent, but using a third ratio, ASH/ASV, would not be independent as it can be formed
from A
AP/ASH
4.4 Source PDF Representations
The normalised moment tensor PDF is five dimensional, and cannot easily be visualised on
paper, or even in three-dimensions. Consequently the PDF is usually split into source-type
and orientation components. There are several methods of plotting source types (Section
2.5), but some care needs to be taken when extending them to plotting the source PDF. The
projection of the PDF onto the chosen plot can be done in two ways, either marginalising over
http://www.xkcd.com/1132
the non-plotted parameters or plotting the silhouette of the PDF, and each has its advantages
and disadvantages.
4.4.1 Marginalised Plots
Projections of the source PDF rarely show all of the parameters. The non-plotted parameters
can be included by marginalising over them, which is the mathematically correct treatment
for representing the full PDF on a reduced dimensionality plot. However, this means that
Figure 4.7: Simple Marginalised
PDF example showing a two
parameter joint PDF, and the plots
of the respective marginalised
probabilities as projections on the
corresponding axes.
the information from the non-plotted parameters are in-
cluded in the plot, so the resultant marginalised PDF can
no longer be used to estimate any of those parameters.
Fig. 4.7 illustrates the effects of marginalisation. The
maximum value of the marginalised PDFs for each para-
meter do not necessarily correspond to each other, so it is
incorrect to state that the maximum joint probability solu-
tion is given by the maximum values of the marginalised
PDFs. For the example in Fig. 4.7, the statements: the
most likely value of x is 0.25; and the most likely value
of y is 1.6, must be kept separate, because the correspond-
ing point (0.25, 1.6) does not necessarily correspond to
the maximum likelihood solution in the full joint PDF
(0.01, 1.6).
Figure 4.8: Source type plots of the marginalised source PDF for an example event, showing that the
most likely source type is close to an opening tensile crack. The first plot shows the Hudson u, v
decomposition, the second shows the Hudson , k plot and the third is the lune plot.
The effects of marginalisation are perhaps clearer when considering source type plots
such as the Hudson type plot (Section 2.5.5) or the lune type plot (Section 2.5.6) . A plot
of the source PDF, marginalised over the orientation, shows the probability of the different
source types (Fig. 4.8). Consequently, the most likely source type from the marginalised
PDF does not have an associated orientation and it is incorrect to assign one to it.
It is more difficult to plot the marginalised orientation information, as there are three
orientation parameters, which cannot be plotted in two dimensions. It is possible to show
the distribution of the T and P axes, marginalised with respect to each other, but this makes
it difficult to interpret the interdependence of the different orientation parameters. Instead,
it may make more sense to consider the orientation PDF in terms of the orientation angles,
strike, dip and rake (or slip), exploring the resultant joint three dimensional PDF. Fig. 4.9
shows a plot of the marginalised joint orientation PDF from the Tape parameterisation (Tape
and Tape, 2012a). However, the orientation is three dimensional and the plot is only two
dimensional, so each plot represents the corresponding joint probabilities marginalised with
respect to the other orientation parameter. Consequently, care needs to be taken with the
interpretation of the plots.
Figure 4.9: Plot of Strike (), dip cosine (h) and slip () joint PDFs for an example event, each one
marginalised with respect to all the other parameters.
4.4.2 Silhouette plots
Figure 4.10: Simple Silhouette PDF
example showing a two parameter joint
PDF, and the projection of the joint PDF
on the corresponding axes, giving the
silhouette plot.
Since the marginalised PDF plots cannot discrimin-
ate over the marginalised degrees of freedom, the plot
does not represent the non-marginalised PDF. Con-
sequently, the maximum probability value may not
correspond to the maximum of the full posterior PDF.
It may in fact be desirable to instead show a slice
through the non-marginalised PDF. However, instead
of a slice through some specific coordinates, a better
approach may be to consider the silhouette of the
PDF (Fig. 4.10). The silhouette slice contains the
maximum probability value for each coordinate point
from the full PDF. Consequently, a silhouette plot
includes information about the full source PDF, such as the maximum full PDF likelihood
value, unlike in the marginalised PDF case (Section 4.7).
Fig. 4.10 shows a silhouette plot for the same PDF as Fig. 4.7. The maxima on each of
the silhouettes corresponds to the maxima on the joint PDF. The effect of the silhouette plot
on a source-type plot shows the maximum values of the PDF for a given source type, which
has a corresponding orientation.
Figure 4.11: Source type plots of the source PDF silhouette for an example event (Same event as
Fig. 4.8), showing that the most likely sources are located in the opening tensile crack and explosive
region. See also Fig. 4.8.
It is still difficult to plot the orientation information, but the silhouette approach includes
the maximum likelihood orientation of the solution. The joint PDFs for the orientation
parameters can be plotted in two different ways, either as the complete silhouette of the
source PDF (Fig. 4.12) or the silhouette of the marginalised orientation PDF (Fig. 4.13).
Figure 4.12: Plot of Strike (), dip cosine (h) and slip () joint PDFs, showing the silhouette of the
source PDF for an example event (Same event as Fig. 4.9).
Figure 4.13: Plot of Strike (), dip cosine (h) and slip () joint PDFs, showing the silhouette of the
marginalised orientation PDF for an example event (Same source as Fig. 4.9).
The first plot shows the orientation of the maximum likelihood solution, and the second
shows the maximum likelihood orientation (marginalised over the source types). There is
a clear difference between the two, with Fig. 4.13 resembling the distribution of Fig. 4.9.
Figure 4.14: Plot of fault planes
for a double-couple source.
Possible fault planes are given by
the lines, with the most likely fault
planes given by the darkest lines.
Manually picked station first
motions are given by upward red or
downward blue triangles.
The double-couple source PDF can be represented by
the distribution of the fault planes plotted on the focal
sphere (Fig. 4.14). This is a good way of representing the
double-couple source PDF, although the plot is a silhouette
type.
4.4.3 Tape Parameter PDF Plots
The Tape and Tape (2012a) parameterisation can be used
as a method of plotting either the full source PDF or the
marginalised PDF, though care must be taken with the
marginalised plots not to describe the maxima as the most
likely source. Fig. 4.15 shows the plots of the parameters
for an example source. These show all five parameters
of the moment tensor, although they are not necessarily a good description of the full
five dimensional distribution with clear variation between the marginalised and silhouette
distributions.
Figure 4.15: Histogram of the marginalised and silhouette of the Tape parameter source PDFs for an
example event (Same event as Fig. 4.8). For the marginalised plots, each plot is marginalised over
the other parameters.
4.5 Summary and Discussion
This Bayesian approach calculates the likelihood of observing the data for different source
parameters. The full source PDF can be estimated using a forward model approach. It
incorporates the measurement uncertainties for a Gaussian noise model, as well as the
combined noise model for the amplitude ratio using marginalisation.
The choice of prior PDF is important for converting to the Bayesian posterior PDF, but
the simplest approach is to use a uniform prior PDF.
Although there are many different possible combinations of data, the choice of data for
the inversion must preserve the data independence, otherwise the PDF can be sharpened
artificially.
There are several different ways of plotting the source PDF. The choice of PDF represent-
ation can have a large effect on the source interpretation, so care must be taken to indicate the
type of plot, because there can be large differences between the marginalised and silhouette
source PDF plots. The Tape parameterisation provides an easily interpretable plot for the
source type, which can also represent the source PDF. This is more easily understood than the
equivalent Hudson plot. Representing the orientation information is more complex, and can
be shown either as joint PDFs between the individual parameters, or perhaps more clearly for
double-couple solutions, by the silhouette fault plane plots. It is also important to consider
the prior distribution of moment tensors, there are many different possible definitions of a
random moment tensor, of which several are discussed in Chapter 2 and Appendix B. This
leads to non-uniform prior distributions in the common source-type plots, and many possible
source-type plots with the desirable uniform distribution. Instead, a parameterisation with
physical meaning should be chosen, such as the one introduced by Tape and Tape (2012a),
rather than more arbitrary decompositions, and the prior distribution should be accounted for
when interpreting the source.
The approach described in this chapter is consistent with that of Brillinger et al. (1980)
and Walsh et al. (2009), and has been written in similar nomenclature and format to the
method in the latter paper. However, this approach has been extended to more complex
data-types, such as amplitude ratios, and any other data type could be added, as long as the
measurements remain independent, and an estimate of the uncertainty in the measurement
can be made. This method also works with sources described by any source model, so can
be used with the double-couple model, the full moment tensor model, or other models, such
as the CDC model described in Chapter 2. The choice of source model constraint reflects the
prior assumptions about the source, so care must be taken when choosing a source distribution
to use as to what prior assumptions this corresponds to. The simplest prior assumption is that
the source should be double-couple, but there are possible sources that are more complex, so
the full moment tensor space should be examined.
Non-double-couple sources, requiring a different physical mechanism from the well-
understood description of slip on a fault-surface, have often been obtained in the literature
(e.g. Frohlich, 1994; Julian et al., 1998b; Foulger et al., 2004;White et al., 2011). However,
these solutions are often limited by uncertainty in the observations, little knowledge of the
velocity model, and a poor distribution of receivers on the focal sphere. As a result, these
non-double-couple sources are often considered to be spurious (e.g. White et al., 2011),
which can be tested using complex synthetic tests (len and Milev, 2006; Vavryuk et al.,
2008; len, 2009). However, these tests can be complex and time-consuming, and must be
evaluated separately to the source inversion. Instead, using the source PDF to evaluate the
Bayesian model evidence for the double-couple and full moment tensor solution provides a
better quantitative estimate of whether the source is double-couple or not. This estimate is
more intuitive than those provided by the Akaike and Bayesian information criteria (Akaike,
1974; Schwarz, 1978), since the resulting value corresponds directly to the posterior model
probability, so can also be used as a test of the hypothesis that an event is non-double-couple.
5 Bayesian Automated Polarity
Estimation
First-motion-based source inversion of earthquake seismograms can be used to constrain
focal planes and to help estimate the source parameters. This was proposed by Nakano
(1923) and first implemented by Byerly (1926). It is commonly used as a simple method
of estimating some of the source characteristics using the seismic waves produced by the
earthquake. However, there is usually still a requirement to pick the arrival polarity manually.
There are several source inversion approaches that utilise first-motion polarities to estimate
the source type, such as FPFIT (Reasenberg and Oppenheimer, 1985), HASH (Hardebeck
and Shearer, 2002, 2003) and FOCMEC (Snoke, 2003), although these are often limited to
a double-couple source model. Manual polarity picking is time consuming, especially for
large microseismic data sets, which are often processed automatically, so the addition of a
slow manual step into the automated workflow is undesirable.
The first motion of a seismic waveform can often be hard to discern from background
noise and filter artefacts, especially for low-magnitude events. A robust first-motion source
inversion requires some understanding of the likelihood of an incorrect measurement. While
the human eye and judgement are often correct inmanually picking the polarity of an arrival on
a seismic trace, it is usually recorded simply as being either positive or negative. Observation
of the first motion using a binary classification does not usually include the assignment of
any quantitative value to reflect the level of measurement uncertainty, and many automatic
approaches for estimating polarities usually produce results with such a binary classification
(Baer and Kradolfer, 1987; Aldersons, 2004; Nakamura, 2004).
A common approach to deal with errors in the polarity picks is to allow a certain number
of mistaken polarities in a fault plane solution(Reasenberg and Oppenheimer, 1985); another
is to provide a probability of a mistaken pick (Hardebeck and Shearer, 2002, 2003). Never-
theless, these approaches do not account for how likely it is that an interpreter picks an arrival
incorrectly. This depends on both the noise on the particular trace and the arrival character-
istics, such as whether the arrival is impulsive or emergent. As a result, the probability of an
incorrect pick varies from waveform to waveform.
The approach described in this chapter eschews this binary classification for the polarities,
instead the probability of the arrival being a positive or negative polarity is calculated from
the waveform. This allows including the uncertainty in the arrival in a quantitative assessment
of the polarity, for later use in source inversion.
This chapter presents a new approach for estimating the first motion polarity of an arrival,
including a quantitative estimate of the uncertainty on the measurement. This approach
is used on data obtained from a local seismic network in the Krafla and Askja regions of
Iceland, along with some synthetic data generated using finite difference modelling (Bernth
and Chapman, 2011). The polarity probabilities described by this approach are also included
in the Bayesian source inversion approach described in Chapter 4.
5.1 Probabilistic Auto-Picking
It is possible to estimate the polarity of a waveform at any point, which retains the polarity
information discarding both amplitude and phase. This reduces the problem of estimating
the arrival polarity to selecting which time (t) is representative of the correct arrival pick.
The basic approach used for manual observation of polarities is to estimate the polarity from
the next stationary value after the arrival time pick.
0.4 0.2 0 0.2
t (s)
Figure 5.1: Histogram of 1791 arrival
time pick differences between automatic
and manually refined picks, the black line
corresponds to the mean shift for each
pick type, the top plot is for P arrivals, and
the bottom S arrivals. Typical frequency
of the arrival is  10Hz . Automatic picks
were made using the coalescence
microseismic mapping method (CMM)
(Drew et al., 2005, 2013) on microseismic
data from the Askja region of Iceland.
Such an approach is straightforward computa-
tionally. The polarity can be described at a given
time t by pol (t), which takes values switching
between 1 and 1, corresponding to the next max-
ima or minima.
5.1.1 Bayesian Method
Arrival-time picks are often imprecise with respect
to the first motion of the arrival, because noise ef-
fects and the choice of picking approach often leads
to a later automatic triggering (Fig. 5.1), and per-
sonal preferences may lead to different positions of
the manual arrival-time pick on the arrival phase.
In the case of an uncertain arrival-time pick, the
identification of positive or negative polarity may
not be meaningful, as it may not refer to the true
first arrival.
The amplitude change of the candidate first ar-
rival can provide an indication of how likely it is to
have been affected by noise, and can be used to give the arrival some quality weighting.
However, it may be clearer to consider how likely the value is to be positive, based on the
measured noise level, i.e. by estimating the probability that the amplitude change of the
candidate first arrival is not due to the noise.
The PDF for a given absolute amplitude change between stationary values, , and noise
standard deviation, m, at a sample is:
p (Y (t) + |m) =
1 + erf
pol (t) (t)
. (5.1)
This has been derived using a similar approach to Eq. 4.9, with the standard deviation mul-
tiplied by
2 , because  corresponds to the absolute amplitude change between stationary
values, rather than the noise amplitude.
This PDF has been marginalised (Sivia, 2000, Section 1.3) with respect to the measure-
ment noise, but retains dependence on the noise standard deviation, m. However, it is also
necessary to marginalise for the arrival-time uncertainty. This uncertainty depends on the
arrival, but could be based on the perceived quality of the pick on the common 0-4 scale
(best - worst) from HYPO71 (Lee and Lahr, 1975). A possible, although perhaps arbit-
rary, probability distribution for arrival-time pick accuracy is a Gaussian distribution around
the arrival-time pick. However, the method described below is independent of the form of
distribution chosen.
For a Gaussian distribution around the arrival-time pick,  , the PDF of the arrival time,
t, given a standard deviation  , is given by
p (t | ,  ) =
22
 (t)
22 . (5.2)
The arrival-time standard deviation,  , can be set either from mapping the pick qualities
(0-4) or from a detection PDF as discussed in Section 5.2.
The PDF for a polarity arrival is therefore given by the product of the amplitude probab-
ilities, the time probabilities and the polarities. The PDF for positive and negative polarites
p (Y = + | t, , m,  ) =
1 + erf
pol (t) (t)
p (t | ,  ) , (5.3)
p (Y =  | t, , m,  ) =
1 + erf
 pol (t) (t)
p (t | ,  ) . (5.4)
If a Gaussian arrival-time PDF (Eq. 5.2) is used, the time dependence can bemarginalised
over analytically by integrating over the entire waveform. However, it is sufficient to take large
limits compared to the width of the arrival-time PDF ( tmin and tmax) where the probabilities
are approximately zero:
p (Y = +|data) =
 tmax
1 + erf
pol (t) (t)
22
 (t)
22 dt. (5.5)
The marginalised probabilities are normalised if the arrival-time PDF (Eq. 5.2) is nor-
malised. The sum of the amplitude probabilities are time independent, because the error
function (erf) is anti-symmetric. For a Gaussian arrival-time PDF the two probabilities sum
to unity:
p (Y = +|data) + p (Y = |data) =
 tmax
2 + erf
pol (t) (t)
+ erf
pol (t) (t)
p (t | ) dt
 tmax
p (t | ) dt,
p (Y = +|data) + p (Y = |data)  1. (5.6)
When the uncertainty in the Gaussian arrival-time PDF (Eq. 5.2) increases, more station-
ary points on the waveform have non-zero pick time probability, and the arrival-time PDF
is flattened. Fig. 5.2 shows that, when the arrival-time pick uncertainty and noise level are
increased, the probabilities for both polarities for a synthetic arrival tend towards 0.5.
Figure 5.2: Distribution of the positive (red) and negative (blue) polarity probabilities for a simple
synthetic trace with added random Gaussian noise. The original waveform was a simple source with
upwards polarity. The time pick position was not changed, however the time uncertainty was
increased as the noise level increased. The dashed line indicates a 50% probability of positive or
negative arrival, corresponding to no net information, and the solid lines indicate a smoothed mean
of the positive and negative polarity probabilities.
This approach is independent of the choice of time PDF. When using automated picking,
the PDF can be chosen based on both the automated picker used and any observed shifting
produced in a manual review of the picks (Section 5.2). A suitable arrival-time PDF should
cover the onset of the arrival well, rather than the whole arrival to reduce the number of
stationary points with significant arrival-time probabilities.
Fig. 5.1 shows a histogram of P and S arrival time shifts for the coalescence microseismic
mapping (CMM) auto-picker (Drew et al., 2005; Drew, 2010) for all pick weights. The mean
shift is non-zero, likely due to poor-quality picks that are improved manually and the CMM
tendency to pick on the peak rather than the onset. Consequently, the choice of a Gaussian
probability around the CMM pick is not a poor one, although the mean could be chosen to
be a small time shift, t, before the automatic pick, as there are more picks that are picked
earlier manually:
p (t | ) =
22
 (t+t)
22 . (5.7)
Fig. 5.3 shows the different steps for evaluating the polarity probabilities for an ex-
ample synthetic arrival with white noise. The calculated probabilities for the pick are:
p (Y = +|data) = 0.33; and p (Y = |data) = 0.67.
Pick(a)
ty (b)
ty (c)
1.8 1.9 2 2.1
Time (s)
Figure 5.3: Plot of trace and the associated
PDFs for the different stages. (a) shows the
waveform and (b) shows the amplitude PDFs
for positive (red) and negative (blue)
polarities having accounted for noise. (c)
shows the time probability function and (d)
shows the combined PDFs for positive (red)
and negative (blue) polarities superimposed
on the waveform (black). The calculated
probabilities for the pick are:
p (Y = +|data) = 0.33; and
p (Y = |data) = 0.67.
0.5 0.6 0.7 0.8 0.9
Prior Probability
Figure 5.4: Polarity probabilities for the
trace from Fig. 5.3 for different prior
probabilities for the manual pick. The red
line corresponds to the positive polarity
probability, and the blue is the negative
polarity probability. The solid lines are when
the manual polarity pick is in that direction,
and the dashed lines correspond to when the
manual polarity pick is in the opposite
direction. The grey line shows the equivalent
manual mispick probability set equal to the
prior probability.
5.1.2 Manual and Automated Picking
This probabilistic approach produces an estimate of the likelihood of the polarity direction,
which can be combined with manual picking by using the manual observations as a prior
probability for the automated measurements. The choice of prior probability for the polarity
can have a large effect (Fig. 5.4). If the manual prior is large, the effect of the polarity
probabilities is negligible, although as it is reduced to the null prior (pprior = 0.5) the effects
become more significant.
The prior has a strong effect, dominating the probabilities even for the incorrect direction,
but there is a clear difference in Fig. 5.4 between the correct (negative) and incorrect
(positive) prior directions, with a much sharper trend towards a value of 1 for the incorrect
prior direction. Consequently, even if the prior probability is large and in the incorrect
direction, the resultant polarity probability for the correct direction will be larger than the
corresponding prior probability value, and the probability is corrected towards the true value.
5.2 Integration With Automated Monitoring
ty (b)
ty (c)
2 2.2 2.4
0.05
Time (s)
1 (b)
2 2.2 2.4
Time (s)
Figure 5.5: Plot of the trace and associated PDFs for the different stages, same type of plot as Fig.
5.3. The left column shows the STA/LTA detection function used as the arrival time PDF, while the
right shows the Gaussian approximation generated from Drew et al. (2013). The STA/LTA
parameters chosen were based on those described in Drew (2010, Chapter 5, p. 78), here the STA
window size is one period of the signal. In both examples, the calculated probabilities for the pick
are: p (Y = +|data) = 0.45; and p (Y = |data) = 0.55.
The fast measurement speed allows for this polarity picking to be integrated into an automated
processingworkflow. This polarity information, in conjunctionwith othermeasurements such
as amplitude ratios or otherwise, can produce an estimate of the event source, allowing for
better data quality control from observations and helping to flag interesting events in near real
time. The accuracy of such an approach strongly depends on the accuracy of the arrival-time
pick. As the arrival-time error is increased, the polarity probabilities will tend towards 0.5
(Fig. 5.2). Consequently, as long as the automated time picking is accurate, the polarity
probabilities produced should show good consistency, and althoughmanual refinement would
still improve the result, the output should improve the source constraints.
The arrival time PDF depends on the approach used to pick the arrival. The CMM event-
detection algorithm (Drew et al., 2013) uses STA/LTA to estimate the arrival time PDF, which
could be used as the arrival time PDF in Eqs 5.3 and 5.4. In CMM, the detection function
is fitted with a Gaussian to produce an uncertainty estimate. However, Fig. 5.5 shows that
using the STA/LTA function (Drew et al., 2013, eq. 1) or its Gaussian approximation often
produces wide arrival probabilities, encompassing most of the arrival rather than just the first
motion. Furthermore, this detection function does not always peak at the onset, and leads to
increased uncertainty in the pick time, and therefore less clear polarity probabilities. Neither
the Gaussian approximation nor the plain STA/LTA display much difference, providing little
constraint on the polarity. Nevertheless, the maximum probability is in the negative direction,
which is consistent with the arrival.
1.5 2 2.5
Trace Time (s)
1.5 2 2.5
Trace Time (s)
Figure 5.6: Example arrival time PDF constructed from different STA/LTA trigger functions. The
onset function is shown in blue, and the example P-wave arrival is shown in black. The first plot
shows the recursive STA/LTA function from ObsPy (Beyreuther et al., 2010; Megies et al., 2011)
The second plot shows the arrival time PDF given by Eq. 5.8 peaked in the middle of the onset. The
third and fourth plots show the truncated CMM STA/LTA detection function and the approximated
Gaussian. The final two plots show the RPA/LPA approach of Zahradnk et al. (2014) and the
approximate Gaussian fit using the same approach as for the CMM STA/LTA function.
Baer and Kradolfer (1987) introduced the concept of phase detectors and phase
pickers. Phase detectors are relatively imprecise, and will be improved by human re-picking.
However, phase pickers should produce results that are comparable to those picked manually.
Consequently, it may be better to use a phase picker to more accurately estimate the onset and
construct the arrival-time PDF. There are many approaches to accurate onset pickers such as
the methods discussed by Baer and Kradolfer (1987), STA/LTA pickers (Allen, 1978, 1982;
Trnkoczy, 2011), auto-regressive pickers (Takanami and Kitagawa, 1988, 1991; Nakamura,
2004), using stationary analysis (Nakamula et al., 2007) and kurtosis based pickers (Ross
and Ben-Zion, 2014; Hibert et al., 2014), right to left pick averaging (RPA/LPA) (Zahradnk
et al., 2014), as well as many others. Withers et al. (1998) provide an overview of several
of these different approaches, as do Di Stefano et al. (2006). Several of the STA/LTA-
based approaches are shown in Fig. 5.6. Determining the parameters for these automated
pickers is not always straightforward, and often must be adjusted based on the general signal
characteristics (Trnkoczy, 2011; Zahradnk et al., 2014).
Many of these approaches produce characteristic functions that have a sharp increase at
the onset (e.g. Ross and Ben-Zion, 2014, fig. 3). Consequently a possible definition of the
arrival time PDF could be a Gaussian using the size and onset of the peak in the characteristic
function, FC, as an indicator of the uncertainty such as:
max  min
FC (t = max) FC (t = min)
, (5.8)
where max is the time of the maximum in the peak, and min is the time of the previous
minimum.
Figure 5.7: Example Polarity PDF plot, the
positive polarity probabilities are above the
axis, and the negative below. The darker the
colour, the better the manual pick quality.
The solid bars correspond to manual polarity
picks, and cross-hatched bars directions
without a manually picked polarity. STA1
has p (Y = |data) = 0.87 and a quality of
0 on the HYPO71 0-4 (best-worst) scale (Lee
and Lahr, 1975), along with a manually
observed negative polarity. STA2 has
p (Y = +|data) = 0.98, a quality of 1 and a
manually observed positive polarity, STA3
has a quality of 2 and no manually observed
polarity, STA4 has a quality of 3 and no
manually observed polarity and STA5 has a
quality of 4 and no manually observed
polarity.
Fig. 5.6 shows an example of this time PDF
for a recursive STA/LTA pick. This PDF encom-
passes most of the clear first arrival, rather than
the whole of the arrival phase. However, both
the CMM based STA/LTA and the RPA/LPA ap-
proach are not as good at resolving the arrival as
the recursive STA/LTA method from Obspy.
5.3 Examples
5.3.1 Polarity PDF Plot
To present both the manual polarity picks and the
polarity probability requires a new type of plot
(Fig. 5.7).
The positive polarity probabilities are repres-
ented as bars above the axis, and the negative
below, the saturation of the shading corresponds
to the manual pick quality. Solid bars correspond
to manual polarity picks, and cross-hatched bars correspond to polarity directions with no
manual polarity picks. This plot allows an easy comparison between the manual polarity
picks and the results from this approach.
5.3.2 Synthetic Examples
The behaviour of the method for different noise levels and time picks was tested using
synthetic arrivals generated by finite-difference modelling (Bernth and Chapman, 2011).
Fig. 5.8 shows the effects of varying the time pick on a trace with random Gaussian noise
added to give a power signal-to-noise ratio (SNR) of 10. The arrival-time uncertainty was
left to the initial value ( = 0.01s = 1 sample), meaning that even small variations in the
trace could cause large changes in the perceived probability. For this example, the probability
of a negative arrival varied between 0.39 and 0.79, some of which are not consistent with the
observed negative polarity of the arrivalwithout noise. However, the inconsistent probabilities
occur due to the arrival-time picks being close to the positive part of the arrival, and later than
the true pick position. This indicates the importance of accurate arrival time PDFs, either as
a Gaussian around the arrival-time pick or using a detection function.
1.8 1.9 2 2.1
0.5 Pick(a)
P(Y=+|d) = 0.30 
P(Y=|d) = 0.70 
0.5 Pick(b)
P(Y=+|d) = 0.30 
P(Y=|d) = 0.70 
1.8 1.9 2 2.1
0.5 Pick(c)
P(Y=+|d) = 0.61 
P(Y=|d) = 0.39 
Pick(d)
P(Y=+|d) = 0.52 
P(Y=|d) = 0.48 
Pick(e)
P(Y=+|d) = 0.21 
P(Y=|d) = 0.79 
1.8 1.9 2 2.1
Pick(f)
P(Y=+|d) = 0.31 
P(Y=|d) = 0.69 
Time (s)
Figure 5.8: PDF plots for varying time picks on the same waveform.The original pick is shown in (a),
with randomly varied time picks in (b-f). The waveform is shown in black, with the positive polarity
PDF in red and the negative polarity PDF in blue. The green line indicates the arrival time pick.
The background noise levels for a given pick time were varied using two different noise
models: Gaussian and boxcar (Fig. 5.9). At low noise levels, the accuracy is good, but as
the noise level approaches that of the signal, it can have very large effects on the observed
waveform and polarity. There is a small dependence on the noise model, with mainly lower
positive polarity probabilities for the boxcar noise. However, the values are still consistent
with those from the Gaussian noise model, suggesting that it is not a bad approximation. In
practice the traces with low SNR (Fig. 5.9 (d), (e) and (f)) would probably be considered
difficult to pick and therefore be assigned a larger time pick error.
These examples also demonstrate why an arrival-time PDF with some shift (Eq. 5.7) may
be better, as the arrival-time pick positions in Figs 5.8 and 5.9 are close to the onset rather
than the first peak. Accordingly, the first motions are more likely to be after the pick time
rather than equally spaced before and after.
As shown in these examples, the approach is robust and can provide a qualitative value on
the probability of the polarity being up or down, but the probabilities are inherently dependent
on the accuracy of the time pick and the noise levels of the trace. Traces with a high SNR
should produce a reliable result, but as the time pick uncertainty gets large, the polarity
probability tends to 0.5 (Fig. 5.2).
1.5 1.6 1.7 1.8
Gaussian
Pick(a)
P(Y=+|d) = 0.94 
P(Y=|d) = 0.06 
1.5 1.6 1.7 1.8
Boxcar
Pick(a)
P(Y=+|d) = 0.94 
P(Y=|d) = 0.06 
1.5 1.6 1.7 1.8
0.5 Pick(b)
P(Y=+|d) = 0.91 
P(Y=|d) = 0.09 
1.5 1.6 1.7 1.8
Pick(b)
P(Y=+|d) = 0.63 
P(Y=|d) = 0.37 
1.5 1.6 1.7 1.8
0.5 Pick(c)
P(Y=+|d) = 0.90 
P(Y=|d) = 0.10 
1.5 1.6 1.7 1.8
Pick(c)
P(Y=+|d) = 0.61 
P(Y=|d) = 0.39 
1.5 1.6 1.7 1.8
0.5 Pick(d)
P(Y=+|d) = 0.91 
P(Y=|d) = 0.09 
1.5 1.6 1.7 1.8
Pick(d)
P(Y=+|d) = 0.68 
P(Y=|d) = 0.32 
1.5 1.6 1.7 1.8
Pick(e)
P(Y=+|d) = 0.67 
P(Y=|d) = 0.33 
1.5 1.6 1.7 1.8
0.5 Pick(e)
P(Y=+|d) = 0.80 
P(Y=|d) = 0.20 
1.5 1.6 1.7 1.8
0.5 Pick(f)
P(Y=+|d) = 0.69 
P(Y=|d) = 0.31 
1.5 1.6 1.7 1.8
Pick(f)
P(Y=+|d) = 0.71 
P(Y=|d) = 0.29 
Time (s)
Figure 5.9: PDF plots for varying Gaussian (left column) and boxcar (right column) noise levels for
the same waveform. The noise levels are: (a) No Noise, (b) SNR=10, (c) SNR=5, (d) SNR=3, (e)
SNR=2 (f) SNR=1.2. See also Fig. 5.8.
Fig. 5.10 shows that the automated approach usually agrees with the manually observed
polarities, especially in the low noise cases. However, as the noise levels increase, the
solutions occasionally disagree with the observed polarities, although this is expected in the
low SNR examples.
Figure 5.10: Polarity PDF plots (Fig. 5.7) of synthetic events for different numbers of stations and
different noise environments, with manually picked arrival times. The first plot has an SNR around
5, while the second has an SNR around 7 and the remaining plot has an SNR around 10.
Figure 5.11: Polarity PDF plot (Fig. 5.7) of solutions from SH phase picking of a synthetic event
with, with manually picked arrival times and a signal to noise ratio of approximately 5.
This approach can also be used when evaluating S-wave polarities, as these require
rotation into the correct ray orientation to measure them. Consequently, they cannot easily
be estimated without an estimate of the hypocentre, unlike the P-wave polarities, so require
re-picking manually after an event has been located. Fig. 5.11 shows an example of SH
measurements from a synthetic event with an amplitude SNR of 5. The SH polarities were
manually picked after the location was estimated by rotating into the ZRT (vertical, radial and
transverse) components. The polarity probabilities show good consistency with the manually
picked polarities.
Figure 5.12: Polarity PDF plot (Fig. 5.7) for the earthquake shown in Table 5.1.
Station Time Pick Quality Manual Polarity p (Y = +|data) p (Y = |data)
ADA Good + 0.94 0.06
BRU Poor + 0.51 0.49
DDAL Poor 0.50 0.50
DYNG Poor  0.50 0.50
FREF Good  0.19 0.81
HELI Poor 0.50 0.50
HERD Good + 0.70 0.30
HETO Good + 0.90 0.10
HOTT Good  0.30 0.70
HRUT Good + 0.99 0.01
HVA Good + 0.94 0.06
JOAF Poor 0.50 0.40
KOLL Good + 0.65 0.35
KRE Good  0.06 0.94
LOKA Good + 0.94 0.06
MIDF Good  0.30 0.70
MKO Good  0.06 0.94
MOFO Good  0.10 0.90
MYVO Good + 0.70 0.30
RODG Good  0.30 0.70
SVAD Good + 0.90 0.10
UTYR Good  0.09 0.91
VADA Good  0.30 0.70
VIBR Good + 0.94 0.06
VIKR Good 0.69 0.31
VISA Poor 0.50 0.50
VSH Good + 0.94 0.06
Table 5.1: Comparison of Automated and Manual Polarity Picks for 2007/7/6 20:47 Upptyppingar
earthquake (White et al., 2011). Missing manual polarities are unpicked. The arrival-time picks were
made manually, with the qualities assigned manually as good or poor with associated time pick
errors of 0.01s and 0.5s corresponding to 0 (good) and 3 (poor) from the HYPO71 pick weighting
(Lee and Lahr, 1975).
5.3.3 Real Data
Table 5.1 and Fig. 5.12 show a comparison of the automated and manual polarity picks for
one event from the July 2007 swarm beneath Upptyppingar in Iceland (White et al., 2011).
6.4 6.5 6.6 6.7 6.8
0.5  Pick
Trace elapsed time (s)
Figure 5.13: Example polarity PDFs
and waveform (grey) for station KRE
from the earthquake shown in Table 5.1.
The positive polarity probability is
shown in red and the negative in blue.
The manual arrival-time pick is shown
by the green line.
The manual observations and automatically estimated
solutions are consistent with the good quality time
picks producing probabilities bigger than 0.7 and of-
ten bigger than 0.9. The poor picks with large time
uncertainty show that the resulting probabilities tend
to 0.5 each as mentioned in Section 5.1.
Fig. 5.13 shows an example polarity PDF for
one of the stations. The arrival has a clear negative
polarity, which is reflected in the associated polarity
PDFs.
The results from two example events from theKra-
fla volcano in the Northern Volcanic Zone of Iceland
are shown in Fig. 5.14 . For the most part, the automated polarity probabilities agree with the
manual polarities, although there a few cases which disagree. Fig. 5.16 shows that arrival at
K030 in the first event may be picked a little to early, as there is a small positive peak before
the large trough. For the second event, the two stations both seem to have incorrect pick
times combined with emergent arrivals, making it difficult to manually estimate the arrival
polarity.
Figure 5.14: Polarity PDF plots (Fig. 5.7) for two example events from the Krafla central volcano in
the Northern Volcanic Zone of Iceland, with manually picked arrival times.
Figure 5.15: Polarity PDF plots (Fig. 5.7) for example events from the Krafla central volcano in the
Northern Volcanic Zone of Iceland, with manually picked arrival times.
11 11.2 11.4 11.6
Trace elapsed time (s)
K030  Pick
11 11.2 11.4 11.6
Trace elapsed time (s)
K030  Pick
12.2 12.4 12.6 12.8
Trace elapsed time (s)
K230  Pick
Figure 5.16: Waveforms for contrasting manual and automated polarity picks from Fig. 5.14. The
left plot corresponds to the first event, and the remaining two the second. The waveforms are aligned
on the manual P-wave pick, and the manually chosen polarity pick is shown as a black arrow. The
positive and negative polarity probabilities are plotted in red and blue respectively.
The results of the automated polarity picking for severalmore events from theKrafla region
of Iceland are shown in Fig. 5.15. For the most part, the automated polarity probabilities
agreewith themanual polarities. In the few caseswhich disagree, there is usually a slight error
in the alignment of the arrival-time PDF similar to Fig. 5.16. The strong agreement of the
observations suggests that this approach works well with real data and not just synthetically
generated events.
5.4 Time benchmarking
0.65 0.7 0.75 0.8 0.85 0.9
Time per seismogram (s)
Figure 5.17: Time benchmarking for different noise levels and time PDFs. Each row corresponds to
a different SNR, from top to bottom: 10, 7, 5 and 2. The blue histogram corresponds to the Gaussian
time window around the manual pick, the red histograms are the STA/LTA detection function
windowed around the manual time pick, and the green histograms are the Gaussian fit to the
STA/LTA detection.
The time taken to run the algorithm is independent of the noise level, as shown in Fig.
5.17. The more complex time PDFs require a slightly longer time to calculate them. These
times were calculated on a single processor using unoptimised Python code, and it is likely
that optimising the code and using a lower-level language could dramatically speed up the
approach.
5.5 Source Inversion
TheBayesian approach discussed in Chapter 4 can easily be extended to include the automated
polarity observations. The PDF for the observed polarity at a given time must be considered
with respect to the source:
p (Y (t) = pol (t) |Ai,  (t) ,  (t)) = H
pol (t)
 (t)
 (t)
. (5.9)
This is dependent on the theoretical amplitude, Ai, the measurement uncertainty,  (t), and
the observed amplitude change,  (t) > 0.
Given that the standard deviation of the background noise, , can be estimated, the
simplest noise model for the uncertainty on  (t),  (t) is a Gaussian distribution, with
standard deviation
2 due to  (t) corresponding to the amplitude change. Marginalising
with respect to the noise model gives
p (Y (t) = pol (t) |Ai,  (t) , ) =
pol (t)
Ai + (t)(t) |Ai|
(t)2
2s d (t) . (5.10)
Following the same approach as in Section 4.2.1, the marginalised PDF is
p (Y (t) = pol (t) |Ai,  (t) , ) =
1 + erf
pol (t) (t) sgn (Ai)
, (5.11)
although this is not marginalised with respect to time.
Eq. 5.11 can be simplified, because the signum function in the PDF is equivalent to
writing the PDF using the Heaviside step function H (x), giving
p (Y (t) = pol (t) |Ai,  (t) , ) = H (Ai)
1 + erf
pol (t) (t)
H (Ai)
1 + erf
 pol (t) (t)
. (5.12)
The modelled amplitude is independent of the time. Consequently, the time marginalised
PDF is given by
p (i|Ai, , ) = H (Ai)i + H (Ai) (1 i) , (5.13)
where i is the time marginalised positive polarity PDF for the arrival (Eq. 5.5).
As with the manually estimated polarity PDF (Eq. 4.9), it is possible that there could be
a receiver orientation error with probability i, leading to a flipped polarity so Eq. 5.13 can
be extended to include this:
p (i|Ai, , , i) = 1 i + (2i  1) [H (Ai)i  2H (Ai)i] . (5.14)
This is the polarity probabilities likelihood, equivalent to Eq. 4.10, and can be included
in the Bayesian source inversion of Chapter 4, by substituting into the source likelihood:
p (d |M ,  ,k) =
i |Aij = aj  M ,i, i, i
Ri = ri |Aij = aj  M ,i, i
p () p () dd, (5.15)
p (d |M ,  ,k) =
i |Aijk = ajk  M ,i, i, i
Ri = ri |Aijk = ajk  M ,i, i
p () p () dd. (5.16)
Care must be taken with the data independence, and the caveats of section 4.3 still apply.
Automated polarity observations must not be used in combination with manual polarity
observations in the inversion, because this artificially sharpens the resultant PDF.
The quantitative method for including the noise in the polarity estimation is unlike the
approaches for manual polarity observations such as that proposed in section 4.2.1 (Brillinger
et al., 1980; Walsh et al., 2009).
The source inversion results for a synthetic double-couple event and a real event from the
Upptyppingar dyke swarm in 2007 (White et al., 2011) were evaluated using the Bayesian
approach from Chapter 4, adapted for automated results (Eqs 4.53 and 4.54).
The results for the source inversion using automated picking resemble those of the manual
picking, although there is usually a wider range of possible solutions, since most of the
proposed solutions have at least a small non-zero probability. In some cases, the solution
can be improved by additional constraint from receivers with no manual polarity pick but
a suitable arrival time pick, as shown by the example from the Upptyppingar dyke swarm,
which has a few receivers without manual polarity picks but with a suitable arrival-time
pick to estimate the polarity probabilities. The double-couple solutions from the automated
polarity inversion tend to have a clearer demarcation between the high-probability solutions
and the lower-probability solutions, as can be seen in Figs 5.18 and 5.19.
Figure 5.18: Lower hemisphere equal area fault-plane plots and marginalised full moment tensor
lune plots showing the comparison of automated polarity and manual polarity source inversions for
two events, a synthetic event (A) and the event from table 5.1 and Fig. 5.12 (B). The first two
columns show the the double-couple constrained and full moment tensor PDFs for the automated
polarity probabilities and the second two show the solutions for the manual polarity observations.
The fault plane plots show the most likely solutions with the darkest lines. The stations are indicated
by circles if there is no manual polarity information used (such as in the polarity probability based
inversions), and upwards red triangles or downwards blue triangles depending on the manually
observed polarity. The lune plots are normalised and marginalised to show the most likely source
type, red regions correspond to high probability and blue low probability.
Acommon approach to dealingwith polarity uncertainties is to allow a blanket probability
of a pick being incorrect (pmispick) (Hardebeck and Shearer 2002, 2003). This is equivalent
to setting the value of i in Eq. 5.14 to either 1  pmispick or pmispick depending on whether
the manual pick is positive or negative. Unlike the automated polarity approach, including an
arbitrary blanket probability of a mistaken pick does not account for the arrivals that are most
difficult to pick being most likely to be incorrect. Consequently the range of solutions is often
not well constrained. The double-couple solutions from the automated polarity in Fig. 5.19
show a stronger demarcation between the low and high probability solutions than those using
a blanket probability of a mistaken pick, although the ranges are similar. The full moment
tensor solutions in Fig. 5.19 have very similar distributions for both of the inversions, with
poorer constraint than the ordinary manual polarity solutions from Fig. 5.18.
Figure 5.19: Lower hemisphere equal area fault-plane plots and marginalised full moment tensor
lune plots showing the comparison of results from automated polarity (first two columns) and
arbitrary probability of a mistaken pick (pmispick = 0.1) (second two columns) for the synthetic (A)
and real (B) events shown in Fig. 5.18. See also Fig. 5.18.
Figure 5.20: Lower hemisphere equal area fault-plane plots and marginalised full moment tensor
lune plots showing the comparison of results from automated polarity using a Gaussian time PDF
around the manual time pick (first two columns) and STA/LTA time picking (second two columns)
for the synthetic (A) and real (B) events shown in Fig. 5.18. See also Fig. 5.18..
Although the choice of time PDF is independent of the approach, the CMM STA/LTA
(Fig. 5.5) was tested as a possible PDF. Fig. 5.20 shows that this arrival-time PDF can
work in low-noise environments, but in a higher-noise environment it may not be possible to
estimate a solution. This is clear in the solutions for the Upptyppingar event, which have no
constraint on the possible source for both the double-couple and full moment tensor inversions
because the variations in probability are too low. The low noise synthetic example shows
good agreement with the east-west plane, but the north-south plane is less well constrained.
However, the approach may provide some constraint in the high noise case if there are enough
receivers sampling the focal sphere. Using an arrival time PDF from a well calibrated onset
picker (Section 5.2) would be a much larger improvement in the source constraints.
Figure 5.21: Lower hemisphere equal area fault-plane plots and marginalised full moment tensor
lune plots showing the comparison of results from automated polarity using a manual prior of 0.85
(first two columns) and manual polarity (second two columns) for the synthetic (A) and real (B)
events shown in Fig. 5.18. See also Fig. 5.18.
Using manual polarity picks as a prior probability further constrains the source PDF,
leading to a sharper solution than the manual polarities (Fig. 5.21). The full moment tensor
solutions are also more constrained by the prior, compared to the equivalent solutions in Fig.
5.18.
5.6 Summary and Discussion
The Bayesian approach to automated polarity estimation proposed here allows a quantitative
estimate of the measurement uncertainties. This method surmounts the inconsistencies in
manual polarity picking, as well as producing a quantitative estimate of the uncertainty on
the source.
The polarity probabilities have a clear dependence on the time pick accuracy and the noise
level of the trace, requiring an accurate arrival time pick approach. When an automated arrival
picking approach is used, it should produce accurate onset picks, with results comparable to a
human, otherwise the resultant PDF is usually too large. However, many of these automated
pickers require parameters that often that are dependent on the arrival characteristics, such
as the frequency and onset characteristics, to produce accurate picks, requiring human input
to correctly set the values.
The choice of arrival time PDF can be adjusted depending on the perceived quality of
the arrival time picking approach. The arrival time PDF can also be adjusted using the pick
quality estimate, such as the pick weight (0-4 range from HYPO71 (Lee and Lahr, 1975)),
although a range with higher discretisation would prove more accurate.
There are fewdifferences between the results obtained frommanual and automated picking
for the events from the Upptyppingar and Krafla volcanic systems in Iceland. The estimation
of the probability of correct first motions produces a quantitative estimate of the uncertainty
of the polarities which carries through to the calculation of the resulting source mechanism
PDF. In the cases where the automated and manual picks seem to disagree, some of that can
be attributed to human error in the arrival time and manual polarity estimation.
The time required for calculating the PDFs is less than one second per arrival, which is
much shorter than that required for manual polarity picking. Consequently, this approach
adds little time to an automated processing work-flow, and can easily be included into near
real-time event detection, unlike the much slower manual polarity picking. Additionally,
the approach is useful for estimating polarities of phases measured on location-dependent
seismogram components, such as the SH phase. These are often ignored in source inversion
due to the requirement to return and pick the polarities after the event has been located.
Polarity probabilities can be incorporated into the source inversion approach of Chapter
4, producing results that are similar to inversions from the manual polarity picks. This
quantitative approach for estimating and including the uncertainties in the source inversion
is better than the qualitative approach for manual polarity observation described in Chapter
4, as well as the inclusion of an arbitrary probability of a mistaken pick.
The results from this method will never be better than those from manual polarities with
no uncertainties, as the manual polarities have massively underestimated the uncertainties on
the measurements, so produce artificially sharp results.
6 The Effects of Uncertainties on Source
Inversion
Source inversion is usually carried out as the final part of a processing workflow involving
detecting, picking and locating the events, and at each step in the chain uncertainties are
introduced and assumptions made.
The uncertainties in the steps preceding the source inversion have been investigated in
the literature (e.g. Lomax and Michelini, 2001; Husen and Hardebeck, 2010; Drew et al.,
2013), and will only be reviewed in this chapter, which is focussed on the effects of different
uncertainties on the resultant source PDF, using synthetic tests controlled for different un-
certainties. The synthetic data are generated using a finite difference approach (Bernth and
Chapman, 2011), for different source types and station distributions, and the posterior model
probabilities (Section 4.2.6) are calculated for the constrained and unconstrained solutions.
6.1 Event Detection and Arrival Picking
Event detectors are used to decide whether a signal is due to an earthquake or not. This can
be easy for larger seismic events, which can sometimes even be detected in space (Garcia
et al., 2013). However, microseismic events are often small in magnitude and frequent,
making manual detection difficult. Additionally, microseismic events are often separated
by only a few seconds, which complicates the assignment of arrivals to individual events.
Consequently, automatedmonitoring approaches, such as those used in early-warning systems
or microseismic monitoring, often try to locate an event as part of the detection step.
Picking the arrival times is sensitive to the random background noise at the station. The
noise may be larger than the first arrival, leading to emergent rather than impulsive arrival
characteristics, which can prove difficult for automated approaches, increasing the arrival
time uncertainty. Arrival picking can occur concurrently with event detection, but the picks
can be refined, either manually, helping to account for false event detection, or automatically
using a more complex picking algorithm. Section 5.2 introduces several of these automated
approaches. Many of the approaches in common usage essentially compare the time evolution
of the signal and whether it differs from the long-term noise environment in such a way that
could be classed as an event.
Modern seismograms are mostly digital, so some interpolation is done between the points
to produce the observed waveform. However, sample rates are usually sufficiently large that
the associated uncertainties from the digitisation are small, compared with the arrival time
uncertainty.
Classifying the arrival phase often uses known characteristics to reduce the uncertainty,
such as the expected orientation of the arrival phases. The phase of the arrival can also
be estimated using polarisation analysis (Baillard et al., 2014; Ross and Ben-Zion, 2014),
requiring a prior estimate of the expected polarisation directions for the P and S phases. This
estimate can be verified after determining the hypocentre location.
The detection step itself does not introduce many uncertainties into the source inversion
workflow, apart from the risk of a false event detection. If the detection approaches are used
to pick the arrival and locate the event, there are corresponding uncertainties for these steps,
as discussed below.
Arrival picking is also when manual or automated polarity determination is carried out.
This depends on an accurate arrival time pick and phase classification to correctly identify the
onset and first arrival polarity. Many automated workflows do not measure arrival polarities
at all, due to the difficulties in determining it, but Chapter 5 describes a method to include
this in an automatic workflow.
6.2 Hypocentre Determination
The phase arrival times are used to estimate the hypocentre of the seismic event. The location
is combined with the observed characteristics of the arrivals (polarity, amplitude etc.) to
estimate the source parameters. The accuracy of the event location also depends on the
network coverage, requiring good constraint to estimate the hypocentre. There is often a
trade-off between estimating the earthquake time and the location. Uncertainties in the phase
arrival times (Section 6.1) can have a large effect on the hypocentre uncertainties.
Most location approaches use a travel-time look-up table to locate the events, although
the type of search algorithm used to estimate the location varies. The HYPO* approaches,
including Hypo71 (Lee and Lahr, 1975), HYPOELLIPSE (Lahr, 1989), HYPOINVERSE
(Klein, 2002) and HYPOCENTER (Lienert et al., 1986), use iterative linearised approaches
based on that ofGeiger (1912). Theseminimise the linearised travel-time residuals, producing
a single solution and linear estimates of the uncertainties. The results can be poor if there
are multiple possible solutions due to weak constraints or where the complete solution is
irregular (Husen and Hardebeck, 2010).
More modern approaches employ the increase in computer power to search the complete
space. NonLinLoc (Lomax et al., 2000, 2009) uses a grid search approach. CMM (Drew
et al., 2013) follows a similar approach, undertaking a spatio-temporal search for the peak
in the coalescence function. These approaches produce complete location PDFs, but can be
computationally demanding compared to the simpler linearised approaches. Nevertheless,
they produce better estimates of the location uncertainties, as well as allowing the full location
PDF to be sampled, so that the location uncertainty can be included as in section 4.2.5.
Hypocentre determination can be improved using relative relocation, either master event
(Deichmann and Garcia-fernandez, 1992) or double-difference (Waldhauser and Ellsworth,
2000; Waldhauser, 2001). These methods are both linearised, and can dramatically reduce
the relative uncertainty between hypocentre locations, yet the absolute uncertainties are not
necessarily similarly reduced. Menke and Schaff (2004) show that the use of waveform cross-
correlation and differential travel-times in relative relocation can produce accurate absolute
locations. However, the uncertainty in the relative relocation is often estimated from the pick
time differences or the cross-correlation and not the corresponding absolute uncertainties
in the initial hypocentres. Alternately, additional information in the waveform such as the
P-wave coda can be used to estimate the separation between events (Robinson et al., 2011),
using coda wave interferometry (Snieder and Vrijlandt, 2005; Snieder, 2006).
If the relative information could be correctly combined with the location PDF, the loc-
ation uncertainties could be reduced further, but care must be taken to maintain the data
independence, otherwise the results could be artificially sharpened1.
Figure 6.1: Marginalised xy and xz plots of the location PDF from NonLinLoc for different
background noise levels. Red corresponds to high probability and blue to low probability. The
station geometery for this event is shown in Fig. 6.2, and the velocity models used to generate the
synthetic and locate it are given by the red and blue lines in Fig. 6.3.
1A cursory examination suggests that the data-types used in relative relocation are not independent of those
used in the inital location.
Noise can affect the detection and picking of an event by masking an arrival or making
the onset of an arrival emergent rather than impulsive and thus difficult to precisely pick
the arrival. Consequently, the noise level can affect the location PDF, as shown in Fig.
6.1. Increasing the noise level increases the lateral uncertainty, and the vertical uncertainty
appears largest in the SNR = 3 case, which may be due to an improved depth constraint
for the SNR = 2 case, due to the different distributions of receivers that it was possible to
manually pick arrivals for. The station geometry for this synthetic is shown in Fig. 6.2.
5 2.5 0 2.5 5
Eastings (km)
Figure 6.2: Location geometry for the synthetic events shown in Fig. 6.1, the source depth was
3.5 km.
Figure 6.3: Plot of the marginalised location uncertainty of a synthetic event, arising from velocity
model uncertainty. The first pair of plots correspond to a percentage variation of  3%, while the
second correspond to a percentage variation of  10% from the true models. For each pair, the first
plot shows the range of velocity models used, with the red and blue corresponding to the VP and VS
models used to generate the synthetic source. The second plot shows the xy and xz marginalised
location PDFs generated by NonLinLoc, including the model uncertainty. Red indicates high
location probability and blue is low location probability.
All these approaches rely on accurate knowledge of the velocity model, at a suitable
resolution for the data. Both CMM and many of the HYPO* type models use simple one-
dimensional models which will not be accurate for the entire region. Those that can use full
three dimensional models, such as NonLinLoc, can have a trade-off between memory usage
and model resolution.
Velocity model uncertainty is often a large (and frequently overlooked in commonly used
software such as the HYPO* approaches and CMM, although others such as NonLinLoc
dont directly include velocity model uncertainty, but it can easily be included) source of
hypocentre uncertainty. Additionally, the uncertainty on three dimensional models can be
hard to quantify and, therefore, difficult to include in a location inversion. Fig. 6.3 shows
that for well constrained models and low noise levels, the effects are minimal. However, as
the range of models increases, the effect becomes larger.
6.3 Source Inversion
Uncertainties can have a large effect on source inversion results. It is important to understand
how all the different uncertainties interact in the inversion process and their resultant effects.
A set of synthetic seismograms, computed from three source types using a finite difference
approach (Bernth and Chapman, 2011), are used to explore the dependency of the PDF
solutions on different types of uncertainty, for both the fully constrained double-couple space
and the full moment tensor space. The three source types used (Fig. 6.4) are a double-couple
source (event A), an opening tensile crack (event B), and a full moment tensor source (event
C). Random Gaussian noise was added to the traces at varying noise levels.
The synthetic data were manually picked for both P and S arrival times and P polarities,
and then located using NonLinLoc (Lomax et al., 2000, 2009) and a simplified version of
the known velocity model. The arrival time picks were used to automatically window and
measure P, SH, and SV amplitudes.
A simpleMonte Carlo based random search algorithm (Algorithm 8.1) using the posterior
PDFs calculated in Section 4.2.5 was used in both double-couple space and the unconstrained
full moment tensor space to produce samples of the solution PDFs. The chosen prior for each
case was the uniform prior, so no preferred source mechanism, or orientation was specified.
Figure 6.4: Lower hemisphere equal area projections, and lune plots (Section 2.5.6) of the example
sources used for the uncertainty tests. Event A is a double-couple source, event B an opening tensile
crack and event C is a full moment tensor source.
Figure 6.5: Lower hemisphere equal area projections, and marginalised lune plots of the source PDF
for three synthetic sources (Fig. 6.4), inverted using polarities for a range of data uncertainties, given
by the inverse of the SNR  = 0,  = 0.1,  = 1/7,  = 0.2,  = 1/3 and  = 0.5. For each source
(pair of columns) The first column shows the source PDF for the solution constrained to be
double-couple only, and the second column shows the source PDF for the full moment tensor
solution. Manually picked station first motions are given by upward red or downward blue triangles.
For the focal sphere plots, possible fault planes are given by dark lines. The most likely fault planes
are given by the darkest lines. High probability regions are shown in red and low probability in blue
on the lune plot. The positions of the different source types on the lune plot are shown in Fig. 2.21.
The posterior model probabilities for the double-couple source model estimated from the Bayesian
evidence are shown in Table 6.1.
6.3.1 Background Noise
The simplest source of uncertainty is the ambient noise at the receivers. This can affect the
detection and picking of an event by masking an arrival or making the onset of an arrival
emergent rather than impulsive and thus difficult to precisely pick the arrival polarity. The
background noise level affects the uncertainty in the polarity observation (Eq. 4.9) and the
amplitude observations for the amplitude ratio PDF (Eq. 4.35) (Sections 3.2 and 3.3).
Noise has a deep effect on the inversion workflow, as it leads to uncertainty in the arrival
time picks, and therefore uncertainty in the hypocentre location. Consequently, it is difficult
to isolate the effects of the noise on the inversion. Adding uncertainty to the observations in
the source inversion step provides some indication of its effect on the resultant PDF (Figs 6.5
and 6.6).
A B C
SNR i ii i ii i ii
 0.77 1.0 ? ? ? ?
10 0.78 0.01 0 0 0 0
7 0.80 0 0 0 0 0
5 0.84 0 0 0 0 0
3 0.91 0.02 0 0 0.01 0
2 0.94 0.01 0 0 0.09 0
Table 6.1: Posterior DC model probabilities
for the source PDFs shown in Fig. 6.5 and
6.6. Each event is split into two columns for
P polarity only (i), and P polarity and P/SH
and P/SV amplitude ratios (ii). ? indicates
events with no double-couple solutions
found.
The measurement error has little effect on the
polarity-only inversion if the solutions are well
constrained (Fig. 6.5). As the uncertainty level
increases, there is some expansion of the cor-
responding range of solutions and an increase in
differentiation between possible solutions due to
the change in the likelihood function (Fig. 4.1),
but the peak remains close to the true source.
There is some deviation in the full moment tensor
solutions, especially for events B and C, although
event B is affected by the random sampling ap-
proach (Fig. 2.28).
Whilst some double-couple solutions are pos-
sible for event B, these are all very low probabil-
ity, as reflected in the posterior double-couplemodel probabilities (Table 6.1). These posterior
model probabilities agree with the true sources, with values bigger than 0.5 for event A , and
non-zero solutions for event C only at the very high noise level solutions.
Including the amplitude ratio observations improves the constraint on the solutions (Fig.
6.6), with the range of possible solutions expanding much less as the measurement uncertain-
ties increase. Event A shows some rotation away from the true source for the double-couple
constrained model, and a deviation from the double-couple point for the full moment tensor
source for the SNR 6 3 examples. The amplitude ratios can suffer from more deviation
as the noise level increases (Section 3.4), which may cause some of these deviations. The
solutions for event C tend towards the closing tensile crack point, with very low probability
double-couple solutions. The double-couple orientations are controlled by the amplitude
ratios, especially at high noise levels, as is evident when compared to Fig. 6.5, and the
polarity misfits in the solutions for event C.
Event A has very low posterior double-couple model probabilities (Table 6.1), despite
the solutions appearing to be good double-couple solutions from a visual inspection. The
full moment tensor solutions are dominated by a few high probability samples, which can
lead to uncertainties in the Monte Carlo integration approach used to calculate the Bayesian
evidence. Events B and C show an increasing range of solutions, and even with some possible
double-couple solutions, the posterior probability of the double-couple source is 0 for both
events, although these distributions are again dominated by a few high probability samples,
however the possible double-couple solutions are much lower probability.
Figure 6.6: Lower hemisphere equal area projections, and marginalised lune plots of the source PDF
for three synthetic sources (Fig. 6.4), inverted using polarities and amplitude ratios at a range of data
uncertainties. The amplitude ratios uncertainties were equivalent to SNR =, SNR = 10,
SNR = 7, SNR = 5, SNR = 3 and SNR = 2, with the polarity uncertainty given by the inverse of
the SNR. The posterior model probabilities for the double-couple source model estimated from the
Bayesian evidence are shown in Table 6.1. See also Fig. 6.5.
The noise has a larger overall effect than that shown in Figs 6.5 and 6.6, as it affects
the ability to make the arrival pick and estimate the location. Reprocessing these synthetic
events at different noise levels shows that adding noise reduces the number of confident picks,
increasing the range of possible solutions, partially due to the network distribution (Section
6.3.2).
Figs 6.7 and 6.8 show the effect of different noise levels on the entire workflow using
manual arrival picking of the synthetic events.
A B C
SNR i ii i ii i ii
 0.77 0 ? ? ? ?
10 0.71 0.99 ? ? 0.45 0
7 0.71 0.70 ? ? 0.43 0
5 0.77 0.84 0 ? 0.48 0
3 0.64 0.82 ? ? 0.62 0
2 0.62 0.58 0 0 0.59 0.47
Table 6.2: Posterior DC model probabilities for
the source PDFs shown in Figs 6.7 and 6.8. Each
event is split into two columns for P polarity only
(i), and P polarity and P/SH and P/SV amplitude
ratios (ii). ? indicates events with no
double-couple solutions found.
The noise reduces the number of stations
with polarity picks, leading to a much lar-
ger range of solutions for the inversion us-
ing only polarity data (Fig. 6.7). The mo-
ment tensor solutions remain peaked around
the double-couple point for event A, while
the distribution for event C shifts towards
double-couple as the noise increases and the
number of receivers is reduced. The mo-
ment tensor solutions for event B remain
non-double-couple but deviate from the true
source as the noise level increases. The pos-
terior double-couple model probabilities are
negligible for event B, showing that no double-couple solutions can fit the event well. Event
A has significant values for all of the solutions, consistent with true source. Event C has sig-
nificant double-couple posterior values for all the solutions with SNR 6 10, although this is
to be expected because the receiver distribution does not prevent the fit of any double-couple
solutions.
While including amplitude ratios improves the source constraint, there can be deviation
from the true source point, as seen in the several of the solutions for event C. This is
possibly due to the deviation in amplitude ratios as the noise level increases (Section 3.4),
which effectively corresponds to a systematic error in the amplitude measurements, and may
require increasing the uncertainty estimates of the amplitudes from just the noise to properly
incorporate it in the source inversion.
The cases with SNR 6 3 in Fig. 6.8 show poor constraint on the double-couple con-
strained solutions for event A, with rotation of the likely orientation from the true source. The
full moment tensor solutions for event A show that adding noise increases the range of pos-
sible solutions, and can move the maximum probability source away from the double-couple
point. This may explain some reported non-double-couple type sources.
Figure 6.7: Lower hemisphere equal area projections, and marginalised lune plots of the source PDF
for three synthetic sources (Fig. 6.4), inverted at a range of noise levels using P polarities only. The
noise levels were SNR =, SNR = 10, SNR = 7, SNR = 5, SNR = 3 and SNR = 2. The
posterior model probabilities for the double-couple source model estimated from the Bayesian
evidence are shown in Table 6.2. See also Fig. 6.5.
The posterior double-couple model probabilities (Table 6.2) for the SNR =  example for
event A is a poor estimate due to the domination of the full moment tensor PDF by a few high
probability samples. The other double-couple examples have posterior probabilities that are
more consistent with the expected values. The probabilities for event B are negligible. Event
C has non-negligible posterior values for the double-couple source when SNR = 2, which is
understandable given the close similarity of the radiation pattern to a double-couple, and the
reduced number of receivers.
Figure 6.8: Lower hemisphere equal area projections, and marginalised lune plots of the source PDF
for three synthetic sources (Fig. 6.4), inverted at a range of noise levels using P polarities and P/SH
and P/SV amplitude ratios. The noise levels are SNR =, SNR = 10, SNR = 7, SNR = 5,
SNR = 3 and SNR = 2. The posterior model probabilities for the double-couple source model
estimated from the Bayesian evidence are shown in Table 6.2. See also Fig. 6.5.
There is some variation in the orientation between the double-couple and moment tensor
solutions (Fig. 6.9), although the general characteristics are consistent between the two.
However, in most cases there are additional peaks in the moment tensor orientations, which
are not present in the double-couple solutions. These results suggest that the full moment
tensor solutions often have a well-constrained orientation, with deviations in the source type
corresponding to the improved fit of the data due to the extra free parameters, rather than any
large trade-off between orientation and source type.
Event A
Event B
0  2
Event C
0 0.5 1
/2 0 /2
0  2
0 0.5 1
/2 0 /2
DC MT
Figure 6.9: Marginalised distribution of the Tape orientation parameters (strike, dip cosine, and
rake) for the SNR = 2 examples from Fig. 6.8. The vertical axis scales are independent, as the
distributions are normalised. The red lines show the values for the synthetic sources (Fig. 6.4).
The deviation and sparseness in some of these solutions arises because the P/SH and
P/SV amplitude ratio solutions often occupy different regions of the source plot due to the
measurement errors (Fig. 6.10). Consequently, the combined distribution can be multimodal,
which is an explanation for why some of the distributions using both the amplitude ratio data-
types can be multimodal (e.g. Figs 6.6 and 6.8). Nevertheless some of the solutions are
multimodal without this effect, such as the SNR = 2 example for P polarities and P/SH
amplitude ratios in Fig. 6.10.
Fig. 6.10 suggests that care must be taken when incorporating amplitude ratios in an
inversion, and they should only be used if consistent and well measured. Without this check,
it is often hard to identify whether the distribution is correct or an artefact of disparate
amplitude ratio distributions.
6.3.2 Network Distribution
In many cases the network of receivers is not ideally distributed for locating the source,
leading to increased location uncertainties. Furthermore, the sampling of the focal sphere
by the receivers often does not constrain the source well, leading to an increased range of
possible solutions (Vavryuk, 2007; Godano et al., 2009; Kim, 2011). Increasing the number
of receivers does not always improve the solution, since the receivers need to improve the
Figure 6.10: Lower hemisphere equal area projections, and marginalised lune plots of the source
PDF for synthetic source A (Fig. 6.4), inverted at a range of noise levels using P polarities and either
P/SH or P/SV amplitude ratios. The noise levels are SNR =, SNR = 10, SNR = 7, SNR = 5,
SNR = 3 and SNR = 2.
sampling of the focal sphere to improve the constraint (Fig. 6.12). It is possible to invert for
the source from a small number of waveforms, but this is not always stable and can lead to
large uncertainties (Kim et al., 2000; Vavryuk, 2007).
The receiver distribution can have a large effect on the resultant PDF for inversions using
only polarity data (Fig. 6.11). The variation in possible orientations is governed by how well
the fault planes are constrained. For all of the sources, the full moment tensor PDFs expand
as the constraint reduces, but remain close to the correct source value. The example for event
C with 22 stations shows the most significant deviation. The full moment tensor distributions
for event B, are peaked slightly away from the true tensile crack value, probably due to the
source sampling (Appendix B). As the constraints from the station geometry decrease, the
range of possible solutions increases, either because the number of stations decreases or the
distance between them increases.
Amplitude ratios greatly improve the source constraint, removing most of the effect of
the network distribution. The full moment tensor solutions for event A remain close to the
double-couple point (Fig. 6.12), and for event B at the tensile crack point. However, event
C shows some variation and deviation from the true source point. There is less of an effect
than in Fig. 6.11, since the amplitude ratios keep the PDF sharp.
Figure 6.11: Lower hemisphere equal area projections, and marginalised lune plots of the source
PDF for three synthetic sources (Fig. 6.4) inverted for different network distributions using P
polarities only. The posterior model probabilities for the double-couple source model estimated from
the Bayesian evidence are shown in Table 6.3. See also Fig. 6.5.
A B C
Stations i ii i ii i ii
200 0.97 0 ? ? ? ?
40 0.40 0 ? ? ? ?
22 0.77 0 ? ? ? ?
15 0.63 1 0 ? 0.32 0
Table 6.3: Posterior DC model probabilities for
the source PDFs shown in Fig. 6.5 and 6.6. Each
event is split into two columns for P polarity only
(i), and P polarity and P/SH and P/SV amplitude
ratios (ii). ? indicates events with no
double-couple solutions found.
The double-couple constrained solutions
are well-constrained for event A, with orient-
ation similar to the source, and there is little
variation in the range of solutions even as
the number of stations reduces. The pos-
terior double-couple model probabilities are
negligible for event A, due to the domination
of the full moment tensor PDF by a few high-
probability samples. The very large likeli-
hoods associated with these samples may be
due to mis-characterisation of the amplitude
ratios and associated uncertainties.
Event C has some possible double-couple solutions for the 15 receiver example, with
orientation consistent with the source (Fig. 6.4). However, the full moment tensor fit is much
better, so the posterior probabilities for the double-couple source model are negligible.
Figure 6.12: Lower hemisphere equal area projections, and marginalised lune plots of the source
PDF for three synthetic sources (Fig. 6.4) inverted for different network distributions using both P
polarities and P/SH and P/SV amplitude ratios. The posterior model probabilities for the
double-couple source model estimated from the Bayesian evidence are all zero. The full moment
tensor solutions are circled in red to make their position clearer. See also Fig. 6.5.
The station geometry is more important than the number of stations, as a low station
coverage case can still provide well-constrained solutions if there are sufficient constraining
station positions (cf. Panza and Sarao, 2000; len et al., 1996). The nodal regions for
full moment tensor solutions are not necessarily planar, unlike the double-couple solution,
therefore more stations are required to constrain the solution well, especially for the polarity-
only cases.
The effect of noise on solutions using amplitude ratios is much stronger than that of the
station distribution and the uncertainty on the measurements, and there is some significant
deviation in the source. Increasing the noise level changes the station distribution, since some
arrivals cannot be seen above the background noise. Consequently, at higher noise levels
the network distribution has an even greater effect, leading to more deviation from the true
source as the number of stations with arrival picks is reduced.
6.3.3 Location Uncertainty
The event location and associated uncertainty can have a large effect on the network distri-
bution on the focal sphere because of the source to receiver ray path variation. As mentioned
in section 6.3.1, the arrival time pick uncertainty can affect the estimated hypocentre. This
hypocentral uncertainty may also have a small effect when using the location to rotate the
seismogram in order to measure the amplitude ratios (Section 3.3), although this is ignored
here. The ray path uncertainties are dependent both on the location uncertainty and the hypo-
central depth. There have been several studies of the effects of location uncertainty on source
inversion, especially for full-waveform based inversrions, such as Duputel et al. (2012a) and
len (2009), who show that in some cases can cause large variations in the solution.
Synthetic seismograms were generated for the double-couple synthetic source (event A)
at different depths and at two noise levels, SNR = 50 and SNR = 5. These were inverted
while including the location uncertainty as described in section 4.2.5.
Figure 6.13: Lower hemisphere equal area projections, and marginalised lune plots of the source
PDF for a double-couple synthetic source (Event A in Fig. 6.4) inverted for different location depths
at two noise levels, SNR = 20 and SNR = 5, using P polarities only . The posterior model
probabilities for the double-couple source model estimated from the Bayesian evidence are shown in
Table 6.4. The fault plane plots also show the scatter of station ray positions on the focal sphere for
the NonLinLoc location PDF, coloured according to the probability of the ray path (darker colour is
more probable) and for the observed polarity with red signifying a positive polarity, yellow no
polarity and blue a negative polarity. See also Fig. 6.5.
The solutions using only polarity data (Fig. 6.13) are mainly affected by the network
distribution, as shown by the source at 7.0 km even at low noise levels. The effects of location
uncertainty at high noise levels are larger, mainly due to the reduced number of receivers. The
full moment tensor solutions are peaked around the double-couple point, and the posterior
double-couple model probabilities (Table 6.4) are all significantly non-zero, with the lowest
value for the 7.0 km depth source.
As seen in the other examples, including amplitude ratios greatly improves the constraint
of the source, and there is little deviation from the double-couple source for the low noise case
(Fig. 6.14). The low noise examples have well-constrained orientation for all the possible
depths, with little uncertainty in the source-receiver ray take off angles and azimuths. The
depth of the source has an effect on the network distribution, with the ray paths moving from
being equatorial on the focal sphere for shallow depths to more vertical paths as the depth
increases. This leads to additional uncertainties arising from clustering of stations on the
focal sphere in the 7.0 km depth example.
Figure 6.14: Lower hemisphere equal area projections, and marginalised lune plots of the source
PDF for a double-couple synthetic source (Event A in Fig. 6.4) inverted for different location depths
at two noise levels, SNR = 20 and SNR = 5, using P polarities and P/SH and P/SV amplitude
ratios. The posterior model probabilities for the double-couple source model estimated from the
Bayesian evidence are shown in Table 6.4. See also Fig. 6.13.
The higher noise level leads to larger uncertainty for both the double-couple constrained
solutions and the full moment tensor solutions, although the network geometry at 3.0 km
depth improves the constraint of the solutions compared to those at shallower depths. The
full moment tensors show some deviation from the double-couple point, apart from the
3.0 km depth example, possibly due to deviation of the amplitude ratios from the true value.
SNR = SNR = 5
Depth (km) i ii i ii
0.5 0.80 0.99 0.57 0
1.0 0.91 0 0.68 0
1.5 0.74 1 0.57 0
3.0 0.95 1 0.57 0.98
7.0 0.38 1 0.58 0
Table 6.4: Posterior DC model probabilities for
the source PDFs shown in Fig. 6.14.
This is reflected in the posterior model estim-
ates (Table 6.4), with the only non-zero value
in the high noise case for the 3.0 km depth
event. The posterior model estimates for the
low noise solutions are biased by a few dom-
inant high probability samples in the moment
tensor solutions. The double-couple solutions
show rotation from the true source for the shal-
low (z < 3.0 km) high noise examples which
is probably affected by both the network dis-
tribution and the noise, similar to Fig. 6.8.
There is an additional effect of a reduced network distribution due to the noise level,
especially affecting the polarity observations with less of an effect on the P arrival time picks.
The high noise case have fewer polarities for the shallow events, as the arrival on the vertical
component was often dominated by noise and the ray path is often not sub-vertical, with a
larger P arrival on the horizontal components.
The high noise examples show some deviation from the true source for the full moment
tensor solutions, although this is more likely to be due to the noise level rather than the
location uncertainty.
SNR = SNR = 5
Depth (km i ii i ii
0.5 0.80 0.99 0.57 0
1.0 0.89 0 0.68 0
1.5 0.63 1 0.57 0
3.0 0.95 1 0.57 0.97
7.0 0.38 1 0.58 0
Table 6.5: Posterior DC model probabilities for
the source PDFs shown in Fig. 6.14.
The effect of the well constrained location
variation on the solution is minimal, with little
difference between both the polarity-only solu-
tions (Figs 6.13 and 6.15 ) and those with amp-
litude ratio (Figs 6.14and 6.16). The largest
effect on the polarity inversion is the change
in receiver distribution with changing hypo-
centre. If the location PDF is not well con-
strained, the effect can be larger, especially for
shallow events with poor depth constraint as
seen in Chapter 9, and to a small extent in the z = 0.5 km, SNR = 5 solutions using
polarities and amplitude ratios. In these cases it is important to include the location un-
certainty, otherwise, as len (2009) shows, hypocentre mismodelling can lead to large
non-double-couple components in the source.
Figure 6.15: Lower hemisphere equal area projections, and marginalised lune plots of the source
PDF using P polarities only for the events in Fig. 6.13, without location uncertainty. See also Fig.
The ray path take-off angles and azimuths are often known to an accuracy of less than
1 degree. This has a very low effect on the corresponding homogeneous response functions
(Appendix D), because the absolute uncertainties are low. Despite these low absolute un-
certainties, the percentage errors can be large, with a corresponding effect on the amplitude
ratios. In some very specific cases, this can lead to very large uncertainties on the resultant
forward modelled amplitude ratio.
The near-field component can have an effect on the source inversion, especially the
amplitude ratios. The shallow event from Fig. 6.14 with low noise levels required a correction
to obtain the result shown in the figure. The near-field component at the nodal planes can
be non-negligible as the far-field component can equal zero, no longer dominating the result.
Furthermore the far-field approximation is not valid at close receivers, so the assumptions for
calculating the amplitude ratios do not hold in either of these cases. However, the effect of the
near-field can be mitigated by increasing the amplitude ratio uncertainties on these receivers,
Figure 6.16: Lower hemisphere equal area projections, and marginalised lune plots of the source
PDF for the events in Fig. 6.14, without location uncertainty. See also Fig. 6.5.
Figure 6.17: Lower hemisphere equal area projections, marginalised lune plots and  and 
distributions of the source PDF for a double-couple synthetic source (Event A in Fig. 6.4) inverted
with and without the near-field correction on the amplitude ratios. The third and fourth columns
show the marginalised distributions of the lune  and  parameters respectively. The fault plane plots
also show the scatter of station ray positions on the focal sphere for the NonLinLoc location PDF,
coloured according to the probability of the ray path (darker colour is more probable) and for the
observed polarity. See also Fig. 6.5.
to account for the uncertainties in the far-field approximation. Increasing these uncertainties
for only those receivers either with low amplitude arrivals close to the nodal plane or in close
proximity to the source, produces a sharp solution which agrees well with the true source
(Fig. 6.17). Without this correction, there are no distinct solutions in both the double-couple
and full moment tensor models.
6.3.4 Model Uncertainty
In many cases the velocity model is not well constrained, especially for local and regional
microseismic events, with simple 1D models used for complex volcanic areas (e.g. White
et al., 2011), so could have a large effect on the source inversion. There have been several
studies investigating the effects of model uncertainty on source inversion, especially for full-
waveform based inversions, including len et al. (2001); len (2004); len and Milev
(2006); Vavryuk (2007); Jechumtlov and Eisner (2008); len (2009); Yagi and Fukahata
(2011), both for variations in velocity model as well as the elastic parameters of the medium.
The model uncertainty has similar effects to the location uncertainty, and can be included
by usingMonte Carlo integration to marginalise over the range of velocity models (Eq. 4.54).
The Monte-Carlo samples can be generated by sampling from the location distributions for
the range of velocity models. although this may lead to computation problems if there are
large variations in the velocity model range, such as for complex three dimensional models.
The model variation can also affect the VP/VS ratio and the corresponding amplitude ratio
corrections (Eq. 4.13), although this is likely to be a second order effect, and negligible
compared to the associated ray path uncertainty, and can be ignored, especially if the Wadati
plot (Wadati, 1933; Wadati and Oki, 1933) provides a good estimate of the VP/VS ratio.
Figure 6.18: Lower hemisphere equal area projections and marginalised lune plots of a synthetic
double-couple source (Event A in Fig. 6.4) at a depth of 1.5 km with a range of velocity models
generated from the true model (Fig. 6.3). The first set of solutions corresponds to a velocity
perturbation of 3%, and the second 10%. See also Fig. 6.5. The fault plane plots also show the
scatter of station ray positions on the focal sphere for the NonLinLoc location PDF, coloured
according to the probability of the ray path (darker colour is more probable) and for the observed
polarity. The posterior model probability for the double-couple source model estimated from the
Bayesian evidence is 0.86 and 0.76 for the 3% and 10% perturbations respectively.
Inverting event A at a depth of 1.5 km using Eq. 4.54, leads to some increased uncertainty
in the source-receiver ray-path angles in Fig. 6.18 for samples drawn from a range of velocity
models, randomly perturbed from the true model (Fig. 6.3) l.
This increased uncertainty causes a slightly increased range of possible orientations for
the double-couple constrained solution for inversions using polarity data only (Fig. 6.18).
Although for a low noise level, increasing the range of models doesnt have a large effect
on the source, it has a clear effect on the range of receiver positions on the focal sphere.
Moreover it is likely that the effect of the model uncertainty will be much larger as the
number of receivers decrease and the noise level increases.
The full moment tensor solution shows little variation in the range of possible source types
compared to just the location uncertainty. In this case a simple one-dimensional velocity
model was used, which was assumed to be consistent for all stations. It is equally possible
to allow the velocity models to vary independently for each station and to combine these,
although this does not have a good physical analogue, except as varying 1Dmodels for shallow
structure to describe the near-surface and site effects (Section 3.1.3), and would increase the
number of location samples required in the inversion.
The effect is even more reduced when amplitude ratios are included (Fig. 6.19), with
little deviation from the double-couple point.
Figure 6.19: Lower hemisphere equal area projections and marginalised lune plots of a synthetic
double-couple source (Event A in Fig. 6.4) at a depth of 1.5 km with a range of velocity models
generated from the true model (Fig. 6.3). See also Fig. 6.18. The posterior model probability for the
double-couple source model estimated from the Bayesian evidence in both cases is 1.
Velocity model uncertainties can lead to non-double-couple components (len, 2004;
len and Milev, 2006; Vavryuk, 2007; len, 2009), so including the velocity model
uncertainty can be important when trying to estimate the source type, but if the model is
well-constrained, the effects are minimal. Topography, site effects and near surface structure
(Section 3.1.3) can have additional affects on the source inversion, especially when using
full-waveforms, amplitudes and amplitude ratios (len et al., 2001), as well as the location
of the event (Bean et al., 2008; OBrien et al., 2011).
6.3.5 Other Uncertainties
There are several other uncertainty sources, which can also affect the source inversion.
Errors in the instrument location can affect both the hypocentre location and the ray paths,
but these are usually negligible as a GPS location can be estimated to very high accuracy. The
instrument orientation can have a larger effect, since it can be difficult to calibrate. The easiest
way to estimate the direction of north is to use a compass and the associated magnetic north
correction, but in many areas the rocks have sufficient magnetism to introduce errors in the
orientation, making it easier to use GPS to calibrate the orientation of the instrument. Despite
this increased uncertainty, the errors arising from thiswill be small, although systematic errors
can be introduced into inversions of a dataset.
While the inversion approach described in Chapter 4 allows for the probability of an
incorrectly wired instrument inverting the polarities, this can usually be calibrated using
large teleseismic events with consistent arrival polarities between close receivers. An effect
on the data measurement that can be much larger than many of these uncertainies is the
site effect, especially when using amplitude data or the whole waveform. This is often
much harder to correct or even calibrate and can lead to large errors in the measurements as
discussed in Chapter 3, but can be incorporated in the measurement error.
6.4 Summary and Discussion
Uncertainties have a large effect on the source, especially the noise level, which influences
the network distribution and the location uncertainty. Consequently, high noise levels lead to
more uncertainty in the source PDF. Inversions using amplitude ratios also show deviation
from the true source point as the noise level increases, which is a possible cause of non-double-
couple source interpretations. This is likely to be due to the deviation of the amplitudes from
the true value as the noise level increases (Section 3.3). The network distribution has an
effect on both the location estimatation and the source constraint, but increasing the number
of receivers does not always improve the results, rather the constraint on the source must be
improved by positioning the receivers where there is more discrimination between sources,
such as close to the nodal plane. However, this is not possible to do with out knowing
the source being observed, so networks are often designed to maximise the discrimination
between the sources, perhaps using some prior estimates of the source (e.g. Coles and Curtis,
2011). The effect of network variation is much stronger for polarities, dominating the source
distribution instead of the noise.
There is an interdependence between the uncertainty on the source from the noise level
and the network distribution. As a general rule, increasing the noise level increases the
uncertainty, and increasing the number of receivers reduces the uncertainty, as long as the
constraint on the source is improved.
The location uncertainty can have an effect on the source PDF, especially as the noise
level increases. The effect is often minimal for well-constrained distributions, with the
largest influence on shallow events, as there is usually more take-off angle uncertainty for
depth variations at shallower depths. The velocity model is often assumed to be correct,
or sufficiently accurate for the data. While model uncertainty can have an effect, at low
noise levels and for low uncertainty in the model, it is often negligible when estimating the
source, much like the location uncertainty, but as the noise level increases or for large model
uncertainties, the effect will be larger. If the location is well-constrained, the effect on the
source distribution is minimal, and is usually negligible compared to the effects of other
uncertainties. The epicentral location is usually well constrained with less uncertainty than
the depth, so the ray paths are usually more constrained in azimuth rather than take-off angle.
However, this uncertainty in ray path angle is dependent on the velocity model at the event
hypocenter, so can be negligible despite the hypocentre uncertainty if there is little variation
in ray path over the range of possible locations. If the velocity model is three-dimensional,
with strong lateral heterogeneity, the sampling approach used here and described in Chapter
4 may struggle, as it is not easy to sample such a range of possible models.
Amplitude ratios make the distributions much sharper, but errors in the amplitude es-
timates can lead to deviations from the true source, since they are very dependent on the
background noise (Section 3.4). The estimates of the amplitude ratio can be uncertain and
incorrect, possibly leading to spurious non-double-couple sources. Consequently, despite
amplitude ratios providing a much better constraint on the source type and orientation, care
must be taken to correctly characterise the uncertainties, including the systematic shift from
the noise, otherwise the uncertainties will be underestimated. Some understanding of this can
be obtained by considering the PDFs arising from the individual amplitude ratios, and check-
ing if they are consistent with those from the polarity solutions, although this is not a rigorous
test. Moreover, due to the sharpness of the solutions when including the amplitude ratios,
the location and model uncertainty distributions require many more samples to estimate the
source PDF. Polarity-only inversions seem reasonably insensitive to both well-constrained
location PDFs, and measurement uncertainty, with the solutions dominated by the effects of
the network distribution. In some cases the results from the amplitude ratios towards different
source types and orientations, which may be an indicator of errors in the estimates rather
than true non-double-couple sources. Additionally, amplitude ratios can be affected by site
effects and near surface structure, and although these uncertainties can be included in the
measurement error, the structure is often not well known.
The posterior double-couple model probabilities agree with visual interpretation of the
source PDFs for the most part, although they can be affected by source PDFs which are
dominated by a few high probability samples affecting the Monte Carlo integration.
Although all of these uncertainties can have a large effect on the source PDF, the strength
of the effect depends on the data-types used in the inversion. Noise has the largest impact
because it influences both the network distribution and the location uncertainty, and directly
affects themeasurement uncertainty. Consequently, high-noise levels increase the uncertainty
in the source PDF and also seem to lead to somedeviation from the true source type as the noise
level increases, which is a possible cause of non-double-couple source interpretations. These
deviations can also be caused by the effects of the noise on amplitude ratio measurements, if
they are included in the inversion. The methods for measuring the amplitudes can introduce
strong systematic shifts (Chapter 3), which lead to spurious non-double-couple solutions, and
sparse solutions due to the peaks for the data types being well constrained about very different
points. However, this deviation from the true source does not appear to be as strong for the
double-couple constrained solutions, perhaps because of the constraint of the orthogonal
nodal planes.
Increasing the number of receivers may seem an easy way to improve the results from
source inversion, however the distribution of receivers must improve the constraint on the
source orientation and type. Poor network constraint can lead to large uncertainties in the
source PDF, especially when only using polarity information, because this has little variation
for different source types. Equally, good network constraint with a few receivers can provide
well constrained solutions especially for the double-couple constrained model. Full moment
tensor sources require more receivers to constrain the source type, because the double-couple
sources can be constrained well by the orthogonal nodal planes, whereas full moment tensors
have nodal lines that vary with the source-type, introducing a trade-off between them.
7 Relative Moment Tensor Inversion
Many seismic events occur in clusters, with waveforms that resemble each other. These
multiplets can have the same or contrasting mechanisms (Tarasewicz et al., 2014). Since
these events are co-located, the rays experience the same earth response between the source
and the receivers. Consequently, the relationship between the observations can help constrain
the moment tensors, as proposed by Dahm (1993, 1996).
This chapter presents an overview of relative moment tensor inversion, discusses the
approaches for making the observations and extends the Bayesian source inversion approach
fromChapter 4 to include relative information. This relative source inversionmethod requires
an estimate of the relative seismic moment, derived here, along with prior distributions for
the relative seismic moment for two common magnitude distributions, those of Gutenberg
and Richter (1949) and Eaton et al. (2014).
7.1 Exploring the Effects of Event Co-location
A seismogram can be modelled as a convolution of different effects (Eq. 3.1). The time-
dependence can be factorised out asF (, t;x, 0), which is dependent on the source location,
x, and origin time, 0, and the receiver location, . F (, t;x, 0) contains the instrument re-
sponse, source-time function, and earth response function. The remaining spatial component
depends on the take-off angle and azimuth of the ray from the focal sphere and the moment
tensor:
u(, t) = F (, t;x, 0)a(x, )M . (7.1)
Events in a multiplet have similar waveforms at each receiver across the network (Fig.
7.1), and since these events are co-located, the ray-path effects contained in F (, t;x, 0)
are the same. If two events have an identical source-time function, either a step function or
0 0.5 1 1.5 2 2.5 3
Relative time (s)
0 0.5 1 1.5 2 2.5 3 0 0.5 1 1.5 2 2.5 3
0 0.5 1 1.5 2 2.5 3
(a) (b)
(d)(c)
gij ESK
Relative time (s)
Relative time (s)Relative time (s)
Figure 7.1: Repeated waveforms with opposite polarity from a magma intrusion at the
Eyjafjallajkull volcano in Iceland. Adapted from Tarasewicz et al. (2014)
otherwise, the ratio of the two waveforms provides a constraint on the moment tensor:
u1(, t)
u2(, t)
F (, t;x, 0)a(x, )M 1
F (, t;x, 0)a(x, )M 2
a(x, )M 1
a(x, )M 2
. (7.2)
Dahm (1993, 1996) introduces an approach for source inversion from relative amplitudes,
with the time-dependent component of the waveform removed completely, leaving only the
ratio of the amplitudes. This approach uses matrix inversion, and is dependent on accurate
event locations and velocity models.
Rearranging Eq. 7.2 gives
u1iaiM 2  u2iaiM 1 = 0, (7.3)
which combined with the arbitrary constraint of the sum of the moment tensor components
for all the events to be equal to some constant (cf. Eq. 3.17),
nk=1
j=1Mkj = c, (7.4)
gives a simple matrix form



2 1 0 0    0
3 0 1 0    0
... . . .
n 0 0 0    1
0 3 2 0    0
0 4 0 2    0
... . . .
0 n 0 0    2
... . . .
0 0 0 0    n1
1 1 1 1    1



. (7.5)
The matrix coefficients

ui1a1
uimam
 (7.6)
are given by the vector of observations for event i multiplied into the six station coefficients
at each receiver for a total ofm receivers (Eq. 7.6). Inverting for the moment tensor using a
suitable left inverse also estimates the relative moments for the events.
However, it is not straightforward to include either location andmodel uncertainties in this
approach or other observations such as first arrival polarities1. Alternatively, the probabilistic
framework of Chapter 4 can be extended to use relative information.
1It is possible to include amplitude ratios in this matrix approach.
5 2.5 0 2.5 5
Eastings (km)
Figure 7.2: Location geometry for the stations shown in this section. The triangles show the
receivers and the red circle the epicentre of the source.
7.1.1 Estimating Event Co-location
Similar waveforms and P-S arrival time delays, PS are a strong indicator of event co-
location. The waveforms will vary as the hypocentre changes, for a synthetic seismogram
from a simple layered velocity model variations of the order of 1.5% in position (100 m) has
little difference in the shape, with some variation in the arrival times (Fig. 7.3). This location
variation corresponds to approximately 20% of the dominant wavelength at this depth, but it
does not lead to much uncertainty in the ray paths, as seen from the similar waveforms.
The P-S arrival-time delays should indicate how well events are co-located:
PS = tP  tS. (7.7)
This delay usually varies faster than the ray azimuth and take-off angle (Fig. 7.4), so PS is a
good indicator of whether two events are co-located.
The hypothesis that a pair of events are co-located can be tested by evaluating the
probability that the two events could have no difference,  , between the arrival-time delays.
Assuming a Gaussian pick-time uncertainty, the PDF for the P-S arrival-time delay PS
is given by the product of two Gaussian distributions, which is also Gaussian. Each P-S
arrival-time delay is an independent observation, so the PDF is a multi-variate Gaussian.
Consequently, the probability of the events being exactly co-located is simply given by the
Vertical
Horizontal
2 2.5 3 3.5 4 4.5
Time (s)
2 2.5 3 3.5 4 4.5
Time (s)
Figure 7.3: Unfiltered synthetic waveforms for 4 different receivers at different locations close to
and distant from the source location. The source location was varied vertically (first column) and
horizontally (second column) by 0.05 km. The black line is a source depth of 7 km. For the vertical
variation, the red corresponds to a depth of 6.95 km and the blue 7.05 km. For the horizontal
variation, the red corresponds to a horizontal shift of 0.05 km along the the east direction and the
blue a horizontal shift of 0.05 km to the west. The receiver locations are shown in Fig. 7.2.
product of
p ( |PS1 , PS2 , P1 , P2 , S1 , S2) =
2P1 + 
P2 + 
S1 + 
((PS1PS2))
+2P2
+2S1
+2S2
(7.8)
for  = 0 over the receivers.
However, this test does not account for how  , the difference in P-S arrival times between
the events, is spatially distributed for a given velocity model. Even if the probability of the
events being co-located is non-zero, it is possible that the events are located sufficiently close
together to use relative moment tensor inversion, with the same source to receiver ray paths
on the focal sphere, and propagation effects. Extending the test to include this is complex, as
it depends on both the receivers and the hypocentre. The range of valid locations is given by
evaluating the integral of Eq. 7.8 between two limits estimated from the spatial distribution of
 for which the difference between the source to receiver ray paths is sufficiently negligible.
Vertical
Horizontal
1 0.5 0 0.5 1
Z (km)
1 0.5 0 0.5 1
X (km)
Figure 7.4: Percentage variation in azimuth, take-off angle and P-S arrival time gap with vertical
(left column) and horizontal (right column) location change. The black line shows the percentage
variation in the arrival time gap (PS) compared to the original location X = 0 or Z = 0. The red
line corresponds to the percentage variation in the ray azimuth is given by the red line. The blue line
shows the percentage variation in take-off angle. The receivers are the same receivers as in Fig. 7.3
(locations shown in Fig. 7.2).
7.2 Observations
The relative ratio between two events, can be estimated from both the amplitudes and wave-
forms, as shown in this section. However, the waveform approach is not easily usable to
extend the Bayesian source inversion approach in Chapter 4, as shown here.
The ratio of arrival amplitudes or waveforms of the same phase between two co-located
events are only dependent on the station propagation coefficients and the moment tensor (Eq.
7.2). However, the observed ratio between the two events,
R12 = 
a(x, ).M 1
a(x, ).M 2
, (7.9)
introduces another parameter, the relative seismicmoment,  , which is the ratio of the seismic
moments for the two events. The amplitude ratio can be estimated using the approaches
discussed in section 3.4.
If the source time functions between the events are identical, Eq. 7.1 describes the ratio
of the aligned waveforms. The arrivals can be aligned using unsigned waveform cross-
correlation, which allows waveforms of opposite polarity, as in Fig. 7.1, to be correctly
correlated. This produces a relative time shift between the two traces, and can be carried out
0 0.1 0.2
5000
 R = 8.181.39A
0 0.1 0.2
 R = 2.620.19
0 0.1 0.2
 R = 2.550.78
0 0.1 0.2
 R = 2.380.47A
0 0.1 0.2
 R = 2.570.26
0 0.1 0.2
 R = 2.850.26
0 0.1 0.2
5000
 R = 2.710.14
T (s)
0 0.1 0.2
2000
 R = 2.110.24
T (s)
0 0.1 0.2
 R = 4.170.25
T (s)
Figure 7.5: Waveform division from two events in the Upptyppingar dyke swarm in 2007. The blue
trace is the waveform for the first event and the red trace is the waveform for the second event. The
black trace is the second trace multiplied by the measured ratio, with the grey region representing the
1 error region.
either in the time or frequency domains. Adding the time shift aligns the waveforms as seen
in Fig. 7.5, so orthogonal distance regression (ODR), a form of least squares type fitting,
introduced by Boggs and Rogers (1990a); Boggs et al. (1992); Zwolak et al. (2004), which
accounts for the error in both the data and the model by minimising the orthogonal distances
of the points from the model, can then be used to calculate the waveform ratio.
The results of waveform division for two events from a dyke swarm beneath the Upptyp-
pingar volcano in the Askja region of Iceland can be seen in Fig. 7.5. In most cases the black
(ratio multiplied line) and the blue are consistent with each other, with small uncertainties in
the calculated ratios.
ODR allows the incorporation of uncertainties in both of the traces based on the observed
noise, but the resultant uncertainty estimates (Boggs and Rogers, 1990b) are often a poor
description of the distribution of results. The distributions of the ODR ratio results for a
synthetic waveform with differing SNR ratios do not resemble a Gaussian distribution, with
the distributions showing both non-zero skewness and occasional multi-modality (Fig. 7.6).
Moreover, the estimate using ODR can deviate significantly from the true value of the ratio.
Consequently, waveform division cannot easily be used for extending the Bayesian approach
for source inversion described in Chapter 4, both due to this deviation, and since the estimated
values for the ratio PDF cannot be estimated from the ODR output.
0.4 0.5 0.6 0.7 0.8 0.9
0 1 2 3
0.2 0.4 0.6 0.8 1 1.2
0.2 0.4 0.6 0.8 1 1.2
0 1 2 3 4
0 0.5 1 1.5 2 2.5
0 0.5 1 1.5 2 2.5
0 0.5 1 1.5 2
Relative Amplitude Ratio
Figure 7.6: Distribution of the relative amplitude ratios for different fractional noise levels for the
two events, calculated using ODR. The cyan line corresponds to the correct value of the ratio, and the
red line indicates the Gaussian distribution from the mean of the ratio and uncertainty calculated
using ODR. The uncertainties for both the numerator and denominator are varied: (a) 1 = 0.1 and
2 = 0.1, (b) 1 = 0.01 and 2 = 0.3, (c) 1 = 0.6 and 2 = 0.5, (d) 1 = 0.4 and 2 = 0.4, (e)
1 = 0.5 and 2 = 0.5, (f) 1 = 0.3 and 2 = 0.1, (g) 1 = 0.6 and 2 = 0.3, and (h) 1 = 0.2 and
2 = 0.4.
7.3 Relative Seismic Moment: The Scale Factor
The relative seismic moment ( ) is a model parameter which depends on both the source
parameters and the observed data, so it cannot be well constrained at just one receiver.
Estimating the scale factor is key for carrying out the relative moment tensor inversion using
a forward model approach, since random sampling of this value can make the computation
infeasible.
7.3.1 Estimation
The likelihood for the amplitude ratio is dependent on the ratio PDF (Eq. 4.27). The
dependence of this PDF on the scale factor is very close to Gaussian, even for high signal to
noise ratio examples (Fig. 7.7).
Assuming no correlation between the two observations ( = 0), the ratio PDF and its
factors (Eqs 4.27 - 4.31) can be re-written in terms of  :
p (r,  ) = b( )d( )
XYa3
b( )
XYa2 (r)
c( )
2 ), (7.10)
0 1 2 3
Figure 7.7: Plots of example likelihood distributions(Eq. 7.10) for a range of  values for variations
in different parameters. The first and second plots correspond to varying X and Y respectively
between three values 2.5 (black), 7.0 (red) and 15 (blue), and the third and fourth plots correspond to
varying the X and Y respectively between 0.125 (black), 0.5 (red) and 1.25 (blue). The final plot
corresponds to varying the observed ratio, r, between 0.5 (black), 1 (red) and 1.5 (blue)
2 + 2X
b ( ) =
c ( ) =
22X + 2X2Y
d ( ) = e
b2( )c( )a2
2a2 . (7.11)
Expanding the exponent in Eq. 7.11 shows that d ( ) is a Gaussian in  :
b2 ( ) c ( ) a2
  Yr
2+2X
) , (7.12)
with mean and standard deviation given by
, (7.13)
2Yr2 + 2X
. (7.14)
0 0.5 1 1.5 2
0 0.5 1 1.5 2
Figure 7.8: Variation in 
2YXr+
2+2X
with  for different.parameters. The left hand plot
shows the variation for a range of noise to signal ratios, with X
= 0.85. The blue line corresponds
to Y
= 0.85, the black line Y
= 0.5 and the red line to Y
< 0.25. The right hand plot
corresponds to varying the value of the measured ratio r for X
= 0.85 and Y
= 0.85. The black
line corresponds to r = 0.1, the cyan line r = 1 the green line r = 5 and the red line r = 100.
Consequently, Eq. 7.10 is equivalent to
RN (r,  |X, Y, X, Y, 0) 
2YXr + 2XY
 2YXr + 2XY
2 + 2X
N (, , ) +
2XYe
, 0,
. (7.15)
The value of 
2YXr+
2+2X
is close to unity if
2YXr + 
XY > 2XY
2 + 2X (7.16)
holds, since  (2) = 0.9772 ' 1. Fig 7.8 shows the constraint holds for most values except
for very small  .
The relative amplitude ratios used are unsigned, so the likelihood depends on the negative
of the observed value:
p ( |r, X, Y, X, Y) = RN (r,  |, X, Y, X, Y, 0) +RN (r,  |, X, Y, X, Y, 0) .
(7.17)
This is equivalent to
p ( |r, X, Y, X, Y) = RN (r,  |, X, Y, X, Y, 0) +RN (r, |, X, Y, X, Y, 0) ,
(7.18)
which is the sum of the PDF from Eq. 7.15 added to the reflection of this PDF in the  = 0
axis. . The approximate PDF for positive  is given by
RN (r,  |, X, Y, X, Y, 0) '
2YXr + 2XY
N (, , )+
2XYe
, 0,
(7.19)
subject to the constraint in Eq. 7.16.
Both the full and approximate PDFs strongly resemble a Gaussian PDF (Fig. 7.9),
suggesting that the full PDF is well approximated by a truncated Gaussian distribution:
p ( ) 
N (,  ,  )  > 0
0  < 0
. (7.20)
The mean and standard deviation of a PDF are given by
 p ( ) d, (7.21)
 2 p ( ) d  2 . (7.22)
From Fig. 7.9, the best approximation is given from the truncated PDF of the positive
lobe alone. Consequently, the mean is approximated by
 ' A
2YXr (2 + 2) + 2XY
, (7.23)
where A is the normalisation constant for the non-truncated PDF.
The standard deviation can be estimated using the same approach. The Gaussian has zero
skewness, so the thirdmoment of theGaussian is given by
3N (x, , ) dx = 3+32.
Consequently, the variance estimation is given by
2 ' A

Y (3 + 32)Xr + 2X (2 + 2)Y
2XYe
+2 . (7.24)
The normalisation constant for the non-truncated PDF is
2YXr + 2XY
N (, , ) +
2XYe
, 0,
2YXr
 + 2XY
2XYe
2 , (7.25)
so the approximate truncated normal distribution is given by
p (r|, X, Y, X, Y) '
22
 )
 . (7.26)
Fig. 7.9 shows that the  dependence is well approximated by Eq. 7.26, although it
may be necessary to include the reflection in  = 0. Approximating the  likelihood by a
truncated normal PDF, constructed from the product of the approximate individual station
PDFs (Appendix F), is a good fit to the simulated data (Fig. 7.10). The approximate
PDF recovers the true value of the scale factor well (Fig. 7.10), although occasionally the
individual PDFs can overestimate the value, due to the slight asymmetry in the ratio PDF.
As the approximate PDF for  is very close to the true PDF, it is no longer necessary to
obtain samples from the  distribution, but instead the probability at a single  value for a
2 1 0 1 2
2 1 0 1 2
Figure 7.9: Numerical Simulation comparing the true PDF from Eq. 7.17 (red line), including the
non-physical negative  region, and the modelled truncated normal PDF. The left example
corresponds to a low signal to noise ratio and the right one a higher ratio. The blue dashed line
corresponds to Eq. 7.26, while the green dashed line is Eq. 7.26 with the addition of the reflection in
the  = 0 axis.
0 11 22 35
0 2 4 6
Figure 7.10: Comparing the true PDF from Eq. 7.17 (red line), including the non-physical negative
 region, and the modelled truncated normal PDF (blue dashed line) for random values of  (left
column) and a fixed value of  = 2.3 (right column). The green vertical line corresponds to the
mean of the modelled truncated normal PDF, while the black vertical line is the true  value. The
first 10 plots correspond to the  PDF for each receiver and the bottom plot shows the combined
PDF for all of these receivers.
given set of moment tensors and observations can be evaluated and the probability for other
values of  is known, greatly simplifying the evaluation of the relative amplitude likelihood.
7.3.2 Prior Distributions
To convert from the data likelihood to the posterior probability, an estimate of the prior
distribution is required. This can be calculated for  from the assumed underlying source
model. There are two prior distributions for the scale factor, either the Gutenberg-Richter
prior (Eqs 7.39 and 7.44) or from the log-normal moment distribution (Eaton, 2011), given
by Eq. 7.46.
0 5 10 15
Figure 7.11: The blue histogram shows the
distribution of random samples drawn from
the Gutenberg-Richter relative magnitude
distribution (M > 0). The red line shows the
calculated distribution (Eq. 7.28).
1 0 1 2 3
Figure 7.12: The blue histogram shows the
distribution of random samples drawn from the
Gutenberg-Richter relative magnitude
distribution (M > 0), truncated at the lower limit
ofM = 4. The red line is the calculated
distribution for a negative lower truncation (Eq.
7.30).
If a Gutenberg-Richter Distribution (Gutenberg and Richter, 1949, pp 16-25)
p (M)  10bM (7.27)
is used as the moment magnitude prior, the relative magnitude prior probability can be
calculated, but the resultant PDF is only analytic if some lower limit is applied to the
magnitudes, in this caseM > 0. The corresponding distribution for the relative magnitude is
p (Mr) =
(Mr + 1)
2 , (7.28)
derived from the ratio of exponentially distributed random variables distribution (Appendix
If the distribution is truncated at some positive lower limit, the distribution is
p (Mr|x0) 
b ln(10)(Mr+1)x0+Mr
Mr(Mr+1)2
eb ln(10)
x0 Mr 6 1
b ln(10)(Mr+1)x0+1
(Mr+1)2
eb ln(10)(Mr+1)x0 Mr > 1
. (7.29)
However, if the truncation is at some negative lower limit, the distribution is instead given by
p (Mr|x0) 


2(b ln(10)x0(Mr+1)+1)eb ln(10)(Mr+1)x0(b ln(10)x0 Mr+1Mr +1)e
b ln(10)Mr+1
(Mr+1)2
Mr < 0, Mr 6= 1
b2 ln (10)2 x20 Mr = 1
2(b ln(10)x0(Mr+1)+1)eb ln(10)(Mr+1)x0
(Mr+1)2
0 6Mr 6 1
2(b ln(10)x0 Mr+1Mr +1)e
b ln(10)Mr+1
(Mr+1)2
Mr > 1
(7.30)
The distribution of non-truncated Gutenberg-Richter Relative Magnitudes can be seen in
Fig. 7.11. However, the distribution changes if the distribution is truncated at some non-zero
lower limit (Fig. 7.12).
0 5 10 15 20 25
Figure 7.13: The blue histogram shows the
distribution of random samples drawn from
the Gutenberg-Richter moment distribution
(M > 0). The red line is the calculated
Gutenberg-Richter moment distribution (Eq.
7.34).
0 2 4 6
Figure 7.14: The blue histogram shows the
distribution of the ratio of random samples
drawn from a truncated Gutenberg-Richter
seismic moment distribution (Fig. 7.13). The
red line shows the calculated PDF for the
truncated distribution (Eq. 7.44).The green
line shows the scaled distribution for the
non-truncated distribution (Eq. 7.40), scaled
to fit to the simulated distribution at
M0r = 1.
Evaluating the distribution of seismic moments for any prior distribution of magnitudes
requires transforming between probability distributions. A simple example shows how this
is done:
g (y) dy = f (x) dx
 g (y) = f (x)
. (7.31)
For multivariate distributions, the PDF is multiplied by the associated Jacobian.
The transformation between the seismic moment,M0, and the moment magnitude,Mw,
(Hanks and Kanamori, 1979) is
log10 (M0) 6, (7.32)
M0 = 10
2Mw+6. (7.33)
If the moment magnitude follows a Gutenberg-Richter distribution (Eq. 7.27), the distri-
bution for the seismic moment is calculated from Eq. 7.31, and substituting for the derivative
gives
p (M0)  M
( 23 b+1)
0 . (7.34)
Normalising the seismic moment PDF (Eq. 7.33) gives an improper integral if the lower
limit is zero. Consequently, it is often simpler to truncate at some non-zero minimum value
to allow for normalisation.
Fig. 7.13 shows that this distribution fits a numerical simulation of seismic moments
from Gutenberg-Richter distributed seismic magnitudes.
The PDF of a ratio, w, given a distribution for both the numerator, x, and denominator,
y, is given by an integral of the joint distribution, g (x, y), following Hinkley (1969):
p (w) =
|y| g (wy, y) dy. (7.35)
Assuming the moment for each event is independent of the other, the joint PDF is simply
the product of the individual PDFs:
p ( ) =
|M02 | p (M02) p (M02) dM02 , (7.36)
M02 > 0, (7.37)
 p ( ) =
M02p (Mo2) p (M02) dM02
( 23 b+1)M
2( 23 b+1)
02 dM02 (7.38)
 (
3 b+1)
 43 b
. (7.39)
The definite integral in Eq. 7.38 is zero if the lower limit is zero, because it is an improper
integral. Fig. 7.14 shows that the un-normalised PDF
p ( )  (
3 b+1) (7.40)
fits the simulated distribution.
If the distribution is truncated at some lower limit y0, the PDF for a distribution of a ratio
(U = X
) can be calculated using
p (U = u) =
|y| p (uy, y) dy u 6 1
|y| p (uy, y) dy u > 1
. (7.41)
The distribution of relative seismic moment with truncated magnitudes is given by
p ( ) 

|M02| p (M02 ,M02) dM02  6 1
|M02| p (M02 ,M02) dM02  > 1
(7.42)


3 b+1)
 43 b
 6 1
3 b+1)
 43 b
 > 1
, (7.43)
 p ( ) =
3 b1  6 1
3 b+1)  > 1
, (7.44)
with the normalisation constant determined from integrating the distribution over the full
range.
This truncated distribution fits the simulated data well (Fig. 7.14), and notably better than
the scaled distribution of the non-truncated PDF (Eq. 7.40).
An alternative magnitude distribution was proposed by Eaton et al. (2014), who suggest
that since observations of bed thicknesses are log-normally distributed and fractures are
strata-bound, the crack radius is log-normally distributed, and therefore there is a log-normal
distribution of crack area. Since seismic moment is proportional to the crack area (M0 =
A), it is also log-normally distributed (Fig. 7.15):
p (M0) =
(ln(M0))
22 , (7.45)
and the magnitude is normally distributed.
0 5 10
Figure 7.15: The blue histogram shows the
distribution of random samples drawn from
the log-normal distribution with  = 1 and
 = 0.8, while the red line represents the
true PDF (Eq. 7.45).
0 5 10
Figure 7.16: The blue histogram
corresponds to the distribution of the ratio of
random samples drawn from the log-normal
seismic moment distribution with  = 1 and
 = 0.8 (green dashed line). The red line
shows the calculated distribution, Eq. 7.46.
The PDF of the relative seismic moment is again determined using Eq. 7.35. Assuming
the moment for each event is independent of the other, the joint PDF is simply the product of
the individual PDFs, so the ratio PDF is given by (Appendix H)
p ( ) =
(ln( ))2
42 . (7.46)
This is a log-normal PDF with zero mean and a standard deviation given by
2, again
fitting simulated data well as can be seen in Fig. 7.16.
7.4 Posterior PDF
The Bayesian framework in Chapter 4 can be extended to relative amplitudes and relative
waveforms. As shown in sections 7.2 and 7.2, the constraint produced by the relative inversion
is a ratio of a sum of the moment tensor components. However, there is the full moment
tensor space for two events to explore, extending the time required to sufficiently sample the
moment tensor space.
The joint probability density function (PDF) for an observed relative amplitude or wave-
form ratio for two events j and k at station i is
i = r
jk, A
i , A
i , 
i , 
i , 
i , A
i , 
i , 
rjki , 
i , A
i , 
i , 
(7.47)
similar to Eq. 4.40, where Aji is the modelled amplitude for event j at receiver i.
Including relative information in the Bayesian approach from Chapter 4, requires extend-
ing the data likelihood (Eqs 4.53 and 4.54) to give the joint likelihood. The joint likelihood
for R events for the non relative data is given by
d |M , t, ,k
i , i
i , i
p () p () dd, (7.48)
d |M , t, ,k
i , i
ikl,
i , i
p () p () dd, (7.49)
although this assumes that each event is co-located with the others, such that the location
uncertainty is not independent between the events.
The joint likelihood can be extended to include the corresponding data for the relative
amplitude ratio or the relative waveform ratio (Eq. 7.47), with appropriate limits so that only
independent combinations of events are taken.
| {M} ,x,,,k
l=j+1
i = r
k , A
ik, A
ik, 
i , 
i , i
i , i
p () p () dd (7.50)
shows the likelihood for a known velocity model, and it is easy to extend this to an unknown
velocity model.
The joint posterior PDF is again obtained by multiplying by the prior PDF distributions
for each events moment tensors:
p ({M} | {d} ,xk,,,k)  p ({d} | {M} ,x,,,k)
p (M i) . (7.51)
To calculate the posterior probability for a given event, the joint posterior needs to be
marginalised (Sivia, 2000, Section 1.3) with respect to the other events:
p (M i | {d} ,xk,,,k) =
p ({M} | {d} ,xk,,,k) dM j 6=i, (7.52)
where the integral is carried out over all possible values of the moment tensors for each event
except event 1, so for the case of three events in a relative moment tensor inversion, Eq. 7.52
would become
p (M 1 | {d} ,xk,,,k) =
p (M 1,M 2,M 3 | {d} ,xk,,,k) dM 2dM 3.
(7.53)
The relative amplitude data provides additional constraint in these marginalised PDFs (Eqs
7.52 and 7.53) because it is including additional data (as long as data independence is
preserved) in the inversion, which has not been used in the absolute inversion.
The increased moment tensor space can become computationally prohibitive to explore,
however appropriate algorithmic approaches are described in Chapter 8.
The relative moment ( ) can be estimated by calculating  and  (Eqs 7.23 and 7.24)
for each receiver and combining to obtain the mean and standard deviation of the combined
truncated Gaussian (section 7.3.1). The joint PDF can be evaluated for this combined mean,
and the corresponding probabilities for other values of the scale factor can be determined
using the approximation from Eq. 7.26.
It is also possible to obtain the marginalised relative moment distribution from the joint
PDF samples by reconstructing the truncated Gaussians, scaling them by the sample probab-
ility, and then marginalising.
7.5 Summary and Discussion
Relative information can help constrain the sources for co-located events (Dahm, 1993, 1996).
Estimating whether an event is co-located depends on the P-S travel time differences, which
can be used to estimate the probability of an event being co-located (Eq. 7.8).
The source inversion uses separately measured arrival amplitudes, although the constant
multiplier between two waveforms at the same station could be used given a suitable estimate
of the resultant distribution. As in previous sections, the combinations of data used for
the inversion must maintain independence, for two events there is a limited number of
combinations of observations that maintain data independence.
An important factor in the relative likelihood is the relative moment magnitude  . The
likelihood has an approximately Gaussian distribution in  for each receiver. These can be
combined to give a resultant estimate of the mean and standard deviation of  , removing
the need for a grid search over  . There are two prior models for the relative moment mag-
nitude dependent on either a Gutenberg-Richter (Gutenberg and Richter, 1949) magnitude
distribution or a log-normal magnitude distribution (Eaton et al., 2014).
The joint posterior PDF includes the relative information in the Bayesian approach from
Chapter 4.
It may not be obvious that the relative information can provide an additional constraint
on the source, but it is important to remember that these observations are new data, not
included in the main inversion, which uses only amplitude ratios and polarities, although care
must be taken if additional data types are included in an inversion. The relative P-amplitude
ratios are an additional independent measure, because five independent amplitude ratios can
be constructed from six amplitude observations for two events, three for each event, so a
possible combination of amplitude ratios is P/SH and P/SV for both events and the relative
P-amplitude ratio.
This approach, while improving the constraint on the source by the addition of new
data to the inversion, increases the dimensionality of the inversion problem, and although
these dimensions are independent, this can drastically affect the approaches used to solve
the forward model. Consequently, the method can not be used indiscriminately due to the
increasing computational burdens, requires judgement before carrying out the joint inversion.
8 Moment Tensor Sampling Algorithms
The Bayesian approach described in chapter 4 uses forward-modelling, which requires a
method of sampling and exploring the source space to evaluate the posterior PDF. The choice
of sampling algorithm can have a large effect on the computational resources required (CPU
time and memory) to gain a sufficient sampling of the source PDF.
The six independent moment tensor components (Section 2.2) describe a basis for the
six-dimensional moment tensor space. Each point in this space uniquely describes a moment
tensor. If the scalar moment magnitude is considered as a separate parameter the moment
tensor can be normalised to unity (Eq. 3.12). In a coordinate system based on the six-vector
form of the moment tensor (Eq. 2.8), this is a vector normalisation, which reduces the
normalised moment tensor solution space to the five dimensional surface of a unit six-sphere.
Since the double-couple source model (Section 2.3.1) is a prime source model candidate,
the search space can be limited to its sub-space. It has three free parameters, rather than the
five of the normalised moment tensor space. This constraint prevents spurious non-double-
couple solutions, but the double-couple space is a very small subspace of the full moment
tensor space. Consequently, sampling from the full moment tensor space is very unlikely to
sample the double-couple subspace at all.
The probability of a double-couple source-type is large when sampling using algorithm
B.1 (Fig. B.4). However, the double-couple point is a single value, consequently since the
source type parameters are continuous values, the probability of obtaining a double-couple
source is zero, as the probability that a variable x is in the range a 6 x 6 b is
P (a 6 x 6 b) =
p (x) dx, (8.1)
so if a = b, the probability goes to zero. However, computers use floating point arithmetic on
a discrete number line, so there is some very small probability that a double-couple source can
be sampled. Sampling from a PDF distribution can be performed by taking a sample from the
uniform distribution between 0 and 1 and using the inverse CDF for the target distribution.
Generating an exact double-couple solution therefore requires sampling the correct CDF
value for the two distributions which is exactly 0.5 in both cases. The probability of obtaining
this sample from the uniform distribution will be very low1. As a result, it is extremely
unlikely that any of the random moment tensor samples will be a double-couple, despite this
1The exact value depends on both the approach for generating random samples and the number of bits used
for the calculation.
being at the peak of the PDF in Fig. B.4. This is an inherent problem of sampling the moment
tensor space.
Although random sampling of the full moment tensor space is very unlikely to produce a
double-couple sample, there will be a lot of samples very close to this (Figs 2.28 and B.4).
Random sampling approaches do not easily allow for a qualitative evaluation of whether the
source type is double-couple or not, since it is unlikely to be directly sampled and certainly
not well-sampled without running such an algorithm for a very long time.
This chapter introduces three different sampling algorithms, and examines the advantages
and disadvantages of each, evaluating the computation resources required for satisfactory
sampling of the source PDF.
There are alternate approaches for forward model sampling, such as neural-network
approaches (e.gKaeufl et al., 2013), alongwith sampling approacheswhich increase in density
in the area of interest, such as the neighbourhood algorithm (e.g. Sambridge, 1999a,b), and
oct-tree (e.g.Lomax andCurtis, 2001). These approaches have advantages and disadvantages:
neural-network approaches produce a fast inversion at the expense of pre-calculating a training
set of possible events and ray paths, and both the neighbourhood algorithm and the oct-tree
approach can struggle with large numbers of dimensions. However, this chapter is presenting
three out of many possible algorithms which could be used for sampling the forward model,
and is not intended as a deep overview of every different possible algorithm. The performance
of these three algorithms is evaluated using three synthetic events, generated using a finite
difference approach (Bernth and Chapman, 2011).
8.1 Monte Carlo Random Sampling
Stochastic Monte-Carlo sampling introduces no biases and provides an estimate for the true
PDF, but requires a sufficient density of sampling to reduce the uncertainties in the estimate.
The sampled PDF approaches the true distribution in the limit of infinite samples. However,
this approach is limited both by time and memory. Some benefits can be gained by only
keeping the samples with non-zero probability.
The dimensions of the moment tensor space require large numbers of samples. 100,000
samples from the five-dimensional surface of a six-sphere amounts to 10 in each dimension,
which can only be thought of as coarse sampling. Doubling the sampling density will increase
the number of samples 32-fold.
The approach can be parallelised and the samples can be evaluated iteratively, discarding
those with zero probability (Algorithm 8.1).
Algorithm 8.1Monte Carlo Random Sampling Search Algorithm
1. Draw new samples x.
2. Evaluate likelihood for samples x.
3. Store moment tensors from x with non-zero likelihoods.
4. Return to 1 until sufficient samples are drawn.
8.2 Markov Chain Monte Carlo Sampling
Markov chain Monte Carlo (McMC) sampling constructs a Markov chain (Norris, 1998) of
which the equilibrium distribution is a good sample of the target probability distribution. A
suitable McMC method should converge on the target distribution rapidly. As an approach it
is more complex than the Monte Carlo random sampling approach described above, and by
taking samples close to other non-zero samples, there is more intelligence to the sampling
than in the random sampling approach.
8.2.1 Markov Chain
0.05 0.75 
Figure 8.1: Markov chain directed graph example
for three states, A, B and C and the associated
probabilities for the transitions between them.
A Markov chain is a memoryless stochastic
process of transitioning between states. The
probability of the next value depends only
on the current value, rather than all the pre-
vious values, which is known as the Markov
property (Markov, 1954):
p (dn|dn1, dn2, dn3, . . . d0) = p (dn|dn1) .
(8.2)
Fig. 8.1 shows a simple Markov chain as a directed graph, with the edges labelled with the
probabilities of going from one state to the others.
8.2.2 Metropolis-Hastings Algorithm
Metropolis et al. (1953) proposed an approach for sampling the Markov chain for the specific
case of the canonical ensemble. It was extended to the general case by Hastings (1970).
TheMetropolis-Hastings approach is a commonmethod forMcMC sampling and satisfies
the detailed balance condition (Robert and Casella, 2004, eq. 6.22), which means that the
probability density of the chain is stationary. New samples are drawn from a probability
density q (x|xt) to evaluate the target probability density p (x|d).
The Metropolis-Hastings algorithm begins with a random starting point and then iterates
until this initial state is forgotten (Algorithm 8.2). Each iteration evaluates whether a new
proposed state is accepted or not. The acceptance, , is given by
 = min
p (x|d)
p (xt|d)
q (xt|x)
q (x|xt)
, (8.3)
which can be expanded using Bayes Theorem (Eq. 4.1) to give the acceptance in terms of
the likelihood p (d|x) and prior p (x):
 = min
p (d|x) p (x)
p (d|xt) p (xt)
q (xt|x)
q (x|xt)
. (8.4)
The acceptance is the probability that the new sample in the chain, xt+1, is the new sample,
x, otherwise the original value, xt, is added to the chain again:
xt+1 =
x probability = 
xt probability = 1 
. (8.5)
The initial starting point can bias the chain, especially when in low-likelihood areas.
Consequently, a large number of initial samples are often discarded, in a process known as
burn-in. During this process, after sufficient samples have been obtained, the transition PDF
parameters can be updated to target the desired acceptance rate. This can take many iterations
during burn in, and the number of required samples must reflect the desired acceptance rate
as low acceptance rates require more samples before updating.
Algorithm 8.2Metropolis-Hastings Markov chain Monte Carlo Sampling
1. Determine initial value for x0 with non-zero likelihood
2. Draw new sample x from transition PDF q (x|xt)
3. Evaluate likelihood for sample x
4. Calculate acceptance, , for x using Eq. 8.4.
5. Determine sample xt+1using Eq. 8.5.
6. (i.) If in burn-in period:
(a) If sufficient samples (> 100) have been obtained, update transition PDF paramet-
ers to target ideal acceptance rate.
(b) Return to 2 until end of burn-in period and discard burn-in samples.
6. (ii.) Otherwise: Return to 2 until sufficient samples are drawn.
8.2.3 Reversible Jump Markov chain Monte Carlo Sampling
The Metropolis-Hastings approach does not account for variable dimension models. Green
(1995) introduces a new type of move, a jump, extending the approach to variable dimension
problems. The jump introduces a dimension-balancing vector, so it can be evaluated like the
normal Metropolis-Hastings shift (Section 8.2.2).
Green (1995) showed that the acceptance for a pair of modelsMt andM  is
 = min
p (d|x,M ) p (x|M ) p (M )
p (d|xt,Mt) p (xt|Mt) p (Mt)
q (xt|x)
q (x|xt)
, (8.6)
where q (x|xt) is the probability of making the transition from parameters xt from model
Mt to parameters x from modelM , and p (Mt) is the prior for the modelMt.
If the modelsMt andM  are the same, the reversible jump acceptance is the same as the
Metropolis-Hastings acceptance (Eq. 8.4). The importance of the reversible jump approach
is that it allows a transformation between different models, and even different dimensions.
The dimension balancing vector requires a bijection between the parameters of the two
models, so that the transformation is not singular and a reverse jump can occur. In the
case where dim (M ) > dim (Mt), a vector u of length dim (M )  dim (Mt) needs to be
introduced to balance the number of parameters in x. The values of u have probability
density q (u) and some bijection that maps xt,u x
x = h (xt,u) . (8.7)
If the jump has a probability j (x) of occurring for a sample (x), the transition probability
depends on the jump parameters(u) and the transition ratio is given by
q (xt,u|x)
q (x|xt,u)
j (x)
j (xt) q (u)
|J | , (8.8)
with the Jacobian matrix
h (xt,u)
 (xt,u)
. (8.9)
The general form of the jump acceptance involves a prior on the models, along with a
prior on the parameters. Combining Eqs 8.6 and 8.8 gives the acceptance for this case:
 = min
p (d|x,M ) p (x|M ) p (M )
p (d|xt,Mt) p (xt|Mt) p (Mt)
j (x)
j (xt) q (u)
. (8.10)
If the jump is from higher dimensions to lower, the bijection describing the transformation
(Eq. 8.7) has an inverse that describes the transformation x  xt,u, and the acceptance is
 = min
p (d|xt,Mt) p (xt|Mt) p (Mt)
p (d|x,M ) p (x|M ) p (M )
j (xt) q (u)
j (x)
J1
. (8.11)
8.2.3.1 Reversible Jump McMC in Source Inversion
The reversible jump McMC approach allows switching between different source models,
such as the double-couple model and the higher dimensional model of the full moment tensor
(Algorithm 8.3). It can be extended to other source models, including those discussed in
section 2.3.
The full moment tensor model is nested around the double-couple point, leading to a
simple description of the jumps by keeping the common parameters constant. The Tape
parameterisation (Section 2.4.2) allows for easy movement both in the source space and
between models. The moment tensor model has five parameters: strike (), dip cosine (h),
slip (), eigenvalue co-latitude (), and longitude (), while the double-couple model has
only the three orientation parameters: , h, and . Consequently, the orientation parameters
are left unchanged between the two models, and the dimension balancing vector for the jump
has two elements, which can be mapped to  and :
h (DC, hDC, DC,u)


MT = DC
hMT = hDC
MT = DC
MT = u1
MT = u2
. (8.12)
Algorithm 8.3 Reversible Jump Markov chain Monte Carlo Sampling
1. Determine initial value for x0with non-zero likelihood.
2. Determine which move type to carry out:
(a) Carry out Jump with probability pJump - Draw new sample x = (x,u) with
dimension balancing vector u drawn from q (u1, u2)
(b) Carry out Shift with probability 1 pJump - Draw new sample x from transition
PDF q (x|xt).
3. Evaluate likelihood for sample x.
4. Calculate acceptance , for x.
5. Determine sample xt+1using Eq. 8.5.
6. (i.) If in burn-in period:
(a) If sufficient samples (> 100) have been obtained, update transition PDF paramet-
ers and jump probability to target ideal acceptance rate.
(b) Return to 2 until end of burn-in period, then discard all burn-in samples.
6. (ii.) Otherwise: Return to 2 until sufficient samples are drawn.
The Jacobian is the identity matrix, with determinant 1, so the acceptance for the double-
couple to moment tensor jump is
 = min
p (d|xMT,MMT) p (xMT|MMT) p (MMT)
p (d|xDC,MDC) p (xDC|MDC) p (MDC)
j (xMT)
j (xDC) q (u1, u2)
, (8.13)
and the reverse jump from the moment-tensor to double-couple is given by
 = min
p (d|xDC,MDC) p (xDC|MDC) p (MDC)
p (d|xMT,MMT) p (xMT|MMT) p (MMT)
j (xDC) q (u1, u2)
j (xMT)
.(8.14)
The choice of values for the model priors, along with q (u1, u2) and the jump probabilities
j (x) are discussed below (Section 8.2.4). The shift acceptance is the same as for the normal
Metropolis-Hastings approach (Eq. 8.4).
8.2.4 Prior PDFs
The basic prior PDF reflects the distribution of randommoment tensors described inAppendix
B. The normalised prior PDFs for the Tape parameters (Section 2.4.2) are:
p () =
cos (3) , (8.15)
p () =
1.1045
( + /2)
, 5.7479, 5.7479
, (8.16)
p () =
, (8.17)
p (h) = 1, (8.18)
p () =
, (8.19)
where  (x, a, b) is the beta distribution (Johnson et al., 1995).
The  and  priors are not uniform distributions, because the distributions of random
moment tensors is not uniform (Appendix B). The prior for  also needs to account for the
variation in area on the sphere, with the normalisation constant numerically calculated to be
1.1045.
The choice of jump probability for the trans-dimensional algorithm is important, as it
affects how often the jump between the models will occur and how many jumps will be tried.
In this case a simple uniform jump probability is used, although increasing the probability of
the moment tensor to double-couple jump close to the double-couple point is an alternative.
8.2.5 Proposal Density
These algorithms are most efficient if the proposal density resembles the target distribution
q (xt|x) = p (xt|d) . (8.20)
This gives an acceptance ratio of unity, as the samples are drawn from the target distribution:
 = min
p (x|d)
p (xt|d)
p (xt|d)
p (x|d)
= 1. (8.21)
The target distribution is hard to quantify, so a Gaussian is often used for the proposal
density. The Gaussian proposal density is symmetric in x and xt:
(xxt)
22 =
(xtx)
22 , (8.22)
so the ratio can be neglected from the acceptance calculation.
Unfortunately, the Tape source parameters have finite upper and lower bounds, so the
proposal densities are more complex. Discarding any samples outside of the range and
redrawing introduces a truncation (Fig. 8.2), which requires a normalisation correction. For
a parameter , with upper and lower bounds max and min, the proposal density is given by
q (|0) =
N (, 0, )
P (min 6  6 max)
. (8.23)
This no longer satisfies the symmetry condition (Eq. 8.22), but instead depends on the
normalisation constants, which must be calculated for each sample.
Alternately, the distribution can be wrapped around the bounds. The strike, , takes
values 0 6  < 2, with a periodicity of 2,  =  + 2n. A new sample can be drawn
from the unbounded Gaussian distribution and wrapped by taking the modulo with respect
to 2 (Fig. 8.3). The wrapped Gaussian has the same symmetry properties as the unbounded
Gaussian distribution and the ratio of proposal probabilities is unity.
0 1 2 3 4 5
Figure 8.2: Limited range Gaussian
distribution between 0.5 and 4
0 /2  3/2 2
Figure 8.3: Wrapped Gaussian distribution
over the range between 0 and 2
Wrapping the proposal density makes sense when the parameter has a periodicity. How-
ever, the other orientation parameters for the source, h and  do not have a straightforward
periodicity. The slip, , is bounded such that || 6 /2, and out of bounds values correspond
to the alternate source plane (Section 2.4.2). Consequently, an out of bounds sample affects
the other parameters (Fig. 8.4), adding complexity to the proposal function. However, the
slip has a periodicity of 2, so it is possible to draw samples from this wrapped distribution,
correcting to the alternate fault plane after the new sample has been drawn.
The dip cosine, h, reflects at the distribution boundaries, introducing a phase shift in the
strike of , which when combined with the slip wrapping can introduce a confusing set of
distributions that describe the orientation.
Fig. 8.4 shows an example of the wrapped proposal distribution for the orientation
parameters. The complexity makes the transition PDF ratio non-symmetrical and difficult to
describe analytically, so it is easier to let the strike proposal density wrap and to sample the
other parameters from truncated Gaussian distributions.
0  2
0 0.5 1
/2 0 /2
0  2
0 0.5 1
/2 0 /2
0  2
0 0.5 1
/2 0 /2
0  2
0 0.5 1
/2 0 /2
Figure 8.4: Distributions of the different orientation parameters (strike, , dip cosine, h, and slip, )
during the different steps of wrapping all the orientation parameters for the source. The initial
parameters are given by the red lines. Column (a) corresponds to unbounded samples from the
Gaussian slip proposal distribution. Column (b) shows the distribution of the parameters when the
slip is corrected to lie within the bounds || 6 2 , using the parameters for the alternate fault plane
(Section 2.4.2). Column (c) includes the random Gaussian samples from the dip cosine distribution,
corrected to lie within the bounds (phase shifting the strike by ). Column (d) shows the complete
Gaussian wrapped distributions for the source orientation parameters.
The source parameters,  and , do not wrap, and so these are again sampled from
truncated Gaussian distributions.
The width parameter, , in the proposal function (Eq. 8.23) defines the range of
parameters that will be sampled. It is related to the distance moved on each trial and controls
the acceptance rate. Increasing the width increases the range of parameters to be tried, but
few are likely to be accepted, lowering the acceptance rate.  is often updated during the
burn-in and initialisation period, so that a desired acceptance rate is obtained.
A large value for  means that as much of the model space is explored as possible,
although this increases the required chain length to sample the target distribution well. In
the limit of an infinite width parameter, this is equivalent to the random sampling algorithm
(Section 8.1).
8.2.5.1 Reversible Jump Balance Vector
There are several possible proposal distributions for the balance vector, including the uniform
distribution, zero mean Gaussians and non-zero mean Gaussians. The closer the balance
vector proposal is to the posterior probability of the data, the better the acceptance rate, but
there is a clear trade-off between introducing more complicated proposal densities and the
required computation time.
As discussed in Brooks et al. (2003), the proposal density for the balance vector can be
altered by adjusting the balance vector and its derivatives to maximise the transition between
models at a central point. However, determining scale parameters for the proposal density
function can often require numerical calculation, adding to the complexity when normalising.
A possible proposal density for the balance vector is a simple uniform probability. While
this can be inefficient, it does help improve the jumps spanning the source-type space, by
connecting regions that ordinarily would not be well connected by a non-uniform distribution.
Brooks et al. (2003) suggest scaling the balance vector proposal to give maximum mixing at
the double-couple point (acceptance is unity for forward and backwards jumps).
Figure 8.5: Distribution of the logarithm of
the ratio of the prior probabilities for , 
(Section 8.2.4) and the balance vector
proposal density (double-couple to moment
tensor jumps), for the uniform and Gaussian
proposal distributions. The Gaussian
proposal distribution has  and  standard
deviations of 0.1. A likelihood ratio bigger
than 1 is required in blue regions correspond
and less than 1 in yellow regions to always be
accepted (with equal model type priors).
The uniform balance-vector proposal distri-
bution requires, for solutions close to the double-
couple point, a likelihood ratio
p(d|xDC,MDC)
p(d|xMT,MMT)
larger than those further away for the moment
tensor to double-couple jump to be accepted
(Fig. 8.5). Moreover, a double-couple to mo-
ment tensor jump at the double-couple point will
always be accepted despite the moment-tensor
model being more complex.
The relative jump McMC approach is pro-
posed so that the effect of any improved fit from
the extra free parameters in the full moment
tensor solution is reduced, which the uniform dis-
tribution does not do. To enable this, a proposal
distribution should be chosen with a maxima at
the double-couple point that is larger than the
prior values for  and . The Gaussian proposal distribution satisfies this condition, and
the likelihood ratio required to accept the jump to the double-couple model increases further
away from the double-couple point. The region where this crosses unity is governed by the
standard deviations in the proposal distribution as well as the model priors. While the stand-
ard deviation can be changed, varying the model priors seems like a more intuitive method
of controlling the scope of this region (Fig. 8.6).
The two free parameters in the Gaussian balance-vector proposal densities, can be unequal
as shown in the bottom row of Fig. 8.6, where the values are scaled so that the percentage
uncertainties on each range are equivalent.
Despite the different choices of balance vector proposal densities, if the model priors are
not too large, when the chain approaches the double-couple model it will jump to that model
for non-negligible likelihood ratios. Equally, a negligible likelihood ratio can counteract the
effect of the trans-dimensional jumping, allowing well constrained PDFs close to double-
couple. Consequently the main effect of the different balance-vector proposal distributions
is to change the required chain length.
Although the uniform balance vector is illogical, the effect is not always evident, as if
the double-couple solutions have sufficiently higher likelihood the peak remains sharp at the
double-couple point, and the two proposal functions produce similar solutions.
Figure 8.6: Distribution of the logarithm of the ratio of the Gaussian balance vector proposal density
and the prior probabilities for ,  (described in section 8.2.4), with varying model priors. The blue
regions correspond to those where a likelihood ratio less than 1 for the double-couple moment
tensors will be accepted for the moment tensor to double-couple jump. The top row correspond to a
circular Gaussian distribution with standard deviations  =  = 0.1 and the bottom corresponds to
 = 3 = 0.25.
8.2.6 Initialisation and Burn-In
The Markov chain requires an initial sample, which can be any non-zero probability sample,
such as the maximum probability sample generated using Monte Carlo random sampling
(Section 8.1). This can be used as the starting point for the burn-in chain, which helps avoid
starting in low-gradient, low-probability regions that bias the Markov chain. The random
sampling approach also gives an estimate of the event quality (section 8.6).
The burn-in period helps move away from any initial bias, as well as allowing the
parameters for the proposal density to be adjusted to achieve a desired acceptance rate. The
rate needs to balance the trade-off between testing the whole space and finding the maxima
of the PDF. The proposal distribution width parameter can be scaled up if the acceptance rate
is too high and down if the rate is too low. This update is done after a sufficient number of
samples have been evaluated to make the acceptance rate estimate valid.
The burn-in length needs to be sufficiently long that the chain is not biased by the
initial sample, and although this is helped by using random samples, it also needs to be
sufficiently long that the proposal density is the correct width for the desired acceptance rate.
Consequently, the required length varies depending on the solution, but it is always better to
have a burn-in length that is too long rather than too short.
8.3 Algorithm Performance
The performance of the different algorithms can be compared, both on the resultant PDF and
the elapsed time. Comparable sample sizes and chain lengths produce better solutions for the
McMC approaches.
Figure 8.7: Lune plot (Section 2.5.6) of the variation in marginalised source PDF for a synthetic
double-couple source with different sample sizes for the random sampling algorithm. Red
corresponds to high probability and blue low probability regions.
The random sampling example (Fig. 8.7) needs sample sizes of the order of 5  107 to
produce a good sampling of the PDF. Increasing the number of random samples improves the
resolution of the source PDF. The solutions have better constraint of the PDF as the number of
samples increases, with the peak becoming clear at 5 108 samples. If the plots use a lower
resolution for the source PDF histogram, the peak is visible at lower numbers of samples.
This holds true for the McMC approaches (Figs 8.8 and 8.9). The results from the different
algorithms are consistent, as is to be expected.
Figure 8.8: Lune plot of the variation in marginalised source PDF for a synthetic double-couple
source with different chain lengths for the Markov chain Monte Carlo algorithm.
Figure 8.9: Lune plot of the variation in marginalised source PDF for a synthetic double-couple
source with different chain lengths for the trans-dimensional Markov chain Monte Carlo algorithm.
The probability of a double-couple source type is 0.34, 0.91, 0.69, 0.67, 0.77, 0.73, 0.68, 0.74,
respectively, for each plot.
A chain length of 50, 000 for the McMC approach is comparable to 1  108 random
samples (Fig. 8.8). There is some discrepancy between whether there are one or two peaks
in the solution, with some examples having two peaks and others only one. This is probably
due to the acceptance rate, which was targeted for these examples at 0.3.
The trans-dimensional McMC solutions (Fig. 8.9) are plotted with a low resolution on the
PDF histogram to make the strong peak at the double-couple point clear. The double-couple
solution is well sampled, unlike in the normal McMC and random sampling approaches, and
there is an associated strong peak at the double-couple point, consistent with the double-
couple synthetic source. The targeted acceptance rate for the trans-dimensional approach is
0.15, because of the dimension jumping. Consequently, more of the space should be explored.
The probability of the source being a double-couple type source from the trans-dimensional
approach varies between 0.24 and 0.99. Despite the sharp peak at the double-couple point,
the non-double-couple samples resemble those for the other examples (Figs 8.7 and 8.8), as
shown in Fig. 8.10.
Figure 8.10: Lune plot of the marginalised source PDF for the trans-dimensional Markov chain
Monte Carlo results shown in Fig. 8.9, showing the non-double-couple distribution only.
Comparing different sample sizes shows that the McMC approaches require far fewer
samples than random sampling. However, the random sampling algorithm is quick to calculate
the likelihood for a large number of samples, unlike theMcMC approach, because of the extra
computations in calculating the acceptance and obtaining new samples. Some optimisations
have been included in theMcMCalgorithms, including calculating the probability formultiple
new samples at once, with sufficient samples that there is a high probability of containing
an accepted sample. This is more efficient than repeatedly updating the algorithm. Despite
these optimisations, the McMC approach is still much slower to reach comparable sample
sizes (Fig. 8.11), and is slower than would be expected just given the targeted acceptance
rate, because of the additional computational overheads.
0 5 10 15
Number Random Samples
0 2 4 6
Markov Chain Length
Figure 8.11: Elapsed time for different sample sizes of the random sampling (left plot) and for the
McMC algorithms (right plot) with different chain lengths. The red dots in the McMC case
correspond to the trans-dimensional McMC algorithm and the blue dots correspond to the standard
algorithm.
The random sampling approach can easily be parallelised. N processors reduce the
required time N -fold. There are techniques for sampling multiple Markov chains in parallel,
by sampling at different acceptance rates and jumping between them (Sambridge, 2013), to
speed up the chain, but these are not as effective because the evaluation of the forward model
is quick for any given model sample.
Including location uncertainty and model uncertainty in the forward model causes a rapid
reduction of the available samples for a given amount of RAM and increases the number of
times the forward model must be evaluated, lengthening the time for sufficient sampling (Fig.
8.12).
0 5 10 15
Number Random Samples
0 2 4 6
Markov Chain Length
Figure 8.12: Elapsed time for different sample sizes of the random sampling algorithm and for
McMC algorithms with different chain lengths. The velocity model and location uncertainty in the
source was included with a one degree binning reducing the number of location samples from 50,000
to 5, 463. See also Fig. 8.11.
Event A Event B
Figure 8.13: Lune plots of the
non-double-couple synthetic
sources used to test the sampling
algorithms.
The location uncertainty has less of an effect on the
McMC algorithms, since the number of samples being
tested at any given iteration are small. Consequently, as
the random sampling approach becomes slower, the fewer
samples required to construct the Markov chain starts to
produce good samples of the source PDF at comparable
times. However, there is an initial offset in the elapsed
time for the Markov chain Monte Carlo approaches due to
the burn in and initialisation of the algorithm.
Different source types can have an effect on the results,
although the results for each approach are consistent. Two
different non-double-couple sources were used to test these algorithms (Fig. 8.13). Figs 8.14
- 8.16 correspond to a moment tensor source close to a double-couple (Event A in Fig. 8.13).
The solution is multi-modal, with the maximum probability source type close to the synthetic
source. Again, as the number of samples increases, the peak becomes clearer.
The McMC solutions are usually consistent with the maximum peak in the random
sampling source PDF and the true synthetic source, but they show only one of the regions,
and the example with a chain length of 105 samples shows a lower probability region. The
lack of sampling of the multiple modes is again due to the acceptance rate of the algorithm,
similar to the example in Fig. 8.8. In this case, the acceptance rate is targeted at 0.3.
Figure 8.14: Lune plot of the variation in source PDF for a synthetic source with different sample
sizes for the random sampling algorithm. The synthetic source (Event A in Fig. 8.13) is close to a
double-couple, with a small amount of closing crack.
Figure 8.15: Lune plot of the variation in marginalised source PDF for a synthetic source with
different sample sizes for the Markov chain Monte Carlo algorithm. The synthetic source (Event A in
Fig. 8.13) is close to a double-couple, with a small amount of closing crack.
The trans-dimensional McMC approach (Fig. 8.16) has a much better sampling of the
multiple peaks in the source PDF and is more consistent with the random sampling source
PDF (Fig. 8.14), with good sampling of the three large regions of non-zero probability and
the peak close to the double-couple source type.
Figure 8.16: Variation in marginalised source PDF for a synthetic source with different chain
lengths for the trans-dimensional Markov chain Monte Carlo algorithm. The synthetic source (Event
A in Fig. 8.13) is close to a double-couple, with a small amount of closing crack.
The only difference between the trans-dimensional McMC approach and the normal
McMC approach in this case is the acceptance rate, since none of the double-couple jumps
were accepted. The targeted acceptance rate for the trans-dimensional approach was 0.15,
suggesting that a value close to this may produce a better sampling of multi-modal source
PDFs.
Figs 8.17 - 8.19 correspond to solutions for a synthetic opening crack source (Event B
in Fig. 8.13). The random sampling solutions (Fig 8.17) are consistent with the synthetic
source, although they show poor sampling due to the relative sparsity of non-zero solutions, as
well as being close to the upper edge of possible random moment tensor samples (Appendix
Figure 8.17: Lune plot of the variation in marginalised source PDF for a synthetic source with
different chain lengths for the random sampling algorithm. The synthetic source (Event B in Fig.
8.13) is an opening crack.
The McMC solutions (Fig. 8.18-8.19) show much better sampling and better constraint
on the solution, with an improved ability to explore regions with a low prior probability. The
source PDF is well constrained for all of the chain lengths as it is a very small region.
Figure 8.18: Lune plot of the variation in marginalised source PDF for a synthetic source with
different chain lengths for the Markov chain Monte Carlo algorithm. The synthetic source (Event B
in Fig. 8.13) is an opening crack.
As with the previous example, the only difference between the trans-dimensional McMC
approach (Fig. 8.19) and the normal McMC approach (Fig. 8.18) is the acceptance rate,
as none of the double-couple jumps were accepted. However, unlike the previous example,
the lower acceptance rate does not seem to improve the sampling of the PDF, since it is so
sharply peaked.
Figure 8.19: Lune plot of the variation in marginalised source PDF for a synthetic source with
different chain lengths for the trans-dimensional Markov chain Monte Carlo algorithm. The synthetic
source (Event B in Fig. 8.13) is an opening crack.
8.4 Estimating the Probability of Double-Couple Source
The trans-dimensional sampling algorithm can also be used to estimate the probability of
a double-couple source, by comparing the proportion of the chain in the different models
(Green, 1995; Sambridge et al., 2013). Fig. 8.20 shows that this approach is consistent with
the estimates obtained from the Bayesian evidence estimation (Section 4.2.6) using a Monte
Carlo sampling approach. In some cases, the solutions can be dominated by a few very large
probability solutions, leading to inaccuracies in the estimates. The second and third synthetic
sources in Fig. 8.20 show this uncertainty. These estimates require much larger sample sizes
to produce a stable estimate.
Apart from the two synthetic sources dominated by a few very high probability samples,
the two approaches agree well on the estimates of the double-couple model probability. The
estimates from the trans-dimensional approach seem to improve as the chain length increases.
The estimates from the trans-dimensional approach also take longer than the random sampling
approach to produce2, although the resulting marginalised source PDFs (Fig. 8.9) provide
a much clearer understanding of the probability values. However, when location and model
uncertainty is included in the inversion, the McMC approaches are often faster to reach
comparable sampling rates.
The McMC approach cannot easily be used to estimate the double-couple model probab-
ilities, unlike both the random sampling and trans-dimensional approaches.
2Around three times longer for comparable sampling densities, such as 5108 samples and 5106 samples
respectively, as shown in Figs 8.7, 8.10 and 8.11
Krafla
Synthetics
Number Samples
Figure 8.20: Comparison of the trans-dimensional and Bayesian evidence (random sampling) based
estimates of the probability of a double-couple source type with number of samples (logarithmic
scale). The values are calculated for various real and synthetic sources. The first column shows
synthetic sources (The solutions shown in Figs 8.7 and 8.9 are shown in the top left plot), and the
second column shows real earthquakes from the Krafla volcano in Iceland. The blue points
correspond to the Bayesian evidence estimate of the posterior double-couple model probability, and
the red points the trans-dimensional algorithm, both evaluated for uniform priors on the two models.
The total number of samples are used for the random sampling approach, and the total chain length
for the trans-dimensional approach.
8.5 Relative Moment Tensor Inversion Search Algorithms
Chapter 7 proposed an extension to the Bayesian framework to include relative data between
co-located events. The algorithms to explore this joint PDF for multiple events can be
developed from those described above. The resultant joint PDF can be marginalised for
individual events by merely ignoring the dependence on the others (cf. Fig. 4.7).
Inverting for multiple events increases the dimensions of the source space for each event.
This leads to a much reduced probability of obtaining a non-zero likelihood sample, because
sampling from the n-event distribution leads to multiplying the probabilities of drawing a
non-zero samples, resulting in sparser sampling of the joint source PDF.
Random sampling of the joint PDF (Fig. 8.21) shows that many samples are required to
produce good sampling of the PDF, with 1011 samples only just having a significant number
of non-zero samples to represent the source PDF. This is expected since 10 samples in each
coordinate for two events would correspond to 1010 samples of a two-event joint PDF. In fact,
the results in section 8.3 suggest that 1015 samples are required to generate a good sampling
of the joint PDF, which can lead to very long durations (Fig. 8.22).
Figure 8.21: Lune plot of the variation in marginalised source PDF for a Monte-Carlo random
sampling inversion including P-relative amplitudes for two events. The top row corresponds to the
first event and the bottom row shows the second event. There were 10 overlapping stations between
the events used for the relative amplitude inversion.
The elapsed time for the random sampling (Fig. 8.22) is longer per sample than the
individual sampling shown in Fig. 8.11, and longer than the sampling for both events due to
evaluating the relative amplitude PDF. Moreover, increasing the number of samples 10-fold
raises the required time by a factor of 10, requiring somemethod of reducing the running time
for the inversion, since, given current processor speeds, 1015 samples would take many years
to calculate on a single core. As a result, more intelligent search algorithms are required for
the full moment tensor case.
0 5 10 15
Number Random Samples
0 5 10 15
Markov Chain Length
Figure 8.22: Elapsed Time for different sample sizes of the random sampling and McMC algorithms
for the relative amplitude joint inversion. See also Fig. 8.11.
Markov chain approaches are less dependent on the model dimensionality. To account
for the fact that the uncertainties in each parameter can differ between the events, the Markov
chain shape parameters can be scaled based on the relative non-zero percentages of the
events when they are initialised. The initialisation approaches also need to be adjusted to
account for the reduced non-zero sample probability, such as by initialising the Markov chain
independently for each event. The trans-dimensional McMC algorithm needs to allow model
jumping independently for each event.
Figure 8.23: Lune plot of the variation in source PDF for a Markov chain Monte-Carlo sampling
inversion including P relative amplitudes for two events. The top row corresponds to the first event
and the bottom row shows the second event. There were 10 overlapping stations between the events
used for the relative amplitude inversion.
Tuning the Markov chain acceptance rate is difficult, as it is extremely sensitive to small
changes in the proposal distribution widths, and with the higher dimensionality it may be
necessary to lower the targeted acceptance rate to improve sampling. Consequently, care
needs to be taken when tuning the parameters to effectively implement the approaches for
relative amplitude data.
Fig. 8.23 shows that the Markov chain provides a much improved sampling of the joint
PDF compared to the random sampling approaches, especially at lower chain lengths than the
equivalent random sampling. Moreover the time required for 106 samples is much improved
compared to the random sampling approach (Fig. 8.22). The acceptance rates for these
solutions varied more, between 0.02 and 0.17, although all within the targeted range.
Figure 8.24: Lune plot of the variation in source PDF for a trans-dimensional Markov chain
Monte-Carlo sampling inversion including P relative amplitudes for two events. The top row
corresponds to the first event and the bottom row shows the second event. There were 10 overlapping
stations between the events used for the relative amplitude inversion.
Chain Length Event 1 pDC Event 2 pDC
1 103 0.0 1.0
2 103 1.0 0.42
5 103 1.0 1.0
1 104 1.0 0.80
5 104 1.0 1.0
1 105 0.78 0.99
5 105 0.98 0.97
1 106 0.95 0.97
Table 8.1: Trans-dimensional pDC values for the
solutions shown in Fig. 8.24
The trans-dimensional McMC solutions
are largely double-couple, with strong es-
timates of double-couple probability (Table
8.1) for both of these events, consistent
with the synthetic double-couple source.
The resultant source PDFs are nearly all
strongly spiked at the double-couple point
(Fig. 8.24). The acceptance rate for these
examples was mainly less than 0.1, which
introduces a requirement for longer chain
lengths compared with the single event solu-
tions.
The Bayesian posterior model probabilities estimate from the random sampling solutions
corresponds to the probability that both events are double-couple. Estimating the individual
model probabilities using the Bayesian posterior model probabilities cannot easily be done
as it would require evaluation of all combinations of the two models for the events. For
two events, this means evaluating the mixed models, such as event one constrained to the
double-couple, while event 2 is a full moment tensor source, adding further to the required
time. From these values it is possible to evaluate the individual probabilities, although this
requires many samples and is very time consuming, unlike the trans-dimensional approach.
8.6 Event Classification
It can be time-consuming to carry out the full inversion, either with random sampling or
Markov chain approaches (Figs 8.11 and 8.12), so an estimate of how well-constrained an
event is can help to decide whether to carry out the inversion. The percentage of non-zero
likelihood samples during random sampling is approximately constant, even for small sample
sizes, as seen in Fig. 8.25.
Number Random Samples
Figure 8.25: Percentage of non-zero
samples for the full moment tensor (12.7%)
and the constrained double-couple model
(18.6%) for different random sample sizes
(logarithmic axis).
0 50 100
Desired nonzero percentage cutoff
Figure 8.26: Actual cut-off values compared
to desired cut-off values for different sample
sizes: N = 1000 (red), N = 5000 (green),
N = 10 000 (blue), N = 40 000 (yellow),
and N = 1 000 000 (purple). The black line
shows when the actual rate equals the desired
rate.
An accurate estimate of the non-zero likelihood percentage requires sufficient samples.
This can be estimated by considering a binomial distribution with x the probability of getting
a non-zero sample, for n samples of which r are non-zero:
p (data|x) =
r (n r)
xr (1 x)nr . (8.24)
Following Bayes theory (Eq. 4.1), the posterior distribution for x depends somewhat on
the prior information for x, the simplest prior is the uniform prior between 0 and 1
p (x) =
1 0 6 x 6 1
0 otherwise
, (8.25)
which leaves the posterior as the likelihood. As Sivia (2000) shows, this prior does not have
a large effect on the results for large sample sizes, so the choice of a uniform prior does not
introduce any strong bias.
The values of n and r are likely to be sufficiently large that direct calculation of their
factorials is impossible, the natural logarithm of the probability is considered instead. Using
Stirlings approximation (Stirling, 1730)
ln (n) = n ln (n) n+O (ln (n)) , (8.26)
gives the probability
p (x|data)  Cen ln(n)r ln(r)(nr) ln(nr)+r ln(x)+(nr) ln(1x), (8.27)
with C determined from the normalisation constraint.
An event can be discarded if the probability that the value of x is above the cut-off is
greater than some value, for example five-sigma accuracy could be desired so an event is
discarded if the probability that x is above the cut-off is greater than 0.9999997. For low
sample sizes the true value of x that is cut-off may not be close to the desired value. Fig. 8.26
shows the relationship between the actual and desired cut-off values. Consequently, 40 000
samples should be a sufficient number to estimate the non-zero likelihood percentage and the
event quality.
Other factors can affect the non-zero likelihood percentage, so this is not always a good
indicator, especially if the polarity probabilities described in Chapter 5 are used. These have
very high non-zero likelihood percentages even if the event is well constrained, making the
use of such a cut-off impossible.
8.7 Summary and Discussion
There are several different approaches to sampling for the forward model. The Monte Carlo
random sampling is quick, although it requires a large number of samples to produce good
samplings of the PDF. The McMC approaches produce good sampling of the source for far
fewer samples, but the evaluation time can be long due to the overhead of calculating the
acceptance and generating new samples. Furthermore, these approaches rely on achieving
the desired acceptance rate to sufficiently sample the PDF given the chain length.
Including location uncertainty increases the random sampling time drastically, and has
less of an effect on the McMC approaches.
The solutions for the different algorithms and different source types are consistent, but
the McMC approach can be poor at sampling multi-modal distributions if the acceptance rate
is too high.
The two different approaches for estimating the probability of the double-couple source
type model being correct are consistent, apart from cases where the PDF is dominated by a
few very high probability samples. For these solutions, the number of random samples needs
to be increased sufficiently to estimate the probability well. The McMC approach cannot
easily be used to estimate the double-couple model probabilities, unlike both the random
sampling and trans-dimensional approaches.
Extending thesemodels to relative source inversion leads to a drastically increased number
of samples required for theMonte Carlo random sampling approach that it becomes infeasible
with current computation approaches. However, the McMC approaches can surmount this
problem due to the improved sampling approach, but care must be taken when initialising
the algorithm to make sure that sufficient samples are discarded that any initial bias has been
removed, as well as targeting an acceptance rate that does not require extremely large chain
lengths to successfully explore the full space.
The quality of an event can be estimated from the non-zero percentage in the random
sampling algorithm. This is consistent even for very low sample sizes, but there are other
effects that can increase the non-zero probability even if an event is well constrained.
Although these approaches are not the only methods for forward model sampling, they
encompass two types of approach, random sampling and directed sampling. For ordin-
ary source inversion with no location or velocity model uncertainty included, the random
sampling approach seems to be better than the McMC approaches, due to the additional
calculations required for accepting the sample and generating a new sample. It is likely that
this difference will hold for other directed sampling algorithms, which usually have some
additional computation steps between evaluating the forward model. In contrast, when loc-
ation and velocity model uncertainty are included, the McMC approaches seem to produce
results on a comparable time-scale, although they are not as easily parallelised, so depending
on the available computation power, may also be slower. Again, this is likely to hold for
other directed sampling algorithms. If the source inversion includes relative data, then the
random sampling approach quickly becomes prohibitively long for running the inversion with
sufficient numbers of samples, additionally it is not possible to estimate the posterior model
probabilities using this method, as each possible permutation of model types for the set of
events must be evaluated, so that the individual model probabilities can be estimated for each
event. However, the McMC approaches produce better results on a shorter time-scale, and
do not require as many samples, so are a better algorithm for this inversion. Additionally, the
trans-dimensional algorithm also provides independent estimates of the model probabilities
for the two events from a single inversion.
An alternative approach, such as that described by Kaeufl et al. (2013), using neural
networks to produce rapid solutions is a possible sampling method. However, this depends
on both the ability to construct a suitable training set to teach the network, which may differ
between events due to varying network geometry, requiring large initial computational costs.
Nevertheless, such an approach is an alternate method to produce samples of the posterior
PDF from Chapter 4, which is a possible future development to produce near-real time
analysis of the events.
9 MTINV - Source Inversion from
Surface Data
This chapter presents an overview of the MTINV code developed in conjunction with this
thesis, and applies it to surface data from a local seismic network in the Askja and Krafla
regions of Iceland. Relative amplitude inversion is used on a pair of synthetic events,
generated using a finite difference approach (Bernth and Chapman, 2011), and a pair of
events that have been investigated by White et al. (2011).
9.1 MTINV
The Bayesian source inversion approach described in Chapters 4, 5 and 7, has been imple-
mented as a Python1 module called MTINV, using the sampling algorithms described in
Chapter 8. Python was chosen as the development language due to its clean syntax and easy
development, and the plethora of existing standard and third party libraries for scientific com-
putation such as numpy and scipy. To improve the runtime and memory usage during
the likelihood calculations (Eqs. 4.53 and 4.54) the Cython compiler2 (Behnel et al., 2011)
is used. Cython is an optimising static compiler for Python and the Cython language, which
supports C functions and C types, helping the compiler generate efficient C code, speeding
up the execution of the Python code.
The random sampling algorithm (Section 8.1) is embarrassingly parallel. This means
that the forward model calculation for each model moment tensor is independent of the
others, so the approach can easily be parallelised. This parallelisation is carried out using
two approaches, MPI and the Python multiprocessing module. The Python implementation
of MPI used is mpi4py3. This allows the code to be run across as many workers as desired
(usually 1:1 ratio with the number of available processors), and merely combines the results
from each worker after each iteration step. For processors on the same physical host, the
Python built-in module multiprocessing allows a queue based system to be used to task
jobs to workers, however this approach only works on the physical machine4.
1Python Language Reference, version 2.7 by the Python Software Foundation. Available at
http://www.python.org.
2Available at http://cython.org.
3mpi4py by Lisandro Dalcin. Available at http://mpi4py.scipy.org/.
4Although the module reports all the available processors across all the nodes on a multi-nodal environment.
http://www.python.org
http://cython.org
 http://mpi4py.scipy.org/
The polarity and polarity probability likelihoods (Eqs 4.9 and 5.10), are relatively simple,
but the ratio PDF is more complex, so using a numerical approximation may provide a large
time speed-up for this approach. Fig. 4.4 suggests that the Gaussian would be a suitable
approximation at low denominator uncertainties, but the errors are still quite large even if the
uncertainty is less than 1%. While this may not have a large effect on the result, this deviation
could lead to very different estimates of the statistics of the solution.
The approximation of the ratio PDF as a Gaussian (Eq. 4.20) has little effect at a signal-
to-noise ratio (SNR) of 10 (Fig. 9.1), but the SNR=5 case shows a shift in the double-couple
orientation and a non-double-couple moment tensor solution, unlike the true PDF. Evaluating
the approximate PDF (Eq. 4.20) is around 1.5 times faster than the full ratio PDF, so may be
worthwhile in cases where it is valid.
Figure 9.1: Lower hemisphere equal area projections, and marginalised lune plots (Section 2.5.6) of
solutions with and without the Gaussian approximation for the ratio PDF (Eqs 4.27 and 4.20) for two
noise levels, SNR=10 (top row) and SNR=5 (bottom row).
Additionally, due to the embarrassingly parallel nature of many of the calculations,
a significant speed-up could be gained from using the graphical processing unit (GPU).
Modern GPUs have tools for parallel GPU computation, such as CUDA and OpenCL, both
of which have available Python modules5, as well as C++ based run-times. This could be
used to parallelise many of the CPU limited calculations, since modern GPUs can have many
available cores enabling many more floating point operations per second (FLOPS)6.
The MTINV approach has been used by Wilks et al. (2015) to invert data obtained
from shallow boreholes and a deep monitoring array during a multi-stage Fayetteville shale
stimulation. These solutions show the difficulty in constraining the source from borehole
data, leading to often large uncertainties in the resultant source PDF. Greenfield and White
5PyCUDA. Available at http://mathema.tician.de/software/pycuda/.
PyOpenCL. Available at http://mathema.tician.de/software/pyopencl/.
6NVIDIA Tesla K80 accelerator can achieve 2.91 teraFLOPS (http://www.nvidia.com/object/tesla-
servers.html), in comparison to the IntelCoreTM i7-3930 which has a theoretical maximum of 182 gigaFLOPS
(http://www.intel.com/support/processors/sb/CS-032814.htm).
http://mathema.tician.de/software/pycuda/
http://mathema.tician.de/software/pyopencl/
http://www.nvidia.com/object/tesla-servers.html
http://www.nvidia.com/object/tesla-servers.html
http://www.intel.com/support/processors/sb/CS-032814.htm
(2015) also used the MTINV module to invert deep earthquakes beneath the Askja volcano
in Iceland.
9.2 Inversion of Surface Data from Iceland
Surface data collected from the Askja and Krafla regions in Iceland, as part of an ongoing
research project with a large seismometer deployment, have also been inverted using this
approach. These events were detected using the coalescence microseismic mapping approach
(CMM) Drew et al. (2013), which uses STA/LTA arrival windowing to identify possible
arrivals, which are then migrated into the subsurface to see if the energy coalesces at a
hypocentre, identifying whether the arrival originates from an event or is just noise. The
arrivals were manually refined and the polarities determined. The events were located using
NonLinLoc, and the arrival-time picks were used to construct windows to automatically
measure the arrival amplitudes.
9.2.1 Krafla
The inversion approachwas used on four local seismic earthquakes from theKrafla geothermal
region of Iceland, investigated as part of an undergraduate research project (Watson, 2013).
These events were at shallow depths ranging from 0.9 km to 1.6 km and part of a larger
group detected on a temporary network between 2009 and 2010. The inversion was carried
out using both the constrained double-couple and unconstrained full moment tensor source
models, using P polarities only, as the S arrival amplitudes could not be reliably estimated.
The location uncertainties from NonLinLoc were included in the inversion (Eq. 4.53),
because the depth was not well-constrained, leading to a large variation of possible take-off
angles (Fig. 9.2).
Fig. 9.2 shows both the range of double-couple solutions and the unconstrained full
moment tensor inversions for these events. The double-couple results for the events B-D have
quite large probability ranges and, even though the full moment tensor solutions do not peak
at the double-couple point, they still have a significant value there. These solutions resemble
those for synthetics with reasonable network coverage (Fig. 6.11).
Event A is clearly non-double-couple and, although it is possible to fit some double-
couple solutions to this source, these are very low probability, with very significant misfit
from the observed data. The full moment tensor solutions show a peak near the closing tensile
crack point, with a large region of very-low-probability solutions reaching the double-couple
point. The full moment tensor source PDF is clearly different from the other three solutions,
which are described well by a double-couple source type, when the effects of uncertainty are
considered.
The orientation of the full moment tensor for event A is poorly-constrained, although
this is to be expected as it is not possible to constrain the orientation of a tensile crack from
polarities alone. However, the orientation of the full moment tensor solutions for the other
events (B-D) is much better constrained.
Figure 9.2: Lower hemisphere equal area projections, marginalised lune plots, and marginalised
distributions of the full moment tensor, h, and  parameters of four events from the Krafla
geothermal region of Iceland, inverted using manual P polarity picks only. The first column shows
the station ray positions on the focal sphere for the NonLinLoc location PDF, coloured according to
the probability of the ray path (darker colour is more probable) and for the observed polarity with red
signifying a positive polarity, yellow no polarity and blue a negative polarity. The second column
shows the source PDF for the solution constrained to be double-couple only and the position of the
stations on the focal sphere the maximum likelihood location. The third column shows the source
PDF for the full moment tensor solution. The fourth, fifth and sixth columns show the marginalised
distributions of the , h, and  parameters for the full moment tensor solutions, respectively.
Manually picked station first motions are given by red (up) or blue (down) markers. For the focal
sphere plots, possible fault planes are given by dark lines. The most likely fault planes are given by
the darkest lines. For the Hudson plots, high probability is red and low probability is in blue. The
posterior model probabilities for the double-couple source model are given in Table 9.1.
Event pDC IBDC  IBMT IAMT  IADC
A 2 103 4.3 6.4
B 0.84 4.1 3.9
C 0.81 4.9 3.8
D 0.73 4.1 3.9
Table 9.1: Posterior model probabilities and BIC
and AIC
values (Section 2.8.2) for the Krafla
events shown in Fig. 9.2. The AIC is defined such that
a negative difference indicates stronger evidence for the
first model, so the AIC column is the difference
between the moment tensor and double-couple model,
so that the values can easily be compared with the BIC.
The posterior model probabilities
for the double-couple source model are
consistent with this interpretation with
a very low value (2 103) for Event A
and values bigger than 0.7 for the oth-
ers (Table 9.1). Consequently, events
B-D can be described best by a double-
couple source, but event A is described
better by a closing-crack source.
Section 2.8.2 shows three different
ways to estimate the source model, the
Akaike information criterion (AIC) (Akaike, 1974), which was extended to give the Bayesian
information criterion (BIC) by Schwarz (1978), and the Bayesian evidence estimate. Both the
AIC and BIC only require maximum likelihood estimates to calculate (Eqs 2.109 and 2.110),
which can be obtained from the full source PDF solution. The posterior model probabilities
from the Bayesian evidence require the full un-normalised source PDF (Eq. 2.111).
Table 9.1 also shows the evaluation of these different approaches for the double-couple
model, which suggest there is evidence for the double-couple model for events B-D and
for a non-double-couple model for event A. However, the values of the AIC and BIC, and
the associated differences are not as informative as the posterior values from the Bayesian
evidence, which provide a more quantitative estimate of the support for the different model
types.
9.2.1.1 Polarity Probabilities
These solutions have also been inverted using polarity probabilities (Chapter 5), producing
very similar results, although there is a slightly larger range of solutions for each event. The
polarity probabilities were estimated using a Gaussian arrival-time PDF around the manual
Event pDC IBDC  IBMT IAMT  IADC
A 2 103 2.21 4.7
B 0.52 4.8 2.7
C 0.72 6.0 3.9
D 0.54 7.3 4.0
Table 9.2: Posterior model probabilities and BIC(
and AIC
values (Section 2.8.2) for the
Krafla events shown in Fig. 9.3. See also Table 9.1.
time pick.
The posterior double-couple model
probabilities (Table 9.2) for the solu-
tions are lower probability than for the
manual polarity picks (Table 9.1). The
information criterion are broadly the
same, although the BIC is mostly lar-
ger compared with the manual polarity
information, partially due to the larger number of observations for polarity probability in-
formation. This increase does not reflect the larger uncertainty in the distributions, unlike the
posterior model probability.
The orientation of the solutions are mostly similar to those in Fig. 9.2, apart from event
D, which shows a very different double-couple source orientation. The full moment tensor
orientation parameters are broadly similar between the two sets of solutions for events A and
C, but the distributions for events B and D differ, due to the increased range of possible source
types.
Figure 9.3: Lower hemisphere equal area projections, marginalised lune plots and marginalised
distributions of the , h, and  parameters of four events from the Krafla geothermal region of
Iceland, inverted using P polarity probabilities only. posterior model probabilities for the
double-couple source model are given in Table 9.2. See also Fig. 9.2.
9.2.2 Upptyppingar
White et al. (2011) investigated events caused by a dyke intrusion beneath the Upptyppingar
volcano in Iceland. They identified a pair of co-located events (2007/07/06 20:47 and
2007/07/06 20:52) close in time with apparent opposite mechanism. This pair of events were
inverted using MTINV with P-wave polarities and P/SH and P/SV amplitude ratios, both
with and without inclusion of the location uncertainty. Both events were located from the
arrival-time picks used byWhite et al. (2011) using NonLinLoc, and the P and S arrival times
were used to automatically window and measure the P/SH and P/SV amplitude ratios.
Figure 9.4: Lower hemisphere equal area projections, and lune plots (Section 2.5.6) of the example
events from Upptyppingar. Event A corresponds to the 2007/07/06 20:47 event and B the 20:52
event. These solutions included both P polarities and P/SH and P/SV amplitude ratios but no
location uncertainty. The red dashed lines are the solutions from White et al. (2011).
Figure 9.5: Lune plots of the
marginalised solutions for different
amplitude ratios for the events shown in
Fig. 9.4. Column i. shows the solutions
for P/SH amplitude ratios and P polarities,
and column ii. shows the solutions for
P/SV amplitude ratios and P polarities.
The solutions for both events without location
uncertainty (Fig. 9.4) are well constrained, although
event A has a strong posterior model probability for
the full moment tensor (pDC = 1 1012), while
B has pDC = 0.98. This may well be linked to
the amplitude ratios, as Fig. 9.5 shows that the
observations are not consistent with each other, as
they peak around very different source-types.
Each of the double-couple solutions are consist-
ent with those shown in White et al., with a slight
difference, possibly due to the use of a different
location method (NonLinLoc). When the location
uncertainty is included (Fig. 9.6), the solutions re-
main consistent, and the full moment tensor solu-
tions, although less constrained are peaked around
the double-couple point. This is reflected in the
posterior model probabilities of pDC = 0.82 for event A and pDC = 0.83 for event B, for
the polarity-only inversion, suggesting both are double-couple solutions. However, including
amplitude ratios results in pDC = 0 for event A and pDC = 0.98 for event B.
There are minimal variations between the posterior model probability estimates with and
without location uncertainty, which when combined with the very similar solutions, suggest
that the location uncertainty in these examples has little effect on the solution, unlike in the
previous examples from Krafla.
Figure 9.6: Lower hemisphere equal area projections, and marginalised lune plots (Section 2.5.6) of
the example events from Upptyppingar. See also Fig. 9.4. These solutions included both P polarities
and P/SH and P/SV amplitude ratios as well as location uncertainty. The first column shows the
receiver distribution with the location uncertainty.
Polarity Polarity and Amplitude Ratios
Event No Location Location No Location Location
A 0.82 0.82 0 0
B 0.83 0.83 0.97 0.98
Table 9.3: Posterior model probabilities for the Upptyppingar events shown in Fig. 9.4 and 9.6.
9.3 Relative Amplitude Inversion
Chapter 8 has shown that random sampling using relative amplitude data (Chapter 7) is
possible, although it requires large numbers of samples. Fig. 9.7 shows the difference
between including relative amplitude data and not on the solution PDF. The constraint on
the inversion is noticeably improved compared to the inversion without relative amplitudes,
although the scale factor estimates are skewed towards 1.2 rather than the true value of 1.
This could be caused by any deviation in the P amplitude estimates, due to the noise (Section
3.3), possibly affecting the scale factor distribution.
The Upptyppingar events (Section 9.2.2) appear to be co-located, so are prime candidates
for the relative inversion approach. The probability that these events were co-located (Eq.
7.8) is non-zero (2 107). Although this is non-zero, it is low due to some uncertain
arrival-time picks making the multi-variate Gaussian wide for the 27 P-S arrival-time gap
pairs. This non-zero probability, along with the similar waveforms observed by White et al.
(2011), indicates that the events are co-located and so relative data can be used to help
constrain the solutions.
Figure 9.7: Lower hemisphere equal area projections, marginalised lune plots, and relative seismic
moment distributions of the example events from Section 8.5, with 10 overlapping stations. The first
two columns show the double-couple constrained source and the full moment tensor source
distribution without relative amplitudes, the second two show the solutions from the relative
inversion using P amplitudes. The last two columns show the distribution of the relative seismic
moment, with the true value given by the blue line.
Figure 9.8: Lower hemisphere equal area projections, marginalised lune plots, and relative seismic
moment distributions, generated using random sampling, of the example events from Upptyppingar
(Fig. 9.4), with 17 overlapping stations. See also Fig. 9.7. These solutions included P polarities,
P/SH and P/SV amplitude ratios, and relative P amplitude ratios, but no location uncertainty. The
full moment tensor solutions for the relative inversion were estimated using a Markov chain Monte
Carlo approach.
Fig. 9.8 shows a comparison of the solutions for these events, using the relative amplitude
ratios leads to a rotation in the double-couple orientation for event A, but little change for event
B. Both solutions are more constrained compared with the relative amplitudes included, but
remain consistent with the non relative examples. The relative seismic moment is consistently
peaked around 0.4, suggesting event B is slightly larger magnitude than event A. No prior was
included in this example, as no information suitable for constructing a log-normal prior was
known, and the effect of the truncated Gutenberg-Richter prior (Eq. 7.44) was negligible,
even for a wide range of possible b-values because the distribution is sharp.
The double-couple relative solutions also show very clearly that these solutions are almost
directly inverted, consistent with the interpretations inWhite et al. (2011) that the events may
be linked to the breaking of a plug in the dyke. However, it is also possible that the
mechanisms could be linked to fractures perpendicular to the dyke similar to those observed
by Rutledge and Phillips (2003), Rutledge et al. (2004) and Rutledge et al. (2015).
9.4 Summary and Discussion
The results from inverting real data, using the MTINV code, are consistent with previous
inversions, and produce a good estimate of whether the event is double-couple or not.
Although this code has been optimised somewhat, there are still several methods that could
speed up the inversion further, which require more development.
The results from the Krafla region show a clear contrast between double-couple and
non-double-couple solutions, in this case, a closing crack. Amplitude ratios were not used
for these inversions due to the difficulty in measuring the arrivals. The solutions using
polarity probabilities are very similar to those for manual polarities only. The pDC values for
the double-couple events (B-D) are reduced, but still significant. This shows that polarity
probability measurements can be used to constrain the source in real data examples.
The results from the different approaches for estimating the correct model largely agree,
although the posterior double-couple model probability provides a much more easily under-
stood value than either the Bayesian information criterion or Akaike information criterion.
This helps to understand the likely models, and can be used to conduct a hypothesis test on
whether the double-couple model is valid. For the Krafla events, it is valid to a significance
of p = 0.01 for events B-D and not for event A.
The Upptyppingar events show very similar double-couple orientations, which agree with
the solutions shown in White et al. (2011). The full moment tensor solutions show some
deviation for event A, although this could be attributed to error in the amplitude measurement
(Section 6.3), as the results are less consistent with the polarity-only inversion. It is difficult
to characterise whether this deviation is due to uncertainties or reflects the true source,
highlighting one of the major issues that can arise when using amplitude ratios. As shown
in Section 6.3, for medium SNR values the amplitude ratios can be well constrained about
the incorrect source. Consequently, although the posterior model probabilities in Table 9.3,
suggest that event A is non-double-couple when amplitude ratios are included, this is unlikely
to be true.
Relative inversion produces improved constraint of the source, in both the double-couple
and full moment tensor solutions, with good constraint of the synthetic source around the
true double-couple point. The scale factor estimates appear to be slightly large compared to
the true value, which may again be linked to uncertainties in the amplitude measurements.
The improvement in the source constraint cannot easily be tested using the posterior model
probabilities as the value corresponds to the joint PDF, so it is necessary to evaluate the cross
models, such as double-couple constrained for one event and full moment tensor for the other.
The relative inversion of the Upptyppingar events produced a very sharply constrained
double-couple constrained source, and a similar distribution to the full moment tensors
solutions including amplitude ratios. The scale factor estimates suggest that event B (20:52)
has a larger moment than event A (20:47). The results from the relative inversion also show
that the two mechanisms are almost directly inverted.
Inversion of the Upptyppingar events shows that care must be taken when including
amplitude ratios as these can lead to deviations from the true source and possible spurious non-
double-couple sources. One of the events has a significant non-double-couple probability,
which can probably be described as a deviation due to errors in the amplitude ratios from the
noise (Chapters 3 and 6). This deviation is caused by an underestimation of the uncertainty
in the amplitudes, because there is a systematic error due to the effect of the noise on the
measurement approach (Section 3.3). This error is difficult to account for, suggesting that
amplitude ratios may not be a reliable data type for source inversion. Perhaps examining
the source PDFs for the individual amplitude ratios to see if they are consistent, along with
testing if variations in the uncertainty of the measurement lead to similar solutions with
less constraint or drastically different solutions, can provide some qualitative estimate of the
confidence in the data. However, this requires a lot of inversions to be carried out so may
only be practical for specific events, rather than a whole data set.
Relative amplitude data improve the source constraint, but large numbers of samples are
required. The distribution of the relative moment in a synthetic test peaks slightly away from
the true value, but this may be linked to the amplitude measurement errors. The relative
amplitude inversion of data from the Upptyppingar dyke intrusion shows two events with an
almost exactly inverted mechanism, consistent with the interpretation of a breaking plug
in White et al. (2011), but these events could also be interpreted as motion perpendicular
to the dyke in the direction of opening (Rutledge and Phillips, 2003; Rutledge et al., 2004,
2015). Both are indicative of melt motion, and even though White et al. (2011) find a large
population of events with a fault plane consistent with the dyke orientation, this would also
hold for events with motion perpendicular to the dyke plane, consequently there is little
difference between the two possible mechanisms.
Source inversions often result in non-double-couple sources, requiring a different physical
mechanism from the well-understood description of slip on a fault-surface. However, these
solutions are often limited by uncertainty in the observations, little knowledge of the velocity
model, and a poor distribution of receivers on the focal sphere. As a result, these non-double-
couple sources should often be considered spurious (e.g. White et al., 2011). This can be
tested using complex synthetic tests (len and Milev, 2006; Vavryuk et al., 2008; len,
2009), but these are not particularly computationally efficient as they must be evaluated
separately to the source inversion. Instead, using the source PDF, as described in Chapter 4,
and evaluating the Bayesian model evidence for the double-couple and full moment tensor
solution provides a better quantitative estimate of whether the source is double-couple or
not. This approach is consistent with the Akaike and Bayesian information criteria (Akaike,
1974; Schwarz, 1978), but the resulting value corresponds directly to the posterior model
probability, so can also be used as a test of the hypothesis that an event is non-double-couple.
A possible future development could be to reduce the sometimes long run times for
this approach using a neural network approach, especially if the training set can be made
independent of the receiver geometry.
10 Conclusions
Seismic source inversion, particularly of microseismic events, often produces non-double-
couple sources, requiring a different physical mechanism from the well-understood descrip-
tion of slip on a fault-surface. However, these solutions are often limited by uncertainty in
the observations, little knowledge of the velocity model, and a poor distribution of receivers
on the focal sphere. As a result, some of these non-double-couple sources are considered to
be spurious (e.g. White et al., 2011), which can be tested using complex, time-consuming
synthetic tests (len and Milev, 2006; Vavryuk et al., 2008; len, 2009).
This thesis has introduced a new approach to Bayesian source inversion, consistent with
the polarity inversion ofWalsh et al. (2009), but including the fullmoment tensor sourcemodel
and including other data-types, such as amplitude ratios. This approach enables the posterior
probabilities for different source models to be calculated, giving a quantitative evaluation of
the different source models and allowing an estimate of whether the source is double-couple
or not. Three approaches for sampling the full source PDF have been examined, using Monte
Carlo sampling andMarkov chains, although there are many other possible search algorithms.
Additionally, a new method for measuring first motion polarities with a quantitative estimate
of the uncertainty has been described, and included in the source inversion approach. The
relative source inversion approach described in Dahm (1993, 1996) has been extended to
work in this probabilistic framework, including a new method for estimating the relative
seismic moment between pairs of events, as well as allowing the relative data to be combined
with the absolute data. This probabilistic approach to source inversion allows the model type
to be quantitatively estimated, although the approach depends on the methods for forward
model sampling, but both are consistent with each other.
The symmetric moment tensor is the common description of earthquake fault sources,
conserving both linear and angular momentum. Often further constraints are added, restrict-
ing the source to a subset of the full moment tensor space, such as the double-couple model,
describing slip on a fault plane. These constraints can counteract the improved fit that arises
from having more free parameters in the full moment tensor model.
Whilst the moment tensor is commonly used as a description of the source, it describes
only the radiation pattern, so it needs to be related to a suitable physical model to estimate
the fault parameters. There are several physical models, including the double-couple model
corresponding to shear in an elastic isotropic medium (Maruyama, 1963; Burridge and
Knopoff , 1964), or the more complex crack + double-couple model describing shear plus
some opening in an elastic isotropic medium (Aki and Richards, 2002; Vavryuk, 2001;
Minson et al., 2007; Tape and Tape, 2013).
Instead of relating the moment tensor to a physical model, it often is decomposed into
different components. This is non-unique for the deviatoric component, resulting in many
different types of source decomposition (Jost and Herrmann, 1989; Riedesel and Jordan,
1989; Hudson et al., 1989; Chapman and Leaney, 2011; Tape and Tape, 2012b). Few of
these correspond to any physical source model, and can complicate the source description.
To understand the source-type and orientation, it can be necessary to account for the elastic
parameters of the source. In an isotropic medium, the elastic parameters do not change the
orientation or source-type, so can be ignored, but for a source in an anisotropic material
these effects can be very large. The moment tensor corresponds to the tensor product of the
potency tensor, describing the physical source-type and orientation, and the elastic stiffness
tensor at the fault surface. These elastic parameters are complicated by the presence of the
fault (Al-Harrasi et al., 2011), deviating from the anisotropy of the bulk medium.
Different anisotropic elastic parameters can cause large differences between the moment
tensor and potency source interpretations, changing the source type and orientation, and so
care must always be taken to account for the elastic parameters when discussing the physical
fault parameters.
Several different observations can be used for source inversion, and the Bayesian frame-
work described in this thesis can be extended to use any combination, as long as the different
observations are independent, and a suitable likelihood can be estimated. The simplest obser-
vation that can be made is the polarity of the first-motion of the arrival, although the signum
function (Eq. 3.14) makes the source dependence non-linear, preventing any linear inversion
approaches from using this observation. Forward-modelling based approaches can use this
observation, often by evaluating the misfit to estimate the source. More complex observa-
tions such as amplitudes and amplitude ratios provide even more source discrimination, but
are very dependent on the background noise (Figs 3.7 and 3.11). These figures show that
estimates of the amplitude ratio can be uncertain and incorrect, possibly leading to spurious
non-double-couple sources.
Walsh et al. (2009) introduced aBayesian approach to double-couple source determination
using polarity observations. The approach derived in this thesis is consistent with theirs, but
can also include amplitude ratios and other observation types, as well as the full moment
tensor source model.
The amplitude ratio noise model is not Gaussian, as is often assumed, but depends on the
noise models of the two phases. If a Gaussian noise model is assumed for these, the amplitude
ratio noise model is given by the ratio PDF (Eq. 4.27), introduced by Fieller (1932), which
for high uncertainties, especially in the denominator, significantly deviates from the Gaussian
distribution.
ABayesian approach can incorporate uncertainties in both themodel and data, as nuisance
parameters, using Bayesian marginalisation (Eq. 4.2), removing the dependence of the
resultant source probability density function (PDF) on uncertainties in the measurements,
location, and velocitymodel. The results in this thesis support the conclusion that considering
the full source PDF rather than the maxima provides a better understanding of the source.
The choice of PDF representation can also affect the source interpretation, with the
marginalised source PDF plot indicating the most likely distributions of the individual para-
meters, while the silhouette plot shows the maximum value at each parameter. The choice
of parameterisation to represent the PDF is often made based on an assumption of equally
probable source areas (Hudson et al., 1989). Alternatively, a parameterisation which can
easily be interpreted, either by a physical meaning, or intuitive parameter definition, can be
chosen.
The posterior model probability of the different source models can be estimated from the
samples of the full source PDF (Eqs 2.111 and 4.55), although this requires sufficient samples
for aMonte Carlo integration to be valid. Unlike the Akaike and Bayesian information criteria
(Akaike, 1974; Schwarz, 1978), the resulting value corresponds directly to the posterior model
probability, so can also be used as a test of the hypothesis that an event is non-double-
couple. Therefore, the approaches for determining statistical significance levels, well known
in hypothesis testing, can be applied, similar to the use of the F-test by Horlek et al. (2010).
The resultant model probabilities help to distinguish between the different types of events
and, consequently, between different physical processes and source interpretations (Baig and
Urbancic 2010). Both the synthetic and real examples examined in this thesis show that the
posterior model probability is a good indicator of source type, agreeing with other approaches
for estimating the source type and visual interpretation of the source PDFs.
Uncertainties on the manual polarity observations are often poorly estimated and hard to
characterise, so are usually left as a user-determined parameter to indicate the confidence in
the polarity pick (Brillinger et al., 1980;Walsh et al., 2009). However, the uncertainty in the
polarity can be incorporated by evaluating the probability that the arrival polarity is positive
or negative, explicitly including the dependence on both the arrival noise and the arrival-time
pick, with their corresponding uncertainties.
The manual polarity pick uncertainty can also be included by treating the manual pick as
a prior for the polarity probabilities. This is similar to including a probability of a mistaken
pick, but means that if the manual polarity is in the incorrect direction, the resultant polarity
probability for the correct direction will be larger than the corresponding mistaken pick
probability. The polarity probabilities can differentiate between hard-to-determine and clear
arrival polarities far better than introducing a blanket probability of a mistaken pick (e.g.
Hardebeck and Shearer, 2002, 2003).
The polarity probabilities require little time to calculate and can be estimated auto-
matically, such as during automated event detection. This requires an accurate automated
arrival-time picker, rather than a broader detection algorithm. Nevertheless, the polarity
probabilities have less dependence on an arrival time pick than other automated polarity de-
termination approaches, which produce results with the binary classification used for manual
polarities (e.g. Baer and Kradolfer, 1987; Aldersons, 2004; Nakamura, 2004).
The automated polarities can also be used on phases requiring rotation of the seismogram
after the event has been located, such as SH- and SV-polarities, without requiring manual
re-picking of the arrival polarities. The example results are consistent with manually picked
arrivals, suggesting it may be a useful approach to provide more information for the source
inversion.
Uncertainties, such as background noise, network distribution, location uncertainty and
model uncertainty, can have a large effect on the source PDF. Noise has the largest impact
because it influences both the network distribution and the location uncertainty, and directly
affects themeasurement uncertainty. Consequently, high-noise levels increase the uncertainty
in the source PDF and also seem to lead to some deviation from the true source type as the
noise level increases, which is a possible cause of non-double-couple source interpretations.
Amplitude observations can have a systematic deviation due to the noise level, which
can also affect the ratio estimate, this leads to deviation from the true source-type, but does
not seem to affect the double-couple-constrained solutions as much. This error is not well
modelled by the uncertainty on the trace, but is somewhat evident when considering the source
distributions for the individual amplitude ratios. This uncertainty means that amplitude ratios
need to be used with care in a source inversion.
The network distribution has an effect on both the location determination and the source
distribution, but increasing the number of receivers does not always improve the results,
rather the constraint on the source must be improved. Poor network constraint can lead to
large uncertainties in the source PDF, especially when only using polarity information, which
often has little variation for different source types.
Uncertainty in the source location causes variation in the distribution of receivers on the
focal sphere, affecting any corresponding ray paths. This effect is often larger for shallow
events, as there is usually more take-off angle variation at shallow depths, which greatly
changes the resultant source type. Higher noise levels lead to more arrival-time uncertainty
and a corresponding larger location uncertainty. However, if the location is well-constrained,
the effect on the source distribution is minimal, and is usually negligible compared to the
effects of other uncertainties.
The velocity model is often assumed to be correct, or sufficiently accurate for the data,
and small uncertainties have little effect, especially at low-noise levels. As the noise level in
the data increases, its effect on the location distribution and therefore the source PDF will be
much larger, both affecting the orientation and possibly causing spurious non-double-couple
sources, consistent with len (2009).
Receivers that are close to the source can sample the near-field, which renders the un-
derlying assumptions used to relate the source models to the observations invalid. However,
classing the near-field component as noise and increasing the measurement uncertainty re-
moves this effect. Consequently, this effect is strong only if the noise level at the receiver is
very low, so may be significant for only borehole receivers.
Relative amplitudes, introduced by Dahm (1996), provide an additional constraint for co-
located events in the source inversion. Co-located events are common in microseismic data-
sets, as there are often repeating or contrasting mechanisms in the same source region (White
et al., 2011; Tarasewicz et al., 2014). Relative data can be included in the Bayesian approach
by evaluating the joint PDF for co-located events, which introduces a dependence on the
relative seismic moment. This relative seismic moment is a function of the proposed source
model, but can be estimated from observations at multiple stations, and has a distribution for
any possible source that is well approximated by a truncated Gaussian PDF. This distribution
enables the source PDF to be evaluated at one value of the relative seismic moment, with the
distribution for other values known.
A forward-model based approach, such as the Bayesian method described in this thesis,
requires a sampling algorithm. Monte Carlo random sampling (Metropolis et al., 1953;
Hastings, 1970) is the most basic approach, but can easily be parallelised. More complex
algorithms such as Markov chain Monte Carlo sampling require fewer samples, but increase
the time taken to carry out each iteration. Since the forward model is quick to calculate,
the effect of the reduced number of samples is offset by this increase in time to generate the
new samples. However, including the velocity model and location uncertainty increases the
time required for evaluating the forward model, making the required times for more complex
algorithms comparable to that for Monte Carlo random sampling. Furthermore, random
sampling of the joint PDF becomes very time consuming, unlike the Markov chain Monte
Carlo approaches.
A trans-dimensional Markov chain Monte Carlo approach (Green, 1995) allows a direct
test of multiple model types. This algorithm jumps between different models using a Markov
chain approach, resulting in an estimate of the model likelihood, based on the proportion
of samples in the different models. The results from this approach are consistent with the
posteriormodel probabilities constructed from theBayesian evidence and visual interpretation
of the results. The trans-dimensional approach is a valid method for estimating the posterior
model probabilities when random sampling approaches require a longer computation time,
such as when using relative amplitude ratios.
The quality of an event can be estimated from the non-zero percentage in the random
sampling algorithm, which is consistent even for very low sample sizes. However, some other
effects can increase the non-zero probability even if an event is well constrained, so it is not
always an applicable approach, especially if used for selecting events to look at in further
detail.
Inversion of surface data from Iceland show that the Bayesian approach can be used to
provide evidence for non-double-couple sources. The events observed at the Krafla volcano
show evidence for both double-couple and non-double-couple sources, supported by the
posterior model probabilities. These posterior model probabilities provide a useful estimate
of the model type, consistent with but more easily understood than the other approaches
for estimating model type, such as the Akaike information criterion (Akaike, 1974) and the
Bayesian information criterion (Schwarz, 1978).
Inversion of the Upptyppingar events shows that care must be taken when including
amplitude ratios as these can lead to deviations from the true source and possible spurious non-
double-couple sources. One of the events has a significant non-double-couple probability,
although this is probably a deviation due to errors in the amplitude ratios.
Relative amplitude data improve the source constraint, but large numbers of samples are
required. The distribution of the relative moment in a synthetic test peaks slightly away from
the true value, but this may be linked to the amplitude measurement errors. The relative
amplitude inversion of data from the Upptyppingar dyke intrusion shows two events with an
almost exactly inverted mechanism, consistent with the interpretation of a breaking plug in
White et al. (2011), but they could also be interpreted as motion perpendicular to the dyke in
the direction of opening (Rutledge and Phillips, 2003; Rutledge et al., 2004).
The Bayesian approach described in this thesis is consistent with the method developed
by Walsh et al. (2009), but can also use amplitude ratios and the full moment tensor source.
Relative data can improve the source constraint, but this often comes at the expense of longer
times to obtain sufficient samples of the joint PDF. This is a limitation of a forward model
approach, which needs a search algorithm to evaluate the full source PDF. Using linear
inversion of amplitude or waveforms will always be faster, but cannot include all of the
uncertainties in the source and data in a reliable way.
Many of the computations in the forward model are independent, so using parallel ap-
proaches, such as parallel processing or graphics processing unit (GPU) accelerated com-
putation to evaluate these calculations, leads to significant improvement in the run times.
However, inclusion of the location and velocity model uncertainties can lead to very slow
computations, due to the large number of samples being tested. Consequently, it may not
be feasible to always include the location uncertainty in the solutions, and it can often be
ignored for well-constrained location PDFs.
While the amplitude ratios can provide massive improvements in the source constraint,
care must be taken when including them in the inversion, as the uncertainty can often be
underestimated due to a systematic shift of the maxima by the noise. This error in the
amplitude ratios can lead to sparse solutions as the distributions for P/SH and P/SV peak
at different source-types, which can indicate errors in the estimates and suggest that the
observations are re-evaluated. Furthermore, the amplitude ratios should broadly agree with
the polarity solutions; failure to agree can be another indicator of errors in the measurements.
The polarity probabilities incorporate uncertainties in the observations, but depend on an
accurate arrival-time pick. This may lead to large amounts of tuning to improve the picking
algorithms. Additionally, the results will nearly always be worse than the corresponding
manual polarity determination, which is equivalent to observing probabilities of 1 in the
manual polarity pick direction. This approach provides a quantitative estimate of the polarity
uncertainty, rather than leaving it as a user-defined parameter, as is common (Brillinger et al.,
1980; Walsh et al., 2009). The polarity probabilities can be used to estimate the uncertainty
on manual polarity picks by treating them as a prior.
Sampling the full source PDF provides a greater understanding of the source and allows
the posterior model probabilities to be calculated. These probabilities provide a quantitative
method to distinguish which source type model is more likely, and can be used in hypothesis
testing. This estimate of the model probabilities provides evidence that many sources of
uncertainty lead to spurious non-double-couple sources, even though the extra parameters in
the full moment tensor improve the fit.
References
Adamov, P., and J. len (2010), Non-double-couple earthquake mechanism as an artifact
of the point-source approach applied to a finite-extent focus, Bull. Seismol. Soc. Am.,
100(2), 447457.
Adamov, P., and J. len (2013), Disputable non-double-couple mechanisms of several
strong earthquakes: second-degree moment approach, Bull. Seismol. Soc. Am., 103(5),
28362849.
Akaike, H. (1974), A new look at the statistical model identification, IEEE Trans. Automat.
Contr., 19(6), 716723.
Aki, K., and P. G. Richards (2002), Quantitative Seismology, second ed., University Science
Books.
Al-Harrasi, O.H., A.Al-Anboori, A.Wstefeld, and J.M.Kendall (2011), Seismic anisotropy
in a hydrocarbon field estimated from microseismic data, Geophys. Prospect., 59(2), 227
Aldersons, F. (2004), Toward a three-dimensional crustal structure of the Dead Sea region
from local earthquake tomography, Ph.D. thesis, Tel Aviv.
Allen, R. V. (1978), Automatic earthquake recognition and timing from single traces, Bull.
Seismol. Soc. Am., 68(5), 15211532.
Allen, R. V. (1982), Automatic phase pickers: their present use and future prospects, Bull.
Seismol. Soc. Am., 72(6B), S225S242.
Backus, G., and M. Mulcahy (1976a), Moment tensors and other phenomenological descrip-
tions of seismic sources - I . Continuous displacements, Geophys. J. R. Astron. Soc., 46,
341361.
Backus, G., and M. Mulcahy (1976b), Moment tensors and other phenomenological Discon-
tinuous descriptions of seismic sources - II. Discontinuous displacements, Geophys. J. R.
Astron. Soc., 47, 301329.
Baer, M., and U. Kradolfer (1987), An automatic phase picker for local and teleseismic
events, Bull. Seismol. Soc. Am., 77(4), 14371445.
Bai, L., L. Medina Luna, E. A. Hetland, and J. Ritsema (2013), Focal depths and mechanisms
of Tohoku-Oki aftershocks from teleseismic P wave modeling, Earthq. Sci., 27(1).
Baig, A., and T. I. Urbancic (2010), Microseismic moment tensors: A path to understanding
frac growth, Lead. Edge, 29(3), 320324.
Bailey, I. W., T. W. Becker, and Y. Ben-Zion (2009), Patterns of co-seismic strain computed
from southern California focal mechanisms, Geophys. J. Int., 177(3), 10151036.
Baillard, C., W. C. Crawford, V. Ballu, C. Hibert, and A. Mangeney (2014), An automatic
kurtosis-based P- and S-phase picker designed for local seismic networks, Bull. Seismol.
Soc. Am., 104(1), 394409.
Bame, D., and M. Fehler (1986), Observations of long period earthquakes accompanying
hydraulic fracturing, Geophys. Res. Lett., 13(1), 149152.
Bayes, T., and R. Price (1763), An essay towards solving a problem in the doctrine of chances.
By the late Rev. Mr. Bayes, F. R. S. communicated by Mr. Price, in a letter to John Canton,
A. M. F. R. S., Philos. Trans. R. Soc. London, 53, 370418.
Bean, C. J., I. Lokmer, and G. S. OBrien (2008), Influence of near-surface volcanic structure
on long-period seismic signals and on moment tensor inversions: Simulated examples
from Mount Etna, J. Geophys. Res. Solid Earth, 113(B08308).
Behnel, S., R. Bradshaw, C. Citro, L. Dalcin, D. S. Seljebotn, and K. Smith (2011), Cython:
The best of both worlds, Comput. Sci. Eng., 13(2), 3139.
Ben-Menahem, A., and S. J. Singh (1981), Seismic Waves and Sources, Springer Verlag,
New York.
Ben-Zion, Y. (2008), Collective behavior of earthquakes and faults: continuum-discrete trans-
itions, progressive evolutionary changes, and different dynamic regimes, Rev. Geophys.,
46, 170.
Benson, P. M., S. Vinciguerra, P. G. Meredith, R. P. Young, P. M. Benson, S. Vinciguerra,
P. G. Meredith, and R. P. Young (2008), Laboratory simulation of volcano simulation
seismicity, Science, 322(5899), 249252.
Bernardi, F. (2004), Earthquake source parameters in the Alpine-Mediterranean region from
surface wave analysis, Ph.D. thesis, Swiss Federal Institute of Technology.
Bernardi, F., J. Braunmiller, U. Kradolfer, and D. Giardini (2004), Automatic regional
moment tensor inversion in the European-Mediterranean region, Geophys. J. Int., 157(2),
703716.
Bernth, H., and C. Chapman (2011), A comparison of the dispersion relations for anisotropic
elastodynamic finite-difference grids, Geophysics, 76(3), WA43WA50.
Beyreuther, M., R. Barsch, L. Krischer, T. Megies, Y. Behr, and J. Wassermann (2010),
ObsPy: a Python toolbox for seismology, SRL Electron. Seismol.
Blanton, T. L. (1982), An experimental study of interaction between hydraulically induced
and pre-existing fractures, in SPE Unconv. Gas Recover. Symp. 16-18 May, Pittsburgh,
Pennsylvania.
Boggs, P. T., and J. E. Rogers (1990a), Orthogonal distance regression, in Stat. Anal. Meas.
error Model. Appl. Proc. AMS-IMS-SIAM Jt. summer Res. Conf. held June 10-16, 1989,
pp. 183194.
Boggs, P. T., and J. E. Rogers (1990b), The computation and use of the asymptotic covariance
matrix for measurement error models, Tech. rep., National Institute of Standards and
Technology Applied and Computational Mathematics Division.
Boggs, P. T., R. H. Byrd, J. E. Rogers, and R. B. Schnabel (1992), Users reference guide
for ODRPACK version 2.01 software for weighted orthogonal distance regression, Tech.
Rep. June, National Institute of Standards and Technology Applied and Computational
Mathematics Division.
Bormann, P., and E. Wielandt (2013), Chapter 4 - Seismic signals and noise, in IASPEI New
Man. Seismol. Obs. Pract., June, pp. 653642.
Bowers, D., and W. R. Walter (2002), Discriminating between large mine collapses and
explosions using teleseismic P waves, Pure Appl. Geophys., 159(4), 803830.
Brillinger, D. R., A. Udias, and B. A. Bolt (1980), A probability model for regional focal
mechanism solutions, Bull. Seismol. Soc. Am., 70(1), 149170.
Brooks, S. P., P. Giudici, and G. O. Roberts (2003), Efficient construction of reversible jump
Markov chain Monte Carlo proposal distributions, J. R. Stat. Soc. Ser. B Stat. Methodol.,
65(1), 339.
Burridge, R., and L. Knopoff (1964), Body force equivalents for seismic dislocations, Bull.
Seismol. Soc. Am., 54(6), 18751888.
Burridge, R., and L. Knopoff (1967), Model and theoretical seismicity, Bull. Seismol. Soc.
Am., 57(3), 341371.
Byerly, P. (1926), The Montana earthquake of June 28, 1925, G.M.C.T., Bull. Seismol. Soc.
Am., 16(4), 209265.
Caflisch, R. E. (1998), Monte Carlo and quasi-Monte Carlo methods, Acta Numer., 7, 149.
Cesca, S., E. Buforn, and T. Dahm (2006), Amplitude spectra moment tensor inversion of
shallow earthquakes in Spain, Geophys. J. Int., 166(2), 839854.
Chakravarti, I. M., R. G. Laha, and J. Roy (1967),Handbook of Methods of Applied Statistics,
Volume 1, Wiley.
Chapman, C. H. (2004), Fundamentals of Seismic Wave Propagation, Cambridge University
Press.
Chapman, C. H., and W. S. Leaney (2011), A new moment-tensor decomposition for seismic
events in anisotropic media, Geophys. J. Int., 188(1), 343370.
Chapman, C. H., and W. S. Leaney (2014), Erratum: A correction to A new moment-tensor
decomposition for seismic events in anisotropic media, Geophys. J. Int., 199, 18081810.
Chouet, B. (1996), Long-period volcano seismicity: its source and use in eruption forecasting,
Nature, 380, 309316.
Christensen, N. I. (1996), Poissons ratio and crustal seismology, J. Geophys. Res., 101(B2),
31393156.
Coles, D., and A. Curtis (2011), Efficient nonlinear Bayesian survey design using DN optim-
ization, Geophysics, 76(2).
Crampin, S. (1985), Evaluation of anisotropy by shear-wave splitting, Geophysics, 50(1),
142152.
Crampin, S., I. Bush, C. Naville, and D. Taylor (1986), Estimating the internal structure of
reservoirs with shear-wave VSPs, Lead. Edge, 5(11), 3539.
Dahm, T. (1993), Relative moment tensor inversion to determine the radiation pattern of
seismic sources, Ph.D. thesis.
Dahm, T. (1996), Relative moment tensor inversion based on ray theory: theory and synthetic
tests, Geophys. J. Int., 124(1), 245257.
Dahm, T., G. Manthei, and J. Eisenbltter (1999), Automated moment tensor inversion
to estimate source mechanisms of hydraulically induced micro-seismicity in salt rock,
Tectonophysics, 306(1), 117.
Damani, A., A. Sharma, C. H. Sondergeld, and C. S. Rai (2012), Mapping of hydraulic frac-
tures under triaxial stress conditions in laboratory experiments using acoustic emissions,
in SPE Annu. Tech. Conf. Exhib. 8-10 October, San Antonio, Texas, USA.
Dasgupta, R., and R. A. Clark (1998), Estimation of Q from surface seismic reflection data,
Geophysics, 63(6), 21202128.
De Barros, L., I. Lokmer, C. J. Bean, G. S. OBrien, G. Saccorotti, J. P. Mtaxian, L. Zuc-
carello, and D. Patan (2011), Source mechanism of long-period events recorded by a
high-density seismic network during the 2008 eruption on Mount Etna, J. Geophys. Res.
Solid Earth, 116, 117.
Deichmann, N., and M. Garcia-fernandez (1992), Rupture geometry from high-precision
relative hypocentre locations of microearthquake clusters, Geophys. J. Int., 110, 501517.
Delouis, B., D. Giardini, P. Lundgren, and J. Salichon (2002), Joint inversion of In SAR, GPS,
teleseismic, and strong-motion data for the spatial and temporal distribution of earthquake
slip: application to the 1999 Izmit mainshock, Bull. Seismol. Soc. Am., 92(1), 278299.
Delpezzo, E., J. Ibanez, J. Morales, A. Akinci, and R. Maresca (1995), Measurements of
intrinsic and scattering seismic attenuation in the crust, Bull. Seismol. Soc. Am., 85(5),
13731380.
Dewey, J., and P. Byerly (1969), The early history of seismometry (to 1900), Bull. Seismol.
Soc. Am., 59(1), 183227.
Di Stefano, R., F. Aldersons, E. Kissling, P. Baccheschi, C. Chiarabba, and D. Giardini
(2006), Automatic seismic phase picking and consistent observation error assessment:
application to the Italian seismicity, Geophys. J. Int., 165(1), 121134.
Di Stefano, R., C. Chiarabba, L. Chiaraluce, M. Cocco, P. De Gori, D. Piccinini, and
L. Valoroso (2011), Fault zone properties affecting the rupture evolution of the 2009 (M w
6.1) LAquila earthquake (central Italy): Insights from seismic tomography, Geophys. Res.
Lett., 38(10), 610.
Doornbos, D. J. (1982), Seismic moment tensors and kinematic source parameters, Geophys.
J. R. Astron. Soc., 69(1), 235251.
Dreger, D. S., H. Tkali, and M. Johnston (2000), Dilational processes accompanying
earthquakes in the Long Valley caldera, Science, 288(5463), 122125.
Drew, J. (2010), Coalescence microseismic mapping: An imaging method for the detection
and location of seismic events, Ph.D. thesis, University of Cambridge.
Drew, J., D. Leslie, P. Armstrong, and G. Michaud (2005), Automated microseismic event
detection and location by continuous spatial mapping, in Soc. Pet. Eng. Annu. Tech. Conf.
Exhib.
Drew, J., R. S. White, F. Tilmann, and J. Tarasewicz (2013), Coalescence microseismic
mapping, Geophys. J. Int., 195(3), 17731785.
Dufumier, H., and L. Rivera (1997), On the resolution of the isotropic component in moment
tensor inversion, Geophys. J. Int., 131(3), 595606.
Duputel, Z., L. Rivera, Y. Fukahata, and H. Kanamori (2012a), Uncertainty estimations for
seismic source inversions, Geophys. J. Int., 190(2), 12431256.
Duputel, Z., L. Rivera, H. Kanamori, and G. Hayes (2012b), W phase source inversion for
moderate to large earthquakes (1990-2010), Geophys. J. Int., 189(2), 11251147.
Dziewonski, A. M., and D. L. Anderson (1981), Preliminary reference Earth model, Phys.
Earth Planet. Inter., 25(4), 297356.
Dziewonski, A. M., T. A. Chou, and J. H. Woodhouse (1981), Determination of earthquake
source parameters from waveform data for studies of global and regional seismicity, J.
Geophys. Res., 86(B4), 28252852.
Eaton, D. W. (2011), Q determination, corner frequency and spectral characteristics of
microseismicity induced by hydraulic fracturing, in 2011 CSPG CSEG CWLS Conv.
Eaton, D. W., J. Davidsen, P. K. Pedersen, and N. Boroumand (2014), Breakdown of the
Gutenberg-Richter relation for microearthquakes induced by hydraulic fracturing: influ-
ence of stratabound fractures, Geophys. Prospect., 62(4), 806818.
Ekstrm, G., M. Nettles, and A. M. Dziewonski (2012), The global CMT project 2004-2010:
Centroid-moment tensors for 13,017 earthquakes, Phys. Earth Planet. Inter., 200-201.
Fedorov, F. D. (1968), Theory of Elastic Waves in Crystals, Plenum Press, New York.
Fieller, E. C. (1932), The distribution of the index in a normal bivariate population, Biomet-
rika, 24(3-4), 428440.
Ford, S. R., D. S. Dreger, and W. R. Walter (2008), Identifying isotropic events using a
regional moment tensor inversion, in Proc. 30th Monitoring Res. Rev.: Ground-Based
Nuclear Explosion Monitoring Tech., 23-25 Sep 2008, Portsmouth, VA sponsored by the
NNSA and the Air Force Res. Lab.
Ford, S. R., D. S. Dreger, and W. R. Walter (2009), Identifying isotropic events using a
regional moment tensor inversion, J. Geophys. Res. Solid Earth, 114(B01306).
Ford, S. R., D. S. Dreger, and W. R. Walter (2010), Network sensitivity solutions for regional
moment-tensor inversions, Bull. Seismol. Soc. Am., 100(5), 19621970.
Fossen, H. (2010), Structural Geology, 481 pp., Cambridge University Press.
Foulger, G. R., B. R. Julian, D. P. Hill, A. M. Pitt, P. E. Malin, and E. Shalev (2004), Non-
double-couple microearthquakes at Long Valley caldera, California, provide evidence for
hydraulic fracturing, J. Volcanol. Geotherm. Res., 132(1), 4571.
Freund, L. B. (1979), The mechanics of dynamic shear crack propagation, J. Geophys. Res.,
84(B5).
Friedman, C. (1999), The frequency interpretation in probability, Adv. Appl. Math., 23(3),
234254.
Frohlich, C. (1994), Earthquakes with non-double-couple mechanisms, Science, 264(5160),
804809.
Frohlich, C., and S. D. Davis (1999), How well constrained are well-constrained T,B, and P
axes in moment tensor catalogs?, J. Geophys. Res., 104(B3), 49014910.
Garcia, R. F., S. Bruinsma, P. Lognonn, E. Doornbos, and F. Cachoux (2013), GOCE: the
first seismometer in orbit around the Earth, Geophys. Res. Lett., 40(5), 10151020.
Geiger, L. (1912), Earthquake location using the methods of Galitzin, Klotz and the least
squares method applied by Geiger, adapted from Dr. L. Geiger, Herdbestimmung bei
Erdbeben aus den Ankunftszeiten, Goettingen, 1910, Bull. St. Louis Univ., 8, 5693.
Gercek, H. (2007), Poissons ratio values for rocks, Int. J. Rock Mech. Min. Sci., 44(1), 113.
Gibowicz, S. J., H. P. Harjes, and M. Schfer (1990), Source parameters of seismic events
at Heinrich Robert Mine, Ruhr Basin, Federal Republic of Germany: Evidence for non
double-couple events, Bull. Seismol. Soc. Am., 80(1), 88109.
Godano, M., M. Regnier, A. Deschamps, T. Bardainne, and E. Gaucher (2009), Focal
mechanisms from sparse observations by nonlinear inversion of amplitudes: method and
tests on synthetic and real data, Bull. Seismol. Soc. Am., 99(4), 22432264.
Golub, G. H., and C. F. Van Loan (1996), Matrix Computations, 3rd ed., Johns Hopkins
University Press.
Green, P. J. (1995), Reversible jump Markov chain Monte Carlo computation and Bayesian
model determination, Biometrika, 82(4), 711732.
Greenfield, T., and R. S. White (2015), Building Icelandic igneous crust by repeated melt
injections, J. Geophys. Res., in press.
Grigoli, F., S. Cesca, T. Dahm, and L. Krieger (2012), A complex linear least-squares method
to derive relative and absolute orientations of seismic sensors, Geophys. J. Int., 188,
12431254.
Gutenberg, B., and C. F. Richter (1949), Seismicity of The Earth and Associated Phenomena,
Princeton University Press.
Hanks, T., and H. Kanamori (1979), A moment magnitude scale, J. Geophys. Res., 84(B5),
23482350.
Hardebeck, J. L., and P. M. Shearer (2002), A new method for determining first-motion focal
mechanisms, Bull. Seismol. Soc. Am., 92(6), 22642276.
Hardebeck, J. L., and P. M. Shearer (2003), Using S / P amplitude ratios to constrain the focal
mechanisms of small earthquakes, Bull. Seismol. Soc. Am., 93(6), 24342444.
Hastings, W. K. (1970), Monte Carlo sampling methods using Markov chains and their
applications, Biometrika, 57(1), 97109.
Havskov, J., and G. Alguacil (2004), Instrumentation in Earthquake Seismology, vol. 22, 360
pp., Springer.
Heimann, S. (2011), A robust method to estimate kinematic earthquake source parameters,
Ph.D. thesis, Hamburg.
Hibert, C., et al. (2014), Automated identification, location, and volume estimation of rock-
falls at Piton de la Fournaise volcano, J. Geophys. Res. Earth Surf., 119, 10821105.
Hinkley, D. V. (1969), On the ratio of two correlated normal random variables, Biometrika,
56(3), 635639.
Hjrleifsdttir, V., and G. Ekstrm (2010), Effects of three-dimensional Earth structure on
CMT earthquake parameters, Phys. Earth Planet. Inter., 179, 178190.
Hong, T. K., and J. Rhie (2009), Regional source scaling of the 9 october 2006 underground
nuclear explosion in North Korea, Bull. Seismol. Soc. Am., 99(4), 25232540.
Horlek, J., Z. Jechumtlov, L. Dorbath, and J. len (2010), Source mechanisms of
micro-earthquakes induced in a fluid injection experiment at the HDR site Soultz-sous-
Forts (Alsace) in 2003 and their temporal and spatial variations, Geophys. J. Int., 181,
15471565.
Houli, N., D. Dreger, and A. Kim (2014), GPS source solution of the 2004 Parkfield
earthquake., Sci. Rep., 4(3646).
Hudson, J. A., R. G. Pearce, and R. M. Rogers (1989), Source type plot for inversion of the
moment tensor, J. Geophys. Res., 94(B1), 765774.
Husen, S., and J. L. Hardebeck (2010), Earthquake location accuracy, Community Online
Resour. Stat. Seism. Anal., (September).
Jackson, J., and D.McKenzie (1999), A hectare of fresh striations on the Arkitsa fault, central
Greece, J. Struct. Geol., 21(1), 16.
Jaynes, E. T. (2003), Probability theory: the logic of science, 727 pp., Cambridge University
Press.
Jechumtlov, Z., and L. Eisner (2008), Seismic source mechanism inversion from a linear
array of receivers reveals non-double-couple seismic events induced by hydraulic fracturing
in sedimentary formation, Tectonophysics, 460(1-4), 124133.
Jechumtlov, Z., and J. len (2005), Amplitude ratios for complete moment tensor re-
trieval, Geophys. Res. Lett., 32(22).
Jeffreys, H. (1998), Theory of probability, 3rd ed., Oxford Univ Press.
Johnson, N. L., S. Kotz, and N. Balakrishnan (1995), Chapter 21: Beta distributions, in
Contin. Univariate Distrib. Vol. 2.
Jost, M. L., and R. B. Herrmann (1989), A students guide to and review of moment tensors,
Seismol. Res. Lett., 60(2), 3757.
Julian, B. R. (1986), Analysing seismic-source mechanisms by linear-programming methods,
Geophys. J. R. Astron. Soc., 84(2), 431443.
Julian, B. R., and G. R. Foulger (1996), Earthquake mechanisms from linear-programming
inversion of seismic-wave amplitude ratios, Bull. Seismol. Soc. Am., 86(4), 972980.
Julian, B. R., and G. R. Foulger (1998), Microearthquake focal mechanisms - A tool for
monitoring geothermal systems, Resevoir Eng., c, 166171.
Julian, B. R., A. D. Miller, and G. R. Foulger (1998a), Non-double-couple earthquakes 1.
Theory, Rev. Geophys., 36(4), 525549.
Julian, B. R., A. D. Miller, and G. R. Foulger (1998b), Non-double-couple earthquakes 2.
Observations, Rev. Geophys., 36(4), 525.
Kaeufl, P., A. P. Valentine, T. OToole, and J. Trampert (2013), A framework for fast probab-
ilistic centroid - moment-tensor determination - Inversion of regional static displacement
measurements, Geophys. J. Int., 196(3), 16761693.
Kagan, Y. Y. (1982), Stochastic model of earthquake fault geometry, Geophys. J. R. Astron.
Soc., 71, 659691.
Kagan, Y. Y. (2005), Double-couple earthquake focal mechanism: random rotation and
display, Geophys. J. Int., 163(3), 10651072.
Kanamori, H., and J. W. Given (1981), Use of long-period surface waves for rapid determin-
ation of earthquake-source parameters, Phys. Earth Planet. Inter., 27, 831.
Kashyap, R. L. (1980), Inconsistency of theAIC rule for estimating the order of autoregressive
models, IEEE Trans. Automat. Contr., AC-25(5), 996998.
Kass, R. E., and A. Raftery (1995), Bayes factors, J. Am. Stat. Assoc., 90(430), 773795.
Kim, A. (2011), Uncertainties in full waveform moment tensor inversion due to limited
microseismic monitoring array geometry, SEG San Antonio 2011 Annu. Meet., (5), 1509
1513.
Kim, S. G., N. Kraeva, and Y. T. Chen (2000), Source parameter determination of regional
earthquakes in the Far East using moment tensor inversion of single-station data, Tectono-
physics, 317, 125136.
Klein, F. W. (2002), Users guide to HYPOINVERSE-2000, a fortran program to solve for
earthquake locations and magnitudes - OFR 02-171, Tech. rep.
Knopoff, L., and A. F. Gangi (1959), Seismic reciprocity, Geophysics, 24(4), 681691.
Knopoff, L., and M. J. Randall (1970), The compensated linear-vector dipole: a possible
mechanism for deep earthquakes, J. Geophys. Res., 75(26), 49574963.
Konca, A. O., S. Leprince, J.-P. Avouac, and D. V. Helmberger (2010), Rupture process of the
1999 Mw 7.1 Duzce earthquake from joint analysis of SPOT, GPS, InSAR, strong-motion,
and teleseismic data: a supershear rupture with variable rupture velocity, Bull. Seismol.
Soc. Am., 100(1), 267288.
Kuge, K., and T. Lay (1994), Systematic non-double-couple components of earthquake
mechanisms: The role of fault zone irregularity, J. Geophys. Res., 99(B8), 15,45715,467.
Kustowski, B., G. Ekstrm, and A. M. Dziewonski (2008), Anisotropic shear-wave velocity
structure of the Earths mantle: a global model, J. Geophys. Res., 113.
Lahr, J. C. (1989), HYPOELLIPSE/version 2.0*: a computer program for determining local
earthquake hypocentral parameters, magnitude, and first motion pattern - OFR 89-116,
Tech. rep., USGS.
Laplace, P. S. (1812), Thorie analytique des probabilits.
Lee, W. H. K., and J. C. Lahr (1975), HYPO71 (Revised): A computer program for determ-
ining hypocenter, magnitude and first motion pattern of local earthquakes, Tech. Rep. 300,
USGS.
Lehman, I. (1936), P, Publ. du Bur. Cent. Seismol. Int. Srie A, Trav. Sci., 14, 87115.
Liddle, A. R. (2004), Howmany cosmological parameters?,Mon. Not. R. Astron. Soc., 351(3),
L49L53.
Lienert, B. R., E. Berg, and L. N. Frazer (1986), HYPOCENTER: an earthquake location
method using centered, scaled, and adaptively damped least squares, Bull. Seismol. Soc.
Am., 76(3), 771783.
Lippitsch, R., R. S. White, and H. Soosalu (2004), Precise hypocentre relocation of mi-
croearthquakes in a high-temperature geothermal field: the Torfajkull central volcano,
Iceland, Geophys. J. Int., 160(1), 371388.
Lomax, A., and A. Curtis (2001), Fast, probabilistic earthquake location in 3D models using
oct-tree importance sampling, in Eur. Geophys. Soc.
Lomax, A., and A. Michelini (2001), Comparision of NonLinLoc and linear earthquake
locations in a 3D model.
Lomax, A., J. Virieux, P. Volant, and C. Berge (2000), Probabilistic earthquake location in
3D and layered models: Introduction of a Metropolis-Gibbs method and comparison with
linear locations, pp. 101134, Advances in Seismic Location, Kluwer.
Lomax, A., A. Michelini, and A. Curtis (2009), Earthquake Location, Direct, Global-
Search Methods, pp. 24492473, Encyclopedia of Complexity and System Science, Part
5, Springer.
Lundgren, P., and J. Salichon (2000), Joint inversion of InSAR and teleseismic data for the
slip history of the 1999 Izmit ( Turkey ) earthquake Izmit / Sapanca, Geophys. Res. Lett.,
27(20), 33893392.
Markov, A. A. (1954), Theory of Algorithms, Moscow Academy of Sciences of the USSR,
1954 [Israel Program for Scientific Translations for the National Science Foundation,
Washington, D.C., 1961].
Marsaglia, G. (1972), Choosing a point from the surface of a sphere, Ann. Math. Stat., 43(2),
645646.
Maruyama, T. (1963), On the force equivalents of dynamical elastic dislocations with refer-
ence to the earthquake mechanism, Bull. Earthq. Res. Institure, 41, 467486.
Megies, T., M. Beyreuther, R. Barsch, L. Krischer, and J. Wassermann (2011), ObsPy - What
can it do for data centers and observatories?, Ann. Geophys., 54, 4758.
Menke, W., and D. Schaff (2004), Absolute earthquake locations with differential data, Bull.
Seismol. Soc. Am., 94(6), 22542264.
Metropolis, N., A. W. Rosenbluth, M. N. Rosenbluth, A. H. Teller, and E. Teller (1953),
Equation of state calculations by fast computing machines, J. Chem. Phys., 21(6), 1087
1092.
Miller, A. D., G. R. Foulger, and B. R. Julian (1998a), Non-double-couple earthquakes 2.
Observations, Rev. Geophys., 36(4), 551568.
Miller, A. D., B. R. Julian, and G. R. Foulger (1998b), Three-dimensional seismic structure
andmoment tensors of non-double-couple earthquakes at the Hengill - Grensdalur volcanic
complex , Iceland, Geophys. J. Int., 133, 309325.
Minson, S. E., and D. S. Dreger (2008), Stable inversions for complete moment tensors,
Geophys. J. Int., 174(2), 585592.
Minson, S. E., D. S. Dreger, R. Brgmann, H. Kanamori, and K. M. Larson (2007), Seis-
mically and geodetically determined nondouble-couple source mechanisms from the 2000
Miyakejima volcanic earthquake swarm, J. Geophys. Res., 112(B10308).
Mukherjee, S., E. D. F. Feigelson, G. J. Babu, F. Murtagh, C. Fraley, and A. Raftery (1998),
Three types of gamma-ray bursts, Astrophys. J., 508, 314327.
Mller, G. (1973), Seismic moment and long-period radiation of underground nuclear explo-
sions, Bull. Seismol. Soc. Am., 63(3), 847857.
Mller, G. (2001), Volume change of seismic sources from moment tensors, Bull. Seismol.
Soc. Am., 91(August), 880884.
Mller, G., and T. Dahm (2000), Fracture morphology of tensile cracks and rupture velocity,
J. Geophys. Res., 105(1999), 723738.
Muller, M. E. (1959), A note on a method for generating points uniformly on N-dimensional
spheres, Commun. ACM, 2(4), 1920.
Nakamula, S., M. Takeo, Y. Okabe, and M. Matsuura (2007), Automatic seismic wave
arrival detection and picking with stationary analysis: Application of the KM2O-Langevin
equations, Earth, Planets Sp., 59(6), 567577.
Nakamura, M. (2004), Automatic determination of focal mechanism solutions using initial
motion polarities of P and S waves, Phys. Earth Planet. Inter., 146(3-4), 531549.
Nakano, H. (1923), Notes on the nature of the forces which give rise to the earthquake
motions, Seis- tool. Bull. Cent. Met. Obs. Japan, 1, 92120.
Norris, J. R. (1998), Markov Chains, 254 pp., Cambridge University Press.
OBrien, G. S., I. Lokmer, L. De Barros, C. J. Bean, G. Saccorotti, J. P. Metaxian, and
D. Patane (2011), Time reverse location of seismic long-period events recorded on Mt
Etna, Geophys. J. Int., 184(1), 452462.
OToole, T. B. (2013), Studies of earthquakes and microearthquakes using near-field seismic
and geodetic observations, Ph.D. thesis, University of Oxford.
Page, M. T., S. Cust, R. J. Archuleta, and J. M. Carlson (2009), Constraining earthquake
source inversions with GPS data 1: resolution based removal of artifacts, J. Geophys. Res.,
114(B01314).
Panza, G. F., and A. Sarao (2000), Monitoring volcanic and geothermal areas by full seismic
moment tensor inversion: are non-double-couple components always artefacts of model-
ling?, Geophys. J. Int., 143(2), 353364.
Peacock, S., C. McCann, J. Sothcott, and T. R. Astin (1994), Experimental measurements of
seismic attenuation In microfractured sedimentary rock, Geophysics, 59(9), 13421351.
Persson, P.-A., R. Holmberg, and J. Lee (1993), Rock Blasting and Explosives Engineering,
560 pp., CRC Press.
Pesicek, J. D., J. len, and C. H. Thurber (2012), Determination and uncertainty of moment
tensors for microearthquakes at Okmok Volcano, Alaska, Geophys. J. Int., 190(3), 1689
1709.
Reasenberg, P. A., and D. Oppenheimer (1985), FPFIT, FPPLOT and FPPAGE: Fortran
computer programs for calculating and displaying earthquake fault-plane solutions - OFR
85-739, Tech. rep., USGS.
Reid, H. F. (1910), TheCalifornia earthquake ofApril 18, 1906: report of the State Earthquake
Investigation Commision, Vol. II, The mechanics of the earthquake, Tech. rep., Carnegie
Inst. of Washington.
Rice, J. R., andM. Cocco (2007), Seismic fault rheology and earthquake dynamics, in Tecton.
Faults Agents Chang. a Dyn. Earth, edited by M. R. Handy, G. Hirth, and N. Hovius,
January 2005, chap. 5, pp. 1621, MIT press.
Richards, P. G. (2005), Equivalent volume sources for explosions at depth: theory and
observations, Bull. Seismol. Soc. Am., 95(2), 401407.
Riedesel, M. A., and T. H. Jordan (1989), Display and assessment of seismic moment tensors,
Bull. Seismol. Soc. Am., 79(1), 85100.
Ritsema, J., H. J. van Heijst, and J. H. Woodhouse (1999), Complex shear wave velocity
structure imaged beneath Africa and Iceland, Science, 286(5446), 19251928.
Robert, C. P., and G. Casella (2004), Monte Carlo Statistical Methods, 2nd ed., 645 pp.
Roberts, G. P., and A. Ganas (2000), Fault-slip directions in central and southern Greece
measured from striated and corrugated fault planes: Comparison with focal mechanism
and geodetic data, J. Geophys. Res., 105(1999), 23,443.
Robinson, D. J., M. Sambridge, andR. Snieder (2011), A probabilistic approach for estimating
the separation between a pair of earthquakes directly from their coda waves, J. Geophys.
Res. Solid Earth, 116(4), 117.
Rodriguez, I. V., M. Sacchi, and Y. J. Gu (2012), Simultaneous recovery of origin time, hy-
pocentre location and seismic moment tensor using sparse representation theory, Geophys.
J. Int., 188(3), 11881202.
Ross, Z. E., and Y. Ben-Zion (2014), Automatic picking of direct P, S seismic phases and
fault zone head waves, Geophys. J. Int., 199(1), 368381.
Rutledge, J., X. Yu, and S. Leaney (2015),Microseismic shearing driven by hydraulic-fracture
opening : An interpretation of source-mechanism trends, Lead. Edge, pp. 926934.
Rutledge, J. T., andW.S. Phillips (2003), Hydraulic stimulation of natural fractures as revealed
by induced microearthquakes, Carthage Cotton Valley gas field, east Texas, Geophysics,
68(2), 441452.
Rutledge, J. T., W. S. Phillips, and M. J. Mayerhofer (2004), Faulting induced by forced
fluid injection and fluid flow forced by faulting: an interpretation of hydraulic-fracture
microseismicity, Carthage Cotton Valley gas field, Texas, Bull. Seismol. Soc. Am., 94(5),
18171830.
Sambridge, M. (1999a), Geophysical inversion with a neighbourhood algorithm-I. Searching
a parameter space, Geophys. J. Int., 138(2), 479494.
Sambridge, M. (1999b), Geophysical inversion with a neighbourhood algorithm-II . Apprais-
ing the ensemble, Geophys. J. Int., 138(3), 727746.
Sambridge, M. (2013), A parallel tempering algorithm for probabilistic sampling and mul-
timodal optimization, Geophys. J. Int., 196(1), 357374.
Sambridge, M., T. Bodin, K. Gallagher, and H. Tkali (2013), Transdimensional inference
in the geosciences, Philos. Trans. R. Soc. A, 371.
Sasaki, S. (1998), Characteristics of microseismic events induced during hydraulic frac-
turing experiments at the Hijiori hot dry rock geothermal energy site, Yamagata, Japan,
Tectonophysics, 289(1-3), 171188.
Sayers, C. M., and J. G. van Munster (1991), Microcrack-induced seismic anisotropy of
sedimentary rocks, J. Geophys. Res., 96(B10), 16,52916,533.
Schuler, J., P. A. F. Christie, and R. S. White (2014), Borehole study of compressional and
shear attenuation of basalt flows penetrated by the Brugdan and William wells on the
Faroes shelf, Geophys. Prospect., 62(2), 315332.
Schwarz, G. (1978), Estimating the dimension of a model, Ann. Stat., 6(2), 461464.
Shearer, P. M. (2009), Introduction to Seismology, 2nd editio ed., 412 pp., Cambridge
University Press.
Shearer, P. M., and J. A. Orcutt (1987), Surface and near-surface effects on seismic waves -
theory and borehole seismometer results, Bull. Seismol. Soc. Am., 77(4), 11681196.
Shimizu, H., S. Ueki, and J. Koyama (1987), A tensile-shear crack model for the mechanism
of volcanic earthquakes, Tectonophysics, 144, 287300.
len, J. (2004), Regional moment tensor uncertainty due to mismodeling of the crust,
Tectonophysics, 383(3-4), 133147.
len, J. (2009), Resolution of non-double-couple mechanisms: simulation of hypocenter
mislocation and velocity structure mismodeling, Bull. Seismol. Soc. Am., 99(4), 2265
2272.
len, J., and A.Milev (2006), Seismic moment tensor resolution on a local scale: simulated
rockburst and mine-induced seismic events in the Kopanang gold mine, South Africa, Pure
Appl. Geophys., 163(8), 14951513.
len, J., and V. Vavryuk (2002), Can unbiased source be retrieved from anisotropic
waveforms by using an isotropic model of the medium?, Tectonophysics, 356, 125138.
len, J., P. Campus, and G. F. Panza (1996), Seismicmoment tensor resolution bywaveform
inversion of a few local noisy records - I. Synthetic tests, Geophys. J. Int., 126, 605619.
len, J., I. Penk, and R. P. Young (2001), Point-source inversion neglecting a nearby free
surface: Simulation of the underground Research Laboratory, Canada, Geophys. J. Int.,
146(1), 171180.
len, J., B. Feignier, V. Teyssoneyre, and O. Coutant (2002), Moment tensor inversion of
regional phases: application to a mine collapse.
Silwal, V., and C. Tape (2013), Seismic moment tensor inversion with posterior samples and
uncertanties, Abstr. S51A-2311, 2013 Fall Meet. AGU.
Sipkin, S.A. (1982), Estimation of earthquake source parameters by the inversion ofwaveform
data: synthetic waveforms, Phys. Earth Planet. Inter., 30, 242259.
Sivia, D. S. (2000), Data Analysis: A Bayesian Tutorial, 189 pp., Oxford Univ Press.
Snieder, R. (2006), The theory of coda wave interferometry, Pure Appl. Geophys., 163(2-3),
455473.
Snieder, R., and M. Vrijlandt (2005), Constraining the source separation with coda wave
interferometry: Theory and application to earthquake doublets in the Hayward fault,
California, J. Geophys. Res. B Solid Earth, 110(4), 115.
Snoke, J. A. (2003), FOCMEC: FOCal MEChanism determinations, Tech. rep.
Song, F., and M. N. Toksz (2011), Full-waveform based complete moment tensor inver-
sion and source parameter estimation from downhole microseismic data for hydrofracture
monitoring, Geophysics, 76(6), WC103WC116.
Stank, F., and L. Eisner (2013), New model explaining inverted source mechanisms of
microseismic events induced by hydraulic fracturing, in SEG Houst. 2013 Annu. Meet., pp.
22012205.
Stirling, J. (1730), Methodus differentialis: sive tractatus de summatione et interpolatione
serierum infinitarum. Auctore Jacobo Stirling, R.S.S.
Strelitz, R. A. (1989), Choosing the best double couple from a moment-tensor inversion,
Geophys. J. Int., 99, 811815.
Stump, B. W., and L. R. Johnson (1982), Higher-degree moment tensors - the importance
of source finiteness and rupture propagation on seismograms, Geophys. J. R. Astron. Soc.,
69, 721743.
Taira, T., R. B. Smith, and W. L. Chang (2010), Seismic evidence for dilatational source
deformations accompanying the 2004-2008 Yellowstone accelerated uplift episode, J.
Geophys. Res. Solid Earth, 115(B02301).
Takanami, T., and G. Kitagawa (1988), A new efficient procedure for the estimation of onset
times of seismic waves, J. Phys. Earth, 36, 267290.
Takanami, T., and G. Kitagawa (1991), Estimation of the arrival times of seismic waves by
multivariate time series model, Ann. Inst. Stat. Math., 43(3), 403433.
Takei, Y., and M. Kumazawa (1994), Why have the single force and torque been excluded
from seismic source models?, Geophys. J. Int., 118(1), 2030.
Tape, W., and C. Tape (2012a), A geometric setting for moment tensors, Geophys. J. Int.,
190(1), 476498.
Tape, W., and C. Tape (2012b), A geometric comparison of source-type plots for moment
tensors, Geophys. J. Int., 190(1), 499510.
Tape, W., and C. Tape (2012c), Angle between principal axis triples,Geophys. J. Int., 191(2),
813831.
Tape, W., and C. Tape (2013), The classical model for moment tensors, Geophys. J. Int.,
195(3), 17011720.
Tape, W., and C. Tape (2014), Supplementary material to erratum: a correction to A new
moment-tensor decomposition for seismic events in anisotropic media, Geophys. J. Int.
Tarantola, A., and B. Valette (1982), Inverse problems = quest for information, J. Geophys.,
50, 159170.
Tarasewicz, J., B. Brandsdttir, R. S. White, M. Hensch, and B. Thorbjarnardttir (2012),
Using microearthquakes to track repeated magma intrusions beneath the Eyjafjallajkull
stratovolcano, Iceland, J. Geophys. Res., 117, 113.
Tarasewicz, J., R. S. White, B. Brandsdottir, C. M. Schoonman, B. Brandsdttir, and C. M.
Schoonman (2014), Seismogenic magma intrusion before the 2010 eruption of Eyjafjalla-
jokull volcano, Iceland, Geophys. J. Int., 198(2), 906921.
Taylor, S. R. (1994), False alarms andmine seismicity - an example from theGentryMountain
mining region, Utah, Bull. Seismol. Soc. Am., 84(2), 350358.
Templeton, D. C., and D. S. Dreger (2006), Non-double-couple earthquakes in the Long
Valley volcanic region, Bull. Seismol. Soc. Am., 96(1), 6979.
Tkali, H., D. S. Dreger, G. R. Foulger, and B. R. Julian (2009), The puzzle of the 1996
Bardarbunga, Iceland, earthquake: no volumetric component in the source mechanism,
Bull. Seismol. Soc. Am., 99(5), 30773085.
Trnkoczy, A. (2011), Understanding and parameter setting of STA/LTA trigger algorithm: IS
8.1, IASPEI New Man. Seismol. Obs. Pract.
Tullos, F. N., and C. A. Reid (1969), Seismic attenuation of gulf coast sediments,Geophysics,
34(4), 516528.
Udias, A., R. Madariaga, and E. Buforn (2014), Source Mechanisms of Earthquakes: Theory
and Practice, 311 pp., Cambridge University Press.
Vasco, D. W. (1990), Moment-tensor invariants: searching for non-double- couple earth-
quakes, Bull. Seismol. Soc. Am., 80(2), 354371.
Vavryuk, V. (2001), Inversion for parameters of tensile earthquakes, J. Geophys. Res.,
106(B8), 16,33916,355.
Vavryuk, V. (2004), Inversion for anisotropy from non-double-couple components of mo-
ment tensors, J. Geophys. Res., 109(B7306).
Vavryuk, V. (2005), Focal mechanisms in anisotropic media, Geophys. J. Int., 161(2),
334346.
Vavryuk, V. (2007), On the retrieval of moment tensors from borehole data, Geophys.
Prospect., 55(3), 381391.
Vavryuk, V. (2011), Tensile earthquakes: theory, modeling, and inversion, J. Geophys. Res.,
116(B12320).
Vavryuk, V., M. Bohnhoff, Z. Jechumtlov, P. Kolar, and J. len (2008), Non-double-
couple mechanisms of microearthquakes induced during the 2000 injection experiment at
theKTB site, Germany: A result of tensile faulting or anisotropy of a rock?, Tectonophysics,
456(1-2), 7493.
Voigt, W. (1910), Lehrbuch der Kristallphysik, B.G. Teubner.
Wadati, K. (1928), Shallow and deep earthquakes, Geophys. Mag., 1, 162202.
Wadati, K. (1933), On the travel time of earthquake waves. (Part II), Geophys. Mag., VII,
101111.
Wadati, K. (1935), On the activity of deep-focus earthquakes in the Japan islands and
neighbourhoods, Geophys. Mag., 8, 305325.
Wadati, K., and S. Oki (1933), On the travel time of earthquake waves. (Part III), Geophys.
Mag., VII, 113137.
Waldhauser, F. (2001), hypoDD  A program to compute double-difference hypocenter
locations - OFR 01-113, Tech. rep., USGS.
Waldhauser, F., and W. L. Ellsworth (2000), A double-difference earthquake location al-
gorithm: method and application to the Northern Hayward fault , California, Bull. Seismol.
Soc. Am., 90(6), 13531368.
Wallace, T. C. (1985), A reexamination of the moment tensor solutions of the 1980Mammoth
Lakes earthquakes, J. Geophys. Res., 90(B13), 11,17111,176.
Walsh, D., R. Arnold, and J. Townend (2009), A Bayesian approach to determining and
parametrizing earthquake focal mechanisms, Geophys. J. Int., 176(1), 235255.
Wang, Z. (2002), Seismic anisotropy in sedimentary rocks, part 2: Laboratory data, Geo-
physics, 67(5), 14231440.
Watson, Z. (2013), Microseismicity within the Krafla caldera, north east Iceland: Seismicity
within a cluster and non-double-couple events, Part III research project, University of
Cambridge.
Wber, Z. (2006), Probabilistic local waveform inversion for moment tensor and hypocentral
location, Geophys. J. Int., 165(2), 607621.
Wennerberg, L. (1993), Multiple-scattering interpretations of coda-Q measurements, Bull.
Seismol. Soc. Am., 83(1), 279290.
Weston, J., A. M. G. Ferreira, and G. J. Funning (2014), Joint earthquake source inversions
using seismo-geodesy and 3-D earth models, Geophys. J. Int., 198(2), 671696.
White, R. S., J. Drew, H. R. Martens, J. Key, H. Soosalu, and S. S. Jakobsdttir (2011),
Dynamics of dyke intrusion in the mid-crust of Iceland, Earth Planet. Sci. Lett., 304(3-4),
300312.
Wilks, M., I. Bradford, M. Williams, I. V. Rodriguez, and D. Pugh (2015), Combined use
of a deep monitoring array and shallow borehole arrays for moment tensor inversion: Th
N108 09, in 77th EAGE Conf. Exhib.
Willemann, R. J. (1993), Cluster analysis of seismic moment tensor orientations, Geophys.
J. Int., 115, 617634.
Withers, M., R. Aster, C. Young, J. Beiriger, M. Harris, S. Moore, and J. Trujillo (1998),
A comparison of select trigger algorithms for automated global seismic phase and event
detection, Bull. Seismol. Soc. Am., 88(1), 95106.
Wolfram Research (1998), The Wolfram functions site, http://functions.wolfram.com.
Wulff, G. (1902), Untersuchungen im Gebiete der optischen Eigenschaften isomorpher
Kristalle, Zeits. Krist., 36.
Yagi, Y., and Y. Fukahata (2011), Introduction of uncertainty of Greens function into
waveform inversion for seismic source processes, Geophys. J. Int., 186(2), 711720.
Yokota, Y., Y. Kawazoe, S. Yun, S. Oki, Y. Aoki, and K. Koketsu (2012), Joint inversion
of teleseismic and InSAR datasets for the rupture process of the 2010 Yushu, China,
earthquake, Earth, Planets Sp., 64(11), 10471051.
Zahradnik, J., J. Jansky, and K. Papatsimpa (2001), Focal mechanisms of weak earthquakes
from amplitude spectra and polarities, Pure Appl. Geophys., 158(4), 647665.
Zahradnk, J., J. Jansk, and V. Plicka (2014), Analysis of the source scanning algorithm
with a new P-wave picker, J. Seismol., 19(2), 423441.
Zecevic, M., L. De Barros, C. J. Bean, G. S. OBrien, and F. Brenguier (2013), Investigating
the source characteristics of long-period (LP) seismic events recorded on Piton de la
Fournaise volcano, La Reunion, J. Volcanol. Geotherm. Res., 258, 111.
Zwolak, J. W., P. T. Boggs, and L. T. Watson (2004), ODRPACK95 : A weighted orthogonal
distance regression Code with Bound Constraints, Tech. rep.
AppendixA Distribution of then-dimensional
inner product
For n-dimensional spherical coordinates, there are n 1 angular coordinates (1, . . . , n1)
and one radial coordinate (r). The angular coordinates take values between 0 and  except
for n1 which takes values between 0 and 2.
Cartesian coordinates can be defined in terms of these angular values, with one of the
relationships given by x1 = r cos1. Without any loss of generality the distribution of the
angle between two (normalised) random vectors can be considered from the distribution of
the inner product between one (normalised) random vector and the pole at x1 = 1, which
simplifies to the distribution of the x1 coordinate. This is proportional to the area of the
surface of the n-sphere at a given angle . The area element of a unit n-dimensional sphere
dA = sinn2 1 sinn3 2 . . . sinn2d1d2 . . . dn1, (A.1)
and the total area can be determined from integrating over all the parameters.
Therefore the distribution of  the angle between the two random vectors is given by
p () =
2 sinn2 
0 sin
n3 2d2 . . .
0 sinn2dn2
0 sin
n2 1
0 sin
n3 2d2 . . .
0 sinn2dn2
(A.2)
sinn2  n > 2
k n = 2
, (A.3)
where the normalisation can be evaluated for the specific dimension. It is clear that the
distribution is proportional to sinn2  (Eq. A.3) for all n > 2.
The distribution of the inner product is dependent on cos so it is necessary to transform
the PDF from Eq. A.3. This transformation just relies on d cos
=  sin, because
p (cos) d cos = p () d
 p (cos) = p ()
d cos
, (A.4)
therefore the PDF for cos is
p (cos)  sinn3 
1 cos2 
, (A.5)
1 cos2 
2 d cos. (A.6)
The normalisation constant (N) can be evaluated for a given dimension, however it is
possible to convert the integrand in Eq. A.6 to a hypergeometric function and integrate
that. This is the approach that many computational integration approaches take, such as
Mathematica and Wolfram Alpha1.
A series,
k=0 tk, is hypergeometric if the ratio of successive coefficients is a rational
function of k, such as
A (k)
B (k)
z, (A.7)
where A (k) and B (k) are polynomials for k. A hypergeometric series can be represented as
a hypergeometric function:
pFq (a1, a2, . . . , ap; b1, b2, . . . , bq; z) =
(a1)k (a2)k . . . (ap)k
(b1)k (b2)k . . . (bq)k
(A.8)
with (a)k the rising factorial function:
(a)k =
a (a+ 1) . . . (a+ k  1) k > 1
1 k = 0
. (A.9)
To generate the hypergeometric function, the ratio of successive terms (Eq. A.7) must be
factorised according to
(k + a1) . . . (k + ap)
(k + b1) . . . (k + bq) (k + 1)
z. (A.10)
For the integrand in Eq. A.6 , substituting x = cos, and using a binomial expansion
gives the infinite series(
1 x2
2 = 1
2 1
(n 3) (n 5)
4 2
(n 3) (n 5) (n 7)
8 3
x6 . . . , (A.11)
with a ratio of successive terms
k + 1 n2
k + 1
x2. (A.12)
The factorisation to obtain the form in Eq. A.10 is arbitrary, and can have cancellations,
so is non-unique. For this case, the chosen factorisation,
k + 32
k + 3n2
k + 32
(k + 1)
x2, (A.13)
gives the Gaussian hypergeometric function(
1 x2
2 = 2F1
. (A.14)
This specific factorisation is chosen to use the differential identity (Wolfram Research,
Citation No: 07.23.20.0032.02)
2 , b; c;x
= 2F1
, b; c;x2
, (A.15)
1See http://reference.wolfram.com/language/tutorial/Integration.html,
http://reference.wolfram.com/language/ref/Integrate.html,
and http://www.wolframalpha.com/calculators/integral-calculator/.
http://reference.wolfram.com/language/tutorial/Integration.html
http://reference.wolfram.com/language/ref/Integrate.html
http://www.wolframalpha.com/calculators/integral-calculator/
but there are a family of hypergeometric functions that could be obtained.
Consequently, the normalisation constant is
1 x2
= 2 2F1
, (A.16)
and the normalised PDF takes the form
p (cos) =
2 2F1
2 ; 1
) (1 cos2 )n32 . (A.17)
The normalisation constant corresponds to different analytical functions for different dimen-
sions. It is also simple to calculate the CDF using the same approach, giving
P (cos < a) =
a 2F1
 2F1
. (A.18)
Fig. A.1 shows that the PDF for cos (Eq. A.17) fits the simulated distribution for a
range of dimensions, confirming that it is the true distribution.
1 0 1
1 0 1
1 0 1
Figure A.1: Simulated distribution of cos for different dimensions (n=2,3,4,5,6, 7,10, 15 and 40).
The red line corresponds to the normalised PDF (Eq. A.17), and the blue bars are the simulated
distribution of the dot product of random n-dimensional vectors drawn according to algorithm B.1
generalised to n-dimensions (Muller, 1959; Marsaglia, 1972). 1,000,000 samples were used for the
random simulation.
Appendix B Random Moment Tensors
The definition of what constitutes a random moment tensor is not straightforward. It depends
on several assumptions about the source, including coordinate independence and the source
type, which are explored below.
B.1 Coordinate Independence
Random sampling of the moment tensor should be coordinate independent, since the moment
tensor is independent of the coordinate system.
Figure B.1: Plot of uniform samples on the
surface of a three dimensional sphere from lines
of longitude() and latitude ()
Anexample of non-coordinate independ-
ent sampling is shown in Fig. B.1. Sampling
uniformly along latitude and longitude will
have higher-density sampling closer to the
poles, which is clearly not uniform, and will
lead to a bias in any estimate of a function
on the sphere surface.
Random sampling can still have a direc-
tional bias. Samples drawn randomly from 
and  on the sphere increase in density close
to the poles (Fig. B.2 (a)).
It is possible to construct samplingmethods that are coordinate independent. The common
approach for sampling on the surface of a n-sphere is to draw samples from the n-dimensional
Gaussian distribution and normalise these to the surface of the sphere (Algorithm B.1,Muller
(1959);Marsaglia (1972)), as seen in Fig. B.2 (b). Another approach for three dimensions is
to sample uniformly in  and cos  as shown in Fig. B.2 (c). This works for three dimensions,
because the distribution of the three dimensional inner product is a special case, as discussed
below.
B.1.1 Sampling from hyper-spherical coordinates
The distributions of the hyper-spherical angular coordinates are given by the distributions
of the inner product from Eq. A.17 for two to n dimensions. This can be derived from the
hyperspherical coordinates (Eq. B.1), where the n  1 angular coordinates are in the range
0 6 k 6  except for 0 6 n1 < 2 and the distribution of the inner product.
Figure B.2: Plots of different random sampling approaches in two dimensions on the sphere. (a)
shows random sampling from 0 6  6  and 0 6  6 2. (b) shows random samples from the
normal distribution normalised to the surface of the sphere. (c) shows random samples from
1 6 cos  6 1 and 0 6  6 2 .
Increasing the dimension of Eq. A.17 to n + 1 corresponds to multiplying the n-
dimensional distribution by sin for the n + 1 dimensional distribution. Therefore, the
distribution of cosk is given by the n  k + 1 dimensional form of the PDF, since the
distribution of xk is given by the n-dimensional and hyper-spherical coordinates
sink1 1 sink2 2 . . . cosk k < n
sink1 1 sink2 2 . . . sinn1 k = n
, (B.1)
where the n 1 angular coordinates are in the range 0 6 k 6  except for 0 6 n1 < 2.
Eq. A.17 is a proportional to sinn3 , so increasing the dimension from n to to n + 1
corresponds to multiplying the n-dimensional distribution by an additional sin.
For n-dimensional hyperspherical coordinates, the distribution of xk for any k 6 n is
given by the n-dimensional form of Eq. A.17, since the hyper-spherical symmetry means all
the Cartesian coordinates have the same distribution. Consequently, the distribution of the
angular coordinates varies, with the distribution of 1 given by Eq. A.17. Eq. A.2 enables
the distributions of the other angular coordinates to be determined, giving
p (k|n) =

2 sinnk1 k
i=1,i 6=k
0 sin
nk1 kdk
0 sin
nk1 kdk
k < n 1
1/2 k = n 1
(B.2)
sinnk1 k k < n 1
k k = n 1
, (B.3)
which is the n  k + 1 dimensional form of Eq. A.17. This can be verified by numerical
simulation, as shown in Fig. B.3, which confirms that
p (cosk|n) =
2 2F1
2+kn
2 ; 1
) (1 cos2 )nk22 (B.4)
is the correct form of the distribution of the cosine of hyper-spherical angular coordinates
(converted following Eqs. A.16-A.18) for random samples on the surface of a 7-sphere.
1 0 1
cos  
1 0 1
cos  
1 0 1
cos  
1 0 1
cos  
1 0 1
cos  
1 0 1
cos  
Figure B.3: Distribution of cosi for the 6 angles on a 7-sphere.
To generate random samples on the n-sphere using this approach requires an iterative
sampling approach because the CDF for the n-dimensional inner product (Eq. A.18) is not
always analytically invertible. This means that the common approach of sampling from an
unknown distribution by drawing a random sample from the range 0 1 and determining the
coordinate by inverting the CDF does not work.
B.2 Generating a Random Moment Tensor
The moment tensor is a three-dimensional symmetric tensor, meaning it has six indepen-
dent components. These components describe a six-dimensional Euclidean moment tensor
space, where each point in this space maps to a unique moment tensor (Voigt, 1910;Chapman
and Leaney, 2011). To simplify this space, the scalar moment magnitude can be separated,
normalising the moment tensor (Eq. 3.12). Therefore the six-dimensional vector corre-
sponding to the moment tensor can also be normalised by scaling the coordinates such that
|M | = 1 means the moment tensor is normalised (Eq. 2.8).
The normalised moment tensor describes a unit 6-sphere in the moment tensor space, so
random moment tensors can be generated from uniformly sampling the surface of the unit
6-sphere. As discussed above, sampling from the appropriate hyper-spherical coordinates
distributions is not practical because the CDFs, generated by integrating Eq. B.3 for suitable
dimensions:
P (cos1 < a) 
1 a2 
1 a2 +
sin1 a+ k, (B.5)
P (cos2 < a)  a
+ k, (B.6)
P (cos3 < a) 
1 a2 +
sin1 a+ k, (B.7)
P (cos4 < a)  a+ k, (B.8)
P (cos5 < a)  sin1 a+ k, (B.9)
are not invertible.
The randomly selected tensor should not depend on the coordinate system chosen, so
that any rotation in the spatial coordinate system should have no impact on the distribution
of random tensors. Furthermore, the samples can be drawn to maintain independence in
the coordinates of the moment tensor space. Therefore samples can be drawn from the
six-dimensional normal distribution (Algorithm B.1), which produces random samples of
the normalised full moment tensor space.
Algorithm B.1 Random Moment Tensors (Muller, 1959; Marsaglia, 1972)
1. Generate random sample
Sample from six dimensional normal distribution:
x  N6 (0, 1)
2. Normalise sample
Sample normalised onto surface of unit six-sphere:
/6 /12 0 /12 /6
/2 /4 0 /4 /2
0 /2  3/2 2
0 0.5 1
/2 /4 0 /4 /2
Figure B.4: Uniformly distributed random sampling of the moment tensor space in the Tape
parameterisation
The distribution of normalised random moment tensors from algorithm B.1 in the param-
eterisation of Tape and Tape (2012a) (Section 2.4.2) is shown in Fig. B.4. An approximation
of the parameter distributions are:
p () =
cos (3) , (B.10)
p () =
( + /2)
, 5.7479, 5.7479
, (B.11)
p () =
, (B.12)
p (h) = 1,
p () =
, (B.13)
where  (x, a, b) is the beta distribution (Johnson et al., 1995). The beta distribution in Eq.
8.16 does not quite fit the simulated data, however it is very close, as is confirmed by the
CDF (Fig. B.5). The values for a = b = 5.7479 have been determined empirically from a
sample of 100, 000, 000 randomly distributed moment tensors.
/2 /4 0 /4 /2
Figure B.5: Plot of the corresponding Beta CDF for Eq. B.11 in red, and a histogram of samples of
random moment tensors.
B.3 Generating a random example of a specific type of mo-
ment tensor
For a specific source model, there is a different approach to describing a random source, since
what is expected now is a random source from the specific source model, not a completely
random moment tensor as described in section B.2. If the source model can be split into
orientation and source type, there is an alternate approach of generating a random source.
The moment tensor is a symmetric tensor and therefore it can be diagonalised to give its
eigenvalues and eigenvectors. Consequently, by sampling the range of possible orthogonal
eigenvectors uniformly (section B.3.1), it is possible to generate a random sample of moment
tensors for a specific type. If there are multiple eigenvalue combinations for a specific type
(e.g. CLVD or tensile crack), each set of eigenvalues can be sampled uniformly according to
the source model. These samples are independent of the spatial coordinate system chosen as
long as the eigenvector sampling is coordinate independent.
B.3.1 Sampling the eigenvector space
For a real symmetric tensor the eigenvectors form a set of orthogonal vectors, which can be
written in a right-handed manner. Algorithm B.2 shows an approach for generating a random
set of right-handed orthogonal eigenvectors.
Algorithm B.2 Random Eigenvector Sampling
1. Randomly select an initial vector
Sample from the three dimensional normal distribution and normalise:
a  N3 (0, 1)
2. Generate an orthogonal vector Select another random vector, x as above, and as long
as it isnt parallel to a, a normalised vector orthogonal to a can be obtained:
a x
|a x|
3. Generate the right handed set of eigenvectors Cross the two vectors to get the third in
a right handed set:
c = a b
This process has generated a randomly orientated set of eigenvectors (Fig. B.2), allowing
random generation of any specific moment tensor type. It is equally possible to sample from
the corresponding distributions of other orientation parameters such as strike, dip and rake,
or the two possible double-couple fault plane normals.
0  2
0 0.5 1
/2 0 /2
Figure B.6: Plot showing the uniform distribution of eigenvectors as their corresponding orientation
parameters from algorithm B.2.
B.3.2 Separated Eigenvalue and Eigenvector sampling
Algorithm B.3 Random Moment Tensor Sampling From an Eigenvalue Distribution
1. Generate Random Eigenvectors
Use algorithm B.2 to generate random eigenvectors: e1, e2, e3
2. Generate Random Eigenvalues
Generate eigenvalues from random sampling of the desired eigenvalue distribution:
1, 2, 3
2. Generate Random Moment Tensor
Rotate the diagonalised eigenvalue matrix, , using the eigenvectors matrix E:
M = EET
Section B.3 suggests another possible approach to generating the source mechanism, which
is to generate eigenvalues from a given distribution and combine these with random orien-
tation components. This means the eigenvalue distribution can be chosen to satisfy certain
conditions, such as the Hudson distribution of eigenvalues 2.98 - 2.100.
As mentioned before, the definition of randomness is hard to describe, especially when
dealing with moment tensors. It is important to have coordinate independence, however both
the method based on that ofMuller (1959) (Algorithm B.1), and that of separated eigenvalue
and eigenvector sampling are independent of the spatial coordinates. Fig. B.7 shows the
distribution of the different MT coordinates for different approaches to random sampling of
the moment tensor space.
1 0 1
1 0 11 0 1
Figure B.7: Comparison of the distribution of different random sampling approaches for the
moment tensor. The first column corresponds to the approach described in algorithm B.1, with the
red line corresponding to the distribution of the dot product of randomly orientated six-vectors on the
hyper-sphere (Appendix A). The second column corresponds to the Tape and Tape (2012b)
assumption that the eigenvalues are uniformly distributed on the surface of the fundamental Lune, L,
and the third column corresponds to the Hudson et al. (1989) assumption of the uniform distribution
of eigenvalues between 1 6  6 1.
As expected, the approach fromalgorithmB.1 is independent of both the spatial coordinate
system, and the six-vector coordinate system. The samples using algorithmB.3with a uniform
eigenvalue distribution over the fundamental lune (Tape and Tape, 2012b), or uniformly in
the range 1 6  6 1 (Hudson et al., 1989), are both independent of the spatial coordinate
system but not the six-vector coordinate system. This adds complexity if a measure like the
angle between moment tensor six-vectors is used as a distance measure between moment
tensors (Section 2.8.1), since the prior distribution of this angle is no longer uniform, but
depends on the initial direction in the six-vector space.
B.3.3 Generating random classical model source
An example source model is the classical source model, which is identical to the CDC model
(Sections 2.3.2 and 2.3.4). A random tensile potency tensor (Section 2.3.6), with opening
angle , can be generated by sampling from the distribution of cos, and then rotating the
resultant diagonalised eigenvalue matrix by some random eigenvectors from algorithm B.2.
This is identical to sampling from randomly orientated fault normals and slip vectors and
combining them into the potency tensor using algorithmB.4, since these are three dimensional
vectors, so the distribution of cos is uniform (Appendix A).
Algorithm B.4 Random crack+double-couple source potency tensor
1. Randomly select the fault normal
Sample from the three dimensional normal distribution and normalise:
n  N3 (0, 1)
2. Randomly select the slip vector
Sample from the three dimensional normal distribution and normalise:
u  N3 (0, 1)
u =
3. Combine to give the Potency Tensor
Combine the outer products to give the unit potency tensor (Eq. 2.28):
unT + nuT
To generate the moment tensor from this source requires the elastic parameters, which
can be combined according to 2.26. This suggests an approach that can be used to generate
random tensile sources. However, generating random sources using this approach may not
realistically be feasible since the elastic parameters have some uncertainty, and anisotropy
can generate a non-linear distribution of moment tensors on the fundamental eigenvalue lune
(Section 2.3.6 and appendix C).
B.4 Summary
Despite the temptation to generate random potency tensors from the classical source model
and convert them to the moment tensor using the elastic parameters, the elastic parameters
are rarely known well enough for this to be done satisfactorily. Additionally, it is not clear
how the presence of the fault volume can change the elastic parameters from those of the
unfractured rock. Instead, random moment tensors can be generated according to either
algorithm B.1, or algorithm B.3 for some assumed eigenvalue distribution.
However, since there is no physical basis for either of the proposed eigenvalue distributions
(Tape and Tape, 2012b; Hudson et al., 1989), and the approach of algorithm B.1 includes
coordinate independence in the moment tensor 6-space of valid moment tensors, this may
be a preferable (and quicker) approach to generating a random moment tensor. The resultant
moment tensor can then be converted to a given potency tensor for some estimated stiffness
parameters, perhaps using the deviation from a tensile source as an indicator of the confidence
in the elastic parameters.
Appendix C Effects of Rock Anisotropies
on the Moment Tensor
Wang (2002) produced a large dataset of vertical transverse isotropic (VTI) elastic parameters
for different materials in different pressure conditions. This appendix contains several dif-
ferent plot types corresponding to the different materials measured by Wang. The first plots
(Fig. C.1 - C.4) correspond to randommoment tensors generated from random crack+double-
couple potency sources and the anisotropy values from the corresponding tables in Wang.
Figure C.1: Plot of the range of moment tensors generated from tensile+shear model potency
tensors for the anisotropy values from Table 1 of Wang (2002). The red plots correspond to the
highest overburden pressure, and the blue the lowest overburden pressure measured by Wang (2002).
The green and cyan lines correspond to the Poissons ratio for the isotropic approximation to the
anisotropic medium (Chapman and Leaney, 2011, eqs 81a and 81b) for the highest and lowest
measured overburden pressures.
Figure C.2: Plot of the range of moment tensors for the data Table 2 of Wang (2002) as described in
Fig. C.1.
Figure C.3: Plot of the range of moment tensors for the data from Table 3 of Wang (2002) as
described in Fig. C.1
Figure C.4: Plot of the range of moment tensors for the data from Table 4 ofWang (2002) as
described in Fig. C.1.
The spherical average isotropic approximation of the anisotropy (Chapman and Leaney,
2011, eqs 81a and 81b) is usually consistent with the distribution of moment tensors. It is
clear that for some materials, the anisotropy has a much stronger effect than others, leading
to a much wider distribution of moment tensors from the isotropic approximation. Also, the
overburden pressure can have an effect, sometimes reducing the anisotropy, as well as rotating
the isotropic approximation Poissons ratio and the associated moment tensor distribution.
There is an additional effect depending on whether the material was brine- or gas-saturated,
however there is no clear trend for this effect between materials.
 D1  E1  E3  E4  E5
 A1  A2  G1  C1  G3
 G5  G28  G30  G32
0 0.2 0.4 0.6
0 0.2 0.4 0.6 B2
0 0.2 0.4 0.6
0 0.2 0.4 0.6
0 0.2 0.4 0.6
Figure C.5: Distribution of Poissons ratios () calculated using an isotropic approximation moment
tensors generated from classical model potency tensors for the anisotropy values from Table 1 of
Wang (2002). The red line corresponds to the Poissons ratio of the spherical isotropic approximation
of the elastic parameters (Chapman and Leaney, 2011, eqs 81a and 81b). The upper plot in each pair
corresponds to the lowest overburden pressure and the lower plot the highest overburden pressure.
 D2  E2b  E5b  G8b  G13b
 G14b  G16b  G17b  E2g  E5g
0 0.2 0.4 0.6
0 0.2 0.4 0.6
0 0.2 0.4 0.6
0 0.2 0.4 0.6
0 0.2 0.4 0.6
Figure C.6: Distribution of Poissons ratios for the data from Table 2 of Wang (2002), as described
in Fig. C.5.
 CC1g  CC2g  CC3g  CC4g  CC5g
 CC6g  CC8g  CC9g  CC10g  CC11g
 CC12g  CC13g  CC14g  CC16g  CC17g
 CC18g  CC19g  CC20g  CC21g  CC22g
 CC23g  CC24g  CC25g  CC26g  CC28g
 CC29g  CC30g  CC31g
0 0.2 0.4 0.6
 CC32g
0 0.2 0.4 0.6
 CC33g
0 0.2 0.4 0.6 GC1g
0 0.2 0.4 0.6
0 0.2 0.4 0.6
Figure C.7: Distribution of Poissons ratios for the data from Table 3 of Wang (2002), as described
in Fig. C.5.
The Poissons ratio estimates (Figs. C.5 - C.8) are mostly consistent with the spherical
average isotropic approximation of the anisotropy, although there are a few cases when they
are distinct, such as in sample E5b (Fig. C.6). The mean can be skewed significantly by
values from samples close to the double-couple point, where small variations in  and  lead
to large changes in .
 CC3b  CC4b  CC5b  CC6b  CC8b
 CC9b  CC11b  CC13b  CC16b  CC18b
 CC20b  CC21b  CC22b  CC23b  CC24b
 CC25b  CC26b  CC28b  CC29b  CC30b
 CC31b
0 0.2 0.4 0.6
 CC32b
0 0.2 0.4 0.6
 CC33b
0 0.2 0.4 0.6
0 0.2 0.4 0.6
0 0.2 0.4 0.6
Figure C.8: Distribution of Poissons ratios for the data from Table 4 of Wang (2002), as described
in Fig. C.5.
 D1  E1  E3  E4  E5
 A1  A2  G1  C1  G3
 G5  G28  G30  G32
0 30 60
0 30 60 B2
0 30 60
0 30 60
0 30 60
Figure C.9: Distribution of 0, the angle between the principle axis triples (Kagan, 1982, 2005;
Tape and Tape, 2012c), for random moment tensors and the potencies generated from the classical
fault model for the anisotropy values from Table 1 of Wang (2002).
 D2  E2b  E5b  G8b  G13b
 G14b  G16b  G17b  E2g  E5g
0 30 60
0 30 60
0 30 60
0 30 60
0 30 60
Figure C.10: Distribution of the angle between the principle axis triples for the data from Table 2 of
Wang (2002), as described in Fig. C.9.
 CC1g  CC2g  CC3g  CC4g  CC5g
 CC6g  CC8g  CC9g  CC10g  CC11g
 CC12g  CC13g  CC14g  CC16g  CC17g
 CC18g  CC19g  CC20g  CC21g  CC22g
 CC23g  CC24g  CC25g  CC26g  CC28g
 CC29g  CC30g  CC31g
0 30 60
 CC32g
0 30 60
 CC33g
0 30 60 GC1g
0 30 60
0 30 60
Figure C.11: Distribution of the angle between the principle axis triples for the data from Table 3 of
Wang (2002), as described in Fig. C.9.
 CC3b  CC4b  CC5b  CC6b  CC8b
 CC9b  CC11b  CC13b  CC16b  CC18b
 CC20b  CC21b  CC22b  CC23b  CC24b
 CC25b  CC26b  CC28b  CC29b  CC30b
 CC31b
0 30 60
 CC32b
0 30 60
 CC33b
0 30 60
0 30 60
0 30 60
Figure C.12: Distribution of the angle between the principle axis triples for the data from Table 4 of
Wang (2002), as described in Fig. C.9.
 D1  E1  E3  E4  E5
 A1  A2  G1  C1  G3
 G5  G28  G30  G32
6030 0 30 60
6030 0 30 60 B2
6030 0 30 60
6030 0 30 60
6030 0 30 60
 (o)
Figure C.13: Distribution of the deviation in opening angle  of the moment tensor from the
potency tensor generated from the classical fault model and the anisotropy values from Table 1 of
Wang (2002). The red line corresponds to no change in opening angle.
 D2  E2b  E5b  G8b  G13b
 G14b  G16b  G17b  E2g  E5g
6030 0 30 60
6030 0 30 60
6030 0 30 60
6030 0 30 60
6030 0 30 60
 (o)
Figure C.14: Distribution of the deviation in opening angle for the anisotropy values from Table 2
of Wang (2002), as described in Fig. C.13.
 CC1g  CC2g  CC3g  CC4g  CC5g
 CC6g  CC8g  CC9g  CC10g  CC11g
 CC12g  CC13g  CC14g  CC16g  CC17g
 CC18g  CC19g  CC20g  CC21g  CC22g
 CC23g  CC24g  CC25g  CC26g  CC28g
 CC29g  CC30g  CC31g
6030 0 30 60
 CC32g
6030 0 30 60
 CC33g
6030 0 30 60 GC1g
6030 0 30 60
6030 0 30 60
 (o)
Figure C.15: Distribution of the deviation in opening angle for the anisotropy values from Table 3
of Wang (2002), as described in Fig. C.13.
 CC3b  CC4b  CC5b  CC6b  CC8b
 CC9b  CC11b  CC13b  CC16b  CC18b
 CC20b  CC21b  CC22b  CC23b  CC24b
 CC25b  CC26b  CC28b  CC29b  CC30b
 CC31b
6030 0 30 60
 CC32b
6030 0 30 60
 CC33b
6030 0 30 60
6030 0 30 60
6030 0 30 60
 (o)
Figure C.16: Distribution of the deviation in opening angle for the anisotropy values from Table 4
of Wang (2002), as described in Fig. C.13.
The change in orientation angle (0) (Figs. C.9 - C.12) is usually small, but there are a
few materials which had some sources with rotations greater than 60, so this rotation could
have a large effect on the source interpretation.
The opening angle variation (Figs. C.13-C.16) is symmetrical, with distributions that
are usually well constrained, suggesting that there is little change between the source-type
interpretation of the moment tensor and the potency tensor. However, some materials had
changes in opening angle bigger than 60, so the source type could be mis-interpreted if
carried out on the moment tensor, rather than the potency tensor.
All of these distributions assume that the anisotropy of the source would reflect the
anisotropy of the bulk medium, which may not be the case.
Many of the examples would be well approximated as an isotropic material, but the
strongest anisotropies can significantly change the orientation and source-type between the
moment tensor and potency tensor.
Appendix D Homogeneous Isotropic Re-
sponse functions
There are several approaches to calculating the response functions, but the far-field response
functions are straightforward to calculate for the simplest structure, a homogeneous isotropic
velocity structure. The different components are related to the direction vectors for the rays.
Considering a system of orthogonal basis vectors on the sphere with  the angle from the
1-axis and  the angle from the 3-axis gives  the radial unit vector,  the unit vector along
lines of longitude, and  the equatorial unit vector:

sin  cos
sin  sin
cos 
 ,  =

cos  cos
cos  sin
sin 
 ,  =

 sin
 . (D.1)
For a point with a take-off angle  and azimuth , the displacement components are given by
(Aki and Richards, 2002)
43r
 TM
 = FP
 TM
uSH =
43r
 = FSH
, (D.2)
uSV =
43r
 = FSV
whereF is the propagation effect, containing the geometric spreading of the wave and effects
of velocity. These could be used to invert for the source in a homogeneous isotropic medium
by constructing a suitable inverse function.
D.1 Radiation Components
The radiation components, dependent on the position on the focal sphere and the moment
tensors are
RP =  TM
RSH = TM (D.3)
RSV = TM
The radiation components can be separated into coefficients for the different moment
tensor components. This allows a calculation of the amplitude for the different components
at the source, which can be used to help determine the moment tensor. The P-wave radiation
components are given by
sin  cos sin  sin cos 

M11 M12 M13
M12 M22 M23
M13 M23 M33


sin  cos
sin  sin
cos 
 ,
RP = M11 sin2  cos2 +M22 sin2  sin2 +M33 cos2 
+M12 sin2  sin 2+M13 sin 2 cos+M23 sin 2 sin, (D.4)
and similarly the SH and SV components can be calculated as
RSH = M11 sin  cos sin+M22 sin  cos sin
+M12 sin  cos 2M13 cos  sin+M23 cos  cos, (D.5)
RSV = M11 sin  cos  cos2 +M22 sin  cos  sin2 M33 cos  sin 
+M12 sin 2 cos sin+M13 cos 2 cos+M23 cos 2 sin. (D.6)
These coefficients describe the amplitude of the different components at the source,
however to extend this further to a measurement at a receiver, propagation effects from Eq.
D.2 need to be accounted for. Where the medium is more complex then the calculation of
the propagation effects becomes similarly more complex.
These amplitude coefficients can be written in terms of the moment tensor six-vector (Eq.
2.8):

 =

sin2  cos2  1/2 sin 2 cos2  12 sin  sin 2
sin2  sin2  1/2 sin 2 sin2  12 sin  sin 2
cos2  1/2 sin 2 0
2 sin 2 sin 1/2 cos 2 sin 1/2 cos  cos
2 sin 2 cos 1/2 cos 2 cos 1/2 cos  sin
2 sin2  sin 2 1/2 sin 2 cos sin 1/2 sin  cos 2



(D.7)
AppendixE MaximumEntropyDistribu-
tion for Mean and Variance
Consider a function g (x) with mean  and variance 2. The entropy is given by
g (x) ln (g (x)) dx, (E.1)
with the maximum entropy at the stationary point of the functional
F (x, g (x) , g (x)) dx = g (x) ln (g (x)) . (E.2)
There are two constraints to apply: g (x) is normalised 
G (x, g (x) , g (x)) dx =
g (x) dx = 1, (E.3)
and the variance is 
H (x, g (x) , g (x)) dx =
(x )2 g (x) dx = 2. (E.4)
The constrained stationary value can be found by maximising the Euler-Lagrange functional
L (x, g (x) , g (x)) =
F + G+ H, (E.5)
where  and  are Lagrange undetermined multipliers. The Euler-Lagrange equation for this
functional is
= 0, (E.6)
 ln (g (x)) + 1 + +  (x )2 = 0, (E.7)
because F , G, and H are independent of g (x). This can be solved to give an expression for
g (x),
g (x) = e(x)
21. (E.8)
Applying the constraints on g (x) (Eqs E.3 and E.4) gives a relationship for  and :
, (E.9)
e1 =
. (E.10)
Substituting for these into Eq. E.8 gives a Gaussian distribution for the maximum entropy
distribution for a mean and variance
g (x) =
(x)2
22 . (E.11)
Appendix F Truncated Gaussian Distri-
bution
The truncated normal distribution, truncated between two boundaries, a lower boundary a
and an upper boundary b has the PDF given by
p (x|, , a, b) =
(x)2
 ()  ()
, (F.1)
where  = a
and  = b
and  (x) = 1
x2dx.
F.1 Moments
The mean of a PDF can be evaluated using
xp (x|, , a, b) dx. (F.2)
In the case of the truncated Gaussian, substituting for u = x
, the mean is
 = + 
 ()  ()
 ()  ()
, (F.3)
where  (x) = 1
ex2 .
Similarly to the mean, the variance of the truncated distribution can be calculated from
the PDF using  b
x2p (x|, , a, b) dx = 2 + 2. (F.4)
Substituting for u = x
and integrating by parts, the variance is given by
2 = 2
1 +  ()  ()
 ()  ()
 ()  ()
 ()  ()
)2 . (F.5)
The third moment of a PDF is b
x3p (x|, , a, b) dx = 1
3 + 3 + 32, (F.6)
where 1 is the skewness. Again substituting u = x , simplifying using the previous results
and then integrating by parts, the skewness term is given by
1 =
(2  1) () (2  1) ()
 ()  ()
 ()  ()
 ()  ()
 ()  ()
 ()  ()
 ()  ()
 ()  ()
)3 6 2
 ()  ()
 ()  ()
. (F.7)
F.2 Combining Truncated Gaussian PDFs
Truncated Normal PDFs combine just like full normal PDFs however the lower limit and
upper limits change. Combining the two truncated Gaussians
p (x|1, 1, a1, b1) =

221
 (x1)
221 1
(b1)(a1)
a1 6 x 6 b1
0 x > b1, x < a1
, (F.8)
p (x|2, 2, a2, b2) =

222
 (x2)
222 1
(b2)(a2)
a2 6 x 6 b2
0 x > b2, x < a2
, (F.9)
gives
p (x|1, 2, 1, 2, a1, a2, b1, b2) 

212
 (x1)
 (x2)
222 max (a1, a2) 6 x 6 min (b1, b2)
0 x < max (a1, a2) , x > min (b1, b2)
(F.10)
The exponents in Eq. F.10 can be combined into another truncated Gaussian distribution by
completing the square (see Appendix H) giving
p (x|, , a, b) =
(x)2
22 1
(b)(a) a 6 x 6 b
0 x > b, x < a
, (F.11)
with the parameters given by:
2 + 221
22 + 21
, (F.12)
 2122
22 + 21
, (F.13)
a = max (a1, a2) , (F.14)
b = min (b1, b2) . (F.15)
It is important to note that the combination happens as for full Gaussian PDFs, i.e. the
new value of  does not correspond to the true mean of the truncated distribution, similarly
2 is not the true variance of the truncated distribution but the corresponding full Gaussian
Appendix G Distribution of the Ratio of
Random Exponential Variables
Samples of an exponentially distributed variable
p (X = x) = ex, (G.1)
can be generated by applying the inverse of the CDF
P1 (u) =
ln (1 u)
, (G.2)
to random values generated from the uniform distribution between 0 and 1. If u is generated
from the uniform distribution U (0, 1) the inverse CDF generates random samples from the
exponential distribution (Fig. G.1).
IfX and Y are independent exponentially distributed variables with the same exponential
constant, the CDF for observing the ratio W = Y
is given by Eq. G.3. The PDF is given
by the derivative of the CDF with respect to w, which can be carried out under the integral
using Leibnizs rule:
p (x, y) dy dx
2e(x+y)dy dx
2ex
1 ewx
dx, (G.3)
p (w) =
P (W 6 w)
ex  e(w+1)x
2xe(w+1)xdx. (G.4)
Integrating by parts gives
p (w) =
(w + 1)2
, (G.5)
which fits a simulated distribution (Fig. G.2). The PDF for the ratio of two exponential
distributions truncated at the lower limit x0 and no upper limit (Fig. G.3), such as
p (x) =
e(x0x) x > x0
0 x < x0
, (G.6)
2 4 6 8 10 12
Figure G.1: Distribution of random samples
drawn from a non-truncated exponential
variables (blue) using Eq. G.2. The red line
shows the PDF from Eq. G.1.
1 2 3 4 5
Figure G.2: Distribution of the ratio of
random samples drawn from two
non-truncated exponential variables (blue).
The red line shows the PDF from Eq. G.5.
can be calculated by considering the conditional probability for the ratio (W = Y
) for allowed
values of x and w:
p (w|X = x) = |x| p (Y = xw)
=  |x| ewx. (G.7)
This conditional PDF can then be marginalised with respect to X . However, care needs
to be taken with the limits as the lower limit of the marginalisation integral depends on the w
value, introducing the Heaviside step function into the equation:
p (w|X = x) = H (wx x0)xewx. (G.8)
Consequently, the marginalised PDF takes the form
p (w) 
p (w|X = x) p (x) dx (G.9)
H (wx x0)2 |x| e(w+1)xdx. (G.10)
The marginalised PDF is dependent on the sign of bothw and x0, due to the step function:
p (w) 
2 |x| e(w+1)xdx w 6 1
2 |x| e(w+1)xdx w > 1
. (G.11)
The integral can be solved: b
xe(w+1)xdx =
x (w + 1) + 1
2 (w + 1)2
e(w+1)x
(G.12)
 p (w) 
(w+1)x0+w
w(w+1)2
x0 w 6 1
(w+1)x0+1
(w+1)2
e(w+1)x0 w > 1
, (G.13)
if the truncation value is positive (Fig. G.4). If the lower limit is set to zero, this is the same
as Eq. G.5.
2 4 6 8 10 12
Figure G.3: Distribution of random samples
drawn from the exponential distribution
truncated at 2.5 with a mean of 4 (blue). The
red line shows the PDF from Eq. G.6.
1 2 3 4 5
Figure G.4: Distribution of the ratio of
random samples drawn from two lower
truncated exponential distributions (blue).
The red line shows the PDF from Eq. G.13.
If the truncation is negative, the integral becomes a bit more complicated. Fig. G.5 shows
the positive and negative values for the Heaviside step function, allowing the different limits
to be determined. There are three clear regimes, w < 0, 0 6 w 6 1 and w > 1, however a
fourth must be considered, that ofw = 1, as this alters the integration. The PDF is therefore
p (w) 


2 |x| e(w+1)xdx w < 0, w 6= 1
 x0
2 |x| dx w = 1
2 |x| e(w+1)xdx 0 6 w 6 1
2 |x| e(w+1)xdx w > 1
, (G.14)
which can be solved using G.12 to give
p (w) 


2(x0(w+1)+1)e(w+1)x0(x0 w+1w +1)e
(w+1)2
w < 0, w 6= 1
2x20 w = 1
2(x0(w+1)+1)e(w+1)x0
(w+1)2
0 6 w 6 1
2(x0 w+1w +1)e
(w+1)2
w > 1
. (G.15)
This fits well with the simulated data (Fig. G.6), showing that the regime changes are real
and consistent with those above.
Figure G.5: Region of non-zero Heaviside
step function for negative x0. The blue
region is the non zero region of H (wx x0)
while the green line is the line corresponding
to x0, 5 in this case, and the red line
corresponds to x0
5 0 5
Figure G.6: Distribution of the ratio of
random samples drawn from two lower
truncated exponential distributions with a
negative truncation value (x0 = 4) (blue)
and  = 1/4. The red line corresponds to the
PDF from Eq. G.15.
Appendix H Distribution of the Ratio of
IndependentLog-NormallyDistributedVari-
ables
The PDF of a ratio (W = X
) can be found from marginalising the joint distribution (g (x, y))
to leave the dependence on the ratio (Hinkley, 1969):
p (W = w) =
|y| g (wy, y) dy. (H.1)
If X and Y are independent and positive, and log-normallly distributed:
p (X = x) =
22X
ln(x)X)
22X , (H.2)
the joint PDF is simply the product of the two individual PDFs. This gives
p (W = w) =
2wy2XY
ln(wy)X)
22X e
ln(y)Y)
22Y dy (H.3)
as the PDF for the ratio
Substituting a = X  ln (w) and
Y + Y2X
2Y + 2X
, (H.4)
 2X2Y
2Y + 2X
, (H.5)
and completing the square gives a distribution related to the integral of the Gaussian:
p (W = w) =
2wXY
aY)
2(2Y+2X)
 (uu)
22u du. (H.6)
A simple substitution
t = uu
gives the standard form for the Gaussian integral:
p (W = w) =
2wXY
aY)
2(2Y+2X)
dt. (H.7)
Substituting for a and u shows that the ratio has a log-normal distribution:
p (W = w) =
22W
ln(w)W)
22W , (H.8)
with mean and standard deviation
W = (X  Y) , (H.9)
2Y + 2X. (H.10)
	Bayesian Source Inversion of Microseismic Events
	Acknowledgements
	Summary
	Contents
	Nomenclature
	1 Introduction
	1.1 Source Processes
	1.2 Probabilities
	1.3 Source Inversion
	1.4 Thesis Outline
	2 Seismic Sources
	2.1 Source Representations
	2.1.1 Stress-Glut Source
	2.1.2 Burridge-Knopoff Slip Discontinuity Source
	2.2 The Moment Tensor
	2.3 Source Models
	2.3.1 Double-Couple Source Model
	2.3.2 Classical Moment Tensor Model
	2.3.3 Crack Model
	2.3.4 Crack + Double-Couple Model
	2.3.4.1 Basic CDC Model
	2.3.4.2 General CDC Model
	2.3.5 Other Source Models
	2.3.6 Potency Tensors
	2.3.7 Elastic Parameters
	2.4 Moment Tensor Decomposition
	2.4.1 Hudson Decomposition
	2.4.2 Tape and Tape Decomposition
	2.4.3 Bi-Axes Decomposition
	2.5 Source Plots
	2.5.1 Two Dimensional Projection Functions
	2.5.1.1 Stereographic Equal Angle Projection
	2.5.1.2 Lambert Azimuthal Equal Area Projection
	2.5.2 Amplitude Plots
	2.5.3 Riedesel and Jordan Type Plot
	2.5.4 Bi-Axes Plot
	2.5.5 Hudson Type Plot
	2.5.6 Lune Plot
	2.5.7 Comparison of Source Plots
	2.6 Volume Change of the Source
	2.7 Moment Tensor Eigenvalue Distribution
	2.8 Non Double-Couple Sources
	2.8.1 Best fitting double-couple
	2.8.2 Distinguishing between source types
	2.9 Summary and Discussion
	3 Source Inversion: Common Data Types and Measurement Uncertainties
	3.1 Seismogram
	3.1.1 Noise Model
	3.1.2 Q Correction
	3.1.3 Surface and Near-Surface Corrections
	3.2 Observations: First Arrival Polarity
	3.2.1 Polarity Inversion
	3.3 Observations: Arrival Amplitudes
	3.3.1 Amplitude Inversion
	3.4 Observations: Arrival Amplitude Ratios
	3.4.1 Amplitude Ratio Inversion.
	3.5 Observations: Full Waveform
	3.5.1 Full Waveform Source Inversion
	3.6 Existing Approaches
	3.7 Summary and Discussion
	4 Bayesian Source Inversion 
	4.1 Bayes' Theorem
	4.1.1 Bayesian Marginalisation
	4.2 Probability Distributions
	4.2.1 Polarity
	4.2.2 Amplitude Ratio
	4.2.3 Amplitude Ratio Noise Model
	4.2.4 Amplitude Ratio PDF
	4.2.5 Posterior PDF
	4.2.6 Posterior Model Probabilities
	4.2.7 Priors
	4.3 Data Independence
	4.4 Source PDF Representations
	4.4.1 Marginalised Plots
	4.4.2 Silhouette plots
	4.4.3 Tape Parameter PDF Plots
	4.5 Summary and Discussion
	5 Bayesian Automated Polarity Estimation
	5.1 Probabilistic Auto-Picking
	5.1.1 Bayesian Method
	5.1.2 Manual and Automated Picking
	5.2 Integration With Automated Monitoring 
	5.3 Examples
	5.3.1 Polarity PDF Plot
	5.3.2 Synthetic Examples
	5.3.3 Real Data
	5.4 Time benchmarking
	5.5 Source Inversion
	5.6 Summary and Discussion
	6 The Effects of Uncertainties on Source Inversion
	6.1 Event Detection and Arrival Picking 
	6.2 Hypocentre Determination
	6.3 Source Inversion
	6.3.1 Background Noise
	6.3.2 Network Distribution
	6.3.3 Location Uncertainty 
	6.3.4 Model Uncertainty
	6.3.5 Other Uncertainties
	6.4 Summary and Discussion
	7 Relative Moment Tensor Inversion
	7.1 Exploring the Effects of Event Co-location
	7.1.1 Estimating Event Co-location
	7.2 Observations
	7.3 Relative Seismic Moment: The Scale Factor
	7.3.1 Estimation
	7.3.2 Prior Distributions
	7.4 Posterior PDF
	7.5 Summary and Discussion
	8 Moment Tensor Sampling Algorithms
	8.1 Monte Carlo Random Sampling
	8.2 Markov Chain Monte Carlo Sampling
	8.2.1 Markov Chain
	8.2.2 Metropolis-Hastings Algorithm
	8.2.3 Reversible Jump Markov chain Monte Carlo Sampling
	8.2.3.1 Reversible Jump McMC in Source Inversion
	8.2.4 Prior PDFs
	8.2.5 Proposal Density
	8.2.5.1 Reversible Jump Balance Vector
	8.2.6 Initialisation and Burn-In
	8.3 Algorithm Performance
	8.4 Estimating the Probability of Double-Couple Source
	8.5 Relative Moment Tensor Inversion Search Algorithms
	8.6 Event Classification
	8.7 Summary and Discussion
	9 MTINV - Source Inversion from Surface Data
	9.1 MTINV
	9.2 Inversion of Surface Data from Iceland
	9.2.1 Krafla
	9.2.1.1 Polarity Probabilities
	9.2.2 Upptyppingar
	9.3 Relative Amplitude Inversion
	9.4 Summary and Discussion
	10 Conclusions
	References
	A Distribution of the n-dimensional inner product
	B Random Moment Tensors
	B.1 Coordinate Independence
	B.1.1 Sampling from hyper-spherical coordinates
	B.2 Generating a Random Moment Tensor
	B.3 Generating a random example of a specific type of moment tensor
	B.3.1 Sampling the eigenvector space 
	B.3.2 Separated Eigenvalue and Eigenvector sampling
	B.3.3 Generating random classical model source
	B.4 Summary
	C Effects of Rock Anisotropies on the Moment Tensor 
	D Homogeneous Isotropic Response functions 
	D.1 Radiation Components
	E Maximum Entropy Distribution for Mean and Variance 
	F Truncated Gaussian Distribution
	F.1 Moments
	F.2 Combining Truncated Gaussian PDFs
	G Distribution of the Ratio of Random Exponential Variables
	H Distribution of the Ratio of Independent Log-Normally Distributed Variables 
