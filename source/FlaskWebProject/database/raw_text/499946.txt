DocumentMiner: 
A Temporal Text Mining Framework for 
Business Intelligence 
A thesis submitted to The University of Manchester for the degree of 
in the Faculty of Humanities 
Haralampos Karanikas 
Manchester Business School/Business Systems 
Table of Contents 
Abbreviations 
1. Introduction 
1.2 The DocumentMiner Approach 
1.2.1 Contribution-innovations 
1.2.2 Motivations 
1.2.3 The DocumentMiner Framework 
1.2.4 The DocumentMiner Architecture 
2.3.4 Metadata Standards 
2.4.1 Linguistics 
2.4.2 Information Extraction 
2.6.3 Text Mining steps and operations .. 
2.7.4 IBM Intelligent Miner for Text 
2.7.5 ClearForest 
2.8 Conclusion 
3. The DocumentMiner Approach 
3.1 Introduction 
3.3 Main Requirements 
3.4 DocumentMiner's Architecture 
3.5 Conclusions 
4.2.7 Evaluation 
5.4 Conclusions 
6.1 Introduction 
6.3 Conclusions 
7. Evaluation 
7.3 The First Case Study 
7.3.1 Competitive Intelligence 
7.3.2 Case Study Overview 
7.4 The Second Case Study MetaOn 
7.4.1 Overview 
List of Figures 
Figure 1 Integration in DocumentMiner 
Figure 2 Combining Information Extraction and Data Mining 
Figure 3 DocumentMiner Architecture 
Figure 4 DocumentMiner Overview 
Figure 5 Major KDT stages of processing. 
Figure 6 Data Elements of Simple Dublin Core 
Figure 7 Morphology 
Figure 8 Syntax 
Figure 9 Structure of an Information Extraction system. [39] 
Figure 10 Framework Architectures 
Figure 11 Acquisition schema 
Figure 12 Main architectural components 
Figure 13 Text Sources 
Figure 14 Linking domain knowledge and textual data 
Figure 15 Core elements of an IE system [54] 
Figure 16 Steps of Document Miner's process model 
Figure 17 Tasks of step Structured textual data preparation 
Figure 18 Tasks under step Modelling-Apply Mining Algorithm 
Figure 19 Star Schema of DocumentMiner metadata 
Figure 20 Classes of general descriptive and storage metadata 
Figure 21 Class diagram of general descriptive and storage metadata 
Figure 22 Knowledge Base 
Figure 23 Document annotations 
Figure 24 Extracted information metadata, logical model 
Figure 25 Example of associations over time 
Figure 26 Sample competitors 
Figure 27 Sample collected documents 
Figure 28 Business problems 
Figure 29 Architecture supporting the Case study 
Figure 30 Sample source document 
Figure 31 Fact extracted (template) 
Figure 32 Fact: CombiChem 4 TechAreaA 
Figure 33 Bioscience Sector A4 TechAreaA 
Figure 34 Bioscience Sector B4 Gene expression 
List of Tables 
Table 7 Collaboration facts 
Abbreviations 
Al: Artificial Intelligence. 
ASCII: American Standard Code for Information Interchange. 
BI: Business Intelligence. 
Cl: Competitive Intelligence. 
CWM: Common Warehouse Model. 
DBMS: Database Management System. 
DCMI: Dublin Core Metadata Initiative. 
DCSV: Dublin Core Structured Values. 
DocMin: DocumentMiner 
ETL: Extraction Transformation and Loading. 
IE: Information Extraction. 
IR: Information Retrieval. 
KB: Knowledge Base. 
KDT: Knowledge Discovery in Text. 
KM: Knowledge Management. 
TEI: Text Encoding Initiative. 
TM: Text Mining. 
TTM: Temporal Text Mining. 
TW: Text Warehouse. 
ML: Machine Learning. 
NLP Natural Language Processing. 
RDF: Resource Descriptor Format. 
OIM: Open Information Model. 
OMG: Object Management Group. 
UML: Unified Modelling Language. 
WWW: World Wide Web. 
THE UNIVERSITY OF MANCHESTER 
ABSTRACT OF THESIS submitted by Haralampos Karanikas for the degree of 
PhD and entitled DocumentMiner -A Temporal Text Mining Framework for 
Business Intelligence 
December 2008 
Text Mining (TM) is the emerging science and industry of applying data mining 
(DM), machine learning (ML), natural language processing (NLP) and information 
extraction (IE) to the problem of finding useful patterns hidden within large textual 
databases. Despite the advances in the area, the field is relatively young and there is 
little understanding of best practices and solutions, regarding the text mining 
process and its operations. Especially the temporal issue is poor or not covered 
currently. 
The reported work in this thesis introduces the DocumentMiner a unified 
database oriented temporal text mining framework. DocumentMiner targets the 
complexity that is associated with the process of text mining. This is achieved by 
providing support for the effective analysis, management and co-ordination of the 
information gathered throughout the stages into a single, consistent, manageable 
database. Our theoretical framework is based on the innovative use of 
information extraction for supporting the text mining process. We apply 
innovatively ontology driven temporal association rules in text and we introduce 
the term Temporal Text Mining which actually is an extension in the current 
concept of Text Mining. An additional contribution is our proposed text mining 
process; a systematic way for conducting the discovery of the useful information. 
Declaration 
No portion of the work referred to in the thesis has been submitted in support of 
an application for another degree or qualification of this or any other university or 
other institute of learning. 
Copyright Statement 
Copyright in text of this thesis rests with the author. Copies (by any process) 
either in full, or of extracts, may be made only in accordance with instructions 
given by the author and lodged in the John Rylands University Library of 
Manchester. Details may be obtained from the Librarian. This page must form 
part of any such copies made. Further copies (by any process) of copies made in 
accordance with such instructions may not be made without the permission (in 
writing) of the author. 
The ownership of any intellectual property rights which may be described in this 
thesis is vested in The University of Manchester, subject to any prior agreement 
to the contrary, and may not be made available for use by third parties without 
the written permission of the University, which will prescribe the terms and 
conditions of any such agreement. 
Further information on the conditions under which disclosures and exploitation 
may take place is available from the Head of School of Informatics. 
To my wife Sofia. 
Acknowledgements 
I would like to sincerely thank my supervisor Dr. Babis Theodoulidis who guided 
me in the years of challenging research work through a professional. friendly. and 
productive collaboration, co-ordination, help, and encouragement. 
Of course I will never forget all of my colleagues in the CRIM research group 
and in the Department of Computation. 
My deepest thanks to my parents who believed in me while providing crucial 
moral and financial support. 
The Author 
Born in 1972, holds a B. Sc. degree in Physics from the Aristotle University of 
Thessalonica, Greece (1996). He has subsequently joined the Department of 
Computation in the University of Manchester Institute of Science and 
Technology (UMIST) to pursue his M. Sc. in Computer Science (1998). Since 
January 2005 he is a researcher at the Knowledge Management Lab of the 
National & Kapodistrian Univ. of Athens. His work aims at advancing the ways 
data mining algorithms are used in order to analyse natural language text in an 
attempt to discover structure and implicit meanings "hidden" within the text. His 
interests in research also include document warehouse, ontology construction, 
data mining, and business intelligence. 
The current research work has provided the formation of research proposals 
related to text mining in collaboration with leading European Universities, 
research institutes and industrial partners. Major one the project PARMENIDES 
(IST-2001-39023/ European Communities) Title: "Ontology Driven Temporal 
Text Mining on Organisational Data for Extracting Temporal Valid Knowledge-. 
http: //www. crim. co. umist. ac. uk/parmenides/, (Funded by EU IST programme 3, 
25 million Euros). Another important project based on the current research work 
is MetaOn (www. metaon. gr), within the Operational Programme "Information 
Society" of the Greek Ministry of Development, General Secretariat for Research 
and Technology, co-funded by the European Union. 
Publications 
The following is a list of publications relevant to the subject of this thesis: 
H. Karanikas, N. Pelekis, Theodoulidis B., I. Kopanakis "A Temporal Text 
Mining Approach in Competitive Intelligence" International Journal of Data 
Warehousing & Mining, 2007, under review. 
D. lakovidis, N. Pelekis, E. Kotsifakos, I. Kopanakis, H. Karanikas and Yannis 
Theodoridis, "A Pattern Similarity Scheme for Medical Image Retrieval, " 
accepted at IEEE Transactions on Information Technology in Biomedicine, 
2008. 
H. Karanikas et al. "MetaOn - Ontology Driven Metadata Construction and 
Management for Intelligent Search in Text and Image Collections", In the 
Proceedings of the International DEXA 2006 on Flexible Database and 
Information System Technology (F1exDBIST-06), IEEE CSP, Crakow, Poland, 
September, 2006. 
H. Karanikas ct al. "Multimedia Annotation System for Intelligent Search". In the 
Proceedings of the International Conference on Telecommunications & 
Multimedia (TEMU'06), Heraklion-Crete, Greece, July 2006. 
H. Karanikas, N. Pelekis, I. Kopanakis and T. Mavroudakis: "Discogering 
Market Trends via Text Mining", Proc. Conf on New Technologies and 
Marketing (NTM'05) Ierapetra - Crete, Greece, May 2005. 
H. Karanikas and T. Mavroudakis. "Text Mining Software Survey". RANLP 
2005, Text Mining Research Practice and Opportunities. Borovets - Bulgaria. 
Sept 2005. 
Karanikas H., Tjortjis C., Theodoulidis B., "An Approach to Text Mining using 
Information Extraction", Proc. PKDD 2000, (KMTA 2000), Lyon, France. Sept. 
2000. 
Mikroyannidis, Haralampos Karanikas, Thomas Mavroudakis and Babis 
Theodoulidis, Web Usage Mining: The MetaOn Approach, TEMU 2008, 
International Conference in Telecommunications and Multimedia, 2008. 
D. K. lakovidis, E. Kotsifakos, N. Pelekis, H. Karanikas, I. Kopanakis, and Y. 
Theodoridis, Pattern-Based Retrieval of Cultural Heritage Images, in Proc. 
Panhellenic Conference on Informatics (PCI), Patras, Greece, 2007, pp. 443-452. 
D. K. lakovidis, N. Pelekis, H. Karanikas, E. Kotsifakos, I. Kopanakis, and Y. 
Theodoridis, "A Pattern Similarity Scheme for Medical Image Retrieval, " in 
Proc. International Conference on Information Technology in Biomedicine 
(ITAB), Oct. 2006. 
1. Introduction 
1.1 Text Mining 
We are living in the "Information Age". The main characteristic is the amazing 
growth of data that are being generated and stored making it difficult for humans 
to understand. In most companies and organisations valuable time and effort is 
wasted in ineffective searches through multiple information sources including 
web sites and other conventional sources. This problem of information overload 
is further exacerbated due to the unstructured format of the majority of the data. 
The vast amount of data found in an organisation, some estimates run as high as 
80%, are textual such as reports, emails, etc. [19] This type of unstructured data 
usually lacks metadata (data about data) and as a consequence there is no 
standard means to facilitate search, query and analysis. On the other hand Web is 
the biggest document collection and most of the current Web's content is 
designed for human beings to read and not for computer programs. To date, the 
Web has developed a medium of documents for people rather than for data and 
information that can be processed automatically. [2] 
While the amount of textual data available to us is constantly increasing, our 
ability to understand and process this information remains constant. A human 
editor can only recognise that a new event has occurred by carefully following all 
the web pages or other textual sources. This is clearly inadequate 
for the volume 
and complexity of the information involved. The need for automated extraction 
of useful knowledge from huge amounts of textual data 
in order to assist human 
analysis is apparent. The rapid adoption of the e-commerce 
business model 
Chapter 1 Introduction 
requires software that will help companies to analyse, manage, manipulate and 
effectively control how business disseminates and leverages information to 
competitive advantage. [19] Knowledge discovery and Text Mining are mostly 
automated techniques that aim to discover high level information in huge amount 
of textual data and present it to the potential user (analyst, decision-maker, etc). 
Unlike the tabular information typically stored in conventional databases, text is 
a form of information that has only limited internal structure. Usually the term 
unstructured text is used for text that does not easily fit into an attribute, such as 
tabular structures in a relational database. The text 26 Kritis Road, found in a 
column labelled address is considered structured because it is a single logical unit 
with an easily distinguished meaning. On the other hand the text Karaiskos 
Dimitris the Physicist from Salonika visited the house at 26 Kritis road, is 
generally considered unstructured because attributes (such as meaning and 
relationships of words) are not easily manipulated with conventional databases or 
other programming tools. Being humans, we are able to understand this sentence 
because the words are organised according to some natural language structures. 
Word order is one of the several natural language structuring mechanisms that we 
will exploit in text mining. Other structures include rules for creating and 
grouping phrases together, ways of varying the meaning of words (by adding 
prefixes and suffixes etc) and restrictions placed by verbs on the number and 
types of nouns that can appear in a sentence. 
While most of the documents we deal everyday are organised collections of 
grammatical sentences we also deal with ungrammatical texts, such as emails. 
Emails are far less formal and often are more similar to verbal conversations, 
with unspoken assumptions, quickly formulated abbreviations etc. In the current 
study we work with grammatical structured text. 
The nature of text is one of the most important problems of knowledge 
discovery. 
The DocurnentMincr approach addresses this problem by supporting KDD 
practices through a complementary framework. 
Chapter I Introduction 
1.2 The DocumentMiner Approach 
1.2.1 Contribution-innovations 
This thesis introduces the DocumentMiner approach that is mostly concerned 
with the task of transforming text into structured data in order to facilitate 
temporal knowledge discovery. Our contributions can be summarised on the 
following issues: 
"A unified database oriented temporal text mining framework for 
supporting knowledge discovery tasks. 
  Integration of IE and DM in order to discover useful patterns from text. 
  Incorporation of the time issue in the discovery process. 
  Incorporation of background knowledge via the use of ontologies in the 
discovery process. 
 A proposed methodology for conducting the Knowledge Discovery in 
Text process. 
  Metadata model for the text repository that transforms it into a document 
warehouse. 
 A novel ontology driven technique for discovering temporal association 
in text. 
The current research work has informed the formation of research proposals 
related to text mining in collaboration with leading European Universities, 
research institutes and industrial partners. Major one the project PARMENIDES 
(IST-2001-39023/ European Communities) Title: "Ontology Driven Temporal 
Text Mining on Organisational Data for Extracting Temporal Valid Knowledge", 
http: /lwww. crim. co. umist. ac. uk/parmenides/, (Funded by EU IST programme 3. 
25 million Euros). Another important project based on the current research work 
is MetaOn (www. metaon. gr), within the Operational Programme "Information 
Society" of the Greek Ministry of Devvelopment, General Secretariat for Research 
and Technology, co-funded by the European Union. 
Chapter 1 Introduction 
1.2.2 Motivations 
Present discussions on business intelligence (BI) are about data warehouses, data 
mining, etc. These techniques have served well the current needs for analysis but 
still they do not address completely the full scope of business intelligence. BI 
provides managers and other decision makers with the information they need to 
understand, manage and direct their organisations. To date numeric and 
categorical (non numeric short character string) information has been the only 
data under investigation in BI area. These structured data leave out the most 
common medium for expressing knowledge: text. Within text xve find marketing 
reports, competitor's advertising campaign, descriptions of new technologies in 
patent applications, scientific papers, experimental reports etc. With traditional 
BI systems we simply cannot get the information we need from text. 
The need to expand the technical capabilities of BI to include textual information 
is apparent. Several tasks in relation to BI need to be supported. First is the 
reduction of the text amount we must read in order to discover relevant pieces of 
information. Second is the isolation of information pieces, such as the names of 
persons, organisation etc. Third is the understanding of relationships among 
terms over a document collection. For example by studying scientific papers, we 
may find the same terms/concepts occurring in several technological areas. A 
fourth need is the incorporation of the temporal issue. Some of the most 
interesting observations involve time. Fifth is the need for metadata integration 
and management. And finally is the integration between text and structured data. 
By extracting key terms/concepts we can link texts to other systems. such as data 
warehouses. 
The DocumentMiner approach addresses these needs by combining information 
extraction and data mining technology under a database oriented framework. 
1.2.3 The DocumentMiner Framework 
The DocumentMiner framework aims to support pre-processing, analysis, 
management of temporal textual data by integrating information extraction and 
mining into the database level. Additionally we include ontologies 
in our 
Chapter I 11rtr0chuc tion 
framework in order to facilitate higher level conceptualisation in various steps such 
as the discovery of associations between entities. 
Information Extraction 
Textual 
Ontologies ---f Metadata 
Database 
Data Mining 
Figure 1 Integration in DocumentMiner 
DocumentMiner framework is analogous to data warehouse with the difference 
that deals with textual data instead of structured data. The framework provides a 
repository of textual information in order to support business intelligence and 
decision support operations. In DocumentMiner, the metadata repository is used 
to hold the information extracted by the Information Extraction module mainly 
and from the ontologies, as one unified representation of the background 
knowledge. Thus by using a common representation, DocumentMiner's 
repository becomes a computational medium to integrate all the domain 
information and facilitate the use of various data mining tasks upon them. 
More specifically the role of the text repository can be summarised as follows: 
" Provides a neutral medium and unified representation towards the integration of 
the various aspects of the text mining process. 
" Provides a mechanism for expressing the interrelationships 
between 
information elements forming the basis for querying and analysis. 
Chapter 1 Introduction 
" Extends the textual metadata in order to cover the temporal characteristics of 
documents and facts mentioned within the documents. 
" Provides a more solid basis as they allow large scale applications in text. 
Another important issue in DocumentMiner is the combination of information 
extraction and data mining techniques. 
Information Text - Data Mining  Rules Extraction 
Figure 2 Combining Information Extraction and Data Mining 
Within our framework Information Extraction is an essential phase in text 
processing. Information Extraction is the process of identifying essential pieces 
of information within a text, mapping them to standard forms and extracting 
them for use in later processing [38]. It facilitates the semi-automatic creation of 
metadata repositories, more or less domain specific. Such repositories can be 
further processed using standard data mining techniques. 
Data mining is the application of algorithms to a set of data to uncover previously 
unidentified connections and correlations. Data mining works with structured 
data, often numerical in nature. Text mining is analogous to data mining in that it 
uncovers relationships in information. Unlike data mining, it works with 
information stored in an unstructured collection of text documents. 
The realisation of these concepts and the communication and management of the 
text mining process is achieved with the DocumentMiner environment. The 
DocumentMiner environment makes effective use of text metadata and background 
information stored in a DBMS. 
1.2.4 The DocumentMiner Architecture 
Chapter 1 Introduction 
The next figure presents the overall architecture of the proposed DocumentMiner 
environment: 
Document Retrieval 
Loading 
Transformation 
Information Extraction 
Pre-processing- IE 
Linguistic 
Resources 
Metadata 
Storage 
Figure 3 DocumentMiner Architecture 
Association Rules 
Temporal Rules 
Queries 
Visualisation 
Analysis - Data Mining 
In the DocumentMiner architecture, we identify three main components: 
The pre-processing/Information Extraction component: The first operation is 
to identify what documents are to be retrieved. Once we have identified the 
source of our documents we need to retrieve the documents (from Web or 
internal file systems). The remaining operations include any kind of 
transformation processes of the original documents retrieved. These 
transformations could aim at obtaining the desired representation of 
documents such as XML, SGML. The results are then processed to provide 
basic linguistic information about the content of each document. The final 
operation is about extracting high-level information (IE-metadata creation). 
The storage component: It holds all the above specifications along with all 
the relevant data that contribute to the overall knowledge discovery in text 
Chapter 1 Introduction 
process. How to store the documents and the related attributes is one of the 
main issues. DocumentMiner's choice is to store the data in a database. 
Especially for very large collections of data we keep only the metadata and 
extracted features, including indexing information and the URLs of internet 
documents or the pathnames for local documents. By this approach we do not 
have to store the actual document and we have distributed document storage. 
9 The analysis/Data Mining component: This component includes patterns and 
relationships discovery within the extracted information. Main text mining 
operations are supported in this component aiming at discovering structure 
and implicit meanings "hidden" within the text. 
Within each component, there are links, which express the inter-connections that 
exist between elements both at the pre-processing and at the analysis component. 
I Storage Area 1 
Ontology 
Document 
conDvertor/Basic 
Information Extraction 
Retrieval 
processing 
Background Linguistic 
Documents knowledge resources 
Document content metadata. 
Search and retrieval metadata. 
Text Mining metadata 
Storage Metadata 
----- 
L------- Storage Area 
Figure 4 DocumentMiner Overview 
The top-level objectives of the DocumentMiner environment are: 
" The ability to utilise ontologies describing domain knowledge. 
" The ability to apply information extraction rules. 
Temporal 
Knowledge 
Discovery 
techniques: 
1. Trends Analysis 
2. Temporal validity 
of knowledge 
1) 1) 
Chapter 1 Introduction 
" The ability to query and edit/update the results of the information extraction 
output, named as templates. 
9 The ability to perform various data mining techniques upon the metadata. 
" The ability to interactively examine and query about the results of the analysis 
stage. 
1.2.5 Case study 
With so much information in a text format, it is not difficult to find sample case 
study for the evaluation of our framework. We utilise a case study concerning a 
specific domain, which is competitive intelligence. 
Competitive intelligence (CI) is the art of gathering information about 
competitor's activities and general market trends. [28] From business intelligence 
point of view CI is the means of gathering and assessing data that describe 
external factors that influence the organisation. In highly competitive markets 
such as biotechnology and pharmaceutical, text mining can provide the base for 
developing competitive information objects that will describe the state of a 
competitor with regards to current market conditions and support decision 
making within the organisation. Usually these information objects represent the 
strategic plans and the tactical operations. 
The strategic plans are composed of financial data, product information, 
organisational structures, and market status. By knowing that a strategic move is 
underway by the competitor is extremely useful. For example when a competitor 
opens operations in new areas or forms alliances with new distributors; these 
pieces of information can lead to get the big picture of his long- term plans. 
Day-to-day operations of competitors are more dynamic than strategic plans but 
collecting such information can be just as useful. This type of information comes 
from advertising and sales information published by the competitor. The 
difference of the tactical operations with the strategic information is the time 
window which is narrower. 
Chapter 1 Introduction 
The company we work with specialises in corporate intelligence products and 
services for the biotechnology and pharmaceutical industry. These products and 
services are targeted mainly to managers in charge of business development 
and/or investment/strategic level issues who wish to make decisions on industry 
and company developments or simply to monitor these on a regular basis. By 
nature, corporate intelligence is highly focused on specific issues of concern for a 
company and as a consequence solutions tend to be custom. The high cost of 
these services however, together with the necessity for continuous access to 
industry developments has given rise for a market for less specific services that 
can answer some of the daily questions of these managers. Examples of simple 
corporate intelligence services that already exist are the various search and 
filtering services as well as the various content providers and aggregators who 
deliver semi-custom information bundles to particular user specifications. The 
need for our partner is clear. On a more sophisticated level to be able to monitor 
specific organisations, technologies or areas of research as well as being able to 
analyse primary data so as to draw conclusions at the level of the company's 
competition, sector or industry. The sources of this primary data are 
predominantly natural language texts that include amongst others press releases, 
scientific articles, conference announcements, patents and others. 
In particular our case study will be applied in a number of documents that 
describe new alliances and partnerships of competitors (documents in appendix 
1). In an intelligent way we will try to discover business trends of competitors or 
the market in general. Competitors moving into markets or the appearance of a 
new market are of paramount importance. This type of analysis provides 
knowledge of competitors and markets so the business analyst is able to support 
the strategic plans of the organisation. The need for the analyst is to identify 
trends; competitors' movements into new scientific areas, potential alliances of 
competitors and the relationships between industry specific sectors and 
technological areas. It is important to mention that these relationships over time 
would add great value for the analyst. These are types of 
business problems that 
initially drive the need for text mining. For example one of the business goal 
might be "Should we move in new technological areas? 
" One of the sub- 
Chapter I Introduction 
questions to answer is "Are competitors moving in technological areas and 
which? " The corresponding text mining goal might be "Discover. according to 
the collaborations they have signed the last year our competitors, relationships 
with new technological areas". 
1.3 Thesis Structure 
Chapter two presents an overview of the state of the art in Text Mining (TM). It 
introduces the area TM with a presentation of characteristic TM methods and TM 
related tools. In the current bibliography, it is possible to find a quite extensive 
list of approaches and techniques for addressing particular issues of TM. Our 
presentation presents current TM trends rather than an inclusive review of all the 
methods, which would be out of the scope of this thesis. This is also true for the 
TM tool support, where we present tools that are relevant to our approach. 
Chapter three outlines the main DocumentMiner concepts. The presentation 
begins with the consideration of core issues that influenced our approach. This is 
followed by the high level description of our approach and its support for text 
mining. One of the essential points analysed is the role of text warehouse in our 
approach and especially the use of information extraction and ontologies in 
combination with data mining. 
Chapter four expands the presentation of our approach with our proposed text 
mining process. We suggest how it is possible to discover knowledge in text by 
following a systematic approach. 
Chapter five is dealing with the metadata repository. The chapter begins with the 
presentation of some fundamental standards being used and continues with some 
of the top-level requirements for this component; then presents the main 
metadata issues and ends with a presentation of our model. In this chapter we are 
utilising developments in data warehouse metadata and 
in information 
management metadata in order to construct the DocumentMiner metadata model. 
Chapter 1 Introduction 
Chapter six presents our method of discovering association and temporal rules in 
text. An important issue is the incorporation of ontologies to assist the discovery 
of useful patterns at the desired conceptual levels. 
The evaluation of our framework with a case study application concerning a 
specific domain is discussed in chapter seven. This discussion encompasses the 
presentation of the remaining aspects of our framework. 
Finally the thesis concludes with a critical analysis and discussion of the 
DocumentMiner approach in chapter eight. 
2. State of the Art 
2.1 Introduction 
This chapter provides an introduction to the area of knowledge discovery in text. 
Additionally is serving as background explanation for our feature classification 
scheme and the software feature table that are included in our survey for text 
mining software. After we briefly explain our use of the terms knowledge 
discovery in text (KDT) and text mining (TM) in section 2.2 we describe in more 
detail each of the stages of the knowledge discovery in text process in sections 
2.3,2.4,2.5. In the following 2.6,2.7,2.8 we present our survey. 
2.2 Knowledge Discovery in Text and Text Mining 
Knowledge discovery in text (KDT) and TM is a new research area that tries to 
resolve the problem of information overload by using techniques from data 
mining, machine learning, natural language processing (NLP), information 
retrieval (IR), information extraction (IE) and knowledge management. As with 
any emerging research area, there is still no established vocabulary for KDT and 
TM, a fact which can lead to confusion when attempting to compare results and 
techniques. Often the two terms are used to denote the same thing. Knowledge 
discovery in text [6], [15] (or in textual databases), Document Mining [44], Text 
Data Mining [10] and Text Mining [24], [20]. [29], [11], [28] are some of the 
terms that can be found in the literature. 
We use the term KDT to indicate the overall process of turning unstructured 
textual data into high level information and knoww-led`ge, while the term Text 
Chapter 2 State of the . 
Mining is used for the step of the KDT process that deals with the extraction of 
patterns from textual data. By extending the definition, for Knowledge disco'Verv, 
in databases (KDD) by Fayyad and Piatetsky-Shapiro [5]. we give the following 
simple definition: 
Knowledge Discovery in Text (KDT) is the non-trivial process of identifying 
valid, novel, potentially useful, and ultimately understandable patterns in 
unstructured textual data. 
0 Unstructured textual data is a set (collection) of documents. We use the term 
document to refer to a logical unit of text. This could be Web page, a status 
memo, an invoice, an email etc. It can be complex and long [28], and is often 
more than text and can include graphics and multimedia content. In this study 
we are only concerned with the textual elements of documents. Documents 
that use extensible markup language (XML), standard generalised markup 
language (SGML) and similar conventions are called semi-structured textual 
data. For our purposes, we will consider both, unstructured and semi- 
structured textual data. 
" Pattern. If we consider our data as a set of facts F (e. g. cases in a database) a 
pattern is a rule expression E that describes facts in a subset FE of F [5]. 
Generally speaking, there are two kinds of patterns the predictive and the 
informative. We use predictive patterns to predict one or more attributes in a 
database from the rest. This kind of patterns make an educated guess about 
the value of an unknown attribute given the values of other known attributes. 
On the other hand, informative patterns do not solve a specific problem, but 
they present interesting patterns that the user might not know. 
KDT is a multi-step process, which includes all the tasks from the gathering of 
documents to the visualisation of the extracted information. The process 
assumed to be non-trivial; that is the result has to 
be qualified as disco\ cry. The 
discovered patterns should be valid on new textual data with some 
degree of 
certainty. The patterns are novel at 
least to the system and should potentially lead 
to some useful actions, as measured 
by some utility function. A main goal of 
KDT is to make patterns understandable to humans in order to 
facilitate a better 
understanding of the underlying 
data [5]. 
Chapter 2 State of the Art 
Text Mining (TM) is a step in the KDT process consisting of particular data 
mining and NLP algorithms that under some acceptable computational of ciency 
limitations produces a particular enumeration of patterns over a set of 
unstructured textual data. 
Text Mining uses unstructured textual information and examines it in attempt to 
discover structure and implicit meanings "hidden" within the text [14]. Main 
objective is the support of the knowledge discovery process in large document 
collections (Web or conventional storage). Text Mining utilises specialised data 
mining and NLP techniques specifically operating on textual data in order to 
extract useful information and its applications imposes strong constrains on the 
usual NLP techniques [24]. For example as they involve large volumes of textual 
data, they do not allow to integrate complex treatments. It is this close connection 
that makes Text Mining a new research area derived from data mining and NLP. 
It is best understood by comparing it with a related technology: Data mining. 
Data mining is the application of algorithms to a set of data to uncover previously 
unidentified connections and correlations. Data mining works with structured 
data, often numerical in nature. Text mining is analogous to data mining in that it 
uncovers relationships in information. Unlike data mining, it works with 
information stored in an unstructured collection of text documents. 
Text Mining techniques are not the only tools to solve the information overload 
problem. Other techniques from different research areas, such as information 
retrieval (IR), natural language processing (NLP) etc. could also be used. Text 
Mining could be used directly or indirectly (similar to Web mining [ 16]). By the 
direct approach we mean that the application of TM techniques directly address 
the problems that derive from the information overload; for example, finding 
relevant information on a huge collection of documents. By the indirect approach 
we mean that the TM techniques are used as a part of a bigger application that 
addresses the problems of information overload. For example. TM techniques 
Chapter 2 State of the Art 
can be used to disambiguate word sense [23] for a search or information retrieval 
application. 
The process of KDT includes three major stages [28], [16] (Figure 5): 
I. Collect relevant documents: The first step is to identify what documents are 
to be retrieved. Once we have identified the source of our documents we need 
to retrieve the documents (from Web or internal file systems). 
2. Pre-processing documents: This step includes any kind of transformation 
processes of the original documents retrieved. These transformations could 
aim at obtaining the desired representation of documents such as XML, 
SGML. The resulting documents are then processed to provide basic 
linguistic information about the content of each document. 
3. Text mining operations: High-level information is extracted (metadata 
creation). Patterns and relationships are discovered within the extracted 
information. 
Collect Pre-processing Text Mining 
Documents Documents 
Figure 5 Major KDT stages of processing. 
2.3 Collect relevant documents 
The technology that mainly addresses the operation of identifying relevant 
documents in large collections of potentially relevant texts is Information 
Retrieval (IR). JR is the automatic retrieval of all relevant documents while at the 
same time retrieving as few of the non relevant as possible. [25] Using this 
approach the computer does not have to understand the document. 
This means 
that, from the IR point of view, documents are just bags of unordered words. 
Usually IR is the first step in the KDT process. 
Chapter 2 State of the Art 
2.3.1 Identify document source 
The first thing we have to consider in a text mining application is where the 
documents are located. There are three main sources. Local areas file systems. 
Document management systems and the Internet. There are different ways of 
identifying particular documents. For example, we can identify the documents 
with a resource list, searches by keywords, searches by topic [28] etc. 
2.3.2 Document retrieval 
Ones we have identified the source of our documents we need to plan their 
retrieval. The retrieval methods can be divided to manual and automatic. In 
general the most appropriate tools are the scripting languages with rich support 
for pattern matching, when we work with internal data sources. Web robots and 
crawlers are used for Web-based information. 
2.3.3 Document storage 
How to store the documents and the related attributes is one of the main issues. 
There are three main choices. The first choice is to store the data in databases. 
Lately major database management systems support text and multimedia data. 
Secondly Files systems provide an alternative for storing the documents. Finally 
an effective approach for very large collection is to keep only the metadata and 
extracted features, including indexing information and the URLs of internet 
documents or the pathnames for local documents. By this approach we do not 
have to store the actual document and we have distributed document storage. 
2.3.4 Metadata Standards 
Stand-alone metadata can exist in any kind of database, and generally provides a 
link to the described resource. This approach is likely to be most practical for 
many types of resources, and is increasingly used for text as wcll, mainly to 
support easier maintenance and sharing of metadata. 
We arc going to examine briefly few important mctadata standards: the 
Common 
Warehouse Metamodel [60], the Open Information Model [62] the Dublin Core 
Chapter 2 State of the .4 rt 
Metadata Initiative [59] and the Temporal Annotation scheme from the TimeML 
work [58]. All contribute to our metadata model initiative of DocumentMiner in 
chapter 5. 
The description of the above standards is not extensive and is limited on the 
elements we are reusing for our model. 
2.3.4.1 Common Warehouse Metamodel 
Common Warehouse Metamodel (CWM) is an Object Management Group 
(OMG) standard developed by IBM, Oracle, Hyperion, Unisys, NCR, and other 
data warehouse vendors. The model describes 11 distinct types of metadata along 
with foundational elements of the model: 
9 Relational data sources. 
" Record data sources. 
9 Multidimensional data sources. 
9 XML data sources. 
9 Data transformations. 
" Online analytic processing (OLAP). 
9 Data mining. 
" Information visualisation. 
" Business nomenclature. 
" Warehouse process. 
" Warehouse operations. 
The main sections of the model are organised into hierarchical structures. 
four main are: 
0 Foundation 
0 Resource 
9 Analysis 
" Management 
Chapter 2 State of the .4 rt 
The final Management package contains the Warehouse Process and Warehouse 
Operation packages. 
Foundations 
Foundation package includes Business Information, Data Types, Expressions, 
Key and Indexes, Software Deployment, and Type Mapping packages. 
It provides the means of expressing business information about other elements 
within a metadata model. For example, Business Information describes offline 
documentation, persons responsible for particular parts of the system etc. 
Resource 
Resource covers the Relational, Multidimensional, Record, XML, Object- 
Oriented packages. 
The Relational class describes the basic elements of a relational database 
management system i. e. tables, columns etc. The Multidimensional class 
describes the data structures used in OLAP databases, such as hierarchies and 
dimensions, and OLAP operations, including aggregation and drill-down. The 
Record class address simple data structures, including both database oriented 
records and programming language structures. Mainly the Record class is used to 
describe older data sources, such as COBOL structures. The XML class 
represents semi structured documents, using constructs such as attributes, 
content, documents, elements types, element content, element types, schema and 
text. 
Analysis 
Analysis includes the Transformation, OLAP, Data Mining, Visualisation, and 
Business Nomenclature packages. The most important for DocumentMiner is the 
transformation, information visualisation, and business nomenclature. 
In the Text Warehouse as in the Data Warehouse users need to understand what 
resources are available in an organisation's 
business intelligence environment. 
Chanter 2 State of the .4 rt 
This includes knowing what information is in the repository. how it can be 
obtained, and where it came from, and this needs to be described in business 
terms and not technical terms. The Business Nomenclature package provides the 
means to represent this type of metadata: 
" Business Domain 
" Concept 
0 Glossary 
" Nomenclature 
9 Taxonomy 
" Terms 
" Vocabulary Element 
The Business Nomenclature package is one of the most important for Text 
Warehouse. These classes can be used to describe not only the structure of the 
warehouse, but its contents as well. The business domain represents a logical area 
of a business. Domains can be quite diverse, depending upon the industry. 
Vocabulary elements are divided into two types: Concepts and Terms. Concepts 
represent ideas in a business domain. Terms are words or phrases that represent 
business concepts in a particular context. The context is defined in terms of a 
concept that is referenced by a term. Concepts can be thought of as a preferred 
terms since they represent a common form of an idea that can be expressed using 
different terms. Terms are organised into two types of nomenclature classes: 
Glossaries and Taxonomies. A glossary is an unordered collection of terms. 
Taxonomies, sometimes called ontologies, are hierarchical organisation of terms. 
Additionally to these classes, the CWM defines several relationships between 
classes. The Synonym to Preferred Term relationship 
links commonly used terms 
that mean the same thing to a single term. This is very useful 
for minimising the 
number of terms that need to be represented 
in a taxonomy or (, lossar,,. The 
Wider to Narrower relationship provides navigation from a general term to a 
more specific. Related terms link terms to other terms providing 
for horizontal 
Chapter 2 State of the _4 rt 
relationships between terms in addition to the vertical, or hierarchical, 
relationships defined in taxonomies. 
Management 
Management package contains the Warehouse Process and Warehouse Operation 
packages. Warehouse process mainly deals with transformation operations. They 
are specific events that result in the execution of a transformation activity. Three 
types are defined: 
" Scheduled 
" External 
" Internal 
Warehouse operations deals with day-to-day operations of the warehouse and 
include three types of events: 
" Transformation Executions 
" Measurements 
" Change Requests 
2.3.4.2 Open Information Model 
The Open Information Model (OIM) was designed to model the metadata needs 
of data warehouses. Unlike the original CWM, the OIM included a wider range 
of information systems, and related operations such as: 
" Analysis and design 
" Components and objects 
" Database and warehousing 
" Knowledge Management 
Analysis and design sub-model include object-oriented design and processes and 
the software development life cycle. The Components and objects 
deals with 
software components and specifies metadata to support component reuse. 
Database and warehousing sub-model covers many of the same concepts as the 
Chapter 2 State of the Art 
CWM. The Knowledge Management is the most interesting sub-model from the 
DocumentMiner's perspective. 
Knowledge Management sub-model 
The main target of the Knowledge Management (KM) sub-model is capturing 
business terminology. For example, capturing semantic relationships between 
objects in a business domain and mapping them to storage structures. Mapping 
linguistic items to data storage structures allows users to manipulate databases 
with natural language front ends rather than SQL or SQL like methods. The 
mapping from natural language to a data manipulation language requires 
understanding of the structure of the database. The knowledge management 
metadata of the OIM standard provides the basis for that understanding. It makes 
use of three elements: 
" Entities, which name a category, such as document, author, publishcr, or 
topic. These categories in turn have instances such as Decision Making 
(Harvard Business School Press). Entities usually correspond to a 
database table or one or more columns in a table. 
0 Relationships that indicate relationships between entities, such as 
Harvard B. S. P. published the review of Decision Making. Association are 
specified by verbs that define relationships. These relationships can also 
be modified by other entities. These additional entities are indicated by 
prepositions in natural language statement. For example David Black 
writes books on chaos theory. The KM sub-model uses a method called 
phrasing to represent different ways of expressing the same relationship. 
The following sentences say the same thing but use different phrasing: 
o Saddam Hussein is the governor of Iraq. 
o The governor of Iraq is Saddam Hussein. 
o Iraq is governed by Saddam Hussein. 
" Dictionaries are collections of words with support for both regular and 
irregular entries. Regular entries are the ones that follow grammatical 
rules, i. e. the past tense of construct is constructed. The irregular entries 
identify words that do not use the general grammatical rules, i. e. the past 
tense of cat is cite. 
Chapter 2 State of the Art 
The role of this sub-model will be limited to provide the same type of natural 
language frond end to text warehouse that it provides for other database 
applications. 
The data warehousing metadata standard provide a solid foundation upon which 
we can build a metadata infrastructure for the text warehouse. Since TW has an 
especially complicated semantic context, we will have to make use of an 
additional standard such as the Dublin Core, to fully satisfy the metadata needs of 
the TW. 
2.3.4.3 The Dublin Core 
The Dublin Core is a metadata standard designed to support resource discovery. 
It is a simple element set for describing a wide range of networked resources. A 
directorate located at the Online Computer Library Center in Dublin, Ohio, 
develops the Dublin Core standard, which was initiated in 1995. Although it 
came from a library science and cataloguing perspective, the Dublin Core is 
becoming one of the most important standards on the Internet resource metadata 
across disciplines. In contrast to some metadata standards that suppose a 
particular area or a specific type, it does not. 
The Simple Dublin Core is a set of fifteen elements as shown in the following 
figure that uses attribute value pairs to describe a resource, such a document: 
Content: Title, Subject, Description, Type, Source, 
Relation, Coverage. 
Intellectual Property: Creator, Publisher, Contributor, 
Rights. 
Instantiation: Date, Format, Identifier, Language. 
Figure 6 Data Elements of Simple Dublin Core 
Chapter 2 State of the Art 
It consists of two classes of terms; elements (nouns) and qualifiers (adjectives). 
which can be arranged into a simple pattern of statements. All data element are 
optional and each may appear an unlimited number of times. Each element also 
has limited set of qualifiers, attributes that may be used to further refine (not 
extend) the meaning of an element. They are all represented as character strings. 
Although DC favours document-like objects, it can be applied to other resources 
as well. Its suitability for use with particular non-document resources will depend 
to some extend on how closely their metadata look like typical document 
metadata and also what purpose the metadata is intended to serve. 
Some implementations using DC have chosen to embed their metadata within the 
resource itself. This approach is taken often with documents encoded using 
HTML, but is also possible with other kinds of documents. On the other hand 
stand-alone metadata can exist in any kind of database, and generally provides a 
link to the described resource. This approach is likely to be most practical for 
many non-textual resources, and is increasingly used for text as well, mainly to 
support easier maintenance and sharing of metadata. 
As we mentioned above if additional information is needed for describing more 
precisely a resource, the simple Dublin Core can be supplemented with qualifiers. 
In July 2000, the DCMI issued its list of recommended Dublin Core Qualifiers 
and recognised two broad classes of qualifiers: encoding scheme and element 
refinement. 
Encoding Scheme qualifiers identify schemes that aid in the interpretation of an 
clement value. They are used to describe the vocabulary or language used to 
describe the metadata. The language specification does not apply to the 
document, but to the metadata itself. Thus a single set of metadata descriptors 
can include subjects and descriptions in multiple languages. 
Element Refinement qualifiers narrow the definition of an element or make it 
more specific. For example. date usually requires 
further classification. By 
Chapter 2 State of the Art 
default the Date element specifies the creation date of a document. But 
additionally we may use the following: 
" Last Modified 
9 Published 
9 Expired 
" Available 
" Verified 
" Accepted 
" Data Gathered 
In addition to specifying more precise meanings of elements with qualifiers, we 
can specify them as sub elements (For example: DC. Date. Created, 
DC. Date. LastModified etc. ). Sub elements are also used in the Relation element 
such as DC. Relations. IsPartOf, DC. Relations. HasPart etc. 
2.3.4.4 TimeML 
For the present purposes the description of TimeML has been kept to the bare 
essentials. TimeML represents a distinct body of research which offers the 
possibility of annotating whichever temporal aspects of documents are 
considered relevant. For example, would a documents publication time be a 
specific enough temporal anchor, or are the more subtle temporal relations 
between events within the document important. It may not be a case of "one size 
fits all" as news feed headlines could only require publication times whereas 
large technical reports should need more fine grained distinctions. 
Which ever level(s) of temporal annotation are targeted, TimeML offers the tools 
to cover the complete range. TimeML is fully compatible with our annotation 
scheme as both have a natural separation between lexical annotations and 
conceptual annotations (see chapter 5). 
Essentially, TimeML uses five tags <E\'ENT>, <TI'MMEX3>, <SIGNAL> 
<LINK> and <DocCreationTime>. The latter is self-explanator`', 
but the scope 
Chapter 2 State of the .4 rt 
of the annotation scheme comes directly from the interaction of the four 
remaining tags. 
Two types of temporal entities are annotated. Verbs. nominalizations. adjectives 
and prepositions that indicate something that happened are marked as 
<EVENT>. Explicit references to times and calendar dates are marked as 
<TIMEX3>. How these two types of entities temporally interact is captured via 
<SIGNAL>, which marks explicit relations ("twice", "during"), and <LINK>, 
which marks implicit relations. The latter is further subdivided into <SLINK>. 
<TLINK> and <ALINK> to characterize different types of temporal relations. 
The temporal annotations operate on two levels. On the lexical level events and 
states are identified, and then the conceptual relations holding between the 
entities are captured by the LINK tags which subsume no text. 
2.4 Pre-processing documents 
Knowledge discovery in large document collections usually require linguistic 
operations that are not related to text mining operations directly. We will refer to 
them in this section and later we will discuss at which extend each tool support 
these operations. 
2.4.1 Linguistics 
Natural language is made of. words, phrases, and sentences. With an 
understanding of how words, phrases, and sentences are structured, we can 
manipulate text and extract information. 
Natural languages are composed from words and rules for combining words. 
From the Text mining perspective, our task is to dev elop systems that use 
language rules and word meanings to produce easily manipulated, categorised, 
and understood representations of texts. 
Linguistics, the study of language. (in 
Chapter 2 State of the . Art 
particular the area generative grammar) has already identified and classified 
many of the rules and principals that underlie language. 
The study of language can be grouped into five main areas: 
" Morphology. Structure and form of words. (Morphological analysers) 
" Syntax. How words and phrases form sentences. (Sentence parser, Part 
of speech taggers) 
" Semantic. The meaning of words and statements. 
" Phonology. Sounds in language. 
" Pragmatics. The study of idiomatic phrases that cannot be analysed with 
strict semantic analysis. 
Most of the text mining applications combine basic linguistic techniques with 
higher level operations such as feature extraction, clustering etc. From the text 
mining perspective we will concern with morphology, syntax and semantics. 
Phonology is important for speech recognition and generation. Pragmatics is a 
challenging area of linguistics in understanding idiomatic phrases but for the 
moment text mining has not utilised techniques of this area. In the near future 
pragmatics may be used in order to analyse e-mails and other informal texts 
(ungrammatical texts). 
2.4.1.1 Morphology 
Words are made up of subparts called stems, affixes and inflectional elements. 
Stems (or roots) are the core meaning elements of words. 
Chapter 2 State of the . 
Figure 7 Morphology 
For example, eatable has the stem eat and the suffix -able. Often words are 
simply stems without affixes or inflectional elements, such as data, document etc. 
There are two types of affixes; prefixes and suffixes. By adding affixes to stems 
we can build more complex word structures and the changes in the meaning of 
steams can be dramatic. For example dispute implies the existence of a 
disagreement, while indisputable means that there cannot by nay disagreement. 
Inflectional elements, change verbs and nouns. For example, in English 
inflections are used with verbs to distinguish past from present and with nouns to 
distinguish singular from plural. In some languages they show gender in nouns. 
Morphological analysers are important and serve many of the text mining 
operations by reducing complexity of analysis and complexity of representing 
word meanings. Simple variations of words (such as the difference between 
singulars and plurals) can be eliminated and as a consequence reducing the 
complexity of later analysis. Feature extraction is one of the operations that 
benefit from morphological analysis. During feature extraction, key concepts 
such as persons, places, and organisations are identified. It is relatively easy to 
identify single terms e. g. Oracle, by using white space, hyphen and other simple 
indicators. For multi word terms such as Department of Computation the 
morphological analyser will need the support of parts of speech analysis in order 
to identify these noun phrases. 
2.4.1.2 Syntax 
The syntax of languages describes how words combine into phrases and phrases 
into sentences. It is syntax that allows us to recognize and utilise phrase 
structures. A sentencc parser analyses the structure of the sentence and 
by using 
the rules of syntax can piece the phrases into a hierarchical structure. 
While 
parsers are identifying phrase structure, at the same time they 
determine the 
relationship between phrases, which can 
be used in later analysis. 
Chapter 2 State of the Art 
For example the sentence, The Mayor of Lamia will hire new employees with the 
new legislation. Using a lexicon for information on parts of speech, a sentence 
parser can identify the noun phrase The Mayor of Lamia, the verb phrase ii ill 
hire, and the prepositional phrase with the new legislation. By using the rules of 
syntax, the parser produces the following hierarchical structure. 
Figure 8 Syntax 
The relationship between verbs and noun phrases is defined by an element of 
syntax called case assignment. The purpose of the case assignment is to identify 
the role that nouns play in a sentence. They anchor the sentence and limit the 
number the number and types of nouns that can be used in a sentence. For 
example, if we know the meaning of the verb to fine, then we 
know that a 
grammatical sentence with it will have an agent doing the 
fining, a recipient 
being fined, and optionally an amount of the fine in. Case assignment information 
can be stored in the lexicon and used to 
drive generalised pattern-matching 
applications. 
Chapter 2 State of the Art 
2.4.1.3 Semantics 
Semantics is the study of meaning. In natural language processing, semantics 
covers both the meaning of individual words and the meaning of sentences and 
documents. The crucial problem is how to represent these meanings. Artificial 
intelligence research has proposed different approaches such as semantic 
networks, deductive logic, and rule-based systems. 
A simple approach is the semantic networks which use node and arcs to represent 
objects, events, concepts and the relation between them. Classification 
hierarchies or taxonomies are actually a limited type of semantic network that can 
represent type-of and part-of relationships e. g. dog is a type of animal, Hellas is 
part of Balkan. 
2.4.1.4 Problems of NLP 
While natural language processing is very useful for most text mining operations 
there are still shortcomings. The most significant problems are: 
" Identifying the correct role of nouns. (Syntactic ambiguity) If we consider the 
sentence "I talked to the person with the instrument ". This could mean that I 
talked to the person, who happened to be carrying an instrument or that I 
talked to the person by using an instrument such as a mobile phone. 
0 Representing abstract concepts. Type-of or Part-of relationships can be easily 
represented in a hierarchy. But as facts become more complex, semantic 
networks become less appropriate at drawing conclusions. As the complexity 
of the representation increases, so does the complexity of drawing deductions 
from representing facts. In addition the enormous number of concepts 
involved in a domain cannot be represented by one hierarchy. 
" Svnony nv and Polseme. Svnomvrns are words or terms with similar 
meanings. Polysemy is the fact that a single word can 
have multiple 
meanings. In almost all the languages there 
is great number of words about 
common objects that mean pretty much the same thing. 
In specialised 
Chapter 2 State of the Art 
domains, the number of similar terms can grow rabidly when practitioners are 
trying to be more accurate in their definitions. 
2.4.1.5 Specific techniques 
Specific linguistic or text processing techniques that been used include: 
Tokenisers separate the beginning and ends of words and sentences. They use 
punctuation, white spaces, and other indicators in order to produce a stream of 
words. 
Text normalisers, put words and terms into standard form for later analysis. For 
example it can include reformatting names such as Bill Black and William Black, 
which can be recognised as the same term and put into canonical form, William 
Black. 
Stop-words list, are list of words ignored in full text indexing because they occur 
so often that they provide no significant information and are not useful for the 
definition of search patterns. 
Full text indexing provides the basic operations and data structures required to 
find text patterns easily. Some tools provide phonetic searching and word 
variation operations, such as soundex functions. 
Two additional issues that may be required to deal with in particular domains arc: 
Multilingualit}y. Multilingual support for different character sets when dealing 
with complex processing languages such as Japanese, Chinese etc. 
Spell checking. Misspelled words on the documents can be corrected before the 
main processing. 
2.4.1.5.1 Character set and format conversion 
The major character sets been used until now for representing characters in 
computer applications are the ASCII and the EBCDIC (by IBM). A newer 
standard, UNICODE has been introduced to exceed other encoding schemes 
because of its ability to represent far more characters than the ASCII or EBCDIC. 
Unicode will replace the older character sets but in the meantime we may have to 
identify the character set of a document and if necessary, convert it to an 
Chapter 2 State of the .4 rt 
appropriate character set for later processing by the other modules. Additionally 
transformations could be needed aiming at obtaining the desired representation of 
documents such as XML, SGML. 
2.4.1.5.2 Language identification and translation 
Some of the operations performed on documents are language specific i. e. 
morphological analysis. So, it is important in the pre-processing stage to identify 
the language that the document is written and if it is not written in a suitable 
language we may have to translate it. 
2.4.2 Information Extraction 
The process of detecting patterns within and across text documents (Text 
Mining) depends mainly upon information extraction techniques. By identifying 
key entities in a text, one can find relationships between terms and identify 
unknown links ('hidden') between related topics. Information Extraction is the 
process of identifying essential pieces of information within a text, mapping them 
to standard forms and extracting them for use in later processing. [38] 
Another way to define is that: Information Extraction is the mapping of natural 
language texts (such as newswire reports, newspaper and journal articles, 
electronic mail, World Wide Web pages, any textual Data base, etc. ) into 
predefined, structured representation, or templates, which, when filled, represent 
an extract of key information from the original text. [39] The 
information 
concerns entities of interest in the application domain (e. g. companies or 
persons), or to relations between such entities, usually 
in the form of events in 
which the entities take part (e. g. company takeovers management successions). 
Once extracted, the information can then be stored 
in databases to be queried, 
data mined, summarised in natural language, etc. 
2.4.2.1 IE (Information Extraction) vs. IR (Information Retrieval) 
IR selects a relevant subset of 
documents from a larger set. IE extracts 
information from the actual text of documents. Any application of 
IE technology 
Chapter 2 State of the Art 
is usually preceded by an IR phase, which selects a set of documents relevant to 
some query. So, EE is interested in the structure of the texts, whereas one could 
say that, from the IR point of view, texts are just bags of unordered words. 
2.4.2.2 Possible restrictions of lE systems 
The main problem with IE is the degree to which knowledge information system 
template like, in the way history was once taught (but is no longer) as factual 
templates of kings, presidents, battles and dates. A major research issue is seeing 
how far the boundaries of templatability can be pushed out. A closely related 
question is how far the construction and adaptation of templates to new domains 
can be made practical and cost effective. 
In addition, for the systems, which use large language dictionaries, there is one 
more point to mention. What such lexicons luck is a dynamic view of the 
language. For example, dictionaries of English normally tell us that "television" 
is as a technology or a TV set, although it is mainly used now to mean the 
medium itself. Modem texts are thus out of step with dictionaries, even modem 
ones. That's why there is a need not just to use large textual and lexical 
resources, but also to adapt them as automatically as possible, to enable them to 
adapt to new domains and corpora. 
Finally an important issue that arises is the accuracy of Information 
Extraction 
and the impact that the noise in the knowledge 
base has on the effectiveness of 
Data Mining techniques. This is particular crucial for systems that combine IE 
and data mining. [45] 
2.4.2.3 Basic Techniques 
The process of information extraction 
has two major parts. 
" First the system extracts 
individuals "facts" from the text of a document 
through local analysis. 
" Second, it integrates these 
facts, producing larger facts on new facts 
(through inference). 
Chapter 2 State of the . 
As a final step after the facts are integrated, the relevant facts are translated into 
the required output format. 
Scenario we shall call the specification of the particular events or relations to be 
extracted. Thus, we distinguish between a general domain, such as financial 
news, and a particular scenario, such as international joint ventures or aircraft 
sales. 
Template we shall call the final, tabular output format of information extraction. 
2.4.2.4 Stages of processing 
We shall look at each of the stages of processing. As we go through the stages, 
we will focus on a simplified version of the MUC-6 scenario. The details we 
present will be those from New York University Proteus Project extraction 
system. 
In this system, and in many other current extraction systems, most of the text 
analysis is performed by matching the text against a set of regular expressions. If 
the expression matches a segment of text, the text segment (constituent) is 
assigned a label, and possibly one or more associated features. 
Associated with some of the constituents in the system are semantic structures 
called entities and events. These structures will be used to construct the 
templates. 
Lexical Analysis 
The text is first divided into sentences and into tokens. Each token is looked up 
in the dictionary to determine its possible parts-of- speech and features. The 
Proteus dictionary includes the Complex Syntax dictionary (a large, (, eneral- 
purpose English dictionary) plus various special dictionaries, such as 
dictionaries 
of major place names, major companies, common 
first names, and company 
suffixes (such as "Inc. "). 
Chapter 2 State of the Art 
Document 
Local text analysis 
Lexical analysis 
Name recognition 
Partial syntactic analysis 
Scenario rattern matching 
Template generation 
Extracted Template 
Figure 9 Structure of an Information Extraction system. [39] 
Name Recognition 
The next phase of processing identifies various types of proper names and other 
special forms, such as dates and currency amounts. 
Names appear frequently in 
Chapter 2 State of the Art 
many types of texts, and identifying and classifying them simplifies further 
processing; names, furthermore, are important as argument values for many 
extraction tasks. 
Syntactic Structure 
Identifying some aspects of syntactic structure simplifies the subsequent phase of 
fact extraction. After all, the arguments to be extracted often correspond to noun 
phrases in the text, and the relationships to be extracted often correspond to 
grammatical functional relations. The system builds structures for noun groups 
and for verb groups. In addition, it builds certain larger noun phrase structures if 
it has semantic information to confirm the correctness of the structure. All of this 
is done using the same regular expression pattern matcher. 
Scenario Pattern Matching 
All of the processing until now has been in a sense preparatory for the scenario 
pattern matching. The role of these patterns is to extract the events or 
relationships relevant to the scenario. 
Co reference Analysis 
Co reference analysis has the task of resolving anaphoric references by pronouns 
and definite noun phrases. For example in a text the word "He" and a name of a 
person may refer to the same entity. 
Inferencing and Event Merging 
In many situations, partial information about an event may be spread over several 
sentences. This information needs to be combined before a template can 
generated. In other cases some of the information is only 
implicit, and needs to be 
made explicit through an inference process. 
For many scenarios time is important: either explicit times must 
be reported, or 
the sequence of events is significant. In such cases, time 
information may be 
derived from many sources, including absolute dates and times 
("on April 6, 
1995"), relative dates and times ("last week"), verb tenses, and 
knowledge about 
Chapter Z State of the Art 
the inherent sequence of events. Since time analysis may interact with other 
inferences about the discourse, it will normally be performed as part of this stage 
of processing. 
2.4.2.5 Issues to be considered 
In this section we are going to present two aspects which we have to discuss in 
the framework of the IE system creation. The first is Portability and the second is 
Multilinguality. 
Portability 
One disadvantage of Information Extraction Systems is the cost of adapting them 
to a new scenario. In general, each application of extraction will involve different 
scenario. If implementing this scenario requires a few months of effort and the 
skills of the extraction system designers, the market for extraction systems will 
remain limited indeed. What we need is to have a tool, which will allow potential 
users to adapt such a system, and create an initial one in days or weeks, not 
months. 
The basic question in developing such a customisation tool is the form and level 
of the information to be obtained from the user (assuming that the goal is to have 
the customisation performed directly by the user rather than by an expert system 
developer). If we are using "pattern matching" system, most of the work will 
probably be focused on the development of the set of patterns. However, changes 
will also be needed to the semantic hierarchy, to the set of inference rules, and to 
the rules for creating the output templates. 
One of the first sites to experiment with this approach was the University of 
Massachusetts at Amherst. Their system as developed for MUC-3, relied on a 
large number of small, lexically triggered patterns, which build 
individuals 
concepts; these concepts were then consolidated to create the 
information needed 
for template filling. For MUC-4, they developed a tool ('`Auto Slog") to create 
such patterns semi automatically 
from the development corpus with its templates. 
Chapter 2 State of the Art 
Given a template slot, which is filled with words from the text, such as a name, 
their program would search for these words in the text and would hypothesise a 
pattern based on the immediate context of these words. These patterns would 
then be presented to a system developer, who could accept or reject the pattern. 
Several of the earlier MUCs involved large training corpora, with over a 
thousand documents and their templates; such corpora encouraged such corpus- 
based approach. However, the centralized preparation of large, consistent training 
corpora proved to be an expensive proposition; this suggested that such large 
corpora would not be available for the most real tasks. 
At NYU, they were building an interactive tool for customizing an extraction 
system. They believe that this will provide the most efficient approach to 
acquisition in situations where the user does not have a large, pre-annotated 
corpus (and in fact, may be refining the scenario in response to new examples), 
and does not have the expertise to create pattern unaided. The user begins by 
providing an example (normally drawn from the corpus) and the fact (template) 
to be extracted. The system responds by using the existing patterns to create a 
structural description of the example. It then can interact with the user to extend 
and generalize the example, both syntactically and semantically. In this way, it is 
possible to quickly extend a single example into a pattern with broad coverage. 
Multilingual Information Extraction 
Most of the IE systems perform an extraction task against texts in one language. 
It is natural to consider how to modify the system to perform the same task 
against texts in a different language. More generally, there may be a requirement 
to do the extraction task against texts in an arbitrary number of different 
languages and to present results to a user who has no knowledge of the source 
language from which the information has been extracted. 
To minimise the language-specific alterations that need to 
be made in extending 
the system to a new language, it 
is important to separate the task-specific 
conceptual knovvledge the system uses. which may 
be assumed to be language 
Chapter 2 State of the. A rt 
independent, from the language-dependent lexical knowledge the system 
requires, which unavoidably must be extended for each new language. (e. g. the 
multilingual M-LaSIE system derived from the LaSIE system) [40] 
2.5 Main Text Mining operations 
Main goal of Text mining is to unable users extract information from large 
textual resources. Natural language processing data mining and machine learning 
techniques work together to automatically discover patterns at the extracted 
information and the metadata the have been derived from the documents. 
Most Text Mining objectives fall under the following categories of operations: 
9 Feature Extraction. 
" Text-base navigation. 
" Search and Retrieval 
" Categorisation (Supervised classification) 
" Clustering (Unsupervised classification) 
" Summarisation 
" Temporal Analysis of Documents (i. e. Trends Analysis) 
" Associations 
9 Visualisation 
2.5.1 Feature extraction 
Primary objective of the feature extraction operation is to 
identify facts and 
relations in text. Most of the times this includes 
distinguishing which noun 
phrase is a person, place, organisation or other 
distinct object. 
Feature extraction algorithms may use dictionaries to 
identify some terms and 
linguistic patterns to detect others. For example, the name of an organisation, 
such as "U: t11ST', may not 
be in a dictionary but a feature extraction algorithm 
could identify' it is a noun and probably significant 
term. Pattern recognition 
algorithms (like Hidden 
Markov Models HMM) are trained to detect patterns i. e. 
Chapter 2 Staic c ?f the -Art 
a noun phrase followed by a verb phrase is often followed by another noun 
phrase as in "UMIST hired lecturers". Of course always a pre- or post- 
processing is required to manually specify significant terms or remove 
unnecessary automatically identified terms. 
The terms extracted usually have to be in canonical or standard form. This makes 
indexing retrieval and the other operations, which follow, more accurate. For 
example "studying" and "study" should be identifying as the same word. 
Additionally this operation should indicate the number of times each term 
appears in a document (word frequency). This mainly supports the classification 
of documents. 
The technology behind feature extraction is the Information Extraction and is 
tend to be the most important component of the Text mining process. We 
consider Feature Extraction (JE) as a TM operation as it may be supported by 
data mining (machine learning) in various stages and in many systems is the 
component that has direct interaction with user. 
2.5.2 Text-base navigation. 
The text-base navigation enables users to move about in a document collection 
by relating topics and significant terms. It helps to identify key concepts and 
additionally presents some of the relationships between key concepts. 
example, when searching for "UMIST, we should be able to quickly move to 
"UMIST courses", "UMIST degrees" and other terms related to the 
" UMIST'. 
The important features about this operation are two. The first is the ability to see 
related terms in context. For example when we notice that two terms seem 
keep occurring together probably there must be an 
important relationship 
between them. The second important feature is the ability to move 
from one pair 
of terms to another. For example if the pair "UMIST and 
"courses" does not 
satisfy us it should be possible to move to another pair such as 
"U. 1 IST"' and 
"degrees". [36] [31 ] 
Chapter 2 
2.5.3 Search and Retrieval 
State of the _Art 
It is used for searching internal documents collections or for the Web. Its main 
characteristic is the various text search options. After indexing which is the first 
step, a wide range of text search options maybe utilised. These include basic 
search options such as Boolean (and/or/not), proximity, wildcard, segment, 
numeric range, etc. as well as some advance search capabilities, such as 
relevancy-ranked natural language searching, fuzzy search, concept search etc. [4] 
[2l] [30] 
2.5.4 Categorisation 
Categorisation is the operation that we use when we want to classify documents 
into predefined categories. Due to this, we are able to identify the main topics of 
a document collection. 
The categories are either pre-configured (by the programmer) or left for the user 
to specify. There are two ways to create the classifications. In the first approach a 
thesaurus can be created that defines a set of domain-specific terms and the 
relationships between them (the most common relationships are broader term, 
narrower term, synonym, and related term). The categoriscr can then determine 
the subject of the text according to the frequency of the domain specific terms 
which has been identified within the text. In the second approach the categoriser 
is trained with sample documents. The categoriser statistically analyse 
linguistic 
patterns (such as lexical affinities, word frequencies) 
from sample documents 
(pre-categorised) that belong in each category in order to create a statistical 
signature for each category. Then it uses the statistical signature 
to classify ncww 
documents. The advantage of the second approach is that a thesaurus, which 
hard to build for large domains, is not needed. 
In order to avoid wrong classification of a 
document it is preferable to provide 
multiple categories per document. 
Additionally it is very helpful when the tool 
supports both inter- and intra-document ranking. 
[ 13] [1] 
Chapter 2 State of the. 4rt 
2.5.5 Clustering 
A cluster is a group of related documents, and clustering is the operation of 
grouping documents on the basis of some similarity measure, automatically 
without having to pre-specify categories. 
The most common clustering algorithms that are used are hierarchical, binary 
relational, and fuzzy. Hierarchical clustering creates a tree with all documents in 
the root node and a single document in each leaf node. The intervening nodes 
have several documents and become more and more specialised as they get closer 
to the leaf nodes. It is very useful when we are exploring a new document 
collection and want to get an overview of the collection. Binary relational 
clustering partitions the documents into a flat structure; each cluster is placed in 
only one set. With Fuzzy clustering all documents are included in all clusters, but 
with a different degree. 
The most important factor in a clustering algorithm is the similarity measure. All 
clustering algorithms are based on similarity measures, and there are various 
types. One type uses words that frequently appear together (lexical affinities i. e. 
Department of Computation) as common features in order to group documents. 
Another type uses extracted features such as person name e. g. Mr. Mitsos 
Karaiskos. [2] [31] [12] 
2.5.6 Summarisation 
Summarisation is the operation that reduces the amount of text in a document 
while still keeping its key meaning. 
With this operation the user usually is allowed to define a number of parameters. 
including the number of sentences to extract or a percentage of the total 
text to 
extract. The result includes the most significant sentences , 
ithin the document. 
[31] [l2] [i] 
Chapter 2 State of the Art 
2.5.7 Temporal Analysis of Documents 
One example of such analysis is trends analysis. This operation is used for 
identifying trends in documents collected over a period of time. Trends can be 
used, for example to discover that a company is shifting interests from one 
domain to another. [ 17] 
2.5.8 Associations Analysis 
Given a set of documents, identify relationships between attributes (features that 
have been extracted from the documents) such as the presence of one pattern 
implies the presence of another pattern. For example, Neurosoft SA, Intrasoft - 
Take over could be a rule that been discovered. An application based on this 
operation is presented by Feldman. [7] [3] 
2.5.9 Visualisation 
Visualisation utilises feature extraction and key term indexing in order to build a 
graphical representation of the document collection. This approach supports the 
user in identifying quickly the main topics or concepts by their importance on the 
representation. Additionally, it is easy to discover the location of specific 
documents in a graphical document representation. [3] [26] 
2.6 Methodology of Survey 
In this section we present the feature classification scheme that we have proposed 
to study knowledge discovery in Text (KDT) and Text Mining tools. Then we 
apply this scheme to review existing tools. Although not exhaustive, we believe 
that the reviewed tools are representative for the current status of technology. 
Features that are relevant to our study are grouped into 3 groups as follows: 
general features of tools, text mining operations supported. 
2.6.1 General features of tools 
The following features are considered as general characteristics of the s}stems: 
Chapter 2 State of the Art 
l. Product name and vendor, home page location on the Web. 
2. Purpose and functionality. 
3. Demo: Specifies if there is a demo version available. (D) Demo version 
available for download on the internet, (R) Demo version available on request, 
and (U) Unknown. 
4 Architecture: The computer architecture on which the software runs. (S) 
Standalone, (C/S) Client/Server and (P) Parallel. 
2.6.2 Text mining approaches 
1. Framework Architecture: (F) File oriented, (C) Component Oriented, (D) 
Database oriented. 
2. Text mining Methodology (Linguistic processing): (HEUR) Heuristic 
approach (lookup techniques)/ (STAT) Statistical approach, (NN) Neural 
Networks, (KNOW) Knowledge-based approach. 
2.6.3 Text Mining steps and operations 
1. (DR) Include Document Retrieval Module. Some of the products include 
components that support the gathering (Retrieval) of relevant documents in 
ordered to be processed. 
2. Pre-processing: (Morph) Morphology, (Synt) Syntax. (Sem) Semantic. 
3. Text mining operations: (FE) Feature Extraction, (TBN) Text-base navigation, 
(SR) Search and Retrieval, (CAT) Categorisation (Supervised classification), 
(CLU) Clustering (Unsupervised classification) (SUM) Summarisation, (TA) 
Temporal Analysis, (ASS) Associations, (DA) Distribution analysis, (VIS) 
Visualisation. 
2.7 Text Mining Tools 
2.7.1 WizSoft WizDoc 
WizDoc is a search engine that enables users to search 
by two methods. The one 
is the traditional `string search' and the other is the 'concept-based search'. 
Chapter 2 
2.7.1.1 Technology description 
State of the Art 
WizDoc operates in two stages. Firstly the indexing stage in which the textual 
data is indexed. Secondly the search stage that the end-user enters texts as a 
search query, and the algorithm retrieves the relevant text sections. 
Indexing operations: At first the tool determines the meaning of the strings in the 
text data. When a string has several possible meanings, WizDoc disambiguates it 
by using syntactic and semantic clues. Then it applies a proprietary algorithm that 
determines the key words in each section. This algorithm takes into account the 
meaning of each word as well as its thesaurus. The key words are determined 
automatically. The tool finalises the indexing stage by determining the relation 
between the key words and the text sections. Data mining technology is applied 
for pattern discovery. 
Searching operations: Determines the meaning and the key words of the text in 
the query by applying the same methods used in indexing the text. Uses the 
patterns discovered in the indexing stage in order to calculate the degree of 
relevancy of the sections to the search query and then sort them. On calculating 
the extent that a text section is relevant to the query, the tool makes use of 
another proprietary algorithm based on data mining. Spelling checker is included 
when analysing the query. 
Currently WizDoc can be applied to text in the English language only. 
2.7.1.2 Basic operations 
Concept-based search (Linguistic processing) 
1. Disambiguation of words. 
2. Automatically extract keywords in each section. 
3. Expand the query by using thesaurus. 
4. Determines the relations between the keywords and the text sections. Also 
an algorithm for calculating the extent that a text section 
is relevant to the 
query. (Data Mining) 
Chapter 2 State of the .4 rt 
Spelling checker 
1. Check the spelling of the words/names in the query. 
2. Displays all the phonetically similar words or names on discovering a 
misspelled word. 
Archiving 
1. Automatic. 
2. Use of spider to reveal the pages in a site. 
3. Dictionary and thesaurus updates. 
2.7.1.3 Comments 
" lt is language dependent. 
0 It is interesting that it includes data mining techniques in order to 
determine the relation between the key words and the text sections. 
www. wizsoft. com/ 
2.7.2 SPSS TextSmart 1.0 
TextSmart is a tool for quantitative analysis of text-based survey responses. It 
basically makes cleaning, filtering and categorisation of open-ended responses. 
The tool uses linguistic technology and statistical algorithms. 
2.7.2.1 Technology description 
Initially there is an Import Wizard, which helps the user importing the responses. 
The next step is to automatically filter the responses by stemming, aliasing and 
excluding unnecessary words. Stemming is a linguistic technique in order to 
derive common root words (i. e., find, finding, finds). The aliasing technique 
combines synonyms automatically. Finally a stop list excludes trivial words (i. e., 
be, is, the, of). It is important to mention that TextSmart is "dictionary -free". so 
there is no need for creating one before the analysis. TextSmart provides tools for 
refining the words produced by the automatic procedures. These tools include 
spell checking for enhancing the automatic categorisation process. 
Ater cleaning and refining the list of terms the next step 
is to create the 
categories by automatically clustering terms that tend to occur together 
Chapter 2 State of the Art 
responses. The user then has to amend and label these categories. The results 
(categories) then can be exported to other statistical packages. 
2.7.2.2 Basic operations 
Pre-processing /Filtering. (Linguistic processing) 
1. Spell Checking. 
2. Stemming. 
3. Stop lists. 
4. Alias list (synonyms). 
Categorisation 
1. Use matrix of similarities. 
2. Hierarchical clustering with maximum distance amalgamation 
(merger). 
3. Multidimensional scaling in two dimensions to scale and chart the 
matrix of similarities. 
2.7.2.3 Comments 
TextSmart is a tool for building categories with automated procedures. It uses 
some basic text analysis techniques plus a Clustering algorithm. There is no 
demo to check, just a movie demonstration, which is not bad. 
http: //www. spss. com/textsmart/ 
2.7.3 Megaputer TextAnalyst 2.0 
TextAnalyst is a software tool for semantic analysis, navigation, and search of 
unstructured texts. Implements a variety of analysis functions based on utilising 
an automatically created semantic network of the investigated text. The semantic 
network is a list of the most important words from the text and the relation 
between them. 
2.7.3.1 Technology description 
Pre-processing is the first step for TextAnalyst. It involves removal of common 
words (Stop words) and the identification of word stems. Common words are the 
words that do not carry any useful semantic meaning for further analysis. 
Chapter 2 State of the Art 
Automatically the tool separates the stems from the prefixes, suffixes, and 
endings. This first step is language dependent in order the tool to apply Stop 
words and identify word stems. 
After the Pre-processing the developed neural network holds all important words 
and word combinations from the text with their frequency of occurrence. At the 
same time the same network assesses frequencies of joint occurrence of different 
semantic elements within certain structural text units (for example sentences). 
The next step is called renormalization. Individual statistical weights of the 
words and relations between them need to be adjusted in order to provide a 
reliable representation of the text. The weights of those words, which are related 
to other frequent words should be boosted and vice versa. This is accomplished 
by using the one-dimensional Hopfield-like Neural Network. The renormalized 
weights of words and relations between them are called semantic weights and the 
resulting structure semantic network (which is a list of the most important words 
and word combinations from the text and relations between them). Words and 
word combination comprising a semantic network have a special name - 
semantic concepts. At the end of this step the text has been prepared for analysis 
and the result is the semantic network. 
There are a number of user functionalities based on the utilisation of the semantic 
network: 
Semantic Analysis: Is the creation of the semantic network, a brief representation 
of the text. 
Text-base Navigation: Key concepts in the text are linked to concepts in the 
analysis. Since these concepts are also in the semantic network the user can 
navigate through the semantic network and hyperlinks 
between an analysis and 
the original text. 
Topic Structure: The system can identify the most important concepts from the 
semantic network and transform the network 
into a tree-like list of nested topics. 
Clustering: Eliminate those links in the topic structure whose strength falls 
below 
a certain threshold. In this way a 
joint topic structure of a collection of texts 
Chapter 2 State of the . -brt 
breaks into groups representing certain largely independent themes, which help 
understand the clusters of information in the investigated text-base. Then 
individual documents can be assigned to different thematic groups, thus 
facilitating clustering of the documents in a text-base. Large documents, which 
probably correspond to different thematic clusters, are treated as multi-topic 
documents or can be split in separate parts. 
Summarisation: The semantic network can be utilised to find the most important 
sentences that can form the summary. It uses weights based upon individual 
words and multiword terms to calculate a sentence weight and then extract the 
most relevant sentences. 
Natural language information retrieval: The user can search for information by 
typing a question using a natural language format. Processing questions is similar 
to processing sentences in documents. Words in the question are scored 
according to their relationship with other words. Based upon the results of that 
scoring, similar sentences are extracted from the source text. 
2.7.3.2 Basic Operations 
Pre-processing /Filtering. 
1. Stop words 
2. Stemming 
3. Creation of semantic network (Neural Network approach) 
User functionality 
1. Text-base navigation. 
2. Topic Structure. 
3. Clustering concepts. 
4. Summarisation. 
5. Natural language information retrieval. 
2.7.3.3 Comments 
Like Oracle Text, TextAnalyst is designed to support information retrieval and 
text-base navigation. Additionally similar to IBM intelligent miner 
for Text 
provides support for a range of text mining operations without a predefined 
knowledge base. The basis for TcxtAnalyst's processing is a neural network 
technique that analyses pre-processed text to 
develop a semantic network. The 
Chapter 2 State of the Art 
semantic network provides the information required for clustering, summarising, 
and navigating documents collections. 
Multiword terms are identified on the basis of the frequency with which they co- 
occur. Unlike IBM Intelligent Miner for Text, which does not usually include 
verbs outside of noun phrases in person, place, and organisation terms, 
TextAnalyst does. This is due to the strictly mathematical approach utilised by 
Megaputer's tool in order to build the semantic network. On the other hand IBM 
is using language-specific heuristic thus identifying terms can be done better. 
TextAnalyst has chosen to limit language-specific operations to the pre- 
processing stage therefore to allow more adaptability in later stages. 
Additional comments include: 
" It is not clear how it treats ambiguity of words. 
" There is language dependency only in the Pre-processing step (Stop 
words, stemming). 
" The most important element is the Semantic network. The entire 
functionality user oriented or not is based on it. It can distil the semantic 
network of a text, without prior development of a subject-specific 
dictionary by a human expert. 
9 Joint-occurrences seem to relate with the lexical affinities by IBM. 
http: //www. me. aputer. com/products/ta/index. php3 
2.7.4 IBM Intelligent Miner for Text 
Intelligent Miner for Text uses a statistical and heuristic approach to text 
analysis. It provides a set of text analysis tools and a search engine enhanced with 
mining functionality and visualisation of results. These tools can work together in 
order to extract knowledge from unstructured information. Basically the tools 
supplement documents with information about their contents. This 
information 
then is used as metadata about the documents and can be used in turn 
for Data 
Mining. 
2.7.4.1 Technology description 
Chapter 2 State of the Art 
The process of using the tools may vary and depends on the specific application. 
Usually the first step is to extract key features from texts. Examples of features 
are the language of text, company names, dates etc. After the feature extraction, 
the next step is to assign the documents to subjects and index them in order to 
facilitate searching. Additionally we should mention that the indexing phase 
includes the regular linguistic processing, such as terms on the base form, stop 
word, synonyms, thesaurus etc. 
The language identification component can automatically discover the 
language(s) in which a document is written by using clues in the document's 
content. Language identification works by analysing patterns in a text. Common 
words patterns, including prefixes and suffixes such as anti-, post-, -ism. and - 
ing, can indicate that a document is written in English. 
The feature extraction component identifies important vocabulary items in text. 
In fact what is found is to a large degree the vocabulary in which concepts 
occurring in the collection. To find these features the tool uses a combination of 
language-specific heuristics and lookup techniques to find significant terms. The 
features automatically recognised are names (of people, organisation and places), 
multiword terms, abbreviations and other vocabulary such as dates and currency 
amounts. When analysing single documents, the feature extractor can operate in 
two possible modes. In the first, it analyses that document alone. In the second 
mode it locates vocabulary in the document, which occurs in a dictionary, which 
it has previously built automatically from a collection of similar documents. 
When using a collection of documents, the feature extractor is able to discover 
the best possible vocabulary by combining facts from all documents. In each 
extracted item is assigned a statistical significant measure within the documents 
in the collection. 
Using heuristic, the name extraction module identifies names in text and 
determines what type of entity the name refers to (person, company, place, 
organisation, etc. ). It does not require a pre-existing 
database of names. It 
discovers names based on linguistically motivated heuristics that exploit 
Chapter 2 State of the Art 
typography and regularities of language. [I ] The term extraction module uses a 
set of simple heuristics to identify multiword terms in a document. Those 
heuristics, which are based on a dictionary containing part-of-speech information 
for English words, involve doing simple pattern matching in order to find 
expressions having the noun phrase structures characteristic of technical terms. 
The abbreviations recogniser identifies these short form variants and matches 
them with their full forms. When the full form has also been recognised by name 
extraction or term extraction then the short form is added to the set of already 
identified variants. Other extractors include number, dates and money. 
Clustering is an automated process that groups similar documents from a 
collection of documents. Hierarchical and binary relational clustering algorithms 
are used. The goal for the clustering analysis is to determine a set of clusters 
(group of documents) such that the inter-similarity is minimised and intra-cluster 
similarity is maximised. Hierarchical clustering groups documents using a tree 
structure, with a single root, leaf nodes, and intervening levels. It is very 
important the notion of similarity between documents. The hierarchical clustering 
tool supports two types of similarity measures, lexical affinities and linguistic 
features. A lexical affinity is a correlated group of words, which appear 
frequently within a short distance of one another. They are generated for each 
document collection, so a predefined thesaurus is not required. A list of lexical 
affinities in each document is used as the basis for its similarity calculation. A 
cluster then can be labelled with the lexical affinities it contains, which allows a 
user to quickly measure the characteristics of the cluster. The other similarity 
measure that is used is based on linguistic features. The 
feature extraction 
program is used to identify key features such as names and terms. 
The advantage 
of this approach is that someone can cluster documents on specific 
type of 
information in them. For example we can perform clustering on the features such 
us places, persons, and organisation and group together 
documents that refer to 
the same organisation. The structure of the 
binary relational clustering is flat and 
the documents are included in only one cluster. Each cluster tends to represent a 
single topic. The user can specify one of three methods 
to calculate similarity. Of 
course all of the methods are 
based on features that represent a document. The 
Chapter 2 State of the Art 
first measure gives more weight to features that appear in a wide range of 
documents. This tends to generate clusters based on common topics. The second 
measure gives more weight to features that appear rarely. This tends to generate 
clusters that are centred on more specific topics. The third measure gives slightly 
less weight to wide-ranging features than features that appear in few documents. 
This tends to reduce the influence of common topics, without leading to large 
numbers of narrowly focused clusters. 
Categorisation assigns documents to pre-existing categories (topics, themes etc. ). 
First it extracts characteristic features form the documents being categorised, then 
comparing these features to a set of features for each category which was 
extracted from the example documents during the training phase. Like the other 
programs in the suite uses statistical techniques along with morphological 
analysis, in order to develop a category scheme describing a topic. 
The summariser tool performs summarisation by extraction using a combination 
of heuristics. At first the feature extraction tool is run to identify structural 
boundaries between words, sentences, and documents. Terms extracted from the 
text are then compared to a reference vocabulary that includes statistics on word 
frequencies. Summaries are built on the basis of word rankings. A word rank is 
calculated according to several factors. Words are considered worth ranking 
they appear in the title, headings, or captions and when they occur more 
frequently in the document than in the reference vocabulary. 
2.7.4.2 Basic Operations 
Pre-processing /Filtering. (Basic Linguistic processing) 
1. Stop word. 
2. Reduce terms to their base form. 
3. Synonyms. 
4. Thesaurus. 
5. Building a feature index. 
User functionality 
6. Feature extraction. 
Chapter 2 
7. Clustering. 
8. Categorisation. 
9. Summarisation. 
2.7.4.3 Applications 
State of the. 4 rt 
Intelligent Miner for Text offers system integrators, solution providers and 
corporate applications developers a set of tools to enrich business intelligence 
solutions. These tools include: 
I I. Text analysis tools. 
2. Full-text search engine. 
3. Web crawler tools. 
4. A Web search solution. 
With these tools a wide variety of applications can be built. For example, we can 
categorize information from news feeds; analyse patent portfolios, customer 
complaints letters and competitor's Web pages; or turn a corporate intranet (or 
website) into a warehouse of knowledge. 
2.7.4.4 Comments 
The architecture of the IBM tool is file oriented. Input is provided to each tool as 
a stream of files or as a file containing fully qualified pathnames. The results of 
the analysis tools are output as streams of semi-structured text that easily 
manipulated. 
One of the benefits of the IBM approach to include language-specific heuristics is 
that identifying terms can be done more cleanly. However there is a restriction 
with the languages to be used. If the feature extraction has to be done in other 
languages than English, then the heuristics need to be redesigned for the target 
language. 
The approach that been adopted for topic categorisation. has the 
benefit that the 
tool can be easily trained for a particular application. 
This tripe of design does not 
need semantic representation, such as taxonomy, to cam, out 
its work. 
Chapter 2 State of the Art 
Lexical affinities are generated for each document collection. so a predefined 
thesaurus is not required. The advantage of this approach is that, although 
additional processing is required before the core clustering operation begin, the 
lexical affinities are specific to the documents being clustered. 
" Lexical affinities similar to Word Permutation (TextQuest) and Joint- 
occurrences (TextAnalyst). 
9 High language dependence as a consequence of the deep language 
understanding. 
2.7.5 ClearForest 
ClearForest's set of tools work with text data. The tools structure the text data, 
extract important information, assign them to taxonomy and define their inter- 
relationships. The result is a structured body of information that users can 
manage and generate patterns in various visual forms. The tools can be used for 
different applications by incorporating a rulebook. A rulebook is set of 
instructions describing specific linguistic patterns relevant to specific domain. 
The rulebooks provide the needed information to the knowledge extractor engine 
what concepts and relationships to retrieve from the documents. 
2.7.5.1 Technology description 
ClearForest's Suite comprises a number of components. First it integrates the 
search procedures required by the user into one module (Administrator). The 
Administrator includes a Search Agent that relays that user's request to the online 
source or search engine. Then proceeds to download the results, compiling them 
into a single file, which is a concatenation of all the documents found. When the 
source data are local archives of documents of a proprietary structure, these are 
converted by the Convertor component into a suitable format prior the 
concatenation. 
The next step is the process of information extraction. Significant terms, names, 
and relationships are identified, extracted and classified into a taxonomy based 
on logical categories found in the text. These are then counted, sorted and 
Chapter Z State of the . 4rt 
analysed for important relationships. The results stored in a knowledge base 
ready for questioning by the user. 
Finally, in the visualisation module of the software, the terms, names and 
relationships are exposed to the user in various visual formats. 
The rules governing how the information extraction is carried out are developed 
with the third primary module, ClearStudio. Using a Visual-C-like interface. 
rules for specific domains can be constructed. A set of rule files is provided With 
the ClearForest suite to deal with general contexts. 
2.7.5.2 Basic operations 
Pre-processing /Filtering. (Basic Linguistic processing) 
1. Convert documents. ( into an SGML format) 
2. Part-of-speech tagging. 
3. Lemmatisation. 
4. Term generation and filtering. 
User functionality 
1. Document retrieval. 
2. Feature Extraction. 
3. Taxonomy construction. 
4. Visualisation. 
2.7.5.3 Applications 
A broad set of applications can be developed with the modification of the 
rulebook. The rulebooks provided include financial services, Business Research, 
Legal, politics, sports and health. This includes graphic visualisations of 
relationships between companies, people and events in the business world. 
ClcarResearch presents a single screen view of complex inter-relationships, 
enabling users to view news and research content 
in context. One example of this 
application is the automation of Patent Research. 
W'e'hen seeking a patent for a 
new product. an inventor must 
first determine the existence of "prior art". 
Chapter 2 State of the Art 
ClearResearch provide the names of key players, graphic and textual 
representation of their comparative distribution, interactive recursive cross- 
references, time-based charts of appearance, and visual maps of links between 
members of any category the user chooses. 
ClearSight provides simple, graphic visualisations of relationships between 
companies, people and event in the business world. It provides real-time updates 
of new product launches, management changes, emerging technologies, etc. in 
any specified context. 
ClearEvents provides web-based monitoring and real time notification of key 
business events including mergers, acquisitions, recommendations, joint 
ventures, etc. To combat information overload it generates one-line summaries of 
recent news. 
ClearAlerts provides highly specific information-notification, personalised to 
each user's needs by industry, event type, geography, content source, ticker or 
specific company. After extracting only the information requested by the user, 
delivers customised snapshots of information to a customer's wireless device or 
ClearCharts matches and marries quantitative and qualitative data. It provides an 
interactive graphic representation of trends in real-time that presents the context 
behind price charts and shows market reaction to specific events. 
2.7.5.4 Comments 
Clear Forest suite's dependency on language and application's 
domain is high. 
This enables the software to provide specific and accurate 
information. 
ClcarForest. com 
2.7.6 Oracle Text (interMedia Text) 
Chapter 2 State of the .4 rt 
Oracle interMedia Text provides a different approach to text analysis. It extends 
the standard relational database to include operators for textual processing. The 
key operations of Oracle's tool are Loading, Indexing and Searching 
2.7.6.1 Technology description 
Oracle Text supports six different methods of storing documents. Direct storage 
loads the contents of a document into a single row of the database table. The 
Master-detail storage method allows documents to be broken down into multiple 
details rows with document-level detail kept in a master table. Nested table 
storage makes use of Oracle's nested table data structure. When this method is 
used, only the file name is stored in the database. The same happens with URL- 
based storage. The URL is stored in the database and the content remains on the 
Internet or Intranet. Finally the sixth method allows users to specify a storage 
procedure that creates text at index time. 
The tool offers several different characteristics for loading the documents in the 
database. The options are SQL Insert, SQL*Loader, and PL/SQL package for 
loading LOBs. The first statement is used only for short text. The SQL*Loader is 
used for large quantities of textual data and works better when along with the 
documents we keep metadata that refer to the documents. SQL*Loader uses a 
configuration file (control file) to specify how the documents should be loaded. 
The most interesting parameters in the control file are the "file_name" field 
which will take the pathname of the file to load, and the "content" which will get 
the text in the document specified by "file_name". Actually the target of the 
SQL*Loader program is to link the file-based area with the database repository. 
The last option, (PL/SQL package for loading LOBs) is used when more 
programmatic control is required. 
Oracle Text offers a number of database objects to index documents. These 
include Filter, Lexer, Wordlist, Section, and finally thematic indexes. Filter 
objects control how word-processing and other formatted 
documents are handled 
by Oracle Text. Lexer objects control how words and sentences are separated. 
This object is used to define properties for language-specific options. 
Wordlist 
Chapter 2 State of the _4rt 
objects are used to specify how stemming and fuzzy matches are performed when 
searching text. Section group provides a mechanism for limiting the range of 
search. For Thematic indexing the tool provides taxonomy of concepts, called 
knowledge base. Thematic indexing is done along with keyword indexing. The 
knowledge base provides a semantic representation for a broad range of terms 
(includes six broad areas), using standard thesaurus relations, including broader 
terms, narrower terms, and related terms. But domain-specific applications may 
require domain-specific thesaurus. A thesaurus consists of a defined vocabulary 
made up of terms, known as indexing terms. Additionally there is support for 
synonym. 
Searching for documents with the Oracle Text tool does not require a separate 
search engine since document indexing is integrated directly into the relational 
database engine. CONTAINS and SCORE are the main functions that are used to 
built queries. Additionally there are a number of text-oriented operators to 
support complex searching. 
2.7.6.2 Basic operations 
Pre-processing/ Filtering. (Basic Linguistic processing) 
1. Reduce terms to their base form. (Stemming) 
Synonyms. 
3. Thesaurus. 
4. Building a feature index. 
5. Multilingual support. 
User functionality 
1. Retrieval of documents. 
Loading documents. 
3. Indexing documents 
4. Searching documents. 
2.7.6.3 Applications 
With Oracle Text we can build two types of applications. A text query 
application and a document classification application 
(document routing or 
Chapter 2 State of the _4rt 
filtering). The purpose of the former is to enable users to find text that contains 
one or more search terms. The text is usually a collection of documents. An 
application can index and search common document formats such as HTN1L, 
XML, plain text, or Microsoft word. The latter application classifies an incoming 
stream of documents based on its content. It makes use of the CTXRULE index 
type, which indexes the rules (queries) that define each class. When documents 
arrive, the MATCHES operator, match each document with the rules that select 
2.7.6.4 Comments 
Working with Oracle Text is fundamentally different from working with the 
other Text Analysis tools since the text processing operations are integrated into 
the relational database. This has as a result that large bodies of text are treated, in 
the same way as another data type. One obvious advantage is that relational data, 
such as metadata can be stored along with the contents of the documents. 
The domains suitable for analysis are constrained by the limits of the knowledge 
base. 
2.7.7 Autonomy 
Autonomy's set of products manages unstructured digital information, including 
word processing and HTML-based files, email messages and electronic 
newsfeeds. By applying concept matching techniques supports information 
retrieval and dynamic personalisation of digital content. 
2.7.7.1 Technology description 
Autonomy's software is based on neural networks and pattern matching 
techniques with conceptual analysis and concept extraction 
in order to automate 
the categorisation and cross-referencing of information, support 
information 
retrieval and enable the dynamic personalisation of 
digital content. The approach 
adopted is called Adaptive Probabilistic 
Concept Modelling (APCM) and is used 
to create Concept Agents. These are pieces of software capable of understanding 
Chapter 2 State of the Art 
the main point of a piece of text and then finding similar documents by analysing 
the patterns of symbols and context. 
The involved technology, enable the software to identify patterns in text and look 
for similar patterns in other sources, automatically. Utilizing Bayesian Inference 
and Claude Shannon's principles of information theory the software can analyse a 
piece of text and identify the key concepts within the document because it 
understands how the frequency and relationships of terms correlate With 
meaning. Bayesian analysis is centered on calculating the probabilistic 
relationships between multiple variables and determining the extent to which one 
variable impacts another. Claude Shannon discovered that "information" could 
be treated as a quantifiable value in communications. Autonomy's approach 
relies on Shannon's theory that the less frequently a unit of communication (for 
example, a word or phrase) occurs, the more information it conveys. It is this 
theory which enables Autonomy software to determine the most important 
concepts within a document. 
At the heart of Autonomy is the Dynamic Reasoning Engine (DRE). The DRE is 
based on advanced pattern-matching technology that exploits high performance 
neural network techniques and provides the foundation for all Autonomy 
products. DRE performs the following main functions: 
1. Concept matching: The engine accepts a source of text as input and returns 
references to documents in another source of text with the highest degree of 
relevance. 
2. Agent creation: The engine accepts text (a training phrase, document or set of 
documents) and returns an encoded representation of the most important ideas in 
the source, including each concept's specific underlying patterns of terms. 
3. Agent retraining: The engine accepts an agent and a set of text and adapts the 
agent using the text. 
4. Agent Matching: The engine accepts an agent and returns similar agents 
ranked by conceptual similarity. This is used to discover users with similar 
interests, or find experts in a field. 
Chapter 2 State of the Art 
5. Agent Alerting: Accepts a piece of content and returns similar agents ranked 
by conceptual similarity. This may be used to discover users who are interested in 
the content, or find experts in a field. 
6. Categorisation: The engine accepts a piece of content and returns categories 
ranked by conceptual similarity. This is used to discover which categories the 
content is most appropriate for, allowing subsequent tagging, routing or filing. 
7. Summarisation: Accepts a piece of content and returns a summary of the 
information containing the most important concepts on the content. In addition, 
summaries can be generated that relate to the context of the original query; where 
the summary varies depending on why the document was retrieved. 
8. Clustering: It takes large sets of documents or profile information and 
automatically calculates the main set of information clusters inherent within the 
data infrastructure. 
9. Taxonomy generation: Automatically understand and create deep hierarchical 
contextual taxonomies of information. Clustering or any other conceptual 
operation can be used as a `seed' for the process. The resulting taxonomy can be 
used to provide insight into specific areas of the information or as a training 
material for the Automatic Categoriser, which then allows information to be 
placed into a formally dictated and controlled category hierarchy. 
10. Standard text search (Retrieval): The engine accepts a Boolean term or 
natural language query and returns a list of documents containing the terms 
ordered be relevance to the query. 
Autonomy's technology is language independent. It does not rely on any intimate 
knowledge of English grammatical structure or that of any particular language. It 
treats words as abstract symbols of meaning, deriving its understanding through 
the context of their occurrence rather than a rigid definition of grammar. 
2.7.7.2 Basic operations 
User functionality 
1. Categorisation. 
2. Clustering. 
3. Summarisation. 
4. Taxonomy generation. 
Chapter 2 State of the Art 
5. Cross-referencing. 
6. Dynamic personalisation. 
2.7.8 SemioMap 
SemioMap is a text mining product that builds visualise concept maps of large 
unstructured text collections using semiotic analysis to identify the relation 
between concepts in different documents. 
2.7.8.1 Technology description 
SemioMap uses SEMIOLEXTM technology and applies computational semiotics, 
the formal study of signs carried by patterned communications. The fundamental 
technologies combine lexical processing, information clustering, and 
visualisation. 
The software extracts automatically all relevant phrases from the text collection. 
It builds a lexical network of co-occurrences, clustering related phrases and 
enhancing the most salient features of these groupings. User you can modify the 
lexicon it builds or create its own. SemioMap uses a technique called "data 
visualization" to display the key concepts and relationships graphically. The 
three-dimensional concept map lets the user move through related concepts, drill 
down to different levels of detail, or jump to the documents referenced. 
The three main steps are: 
" Text indexing and extraction. Automatically extracts all relevant phrases 
from a large document collection. 
" Concept clustering: Once the phrases are extracted the software then 
identifies the relationships between these phrases, and builds a "lexical 
network" grouping related phrases and enhancing the most important 
features of these groupings. 
0 Graphical display and navigation. In the final step the tool present the 
relationships between concepts graphically. This graphical map allows 
users to navigate through the key phrases and relationships within a 
corpus of text, and drill down to specific 
documents when desired. 
Chapter 2 State of the . 
There are two main components. The first is the SemioMap server. which 
processes large collections of unstructured texts and builds an index of key 
phrases. This list of phrases (known as lexicon) is built automatically without the 
need to hand-build topic trees or indexes. The second component is the 
SemioMap client. Relationships within text are delivered to the user in the form 
of a navigable Java map. The client maps allow user to navigate through 
information. 
2.7.8.2 Basic operations 
Pre-processing/ Filtering. (Basic Linguistic processing) 
1. Lexical processing. 
2. Lexical network. 
User functionality 
1. Visualisation. 
2. Text-base navigation. 
2.7.8.3 Applications 
SemioMap empower and support the knowledge worker. It displays the 
relationships of concepts in text collections; it is up to the knowledge worker to 
provide the meaning and relevance to that information. The tool facilitates the 
transfer of information into knowledge. 
2.7.9 dtSearch 
dtSearch line of products targets on text search and retrieval. It is used for 
searching internal documents collections or for the Web. Its main characteristic 
the different text search options. 
2.7.9.1 Technology description 
dtSearch product line includes built-in file parsers that can 
index, search and 
display with highlighted hits a wide range of file types. 
Supported file types 
include word processor, spreadsheet, database. RTF, 
PowerPoint, email message 
stores, ZIP, PDF, HTMML and 
IHML. 
Chapter 2 State of the Art 
After indexing which is the first step dtSearch offers a wide range of text search 
options. In addition to basic search options such as Boolean (and/ornot), 
proximity, wildcard, segment, numeric range, and phonic, dtSearch has the 
following special search capabilities: 
" Concept/synonym/thesaurus searching. The tool performs automatic 
query expansion using a semantic network of the English language with 
variable levels of expansion (user-defined synonyms, built-in synonyms, 
or built-in synonyms + related words). Concept searching also known as 
synonym or thesaurus searching expands a single search request into 
multiple conceptual dimensions. For example, when searching for 
"incendiary" the query is automatically expanded to include synonyms 
such as "arsonist" and "inflammatory". The combined built-in thesaurus 
and the user-defined thesaurus allow automatic synonym expansion 
covering a wide range of search terms. Additional expansion. for 
example, to include with "incendiary" the words "inflammatories" and 
"inflammatory" can be obtained by stemming. Stemming uses a built-in 
algorithm that is familiar with the native language, to expand a search 
request to include word derivatives automatically. 
" Relevancy-ranked natural language searching. Natural language searches, 
also known as query-by-example, look for all words in a search request 
and return results based on automatic term weighting. Using the "Vector 
space" method, relevance ranking takes into account the frequency of hits, 
relative frequency of the search terms in the index, and hit density in 
retrieved documents. 
" Variable term weighting. Additional to automatic relevancy ranking in 
natural language search request, dtSearch provides the ability to specify 
relative weights. These weights can be positive or negative. For example, 
a user might assign a positive weight of 3 to the word green and a 
negative weight of five to the word orange. 
0 Fuzzy searching. Fuzzy searching uses an algorithm to 
find search terms 
even if they are misspelled. Search fuzziness adjusts 
from 0 to 10 to 
correspond to the level of typographical or 
OCR errors in files. For 
Chapter 2 State of the Art 
example, with a fuzziness level of 1, a search for "alphabet" would find 
``alphaget". 
2.7.9.2 Basic operations 
Pre-processing/ Filtering. (Basic Linguistic processing) 
1. Stemming 
2. Synonyms/Thesaurus. 
3. Index. 
User functionality 
1. Concept/Synonym/thesaurus searching. 
2. Relevancy-ranked natural language searching. 
3. Variable term weighting. 
4. Fuzzy searching. 
5. Basic search options (Boolean, proximity, wildcard etc. ). 
2.7.9.3 Applications 
Companies and individuals that need to search large internal document 
collections are using dtSearch Desktop and dtSearch Network (or LAN). 
dtScarch Publish allows CD and DVD producers to use the dtSearch front-end 
for document access, search and display. Customers looking to provide very fast 
searching over a large collection of documents on an Internet or Intranet site are 
using dtSearch Web. FindPlus provides distributed searching. In other words it 
allows a single search to span everything from a local drive to remote 
organisation servers. The last component dtSearch Spider enhances the product 
line's Internet/Intranet reach by indexing and searching other organisation's Web 
sites. For software companies seeking to embed dtSearch indexing and searching 
algorithms dtScarch offers the dtSearch Text Retrieval Engine. 
2.7.10 Thunderstone's Texis 
TEXIS is a fully integrated SQL RDBMS that queries and manages databases 
containing natural language text, standard data types, geographic information, 
images, video, audio, and other payload data. 
Chapter 2 State of the Art 
2.7.10.1 Technology description 
Texis is a relational server that specialises in managing textual information. It can 
be stored text of any size, and it is possible to query that information in natural 
language. Texis's text search operators have an English language vocabulary of 
250,000 word and phrase concept associations for natural language queries. It 
also provides proximity control, fuzzy searches, true regular expressions 
matching, and written numerical value searches. 
Unique features to Texis include: 
" Zero latency insert. When a new record is added or updated in a table it is 
available for retrieval immediately without needed indexing. 
" Variable sized records. Most databases allocate disc space in fixed size 
blocks that each contains a fixed number of records. In contrast Texis 
stores only what the user use and not what he might use. This has as 
consequence to remove the limitation of having to specify the maximum 
length of a variable length field. In Texis any variable field can contain up 
to a Gigabyte. 
" Indirect fields. These are byte fields that exist as real files within the file 
system. This field type is usually used when you are creating a database 
that is managing a collection of files on the server. They can also be used 
when the one Gig limitation of fields is too small. Additionally they may 
be used to point to files anywhere on the file system. 
" Variable length index keys. Traditional database systems allocate their 
indexes in fixed blocks. English language contains words of extremely 
variant length and traditional Btrees have fixed length keys. This cause a 
problem that Texis is dealt with the variable length key Btree in order to 
minimize the overhead while not limiting the maximum length of a key. 
2.7.10.2 Applications 
Texis is used in many diverse Internet, real-time, and integrated applications such 
as, message profiling & handling, image library management, 
help-desk support. 
online news retrieval, business intelligence, research 
libraries, litigation support. 
and Internet retail operations. An example of the type of applications that can 
Chapter 2 State of the Art 
built around Thunderstone's Texis RDBMS is the Webinator, a Web index and 
retrieval package. It allows the website administrator to create and provide a high 
quality interface to collections of web documents no matter where they reside. 
2.7.11 InFact 
Insightful is merging the mining and search of structured and unstructured data, 
creating a search and categorisation system. InFact analyses and searches text, 
tables, charts, and images. Main characteristic is the retrieval of facts. It applies 
text mining to extract equivalent meaning from unstructured information. 
2.7.11.1 Technology description 
InFact consists of three main systems: an indexing system, a database, and a 
search system. 
The indexing system reads documents and normalises the information into useful 
abstractions. The process takes place off line. The presence of graphs, tables is 
recognised. Communicate with remote Optical Character Recognition (OCR) 
Servers to determine text content visible within images. Information extracted at 
read time is stored in a database for use in real time by the search system. The 
indexing system is scalable and operates at three levels: morphological, semantic 
and syntactic. It can integrate information from a variety of different sources, 
including plain text, HTML, XML, SGML, pdf, MS Word, MS PowerPoint, and 
MS Outlook. 
The search system utilises data generated by an indexing system. It performs 
searches using a LRU (Least Recently Used) cache. This 
is possible because 
InFact reduces the many different forms that a user might ask a question 
into one 
canonical representation. 
Morphological module: InFact normalises keywords by removing 
inflections. 
derivations, word compounding, etc. and maps them to the common root. 
employs a conflation technique 
based on the n-grams, and determines word roots 
based on the number of shared unique n-grams. 
For example, the words "hunter", 
Chapter 2 State of the . 
"hunters" and "hunting" share three unique diagrams ('`hu". "un", "nt") and two 
unique trigrams ("hun", "unt"). Therefore these words can all be mapped to the 
same concept root "hunt". Additionally InFact employs hierarchical 
agglomerative clustering techniques to produce a morphological map of word 
roots for each language. Depending upon the language, it clusters terms using a 
single or complete link clustering method. Every language has a unique 
morphological signature. The morphological model is language independent, and 
robust to spelling and OCR errors. 
Semantic module: It uses a process of inductive reasoning to learn word 
meanings from the context in which they occur. Semantic normalisation involves 
recognition of keyword relationships such as synonyms, antonyms and 
metonyms. The process is based on an InFact's patent algorithm, LSR (Latent 
Semantic Regression). LSR is an unsupervised dimensionality reduction and 
classification technique based on a multivariate analysis of text content. We can 
understand it with the following example. We create a word index from a set of 
articles on England. From 15 articles we produce a "term-document matrix"; that 
is how many times a word occurred in each article excluding words such as 
"the", "he" etc. The results may look like the following table. 
Words - Documents 1 2 3 ... 
Say 1 1 2 2 3 
England 2 3 2 2 3 
Shakespeare 0 3 1 0 0 
Tempest 2 1 1 0 1 
Table I Words - Documents 
The numbered columns in the matrix are the articles. InFact applies a number of 
statistical entropy measures to emphasize the most diagnostic terms, or 
emphasise words that have low diagnostic value, such as "say" or "England" that 
occur across the entire collection. By following each row we can 
discover some 
interesting word associations. For example: a) all documents that contain the 
word "Shakespeare" also contain "tempest" etc. 
By keyword searching for the 
Chapter 2 State of the Art 
word "Shakespeare" most search engines would look up the row for 
"Shakespeare" and return only articles 2 and 3. Because InFact establishes a 
weighted semantic link between "Shakespeare" and "Tempest", it %vill 
additionally return article 1 and 15. The additional documents are linked to 
"Shakespeare" through other semantically shared terms. The process of linking 
documents by topic is optimised by computing semantic axes that reduce the 
dimensionality of the index, based on a patent algorithm for fast matrix 
decomposition and clustering. The semantic network that is creating by the 
algorithm supports the search procedure. 
Syntactic Module: Facts are expressed by the way in which words are put 
together to form phrases and sentences. InFact employs patented transformational 
rules that recognise the semantic equivalence of multiple sentence structures. It 
uses rule-based and stochastic parsers to recognise not just noun phrases but also 
grammatical structures and part of speech attributes, therefore can determine the 
governing verb, subject, object and their modifiers for any sentence. 
Categorisation: InFact automates processes; from filling, filtering, routing and 
organising emails, natural language texts or multimedia documents, to searching 
the web for news and articles. 
2.7.11.2 Basic operations 
Pre-processing/ Filtering. (Basic Linguistic processing) 
1. Morphological analysis 
2. Semantic analysis 
3. Syntactic 
User functionality 
1. Search 
2. Categorisation 
2.7.11.3 Applications 
The InFact technology provides text mining, meaning information extraction and 
analysis; Information Retrieval, which 
delivers answers to questions rather than a 
Chapter 2 Stare of the Art 
list of possible document sources; continual ingestion of incoming documents; 
and categorisation of information by themes or topics. 
" Customer self-support via the Web. 
" Help-desk answers system. 
" For Corporate knowledge Management. (Publishers, eCommerce etc. ) 
http: //www. insiL)-htful. com/products/infact/ 
2.7.12 LexiQuest 
LexiQuest products combine natural language technology with data mining in 
order to facilitate main text mining operations such as categorisation, search and 
clustering of concepts. 
2.7.12.1 Technology description 
LexiQuest includes linguistic analysis into all their products as a method of 
giving some structure into free text. First they use dictionaries to perform 
morphology of the word. Then they look at syntax and determine part of speech 
the word is: noun, verb, modifier, etc. Next semantics takes into account the 
meaning of the word in the broader context. 
LexiQuest Categorize extracts textual information from the documents and by 
utilising it assigns documents into categories. It matches the entities extracted 
from the documents against descriptions of categories organised according to a 
predefined list or tree of categories which is called taxonomy. The initial category 
descriptions are obtained using sets of learning documents which are used to train 
the taxonomy. A series of terms are found and weights assigned based on the 
uniqueness of the term. For example a common word like "view" would have 
low weight as it can appear in documents assigned to many categories whereas a 
phrase like "fuselage" would receive higher weighting due to its more exclusive 
use in the aerospace category. LexiQuest Categorize consists of the following 
components: 
0 Categorizer Engine is the main component of the categorisation system. 
The categorizer provides the input-output interface and handles all dialogs 
with the internal components. 
Chapter 2 State of the Art 
" Extractor module. Examines the morphology of words in a document and 
extracting words from the document so that these words may be 
subsequently transformed and used to match suitable categories for the 
document. The extraction process for documents is mostly a process of 
identifying the right subset of words relevant to the chosen subject. 
" Taxonomy Manager is a graphical user interface that will permit the 
creation and maintenance of the category set. Training with sample 
documents, and customise the linguistic resources to adapt the 
terminology extractor. 
LexiQuest Guide is providing search operations over a document collection. 
Users can type a full question into the dialogue box using natural language. The 
system will prompt for a misspelling and offer alternatives. Term expansions take 
place by a semantic dictionary lookup. There are two types of expansions. The 
first Morphological Expansion will expand to the other possible forms of the 
word. For example "policy" will expand to "policies". The second form of search 
expansion is Semantic Expansion, which will expand the word to related 
meanings, similar to a Thesaurus approach. For example, "policy" will expand to 
"rules", "procedures", "guidelines" etc. An additional feature is that user may 
select a meaning based on the word's domain when there are not enough words 
in the query to avoid ambiguity. For example a search against "mouse" without 
the additional context will keep the two possible meanings: rodent and computer 
device. Specifying a domain "life science" or "manufacturing" industry can then 
handle this query accordingly. 
The dictionary model establishes a clear distinction between the linguistic level 
(how words can occur in a sentence) and the conceptual 
level (how concepts 
relate to words and are organised in a semantic network). 
LexiQuest comes with 
a standard dictionary, which includes approximately 
100,000 terms (including 
single and compound words) and is available 
in English, French, German, Dutch 
and Spanish, as well as vertical and 
functional-specific areas such as medical, 
legal, and human resources. In addition, to the 
base dictionary, a vertical (e. g. 
Chapter 2 State of the _4 rt 
healthcare) or functionally-specific (e. g. human resources) dictionary can be 
added if required by a specific application. 
The third product is the LexiQuest Mine and the target of this product is to 
graphically present the main concepts found in the document. Mine can work 
with input formats like HTML, XML, MS Office, PDF, and email and is 
available in English, French and German. LexiQuest Mine works by employing a 
combination of dictionary-based linguistics analysis and statistical proximity 
matching to identify key concepts, including multi-word concepts. Then, based 
on a linguistic analysis of the context and semantic nature of the words, it 
identifies their type (organisation, product, etc. ) as well as the degree of 
relationship between them and other concepts. These relationships are displayed 
in a dynamically produced graphical map, which can be used to develop a query 
based on the connections shown. LexiQuest Mine is composed of the following: 
9 Database Manager. Available from any web browser, the database 
manager provides the functions for managing LexiQuest Mine's 
databases. Additionally allows remote administration of the LexiQuest 
Mine environment. 
" LexiQuest Mine. Available from any Java-compliant Web browser, the 
application is accessible from the user's desktop and provides the concept 
extraction, graphical interface, trending analysis and access to 
customisable administrator features. 
0 Database Server. Server application that performs statistic processing for 
dynamic clustering. Essentially serving as the engine which drives the 
identification of relationships between concepts. 
" LexiQuest Base for Text Mining. Utilising the core LexiQuest Language 
Recognition technology. Base supplies the lexical information for the 
linguistic analysis of documents in French, English, German and Spanish. 
" Search Engine. Allow documents to be retrieved 
from the LexiQuest 
Mine maps and attached terms. 
2.7.12.2 Basic operations 
Pre-processing/ Filtering. (Basic Linguistic processing) 
Chapter 2 
I. Morphological analysis 
2. Syntactic 
3. Semantic analysis 
User functionality 
1. Categorisation 
2. Search 
3. Clustering 
2.7.12.3 Applications 
State qf the .4 rt 
Mine is for applications that support the knowledge worker who needs to review 
large volumes of documents to identify key elements for further exploration. 
Additionally competitive and market intelligence, general scientific and medical 
research and finally investment research are areas that can benefit from it. 
Categoriser on the other hand is useful for keeping taxonomies and search 
systems up to date. 
2.7.12.4 Comments 
LexiQuest is one of the interesting tools cause is of the few that combine 
linguistic techniques with statistical and mining processes. 
Another interesting feature is the ability to manage metadata database. This in 
general can facilitate many data mining tasks. For example it is important that 
LcxiQuest Mine can be accessed directly from Clementine's (SPSS's data mining 
tool) user interface. 
LexiQuest Categorize. Main idea behind is the combination of linguistic-powered 
term extraction and comparative weighting. 
http: %%NN, \-N, N-. spss. com/spssbi, 7cxiqucst/ 
2.8 Conclusion 
Chapter 2 State of the Art 
In this section we will discuss our survey findings, limitations of current 
technologies and focus on two main issues, the methodologies adopted and 
finally the frameworks and repositories architectures 
2.8.1 Text Mining Methodologies 
The objective of Text Mining is to exploit information contained in textual 
documents in various ways, including the type of analyses that are typically 
performed in Data Mining: discovery of patterns and trends in data, associations 
among entities, predictive rules, etc. [45]. Most approaches to text mining 
perform discovery operations either on external tags associated with each 
document, or on a set of words (single and multi terms) within each document. 
Some others apply discovery operations on phrases and facts, as extracted from 
the documents (JE). [47] 
Text Mining can be performed by a collection of methods from various 
technological areas, not just a single one. However, all of these methods can be 
roughly grouped under two main headings. The two broad groups of approaches 
for the development of systems that aim to extract information and knowledge 
from text are performance-based and knowledge-based [28] (linguistic-powered 
[43]). In the former case designers are concerned with the effective behaviour of 
the system and not necessarily with the means used to obtain that behaviour. The 
most common performance-based algorithms are statistical methods and neural 
networks. 
Statistical Methods: These approaches usually rely on an explicit underlying 
probability model. They are not context sensitive. 
Neural Networks: Neural Networks are a class of systems modelled after the 
human brain. As the human brain consists of millions of neurons that are 
interconnected by synapses, neural networks are formed from large numbers of 
simulated neurons, connected to each other in a manner similar to 
brain neurons. 
Like in the human brain, the strength of neuron interconnections may change (or 
he changed by the learning algorithm) in response to a presented stimulus or an 
Chapter 2 State of the Art 
obtained output, which enables the network to learn [8]. Neural networks are 
highly suited to textual input, being capable of identifying structure of high 
dimensions within a body of natural language text. Neural networks work best 
with data that contain noise, has a poorly understood structure and changing 
characteristics (all are present in textual data). [46] 
Knowledge-based or linguistic-powered systems on the other hand use explicit 
representations of knowledge, such as the meaning of words, relationships 
between facts, and rules for drawing conclusions in particular domains. Common 
knowledge representation schemes include inference rules, logical propositions 
and semantic networks such as taxonomies and ontologies. The ability to 
understand human language is provided by linguistics, commonly referred as 
Natural Language Processing (NLP). Morphology, syntactic and semantic 
analyses are some of the techniques being used. [43] 
Oracle Text is knowledge-based tool. The thesaurus provided in the tool drives 
thematic classification of documents. With a predefined set of terms and well 
defined relationships between terms, such as "stock is a narrower term for 
financial investment"; there is no need for the statistical analysis of a corpus (as 
with TextAnalyst and Intelligent Miner for Text etc. ). With a taxonomy 
dependent analysis tool we have to create a domain specific taxonomy to get the 
level of document discrimination needed for the application, which is a difficult 
process. Taxonomies however are not the only kind of knowledge representation. 
Pattern matching rules are also used extensively. Heuristics are often represented 
as pattern matching rules. For example, "If the first two words of the sentence are 
in conclusion then add the sentence to the summary". These types of rules can 
represented using IF-THEN conditions and regular expressions pattern matching. 
Like taxonomies, which need to be domain specific, text analysis heuristic tends 
to be language specific. Adopting a text mining tool to additional 
languages, 
then, requires the creation of another language specific rule 
base. 
Most of the tools combine performance-based with 
knowledge-based techniques 
in order to balance the flexibility and adaptability of statistical 
techniques with 
Chapter 2 State of the Art 
the domain and language specific knowledge provided by thesauri and heuristic. 
Choosing between statistical oriented and knowledge oriented tools depends on 
the domain. For rapidly changing areas such as genetics research would be 
facilitated by a performance based tool. On the other hand, for relatively stable 
areas such as finance and politics a knowledge-based tool can operate better. The 
ideal though would be to work with taxonomies that can automatically adjust to 
accommodate new terms, issue that none of the current technologies support. 
2.8.2 Framework architecture 
In this subsection we will compare how the text mining tools are integrated with 
each other and how well they work with third-party tools. From our survey we 
have identified three kinds of architectures: 
9 Component-oriented architecture. For example ActiveX components. 
0 File-oriented architecture. Suite of independent applications. Input is 
provided to each tool as a stream of files or as a file containing fully 
qualified pathnames. The result of the analysis tools is output as streams 
of semi-structured text that can be easily manipulated. 
0 Database-oriented architecture. Processing operations are integrated into 
the relational database. 
At one end of the component range we have TextAnalyst (Megaputer) with 
ActiveX suite for dealing with text and semantic analysis. At the other end reside 
Oracle Text and Thunderstone's Texis, which are fully embedded 
into a 
relational database engine. In the middle and closer to Oracle 
Text are InFact and 
LexiQuest. In the middle, and closer to TextAnalyst, is Intelligent Miner 
for Text, 
ClearForest suite, etc. (Figure 10) 
Tools with component-oriented architecture like 
TextAnalyst are appropriate for 
applications requiring embedded text analysis. 
For example in TextAnalyst the 
simplicity with which the ActiveX components can 
be integrated using common 
programming tools, such as 
Visual Basic, C++, and Python make this an 
appropriate choice for client and server 
based applications. 
Chapter- 2 State of the Art 
Intelligent Miner for Text provides a suite of independent applications. The key 
to integration is a standard output format that is compatible with all the programs 
in the suite. By this approach is easy for example to use the feature extraction 
tool with the clustering tool. Additionally with this semi-structured output format 
is easy the 
t. Intelligent 
miner for text 
2. ClearForest 
3. Autonomy 
I. Oracle 1. InFact 4. WizDoc 
I. Text Analyst 
Text 2. LexiQuest 5. TextSmart 
2. Texis 6. SarnoMap 
7. dtSearch 
Integrated Reusable 
Framework components 
Figure 10 Framework Architectures 
integration with third-party tools, and custom applications. Finally the results can 
easily be stored in relation database and processed by other applications. 
When using Oracle Text or Texis it is already assumed that we are working With 
the relational database. Although we can store the documents outside the 
database (by keeping only the pathname or URL in the database), all processing 
is controlled from within the database. 
When we have to analyse small amount of text then the tools with component- 
oriented architecture are more appropriate. For large-scale document collections, 
the benefits of using tools with database-oriented architecture are obvious. The 
documents and the related metadata can be stored and queried together. Secondly 
text indexing is integrated into the database, so the query optimiser can use 
statistics about both structured and long objects columns when 
formulating a 
query execution plan. Thirdly. SQL has been extended to support a range of text- 
specific selection criteria such as proximity searches, multilingual searches, 
thematic searches, and word variation searches. 
JC" 92 
Chapter 2 State of the .4 rt 
The key criteria from the integration perspective are to determine how much of 
the control and the results produced from text analysis need to be integrated with 
other applications. 
Oracle Text and Texis from Thunderstone provide the most extensive support for 
repository operations, including a wide-ranging query capability. InFact and 
LexiQuest stores all the information extracted in a database for use by the other 
subsystems. This allows a number of mining operations to be performed and 
great flexibility. 
Intelligent miner for Text does not include a relational database engine but the 
output from the various text analysis tools is semi-structured that makes it easy to 
load into relational tables. 
TextAnalyst is not geared for tight integration with databases, but it does provide 
its own storage mechanism. The ActiveX architecture, however, provides 
flexibility for programmatic control so that the lack of direct support for 
relational database does not prevent using the tool with a database. 
2.8.3 Critical Discussion 
Currently available tools adopt a single technique or a limited set of techniques to 
carry out text analysis. From our research we have concluded that there is no best 
technique, therefore the issue is not which technique is better than the other, but 
which technique is suitable for the problem to be solved. A proposed framework 
though will have to provide a wide range of different techniques. Users should be 
able to choose and easily apply the techniques that are suitable for their analysis 
scenario. 
There is a strong commercial desire to utilise data mining techniques on the 
masses of textual data. [44] A problem with many knowledge workers is that 
may not know exactly what they are looking for, from the textual database; and 
mining is incredibly effective at retrieving interesting facts from a database. A 
closer and easier tic of data mining techniques with linguistic and information 
Chapter 2 State of the Art 
extraction techniques are what need to be done. Currently tools that work closer 
with the database (InFact, LexiQuest etc. ) will have the advantage, as they 
facilitate efficient applications of various data mining techniques. 
For a demanding, domain specific knowledge discovery task, an additional 
linguistic processing requirement appears; and is necessary to perform semantic 
analysis to derive a sufficiently rich representation of relationships between fact 
and concepts described within the documents. [28] Semantic analysis is 
computationally expensive and according to our survey few tools currently utilise 
it. It is a challenge for text mining tools to include an efficient and scalable 
semantic analysis. 
An issue that may restrict the use of some of the tools we have included in our 
survey is multilingual text mining. Tools that do not "understand" the text (non 
linguistic tools) will not be able to multilingual access document collections. 
This is an issue to be resolved as the information that the user is looking may rely 
on document in various languages. 
Domain knowledge and the way that can support the linguistic pre-processing 
steps plus the discovery tasks is an interesting topic to explore. Most of the 
current tools make use of taxonomies that mainly are building manually or semi- 
automatically. Richer taxonomies such as Ontologies to be utilised and the 
automatic construction seems to be the future direction. 
Another identified drawback of current tools is the inability to capture the 
temporal issues. The majority of the tools do not address temporal aspects at all, 
while the rest do so very poorly. Tools are expected to deal with issues such as 
information temporal validity and trends. 
' C/) V] C/] V) C/) C/] C/) 
v U U U r U U . U 
 v v v v v v v v v v 
G +" c ' i 
_ v v 
7 G v - 
Y cC -r v ca 
L y v, - G cC ^ U  
y -c C3 C) -  f1 .v 'CJ 
C v rz 
9- v L 0 RS 
O u Z C O"".  r- 
u u t,  C 
cC  = 
p O cJ a .. 
L a cC U O 
 O cn O  c r, c'"' 
- l- z 
- ' ) ' d LA Z- - Z: 
C A b v ^0 
O Z ri 
j, 4? G c r- v 
 , O A >  CZ 
p v c co 4 
M v cC x 
Z .D G 
Y .G 'Y  cC Y 
p u v: 
Cd M Y . 
{", '., U "U 
L G'C C3 
U . '. 
C d  Y - p 
 uL d aA U 
10 CZ 
cd v O . a cC 
Ca p . v 
u " u U 
'G L1 u4 
O - J oA 
4i C 
k C X bA O = cC 
a SC cd  cC c, cd 
b `r 'C3 C u '- cz W. 
u oA G q o" fj 
C' dU c - 3 A U y 
L .c c 
u   
iC HO 
u C " o 
CD r_ , 
s v Z w L 
v CZ  )  b , ` 
C >, - 
cu ", o a O - 
4- .r u L x cd r- m; 
9-1 c4 C E Z cz . s. - c. ) L . - 
o. E C) - a) v _ L u C v' 
tu O Y u 
O C u '' n n v i v- cn a ,Y J L 
m  cC 
' CYC 
U U Q .. ' C . / . il 
OA TS L 
cz Q) cn G C C j 
c> cz 
  O Y LO pfd O 
p v E - 
rr. p y _E dA En 
C, ' Cl) u c3 u 
L U L Y CC U (D 
= U VlJ 
u u Q ) O 
' n " 
r- CZ. C 
. CIS V " 
I-- E 
u 73 r_ 
(O "Y 
-- cu 
. U - v: 
 t o 
; ., \ p G 
CO CG L 
 C CJ  C Y 
U CD cz 
^. X G 
O`,, 
E u p 
L V--i 
a O k- 
. ... 
= g- ri. 
u c Y 
tCS O p n O u C u V] v p v 
CD , v iG f1 
3 C 3r 
Cj t r u p 
j U r 
Y CI Y 
N r '7 v D [ 
oC Q - C - 
. 5  
o Uy 
'C OO 
a) "C7 4-0 
rod c' 
o ate, 
au F-4 
Ay  
>1111, 
~tio X C 
X X X x X 
V cd v C. )  L G) 
fv ( 
j y iG 
-- N M c [- 00 O\ 
Cri U 
.0 cd 03 
ter., -0 
bJJ T 
`-msz 
- NZ 
--" ., 
4) U) 
X X X X 
 X X X X 
V X X x x X >G X 
z x V X V V X V V 
V X X V X X X 
SO rA 
X X x x X V X X V X I X  
X X V 
E O N "-- 
O  - 
-. N r n C [- x 
GL CZ Z 
cu ID ( .O 
U" 
y, c 
UO' 
'Z tp J CID 
c) cr, 
c - d 
12.00 cn 
-- N r'' 
3. The DocumentMiner Approach 
3.1 Introduction 
This chapter introduces the DocumentMiner's approach. As it was mentioned in 
chapter one, the goal of the DocumentMiner is to support temporal text mining 
and knowledge discovery in text. More specifically, the proposed framework 
supports better pre processing, management, and analysis of textual information. 
which is summarised within the overall discovery process. 
The introduction of DocumentMiner begins with the discussion of related issues 
and the requirements for proposing the framework. The following sections 
present the main elements of the DocumentMiner approach and the role of 
textual repository, information extraction, ontology and data mining 
components. 
3.2 The DocumentMiner support for TM 
This section presents related issues of the DocumentMiner framework and 
environment. The information of a text mining application and especially 
information processed by knowledge workers involved in a knowledge discovery 
task is multifaceted, voluminous, and comes in many different forms in order to 
cover all aspects concerning the problem domain. The DocumentMiner approach 
is mainly concerned with the information that is acquired throughout the TM 
process. 
Chapter 3 The DocumentMiner Approach 
In DocumentMiner we view this information essentially covering the following 
main aspects: 
" Information about the textual sources. 
  Information about the content of the documents: textual metadata 
  Information about the background knowledge mainly in ontologies: the 
business structure, its processes, actors, goals, market sectors, products, etc. 
  Information about the analysis stage 
3.2.1 The Warehouse Role of DocumentMiner 
One of the main priorities of the DocumentMiner approach was to address the 
problem of heterogeneous representations of all the information elements 
produced as a result of the text mining processes. 
DocumentMiner framework is dealing with huge amount of textual information 
and is analogous to data warehousing as a method for dealing with large volumes 
of numeric data. Data warehouses are excellent for working with structured data 
and answering who, what, when, where, and how much questions. They do not do 
so much with textual data and as a consequence with why questions and these are 
DocumentMiner's main target. It can gather and process text from any source, 
internal or external, and this is the key to support strategic management that 
looks beyond the internal operation to the external factors that influence an 
organisation. 
3.2.2 DocumentMiner's main characteristics 
DocumentMiner includes a repository of textual information designed to support 
business intelligence and decision support operations. There are three main 
characteristics (Table 5): 
" No single text structure. Any t\pe of text can be used in the text 
repository. But while we will include a wide range of text types, 
Chapter 3 The Document finer Approach 
(document types), the specific type of processing and information 
extraction techniques applied will depend upon the individual text type. 
o Multi-source document gathering. We can structure documents to support 
retrieval and analysis from multiple perspectives based upon the semantic 
content of the document (internal attributes such as abstract, key features, 
topics), and not only on external attributes (such as directory path, 
creation date, file size). The Internet is the single largest potential source 
of documents and by nature it is considered a distributed source. 
" Document content understanding. Supported by basic NLP processing 
and Information Extraction. Essential features of documents are 
automatically extracted and stored in the text repository. This is the most 
crucial step in the framework. 
o Extracting key features (Information Extraction). These include 
the names of persons, places, and organisations as well as 
relationships between those entities (facts). Key features identify 
what a document is about but can provide more detail than broad 
themes can. For example a document with terms such as nucleic 
acid, protein expression, etc. might be categorised as a genetic 
engineering document. 
DocumentMiner 
1 Multiple document types 
2 Distributed textual sources 
3 Document Content Understanding 
Table 5 DocMin repository main characteristics 
3.2.3 Textual Information included in text repository 
The text repository may include the following textual information: 
1. The actual documents (Complete documents). 
2. Summaries of documents (Automatically generated). 
3. Translations of documents. 
4. Metadata which label documents 
Chapter 3 The DocumentMiner Approach 
a. Author's name, publication dates and subject keywords). 
b. Key features/facts (automatically extracted from the documents). 
These may include proper names (persons, companies etc. ). terms 
(meaningful sequence of words i. e. Department of computation). 
events (facts of interest which are mainly combination of terms) 
and other conceptual structures which are basically the output of 
the information extraction stage. 
5. Clustering information about similar documents. 
6. Thematic or topical indexes. 
The process starts with richly complex units, the documents; and extract 
information by applying basic linguistic processing, Information Extraction and 
Data Mining. Documents' contents are often referred to as free-form text because 
we cannot fit the contents into a fixed relational structure, at least not a useful 
one. We can put the entire document into a binary long object, but this is useful 
only if we are developing document management systems. In order to support 
decisions we need to understand what is inside those documents. 
3.2.4 Content Integration in DocumentMiner 
Dimensional models such as the star schema and its snowflake variations are 
designed around two basic types of entities: fact tables and dimensions. Fact 
tables are used to store information about particular measurements of a business 
process such as collaborations, acquisitions, sales, mergers and other financial 
events of interest. Dimensions are used to organise facts. For example, an 
acquisition is a fact about gaining a particular company by another particular 
company at a specific time, with a specific amount. 
Chapter 3 The DocumentMiner Approach 
Companies Time 
Acquisition Fact 
Amount of 
Money 
Figure 11 Acquisition schema 
For our purposes the dimension tables are the starting points of text repository 
integration. Each dimension can contain a vast amount of descriptive 
information. For example the company dimension could contain description of 
company, company categories, related companies etc. 
3.3 Main Requirements 
One of the most important issues when discussing requirements in text mining is 
to understand user's areas of interest and their functional needs. Depending upon 
the case study the end users could be a homogeneous group of analysts focused 
on single, narrow domains or could be a wide range of managers and analysts 
concerned with broad areas. With an understanding of their functions and 
interest, we can begin to define what documents they need. 
Users may view functionality needed with a variety of perspectives and 
objectives, but they frequently share a need for the same type of documents. For 
example marketing and sales departments of a company may both need 
competitive intelligence information. 
Time influences the business intelligence value of text in various ways. For 
example, market conditions and other external factors can determine the value of 
a document. Additionally the value of information often 
determined when 
someone sees it, as a consequence information should 
be delivered on time. Text 
Chapter 3 The DocumentMiner Approach 
itself contains information that limits its value. For example, a proposal 
document may have an expiration date or a news item may refer to an event 
which is true for a specific period. 
An additional important issue to consider as part of the requirements is the 
location of the source documents. In general documents of interest can be found 
on organisation's file systems, the World Wide Web, including external services. 
such as newsletters, research reports, etc. 
The vast amount of business data found in an organisation, some estimates run as 
high as 80%, are textual such as reports, emails, etc. [19] Internal file systems 
alone are a rich source of textual data. The WWW is the biggest repository of 
textual data. Another important source of textual data are third part providers 
such as news feeds, research reports etc. One of the most time-consuming tasks is 
defining what should be included and what should be excluded from the textual 
repository. 
3.4 DocumentMiner's Architecture 
There are several components associated with main operations of the framework: 
" Sources of text. 
" Background knowledge(mainly ontologies) 
" Pre-processing text components. 
" Text and metadata repositories. 
" Analysis operations components. 
Chapter 3 The DocumenLWiner Approach 
Textual Sources Document 
Collector processing 
Ontologies 
Textual 
Metadata 
Analysis 
operations 
Figure 12 Main architectural components 
In the following section we are going to present the role of each of these. 
3.4.1 Sources of text 
In general there are two types of textual sources we have to consider; internal to 
organisation and external sources including the internet and subscription services. 
Organisation 
1. File servers 
2. Document repositories (i. e. Lotus 
Notes) 
3. Document management systems 
Figure 13 Text Sources 
External sources 
Subscription 
Services 
3.4.1.1 Internal to organisation sources 
Within an organisation text is spread throughout on file servers, in document 
repositories and in document management systems. While these systems satisfy 
much of the basic old needs they do not provide much to the current and future 
more demanding needs for intelligent analysis. 
Although file servers store great volume of business's text they provide very little 
Support for document management. In general file servers provide storage, 
basic 
navigation and search services. These hierarchical storage systems allow quickly 
Chapter 3 The DocumentMiner Approach 
navigate to areas of interest. But these systems assume that we know exactly 
what we are looking for. When we do not know the exact location of a file we 
can use the search facilities and basic search operations are supported in most file 
systems. 
Document repositories, such as Lotus Notes provide more advanced search 
facilities and are easier to navigate. One of the benefits of Lotus Notes over file 
system is that views and folders are customisable by users in order to organise 
documents. Also navigation tools can display descriptions and other metadata 
about documents. Document management systems focus on large volumes of 
similar documents often combining text storage with workflow to support 
specific document processing tasks. 
3.4.1.2 External to organisation sources 
Internet is the largest single integrated source of text. There are various types of 
Web sites including news sites, industry specific sites and government 
information sites which may provide text for text analysis purposes. 
News sites provide mainly general news of broad interest. Usually, general news 
sources are some of the best sources for information on issues outside normal 
markets that have an effect on industry. For example, international investment 
decisions are influenced by political risk assessment. Business news sites provide 
narrower ranges of information, often focusing on business and economic news. 
Some of these sites often tend to refer to specific regions. They provide more 
business oriented information than general news sources and often in more depth. 
For detailed coverage of a particular market industry specific sites provide the 
most. Industry specific sites may vary from broad professional associations to 
member only sites providing proprietary information. 
Government information sites provide usually valuable information not found 
elsewhere. Key industry information or founding related reports and technologies 
may provide text for interesting analysis. 
Chapter 3 The DocumentMiner Approach 
Finally, an important external textual source is the subscription services which 
provide access to pre-processed textual data often semi-structured. 
3.4.2 The Role of Ontologies in DocumentMiner 
One of the main priorities of our approach was to address the problem of 
heterogeneous representations of all the information elements produced as a 
result of the text mining processes. At the implementation level, the utilisation of 
a text capable database management system and a common schema is the answer. 
However, various formats also existed at the conceptual level. Models and other 
business related conceptual structures can be regarded as the primary forms of 
representation concepts of the problem domain. Since DocumentMiner aims at 
the effective analysis, management, and communication of all these elements, a 
solid paradigm had to be adopted to form the basis of our approach. 
The concept of Ontology emerged as a research topic mainly in the areas of 
Knowledge Base System integration and Artificial Intelligence Knowledge 
Representation approaches [53]. The standard definition for ontologies states that 
"Ontologies represent a formal and explicit specification of shared 
conceptualisation", where: 
  Conceptualisation refers to an abstract model (by identifying the relevant 
concepts) of some phenomenon/situation in the world. 
  Explicit means that the type of concepts being used (and the relevant 
constraints) is explicitly defined. 
  Formal refers to the fact that the ontology should be machine-usable. 
  Shared means that ontology captures knowledge which is not an 
individual's point of view. In contrast ontologies are explicit 
specifications of conceptualisations that are commonly agreed. 
In our approach the concept of Ontologies was adopted within the context of text 
mining and the main reasons are summarised below. 
Chapter 3 The DocumentMiner Approach 
In DocumentMiner Ontology models differ slightly from traditional conceptual 
models. This is because Ontologies are driven from the need to describe the 
elements that exist in the text mining process in a generic and explicit manner, so 
future references have to commit to these conceptualisations. Within the 
framework the role of the ontology is double. First the ontology drives the 
linguistic pre-processing upon the documents, by providing the necessary 
background information. For example when extracting from documents terms 
such as "zip code" and "post code", the program should understand that these 
two terms refer to the same concept. The second important role of ontologies in 
DocumentMiner is to be utilised by the user during the analysis (mining) step in 
order to specify the conceptual levels of the discovered patterns. For example, the 
attribute "lecturers" may include person names such as Dimitris Karaiskos, 
Aleksis Iwannidis, Ioannis Kakos, etc. For a particular analysis task, such as 
associations we may be interested to perform the analysis in a higher conceptual 
level than lecturers, which could be the level that specifies that the persons are 
employees in a particular sector. So, we would generalise Dimitris Karaislw , 
Aleksis Iwannidis 4 IT employees' category and Ioannis Kakos into 
Management employees' category. 
Traditional conceptual models on the other hand are more specific in abstracting 
and describing the problem domain in the context of developing a system. In this 
sense, the content of Ontologies may be viewed as a superset of these models; 
example, a specific data model of a problem domain may 
be based on the 
Ontology of the domain of interest. Thus, we view Ontologies as the 
ideal result 
of the text mining process where there is an effective exploitation of 
the domain 
concepts and knowledge elements. 
Ontologies arc lists of the kinds of things that are presumed to exist 
in a given 
domain together with a formal description of the salient properties of those 
things 
and the salient relations that hold among them. 
In DocumentMiner Ontologics 
are used by the program machine and 
by the user. Thus they hold domain 
knowledge in one unified representation. As a consequence 
by using a common 
Chapter 3 The DocumentMiner Approach 
representation, ontologies become a computational medium to integrate all the 
information to describe domain knowledge. 
Generalise, Generalised Textual Data 
Figure 14 Linking domain knowledge and textual data 
The link between the ontology and the actual data is the generalisation method 
we are using. In the following section we describe the method that we utilise in 
order to generalise our textual data with the ontology. 
3.4.2.1 The generalisation method 
Most of the times applying generalisation before any data mining task reduces 
substantially the computational complexity of the mining step as it reduces the 
complexity of the data set. The second equally important reason, especially in 
text mining, is that by applying generalisation the data set becomes more abstract 
and meaningful. Additionally the user is able to direct the analysis to the required 
conceptual levels. 
By using generalisation rules values of the attributes are generalised at multiple 
concept levels. As a result of this process, a set of attributes is generated for 
every generalised attribute of the original data set. The generated attributes are 
instances of the same attribute but in different concept levels. Each of these 
generated attributes is the generalisation of the attribute of the previous concept 
level and its values are the basis for further grouping and generalisation in order 
to form the attribute of the next higher concept level. 
It is important not to over generalise or oversimplify the processed attribute. As a 
consequence one of the critical issues is to 
find the minimum desirable level in 
Chapter 3 The Document. tlirrer Approach 
which the process should be stopped. On the other hand, generalisation process 
must not stop too soon as it will result in an attribute with too many values that 
have little support in the database. In standard attribute induction the desirable 
concept level is found by the use of a threshold at each concept level. Therefore, 
the desirable minimum level is the first concept level in which the number of 
distinct values of the generalised attribute is less than the corresponding 
threshold of that level. This method works independently from the undergoing 
data mining process and requires the user to specify a series of thresholds, one for 
each concept level. 
The method we are utilising (and extend) [52] overcomes the problem of 
manually adjusting the required thresholds by the adaptation to the undergoing 
data mining task, which also appears to be the main issue that distinguish this 
method from the standard attribute oriented induction. The key idea is that from 
the generated attributes after applying generalisation, the one that should be 
chosen is the one that is the most relevant to the undergoing data mining task. 
The most relevant of these attributes, which we find with relevance analysis, 
represents the least concept level in which we should stop. This chosen attribute 
replaces the original attribute in the original data set. 
In order to generalise, a set of generalisation rules has to be defined. Thesc 
generalisation rules are the elements that ontologies are consists of. For example: 
Cat, dog, donkey, etc. - Animals 
Red, green, yellow, etc. - Colours 
3.4.3 Role of the Pre-processing text components 
There are three main components: 
" Document collection 
" Document transformation 
0 Information Extraction 
In the following sections we refer to each of these components. 
Chapter 3 The DocumentMiner Approach 
3.4.3.1 Document collection 
Document collection component's main purpose is to gather the relevant data and 
always depends on the requirements the way this will be done. Mainly there are 
three ways supported by our framework. 
The first way is to store in the database a list of predefined sources and each time 
the collection modules transfer the necessary documents to the text repository. 
The basic information that we will need to maintain for each store is the location 
of the documents, the collection dates and times, and finally the amount of 
documents to gather. 
If we are not sure about the documents we will need in advance then we should 
include in the retrieval process a predefined list of topics in order to retrieve only 
a fixed number of documents and the most relevant for our purposes. 
The last way is to allow users manually to be able to load documents in the text 
repository. This is particular important for internal to the organisation documents 
with restricted information. 
3.4.3.2 Document transformation 
The work of these components includes the first basic operations upon the 
documents before we start the deep understanding of the content. For example, 
identifying document formats and if needed convert to a format compatible to 
other processing components. Additionally the language of the text may need to 
be identified or particular sections of the document extracted (such as headings, 
abstract/summaries etc. ) for further processing. The later, in general called zoning 
seems to be particularly important for specific analysis tasks. Particular sections 
or zones may include: sentence, paragraph, metadata zone, headings etc. 
3.4.3.3 Role of Information Extraction 
This is one of the most crucial faces. The goal of these components is to distil the 
necessary information for efficient management and mining and 
depends mainly 
upon information extraction techniques. By 
identifying key entities in a text, one 
Chapter 3 The Document tiliner Approach 
can find relationships between terms and identify unknown links (`hidden') 
between related topics. Information Extraction is the process of identifying 
essential pieces of information within a text, mapping them to standard forms and 
extracting them for use in later processing. 
IE is not a stand-alone task that human analysts typically perform. It is an 
abstraction from such tasks that at the end achieve a specific goal. Usually the 
success of the application depends on a) the nature of the texts, b) the complexity 
and variety of the kinds of information needed to be extracted, c) the suitability 
of the chosen output representation to the information requirements of the task. In 
general 60% is considered to be the upper bound of accuracy [54] for extracting 
events. 
Although IE systems that are built for different tasks often differ from each other 
in many ways, there are core elements that are shared by nearly every extraction 
system. The following figure shows the four primary modules that every IE 
system has, namely a tokeniser, some sort of lexical and or morphological 
processing, syntactic analysis, and some sort of domain-specific module that 
identifies the information being required in that particular application. 
Tokenisation 
Morphological and Lexical 
Processing 
Syntactic Analysis 
Domain Analysis 
Figure 15 Core elements of an IE system [54] 
Chapter 3 The DocumentMiner Appruuc h 
Depending on the requirements of the particular application, it is likely to be 
desirable to add modules to the above system. In summary the following factors 
will influence whether a system needs additional modules: 
1. The language of the text. Some languages may require morphological and 
word segmentation processing that English does not require. 
2. Type of input data. Extracting information from speech transcripts 
requires different techniques than text. For example, one may need to 
locate sentence boundaries that are not explicitly present in the transcript. 
3. Properties of the text. Very long texts may require information retrieval 
techniques to identify the relevant sections for processing. 
4. Tasks to be done. Tasks like entity identification are relatively simple. If 
one wants to extract properties of entities, then the text needs to be 
analysed for fragments that express the property. If the task involves 
extracting events, then the entire section may have to be analysed 
together. 
The last issue is crucial to our framework as there is a special requirement for 
temporal annotations. For a temporal knowledge discovery we suppose that 
temporal expressions in relation with events exist. The aim is to identify events 
(facts of interest), determine their relative temporal order and, if possible, 
position them in calendrical time. Event is something that happens, something 
that one can imagine putting on a time map. For example, the sentence; a small 
single-engine plane crashed into the Atlantic Ocean conveys an event. Time like 
events can be viewed as having extent (intervals) or as being punctual (points). 
Both are treated as time objects. A time object must, however, be capable of 
being placed on a time line. Examples of time objects are last Tuesday, April 4, 
1998 and March 1997 and also the referents of more complex expressions like 17 
seconds after the crash. 
Clearly the methodology followed by the information extraction module is out of 
the scope of this study, thus we present it brieilyy. What 
is important for us is the 
output of the IE module and this one is defined 
by the metadata schema in the 
corresponding chapter. 
Chapter 3 The DocumentMiner Approach 
3.4.4 Text and metadata repositories 
There are various options for storing text: 
" Textbases, such as Lotus Notes, Domino etc. The advantages of such 
applications are easily configured interfaces, a great number of text- 
searching operations, efficient programming tools and architectures that 
allow scalable solutions. 
" File systems. Many of the NLP tools work directly with files; so storing 
in file systems is an alternative storage option. Especially for the initial 
steps such as, loading, transformation it may be particular useful. Of 
course maintenance and metadata management is more demanding and 
the functionality by the file systems is not enough. 
" The third option is databases, which has been extended to support text 
such as Oracle Text. These databases combine the benefits of the 
relational database with data types for text and text retrieval operators. 
The advantages of this option are numerous. First of all integration with 
other applications of relational databases is easy. Additionally scalability 
and portability are issues that are strongly supported. Finally most of the 
operations that are utilised in data warehouses seem to be useful for the 
text repository. There are also some applications that combine file 
systems and database functionality such as the Oracle's iFS. 
In our framework we have adopted the database storage option. Apart from the 
above advantages the most important is that within our framework we are not just 
interested in treating documents as solid units or hierarchies of subdocuments but 
we are aiming to understand and exploit the semantic content of the documents. 
All this information extracted from documents (metadata) need to be stored and 
accessed by the various component such as mining, management etc. A detailed 
discussion of the issues that concern the metadata model is presented in the 
"Metadata model of the Document. l finer" chapter. 
3.4.5 Role of Data Mining- Analysis components 
Chapter 3 The DocumentMiner Approach 
Once the pre-processing and the metadata creation have been completed the text 
mining operations may be applied. Main goal of Text mining is to enable users 
extract information from large textual resources. Natural language processing, 
data mining and machine learning techniques work together to automatically 
discover patterns at the extracted information and the metadata that have been 
derived from the documents. 
Data mining is a new name for the old process of finding patterns in data. Usually 
a data mining analyst is presented with a problem to solve and an existing 
database that is relevant to the problem, and the analyst needs to find patterns in 
the data that can be used to solve the problem. As such data mining is the 
extraction of patterns or models from observed data [8]. Data mining is 
considered one step of the overall knowledge discovery process. Data mining and 
text mining tasks are similar. The most important difference is the initial input, 
which in the case of TM is text. 
At the core of the Knowledge Discovery in Text (KDT) process, which we have 
defined in previous chapter are the text mining methods for extracting patterns 
from data. These methods can have different goals, dependent on the intended 
outcome of the overall KDT process. It should be noted that several methods 
with different goals may be applied successively to achieve a desired result. Most 
Text Mining objectives fall under the following categories of operations, which 
we have described in previous chapter: 
" Feature Extraction. 
" Text-base navigation. 
" Search and Retrieval 
" Categorisation (Supervised classification) 
" Clustering (Unsupervised classification) 
" Summarisation 
" Temporal Analysis of Documents 
(i. e. Trends Analysis) 
" Associations 
" Visualisation 
Chapter 3 The DocumentMiner Approach 
3.5 Conclusions 
As mentioned before, the general framework proposed by DocumentMiner 
supports the text mining process and methods as defined in chapter two. Our 
view for text mining combines three fundamental technological areas such as 
Information Extraction, Data Mining, and Ontologies. We argue that complexity 
in analysis, and management of information during the text mining process, may 
be addressed using a variety of methodologies and approaches based on database 
oriented architecture. In DocumentMiner we propose a framework in which by 
using these approaches we may follow an easier and more secure path to 
transform the original unstructured or semi-structured text, into useful patterns 
and knowledge for the analyst/knowledge worker that has an interest upon the 
documents. 
Except the combination of information extraction and data mining another 
important issue is the introduction of the temporal issue which supports the 
knowledge discovery process. The information gathered throughout the process 
might be quite complex. Thus a systematic approach for storing this information 
is quite crucial and the schema for the database has to be as generic as possible. 
In the following chapter we are presenting our methodology for supporting the 
knowledge discovery process. It covers tasks necessary for our framework such 
as information extraction etc. 
4. The Text Mining process in DocumentMiner 
4.1 Introduction 
In text mining any task that is repeated several times and by many persons can 
benefit from a methodology for conducting the process. The goal of this chapter 
is to propose a model to support the Knowledge Discovery in Text process. 
DocumentMiner's process is iterative and involves a number of steps, beginning 
with the understanding and definition of business problem and ending with the 
analysis of results and a strategy for using the results to gain competitive 
advantage. There are usually two groups involved in the process for knowledge 
discovery in text. These are the text mining analysts and the domain experts. 
Usually the group of the text mining analysts consists of NLP and data mining 
experts but nothing prohibits these to be the same persons. In most cases the 
problem to solve is well known by the domain expert, and the process begins 
with the experts trying to explain the problem to the analysts. In parallel is always 
useful that the analysts explain the text mining process to the experts. Once they 
have together defined the problem to solve, in the following steps the analysts 
must collect and understand the relevant textual data, pre-process and prepare the 
textual data in order to structure them as to be maximally useful with the problem 
in hand, apply a mining algorithm, and explain the results to the experts. The 
process is iterative and has many cycles. 
Our proposed process includes concepts derived 
from the CRISP-DM [48] and 
Fa`yad U. and Piatetskv--Shapiro G. [5]. It is described in terms of a 
hierarchical 
Chapter 4 The Text Mining process in Document liner 
process model consisting of sets of tasks described at different levels of 
abstraction (steps, tasks etc. ). At the top level. our text mining process is 
organised into eight steps (Figure 16); each step consists of several generic tasks. 
We use the term generic for the second level tasks in order to indicate that these 
tasks are general enough to cover all possible text mining situations, which of 
course comply with our proposed framework. In other words these generic tasks 
should cover both the whole process of text mining and all possible applications 
of our approach. 
The description of steps and tasks performed in a specific order represents an 
idealised sequence of events. In reality many of the tasks can be performed in a 
different order and it will often be necessary to frequently go back to previous 
tasks and repeat specific actions. The process model we propose does not attempt 
to describe all of the possible routes through the text mining process of our 
framework because this would require a too complex model. 
Chapter 4 The Text Mining process in DocumentMiner 
Problem understanding and definition 
Collecting and understanding unstructured 
textual data 
Unstructured textual data preparation 
Converting unstructured textual data to 
structured data 
. -- - -- 
Structured textual data preparation 
Modelling-Apply Mining Algorithm 
Evaluation 
Chapter 4 The Text Mining process in DocumentMiner 
4.2 DocumentMiner's discovery process 
4.2.1 Problem understanding and definition 
It seems trivial to mention the step of problem understanding but as part of the 
whole discovery process can take a significant amount of time and its importance 
is paramount. This first step deals with the preparation of the business 
requirements and the definition of the method to address these requirements 
under the knowledge discovery framework. 
Text mining differs from other analytical processes including data mining in the 
sense that it is applied to unstructured data. A significant effort is needed to give 
some structure into the textual data. For text mining (as in data mining) it is 
relatively easy to apply a mining algorithm once we have our data properly stored 
in the database and get some results, but without a deep understanding of the 
problem domain the results may not be useful. It is important to understand the 
problem clearly and this domain specific knowledge has to be transferred from 
the domain expert to the text mining analysts. 
By definition of the method to address these requirements, we mean that the text 
mining analyst should join forces with the domain expert in order to define the 
problem detailed enough to be solvable. This in turns will make the results 
measurable. The problem may be clear to the domain expert but most of the times 
there are many ways to solve a specific problem and all these ways have to be 
evaluated under certain criteria. 
This first step of the process can be broken down in various tasks such as identify 
business objectives, assess state by analysts, define text mining objectives and 
finally the development of the business plan. 
The first task is to understand what the users want. Usually this 
is described in 
business terms. The business objectives may vary from single main objective to 
more complex secondary ones. 
It is important here to clearly specify 
interrelationships of the objectives as a single solution may satisfy various needs. 
Chapter 4 The Text Mining process in Document_tliner 
Additionally the success criteria of the project have to be defined and ensure that 
the right problems are identified. 
In our case study for example some of the business questions to be satisfied are 
the following: 
What are the competitors doing? 
Are they moving in new technological areas? 
Are they signing new collaborations? 
How have they expended their product line? 
These are types of business problems that initially drive the need for text mining. 
After an early assessment of the business objectives the analysts moves to a more 
detailed assessment than the first task. The objective here is to develop a list of 
human, data and computing resources. The final list of requirements is prepared 
at this point along with known constraints such as temporal, legal, technical, 
personnel etc. In addition to constraints a list of risks that may cause problems in 
the project is something useful. 
Unlike the business goal, which states the objectives in business terminology a 
text mining goal is an operational definition of the goal. In other words the text 
mining goal states project objectives in technical terms. For example in our case 
study the business goal might be "Should we move in new technological areas? " 
One of the sub-questions to answer is "Are competitors moving in technological 
areas and which? " and the text mining goal might be "Discover according to the 
collaborations they have signed the last year our competitors, relationships with 
new technological areas" As a consequence the business competition planning 
could be supported by a text mining operation such as trends analysis. The text 
mining analyst will include the description of the intended output of the project 
that enables the achievement of the business objectives. Additionally to the text 
mining overall goal it is really important an initial specification of the pieces of 
information (Name entities, facts etc. ) that we indeed to extract from the 
documents in order to feed the mining process. These initial specifications will 
greatly support the assessment of linguistic tools and resources. 
er 4 The Text Mining process in DocumeniMiner 
The final task of this first step is the preparation of the project plan. This 
includes time schedules, resources and other facts that influence the project. After 
defining in general the text mining goal and in particular the type of information 
we need to extract from the text, an initial assessment of the tools and techniques 
is done at this stage to unsure that the project can utilise essential algorithms and 
approaches. This is fundamentally important in text mining due to the strong 
dependency on the linguistic pre-processing. The tools and resources for this pre- 
processing usually require lots of time in order to be adapted or created for the 
domain with the problem at hand. Careful planning and good knowledge of the 
existing tools and resources is needed during this task. 
4.2.2 Collecting and understanding unstructured Textual Data 
This step deals mainly with the acquisition and evaluation of the textual data sets 
that will be processed. Once the problem has been defined we are in a position to 
specify and collect the relevant textual data (documents). In general the sources 
may be the Internet or internal file systems (including document management 
systems). Within a company each division collects the documents that it needs to 
perform its job efficiently and as a result usually there is no single resource. We 
have to build a common central text repository, most of the times, to maintain the 
documents that we need for our knowledge discovery tasks. 
The first task of this step involves the gathering of the relevant documents from 
the sources to the text repository. Secondly the description of these documents 
with the necessary source metadata such as Publisher, Published Date, Tipe, 
Format, Language etc. (In the next chapter we give more details about the source 
metadata). Thirdly it is important to assess the quality of the textual data we have 
collected. We have to ensure that the textual data set sufficiently represent the 
area being studied. For example, in our market competition case study if we are 
interested to check that competitors are moving to specific territories we should 
ensure that we include in the text repository market document about these 
specific territories. 
Chapter 4 The Text Mining process in DocumentA liner 
In general at this step various important issues will arise. One example is to 
verify that the known sources are enough or we have to search for additional 
content providers. The frequency of collecting documents is an additional issue 
and finally most of the times we have to check if sources are publicly available. 
Actually these issues are to be resolved during the last task of this step which is 
the exploration of the textual data. Once we have collected the relevant data it is 
important to spend some time exploring the document collection. First the analyst 
should familiarise himself with the textual data; not just knowing the topic names 
and the labels and what they intend to mean but the actual contents of the textual 
data. Additionally there are many sources of error when collecting data from 
multiple sources into a single textual repository and the analyst should perform 
several checks to validate the gathered textual data. 
  Identify documents sources 
  Retrieve documents 
  Storage options 
4.2.3 Unstructured Textual Data preparation 
In this step we will discuss the preparation of the textual data on the document 
level. The relevant documents are collected. Before we start to 
understand/process their content, is necessary to perform some operations upon 
them in order to be prepared for the next step of converting the unstructured 
textual data to a structured data format. Various operations due to the textual 
nature of the data may be needed but the most important concern the character 
format, the desired representation of documents and the language that the 
documents are written. 
Coded characters sets are used to represent alphabets 
in computers. The two most 
commonly used arc ASCII and Unicode. Unicode 
is replacing the older character 
sets but in the meantime we may have to 
identify the character set of a document 
and if necessary, convert it to an appropriate character set 
for later processing by 
the other modules. 
Chapter 4 The Text ; inning process in DocumentAfiner 
Natural language text by default has a natural structure; the grammatical structure 
(word, phrase, sentence etc. ). On top of this a number of artificial structures have 
been utilised via markup languages. Markup languages generally define 
structures and describe formatting. For example, XML is widely used for data 
exchange because it provides a flexible and easily parsed structure for arbitrarily 
data structures. Unlike natural languages that have predefined elements, such as 
nouns, adjectives, and verb phrases, markup languages (XML, SGML, etc. ) allow 
users to define the structuring elements of a document. Almost always 
transformations are needed aiming at obtaining the desired representation of 
documents such as XML, SGML for the text analysis components. 
Some of the operations performed on documents are language specific i. e. 
morphological analysis. So, it is important in the pre-processing stage to identify 
the language that the document is written and if it is not written in a suitable 
language we may have to translate it. 
  Character set conversion 
  Obtain desired document representation 
  Language identification, Translation 
With these tasks complete, the process moves to the first major processing of the 
actual documents. 
4.2.4 Converting unstructured textual data to a structured data 
format 
This step mainly concern IE tasks that aim at extracting specific pieces of 
information from the text in order to populate the database. The tasks involved 
depend on the approach we will follow for the IE system. 
There are two basic 
approaches to the design of the IE system, which are 
labelled as the Knowledge 
Engineering Approach and the Automatic Training Approach [54]. 
The Knowledge Engineering approach is characterised 
by the development of the 
grammars used by a component of the 
IE system by a text mining analyst i. e. this 
r4 The Text Mining process in DocumentMiner 
person is familiar with the IE system, and the formalism for expressing rules for 
that system, who then, either on his own, or in discussion with the domain expert. 
writes rules for the IE system component that mark or extract the information 
pieces of interest in the text. Typically the text mining analyst will have access to 
a reasonable size corpus of domain-relevant texts and his'her own instinct. A 
reasonable size corpus means a corpus that a person could be expected to 
personally examine. It is very important to mention that the skill of the text 
mining analyst plays a large role in the level of performance that will be achieved 
by the overall system. Building a high performance system is usually an iterative 
process whereby a set of rules is written, the system is run over a training corpus 
of texts, and the output is examined to see where the rules under- and over- 
generate. The text mining analyst then makes appropriate modifications to the 
rules, and iterates the process. 
  Acquire a corpus of domain-relevant texts. 
  Write rules. 
The Automatic training approach is quite different. Following this approach it is 
not necessary to have someone on hand with detailed knowledge of how the IE 
system works, or how to write rules for it. It is necessary to have someone who 
knows enough about the domain and the task to take a corpus of texts, and 
annotate the texts appropriately for the information being extracted. Typically, 
the annotations would focus on one particular aspect of the system's processing. 
For example, a name recogniser would be trained by annotating a corpus of texts 
with the domain relevant proper names. Once a suitable training corpus has been 
annotated, a training algorithm is run, resulting in information that a system can 
employ in analysing novel texts. 
  Acquire an annotated corpus of domain-relevant texts. 
  Run a training algorithm 
Both approaches share some common tasks that have to do with the 
following: 
  Assess the information extracted according to technical criteria 
Chapter 4 The Text alining process in DocumentMiner 
" Evaluate according to what pieces of information we where expecting, to 
extract. The key questions once information has been extracted are: 
o Are these facts relevant to the business objectives? 
o Are the facts actually true? 
o What pieces of information are missing that increase the risk of 
acting on these facts? 
What is gained by this step are pieces of information that drive business decision 
making and feed the mining algorithms in order to discover knowledge. 
4.2.5 Structured Textual Data preparation 
Once the textual data have been transformed into structured data the classical 
data preparation tasks can take place in order to prepare data for use With data 
mining tools. The output of the previous steps is a database containing all the 
information that has been extracted from the documents. 
The first important issue that we should consider under this step is the selection 
of attributes and records to include in the mining phase. Choosing which 
attributes to work with is a difficult task and usually depends on the type of 
analysis we want to perform. There are some techniques that can support this 
task, such as relevance analysis (the purpose is to rank the available attributes 
based on their relevance with each other), but usually in text mining the selection 
is up to the domain expert. Additionally in our framework this is feasible with the 
use of an ontology. Via the ontology the user is able to choose the entities for 
which further processing will follow. Selecting records refers to the sampling 
problem and some statistical techniques often are used. 
After selecting the next task is cleaning the selected data. There are several Ways 
to do it. If necessary, depending on the mining algorithm, missing values can 
substituted with default values. Additionally 
in text mining correcting is 
particularly important as the previous linguistic pre-processing steps 
introduce 
great number of errors in the precision of the extracted 
information. Correcting 
the features and facts that have been extracted 
from the unstructured textual data 
by the domain expert helps the text mining analyst to 
improve the performance of- 
Chapter 4 The Text Mining process in DocumentMiner 
the NLP tools. Usually the domain expert selects a set of documents in order to 
examine the metadata extracted. 
The following task includes any possible transformation of the available data in 
order to satisfy the needs of a particular analysis. For example, in our data the 
values of the attribute "colour" may be represented with categorical values e. g. 
green, red etc. or by numerical values such as 1 for green, 2 for red etc. This type 
of transformations depends highly on the type of analysis we want to perform and 
the required knowledge to be extracted. In some other cases derived values need 
to be calculated and most of the times the values are limited to calculations 
within a single record. As a consequence entire records may be added to the data 
set. In our framework under this task we include any generalisation of the value- 
attributes in order to gain higher level of conceptualisations. For example, the 
attribute "companies" may include company names such as IBM, Oracle, 
Unilever, etc. For a particular analysis, let's say looking for associations we may 
be interesting to perform this analysis in a higher conceptual level than company 
names, which could be the market sector of the companies. So, we would 
generalise IBM, Oracle - IT category and Unilever into Food category. 
The integration task takes place when there are multiple data sources that need to 
be integrated in order to facilitate analysis. It is particular important when it 
involves integration of data extracted from documents and structured data already 
stored in a database. Finally reformatting the data may be necessary to meet the 
needs of the mining tools. Reformatting at this stage has to 
do only with the 
syntactic properties of the data and not with the meaning of the 
data. 
Chapter 4 The Text Mining process in DocumentMiner 
Select 
Clean and correct 
Construct and Generalise 
Integrate and Format 
Figure 17 Tasks of step Structured textual data preparation 
At this point the data are ready to be processed with the mining techniques. 
4.2.6 Modelling-Apply Mining Algorithm 
In this stage the actual mining tasks take place. All the previous steps in some 
way pre-process and prepare the data for the algorithm. The first task under this 
step is the identification of the approach and the algorithm that will satisfy the 
initial needs and the text mining goals specified in the first step. For example if 
the initial need was for prediction based upon classification then we should 
decide for, a decision tree algorithm or a neural network etc., to use. There are 
many algorithms to satisfy specific needs and the choice of one will have 
significant effect on the quality of the discovered patterns. Even a single 
algorithm is chosen, there are still several parameters to set. It is true that in some 
cases the available data mining algorithms may not be directly applicable to the 
problem at hand and some customisation may be needed in order to adapt the 
algorithm. Usually this is the case when the tool set of the mining algorithms is 
limited or we apply mining to new types of problems for which algorithms have 
not yet been invented. This is one of the most important 
drawbacks in text 
mining and in particular in temporal text mining. The need 
for new algorithms 
that will satisfy the specific needs of these areas 
is significant. Up to now existing 
mining algorithms have been used and this 
limits in some way the profits of 
Chapter 4 The Text Mining process in Document_1liner 
applying mining to text. As long as innovative uses for data mining are explored. 
the need for new algorithms will be greater. 
The next task is about the test design of the proposed model. Before we actually 
build a model, we need to generate a method to test the model's quality and 
validity. For example, in supervised data mining tasks such as classification, it is 
common to use error rates as quality measures for data mining models. Therefore, 
we typically separate the dataset into train and test set, build the model on the 
train set and estimate its quality on the separate test set. After specifying the 
method to test the model we proceed with the creation of the actual model. 
Along with the model all the necessary parameters should be specified. The last 
task of this step has to do with the assessment of the created models. The text 
mining analyst assesses models by checking the technical criteria that have been 
specified and the domain expert in terms of its descriptive properties. 
Identify approach and 
algorithm 
Generate test design of the 
model 
F Create the model 
Assess the model 
Figure 18 Tasks under step Modelling-Apply Mining Algorithm 
4.2.7 Evaluation 
The evaluation step follows the modelling and consists of various tasks. 
Usually 
the methods used to evaluate depend on the type of problem 
being solved. 
Numerically evaluating the performance of the model 
is crucial, but we have to 
consider also other less objective methods. 
It is always necessary that the model 
Chapter 4 The Text Mining process in DocumentAliner 
is being interpretable by the domain expert. Although performance results may 
indicate some confidence on the results of the model, without the domain expert 
being comfortable and have some understanding of what the model is doing. it 
will be really difficult to use it in order to support crucial business decisions. 
Evaluating the results is different from the assessment of the model in the 
previous step. Now the evaluation takes place in terms of the business objectives 
that the mining goals aim to fulfil. If the domain expert is satisfied by the model 
but there are some unexpected patterns or he/she is not satisfied by the model 
then further analysis is needed and refinement of problem, data or the process 
he/she had followed. 
The most important reason for starting iteration in the text mining process is at 
this point, when the domain expert does not see what he/she was expecting. Quite 
often it is better to have initially one quick pass through the entire process in 
order to show some results to the domain expert. This familiarise the expert with 
the entire process and in the second iteration we can have a better understanding 
and more accurate settings. In some cases the expert may not like the results, but 
there is nothing wrong with the definition of the problem or the approach 
defined. Thus new attributes may be added in the model or errors to be 
eliminated in the extraction phase. 
  Evaluate results 
  Refine problem, data and process 
After a few refinement iterations and with the domain expert comfortable with 
the model, we can pass in the next step which is the use of the results. 
4.2.8 Deployment 
Until now we have discussed about extracting 
information from text, discovering 
patterns in the extracted information, and all the necessary steps 
that are needed 
to pre-process, prepare the data and 
finally verify that the discovered patterns are 
potentially useful. Once we have made a 
final pass through the entire text mining 
process we are ready to plan 
how to take advantage of the discovered patterns, 
and the way they will 
influence the decision making, in the organisation. We 
should have in mind that this 
is heavily up to the domain expert and just having a 
I, N0 
Chapter 4 The Text Mining process in DocumentMiner 
model can be a long way from being ready to take actions based on it. Usually the 
tasks involved have to do with the plan of the deployment which is simply a 
formal documentation of the approach to use the model. In a similar way we 
document the steps for monitoring and maintaining the model and all the 
necessary parts of the text mining process. This is particularly important in text 
mining (in contrast with data mining) because the process rely in a great number 
of resources such as dictionaries, rules for information extraction, thesaurus and 
other linguistic resources that need to be updated regularly. Text mining will 
greatly benefit from a formal way for monitoring and maintaining these 
resources. On the other hand the actual data, the documents are more sensitive to 
change than the conventional structured data for data mining and frequent 
adaptation of the various components that take part in the process is necessary. 
As final tasks we include the final report and project review which gives the 
opportunity to document the results of the entire process. 
4.3 Conclusions 
Many of the tasks that are involved in the KDD (Data mining) process are 
applicable to text mining with some variation. In this chapter we have considered 
each step in detail and we have identified differences. For example the first step 
of "Problem understanding and definition " is identical in both processes. This is 
due to the generality of the step in any type of development or analysis project. In 
the second step "Collecting and understanding unstructured Textual Data " 
mining and text mining have similar requirements. The most 
important difference 
is the nature of the initial input. In data mining we have structured 
data and in 
text mining semi or unstructured data (document). Thus 
in our proposed 
methodology an additional step is added. This 
is "Converting unstructured 
textual data to a structured data format" which 
is necessary due to the 
unstructured nature of the textual data. Actually this step 
involves Information 
Extraction in the overall discovery process and supports the transformation of 
unstructured data into structured. It 
includes various tasks depending, on the IE 
approach we will follow. 
Chapter 4 The Text Mining process in Document. diner 
As we have stated above main difference is on the initial input of the process and 
thus the pre-processing step of the TM process is more complex and requires 
support of linguistic tools. This is the reason we have split the preparation step 
into two steps; initially working with documents and later with pieces of 
information extracted from texts. These steps are time consuming but necessary 
in order to understand the meaning of the text and extract valuable pieces of 
information for assisting further mining analysis. 
The tasks that are involved with the construction and maintenance of the 
Ontology that is necessary for the framework are not included in the proposed 
methodology. The ontology is considered background knowledge already 
existing. 
5. Metadata model of the DocumentMiner 
5.1 Introduction 
DocumentMiner framework is dealing with huge amount of textual information 
and is analogous to data warehousing as a method for dealing with large volumes 
of numerical data. Metadata of the DocumentMiner is information about the 
contents and processes of the proposed framework. 
The term "meta" comes from a Greek word that denotes "alongside, with, after, 
next. " More recent Latin and English usage would employ "meta" to denote 
something transcendental, or beyond nature. Metadata, then, can be thought of as 
data about other data. 
Metadata is essential part of the DocumentMiner framework. The goal of the 
framework is to provide not only access to the information hidden within text but 
also a full understanding of the life cycle of that information and how it is 
acquired. Metadata is the key to meeting that objective. Although we could 
randomly define the document metadata we have chosen to reuse and adapt few 
of the existing standards as a basis. The main reason for this is the rising 
awareness for the need for metadata on the web and all business intelligence 
applications in organisations. Such applications interact to each other and a 
common ground is paramount. Our model includes concepts derived from 
various existing standards such as the Dublin Core [59], Common Warehouse 
Metamodcl [60]. Message Understanding Conference (MUC7) [57], TimeM L 
[58], Text Encoding Initiative TEI [56] etc. Each one assists the deN clopment of 
Chapter 5 Metadata model of the Document. lliner Framework 
the DocumentMiner's model. Thus initially in this chapter we will discuss in 
brief the elements of the existing standards that we are utilising. and later we will 
present our model. The current description has only an illustrative purpose, in 
particular the set of metadata elements introduced needs to be further expanded 
and the attributes of all elements need to be verified. 
5.2 Metadata standards 
In various disciplines metadata standards have been proposed, although the 
common purpose of all of them is to provide a common description of the 
contents of an item that supply data in various formats i. e. picture, video, audio 
etc. For our purposes this item is the document. Usually the producer or the 
author of an item creates the metadata. The exact contents of the metadata can 
vary even for a single domain and one objective of metadata standards is to create 
a common language that will improve interoperability. In DocumentMiner, 
metadata is information describing documents and is one of the most crucial 
components. 
Existing metadata standards that contribute to our work have come out from three 
directions mainly. Firstly, data warehousing have created data and system 
modelling metadata standards such as the Common Warehouse Metamodel etc. 
Secondly, appears the resource standards for which library and Information 
Science professionals have developed standards such as the Dublin Core etc. 
Thirdly, we will consider separately the NLP community, which offer linguistic 
and IE standards. 
The Common Warehouse Metamodel (CWM) describes essential data 
warehousing processes and objects. The Open Information Model (OIM. which 
now has joined CWM) developed a distinct metamodel, including 
warehousing and other software engineering processes. Dublin 
Core Mctadata 
Initiative (DCMI) focuses in describing the contents of an Internet resource and 
does not include issues of text mining or 
data warehousing processes. From 
MUC7 we are utilising the existing Named Entity annotation scheme. and 
Chapter 5 Metadata model of the DocumentMiner Framework 
accordingly the temporal annotation scheme from the TimeML work, while 
tentatively suggest possible specific extensions for our framework. TimeML 
offers the possibility of annotating whichever temporal aspects of documents are 
considered relevant. For example, would a documents' publication time be a 
specific enough temporal anchor, or are the temporal relations between events 
within the document important. News feed headlines could only require 
publication times whereas large technical reports should need more distinctions. 
Additional source that have been considered is the Text Encoding Initiative 
(TEI), which is used to define the organisation of the document into head, body, 
sections, paragraphs, sentences and tokens. 
5.3 DocumentMiner metadata model 
The main purpose of the metadata repository in DocumentMiner is to store 
information that describe the documents of interest and information extracted 
from those documents. We propose the following general categories of metadata 
under DocumentMiner framework: 
" General Descriptive Metadata of Document 
" Storage or maintenance metadata 
" Extracted Information Metadata of Document 
General Descriptive Metadata of documents includes search, retrieval and other 
external metadata related to the document. Storage or maintenance metadata 
used for managing the contents of the text warehouse. 
The last type of metadata 
is basically the output of the Information extraction step and refers to the 
concepts and information that is mentioned within the 
document. 
At each step we define metadata needed 
in DocumentMiner in terms of the 
metadata elements specified within one of the standards 
discussed in the above 
sections. 
Chapter 5 Metadata model of the DocumentMiner Framework 
DocumentMiner's logical model has similarities to the star schema architecture 
used so often in data warehouses. As shown in figure 19, at the heart of the 
logical model is the document data model. Content, source, search and retrieval, 
information extraction and storage metadata correspond to dimensions in a 
dimensional model. 
r------------------------------ 
General descriptive metadata 
Search and 
I Content Source 
retrieval 
--------------- --------------- 
Document 
Extracte d 
Storage Information 
metadata 
Figure 19 Star Schema of DocumentMiner metadata 
5.3.1 General descriptive and storage metadata 
Although we can define randomly the general descriptive metadata needed to 
describe the documents, we use one of the widely adopted standards as a basis 
and we extend it. This is suggested by major warehouse designers. The Dublin 
Core metadata standard was developed with Internet resources in mind and is one 
of the best options to start with as the main source of textual information for 
DocumentMiner is the Web. 
Firstly the classes with the associated attributes are introduced and after the class 
diagram is presented. Obvious descriptions of attributes will not be included. 
Content 
Content describes the subject of a document as well as its intellectual property 
and instantiation. The attributes used are explained 
bellow: 
Chapter S Metadata model of the Documentllliner Framework 
Creator 
- Person, group or other organisation responsible for the 
document. For a single document we can include both persons and 
organisation. 
Subject - List of terms from a controlled vocabulary that describes the 
contents of the document. For example, a corporation managing an 
intranet should use terms from a corporate thesaurus when specifying 
subjects. This is particularly useful because it improves keyword search. 
Title - The formal name of a Document. 
Description - They are not limited to controlled vocabularies and can 
include summaries, tables of contents (for long documents), or other ways 
of providing more details about the contents of a document that is 
available from the subject element. 
Publisher - Person or organisation responsible for publishing the 
document. 
Contributor - Person, group or organisation that makes substantive 
additions to the content of a document in addition to creators. 
Date - Define the time of significant events 
in the life of a document, such 
as the creation and publication. The DCMI recommends following the 
ISO 8601 profile, which uses the YYYY-MM-DD format. The Date 
element is frequently repeated but qualifiers are used to 
distinguish 
different events. We make use of Creation Date, Publication Date and 
Last modified Date. 
Type - Describes the general category of a 
document. The DCMI 
recommends using a controlled vocabulary such as: 
(Text. Image, Sound, 
Dataset, Software, Interactive, Event, and Physical Object). Additionally 
Chapter 5 Metadata model of the Document liner Framework 
to the recommended we make use of few other types such as web page 
Format - Describes the way in which the document is stored and 
represented within a file. This information generally is used to determine 
the type of software needed in order to use a document. (Word, Adobe 
A. R., Latex etc. ) 
Language- Describes the language of the text in the document. DCMI 
recommends using the ISO 639 two-character standard abbreviations. 
Rights - Describes the legal rights held over the document by its creators 
or publishers. Sometimes this element contains the name of a service 
providing management of the owner's rights. 
Search 
We have included search metadata about the documents as different class. Unlike 
the features of a particular document, the search metadata describes how the 
document comes to the repository; this is defined by source metadata which 
includes URLs for online sources or for local files, pathnames, and frequency of 
search, search criteria etc. 
The basic list of search metadata that we make use is: 
URL Pattern - Identifier for online sources. 
The values of these elements 
should always be defined in accordance with a formal standard, such as 
Universal Resource Indicators (URI, which includes Universal Resource 
Locators [URL]) 
File Path - For local files we 
include the file system pathname 
Chapter 5 Metadata model of the DocumentMiner Framework 
Depth 
Span Site Indicator 
Number of Tries 
Time out 
Wait between Retrievals 
HTTP UserName 
HTTP Password 
Proxy UserName 
Proxy Password 
FSUserName 
FSPassword 
Reject List 
Include Directories 
Exclude Directories 
Search Engine 
Source 
The kind of source that the document came from is an important issue. Quality of 
the retrieved document is of paramount need of the knowledge worker. Two 
indicative attributes that can specify quality is the timeliness and reliability. Neves 
sources are in general very timely sources as they are in constant competition of 
providing breaking news, which means the latest the better. On the other hand 
some sources may be timely, but the accuracy of their content is questionable. 
Always depends on the application which of the two attributes of measuring 
quality; timeliness or reliability is the most important. 
Name - The formal name of a source 
Description - More details about the source of the 
document. 
Timeliness - Indicates a timely source. 
How quickly a reported event is 
made known. 
Reliability - Indicates the accuracy of the 
information provided in text. 
InternalSourceIndic - Indicates if the source 
is internal to the organisation 
(in example a file system) 
Chapter 5 Metadata model of the DocumentMiner Framework 
ExternalSourcelndic 
- Indicates if the source is external to the 
organisation (In example the web) 
SourceType 
- Indicates the type of the source. (In example news stories, 
Academic papers etc. ) 
PurgeMultipleVersion 
- Indicates if multiple versions will be kept. 
Document 
We use the term document to refer to a logical unit of text. This could be Web 
page, a status memo, an invoice, an email etc. It can be complex and long [28], 
and is often more than text and can include graphics and multimedia content. In 
this study we are only concerned with the textual elements of documents. 
Contents - Includes the contents of the document. 
Summary - Aims to briefly give an idea of what the document is about. 
SummaryExpires_On - The date that the summary expires. 
DateLoaded - Date that the document was loaded. 
LastVerified - Date that was last confirmed. 
Version - Identification in case of keeping multiple versions of 
documents. The technique we propose to use for versioning is to 
distinguish different versions with the creation time of a document on its 
original source. 
Storage 
Storage describes how a document should be handled once it has been retrieved 
and analysed. One of the main purposes is how to represent the document and 
when to change representations. For example, we can store the entire document 
or for some documents we may decide to keep just a link (URL) to the actual 
document along with information extracted. Documents may need to 
translated and reformatted. In general, the question is how to 
handle various 
representations of the same content. The process of reducing the 
document 
representation within the text repository 
is called pruning. For c\ample, one 
approach is to store the entire document at 
first and then store only a summar\ 
and at the end just a URL or other reference to 
document's location. 
Chapter 5 Metadata model of the DocumentMiner Framework 
Some basic storage metadata to start with include: 
Entire Document Indicator - Indicates if entire document is stored. 
Summary Indicator - Indicates if a summary is stored. 
URL Indicator - In case just a URL is kept. 
Pathname Indicator - Indicates that the pathname is kept. 
Prune Full Document 
Prune Full Document after 
Prune Summary Indicator 
Prune Summary after 
Keep Full translation 
Keep Summarised Translations 
Translation Review Required 
Storage metadata such as these are associated with a set of documents (particular 
document types, specific source etc). 
Chapter S 
Documents 
Document ID 
StorageMetadata_ID 
Source ID 
ContentMetadataID 
Contents 
Summary 
SummaryExpires_On 
DateLoaded 
LastVerified 
Version 
Storage 
Storage_I D 
StoreEntireDoclndic 
StoreSummarylndic 
StoreURLIndic 
StorePathnamelndic 
PruneFullDoc 
PruneFullDocAfter 
PruneSummary 
PruneSummaryAfter 
KeepFullTranslations 
KeepSummarisedTranslations 
Translation Review Required 
Content 
Content_ID 
Creator 
Subject 
Title 
Description 
Publisher 
Contributor 
CreationDate 
PublishedDate 
LastModifiedDat e 
Format 
Language 
Rights 
Metadata model of the DocumentMiner Framework 
Source 
Source ID 
Description 
Timeliness 
Reliability 
InternalSourcelndic 
ExtemalSourceIndic 
SourceType 
PurgeMultipleVersion 
Search 
DocSourceID 
URLPattern 
FilePath 
Depth 
SpanSite 
NumberOfTries 
TimeOut 
WaitBetwRetrievals 
HttpUserName 
HttpPassword 
ProxyUserName 
ProxyPassword 
FSUserName 
FSPassword 
RejectList 
IncludeDirectories 
ExcludeDirectories 
SearchEngine 
Figure 20 Classes of general descriptive and storage metadata 
Chapter S 
How come 
Search 
Metadata model of the DocumentMiner Framework 
Documents  
Content 
is connected to 
Kind of source 
Source 
Figure 21 Class diagram of general descriptive and storage metadata 
Although specific metadata would often relate to only a single document, when 
multiple versions are stored as this is the case in our framework, it may relate to 
several documents. 
5.3.2 Extracted Information Metadata 
DocumentMiner framework is based on the innovative use of infrnnution 
extraction for supporting the text mining process. Within this framework 
Information Extraction is an essential phase in text processing. It facilitates the 
automatic or semi-automatic creation of more or less domain specific metadata 
repositories. Such repositories are further processed using standard data mining 
techniques. As mentioned in chapter 3 the methodology followed by the 
information extraction module is clearly out of the scope of this study. What is 
important for us is the output of the IE module and this is defined by the 
proposed metadata schema. In the following sections we are going to present the 
schema using the linguistic notation and as such we will refer to annotations 
Describes 
Stores 
Chapter 5 Metadata model of the Document. %liner Framework 
upon the documents. Where an annotation represents a form of meta-data 
attached to a particular section of document content. An annotation has a type (or 
a name) which is used to create classes of similar annotations, usually linked 
together by their semantics and annotation set holds a number of annotations. 
The examples given will look XML like format for a matter of convenience. The 
entities introduced are demonstrated in the document collection, which act as the 
corpus of the framework. [Appendix 1] The list of entities introduced is by no 
means complete but serves as the starting point, upon which to build a picture of 
the domains from information types they contain. We have to make clear that 
always the most crucial step is the users to clarify types of relevant information 
while the text mining analysts formulate the definition in practical terms. 
The most basic elements of information extraction metadata, in which we will 
refer in the following sections as conceptual annotations in more detail, include: 
" Entities. 
9 Relations 
Entities represent main points of a document and may be terms or concepts such 
as person names, locations, event's action and other information pieces of 
interest within the documents. They may be stored in a database and managed as 
attributes of the documents with some interesting measures such as the frequency 
within the document or other measures that specify their importance. 
Relations describe the relationships between terms or concepts such as 
organizations, persons, places, etc. For example a text from a news item: 
Saddam Hussein the president of Iraq announced that Iraqi people will 
defend their country against USA aggression. 
Two relations can be identified in this text: 
Entitil (Saddam Hussein), Relationship (President of), Entitv2 (Iraq) 
Entity] (Iraqi people), Relationship (defend against), Entitv2 (USA) 
Chapter S Metadata model of the DocumentMiner Framework 
Combining entities and relationships we can define facts of interest within the 
document collection. Information extraction can extract these relationships as 
well as entities participating in these relationships, such as proper names. 
organisations, time expressions etc. 
5.3.2.1 The Role of the Ontology 
The Ontology provides the essential Domain Knowledge by classifying and 
organising the concepts of the domain. 
All Conceptual Entities in our framework will be linked to the Ontology via a 
Type attribute, which can be considered as a pointer to the most specific concept 
in the Ontology which includes them. Quite obviously, structural entities do not 
need to be defined in such a way. In the examples we have used the simplifying 
assumption that a string (such as "person", "organisation") can be considered as a 
pointer in the Ontology (it is to be taken as the name of a concept). In fact it 
might be the case that we will need to use more complex identifiers for this 
purpose. 
One point which needs to be discussed is whether the Lexical Entities should be 
considered as 'instances' of specific concepts or rather simply one of the many 
ways to refer linguistically to the corresponding entities (and thus all instances of 
something like 'Name' or 'String' or 'Term'? ). The relational entities will need to 
have restrictions as for the possible values of their domain and range in terms of 
types within the ontology. 
In order to avoid confusion, it is useful to state here the definitions of some terms 
that will be used. 
Taxonomy is a set of concepts organised into a hierarchy 
by the IsA relation. 
Concepts have no explicit definition and no content (they are 
just empty labels). 
Ontology is a Taxonomy whose concepts are 
defined, either formally by some 
kind of formal language. in which case you would 
have a "formal ontology", or 
Chapter 5 Metadata model of the DocumentMiner Framework 
informally (by a gloss). In an Ontology (but not in a Taxonomy). concepts can 
have definitional content and can be linked by additional relations (e. g. 
meronymy, antonymy). The main function of an Ontology is to provide a 
reference vocabulary for a given domain. More ambitious Ontologies aim at 
providing a common core set of concepts for all domains of human knowledge. 
Thesaurus is also a generalization of taxonomy. Concepts can be connected to 
each other with more types of relations than the IsA. However, the concepts still 
lack an explicit definition, i. e. they are just labels (and obviously have no 
instantiation). 
For example, the concept of "Company", which could be defined (informally) as 
"a legal entity engaged in some kind of business" (the same definition could be 
written in some formal specification). Now this concept could have a slot for 
"CEO", whose filler would be of type "manager". a subtype of "human". Now 
consider the instance "Microsoft", for this instance the filler would be "Bill 
Gates" which is an instance of human. 
In Ontology we would have the concepts of Company, of Manager and of 
Human, but not the instances "Microsoft" and "Bill Gates". When instances are 
considered as part of the picture the resulting object should be called Knowledge 
er 5 Metadata model 
Figure 22 Knowledge Base 
5.3.2.2 Categories of annotations 
the DocumentMiner Framework 
The set of annotations we are utilising for our framework is organised in three 
levels: 
Structural Annotations 
This category of annotations is used to define the physical structure of the 
document. For example the organisation into head, body, sections, paragraphs, 
sentences and tokens. This is particular useful when we have to guide our 
analysis in specific parts of the document. For example, if we are planning to 
utilise research papers in a knowledge discovery process, just working with 
the abstract section could be enough. 
Lexical Annotations 
These annotations are associated to a short span of text (smaller than a 
sentence), and identify lexical units that have some relevance for the 
framework: Named Entities, Terminology, Time Expressions, etc. 
Semantic/Conceptual Annotations 
This type of annotations is not associated with any specific piece of text. They 
refer to lexical annotations via co-referential Ids. 
They (partially) correspond 
Knowledge Base 
Chapter 5 Metadata model of the DocumentMiner Framework 
to what in MUC7 was termed 'Template Elements' and 'Template Relations' 
(Template represents the final, tabular output format of information extraction 
process). Ideally they should correspond to instances. 
Figure 23 Document annotations 
All annotations are required to have a unique ID and thus will be individually 
addressable; this allows lexical and conceptual annotations to point to the entities 
to which they correspond. Conceptual Annotations themselves are given a unique 
ID, and therefore can be elements of more complex annotations. 
The temporal aspects of documents that are considered relevant to our analysis 
are captured with TimeML. Basically, TimeML uses five tags <EVENT>, 
<TIMEX3>, <SIGNAL>, <LINK> and <DocCreationTime>. The latter is 
self-explanatory, but in our framework this information is captured by the Dublin 
core metadata Date which except Creation Date also includes Publication and 
Last modified Date as explained in above sections. The scope of the annotation 
scheme comes directly from the interaction of the four remaining tags. 
Two types of temporal entities are annotated: 1) Verbs, nominalizations, 
adjectives and prepositions that indicate something that 
happened are marked as 
<EVENT>. 2) Explicit references to times and calendar 
dates are marked as 
<TIMEX3>. How these two types of entities temporally 
interact is captured via 
Chapter 5 Metadata model of the DocumentMiner Framework 
<SIGNAL>, which marks explicit relations ("twice". "during"), and <LINK>, 
which marks implicit relations. The latter is further subdivided into <SLINK> 
<TLINK> and <ALI'NK> to characterise different types of temporal relations. 
The temporal annotations operate on two levels. On the lexical level events and 
states are identified, and then the conceptual relations holding between the 
entities are captured by the LINK tags which subsume no text. Therefore we 
consider the <EVENT>, <TIMEX3>, <SIGNAL> as belonging to the set of 
Lexical Annotations, while <LINK> (together with the related 
<MAKEINSTANCE> tag) belong to the set of Conceptual Annotations. 
A detailed description for each of the types will be presented in the following 
sections. 
5.3.2.2.1 Structural Annotations 
The structure of the documents will be marked using a general scheme which 
may require further adaptations to specific documents. For convenience and 
compatibility we plan to apply a set of elements based of the high-level structure 
specified in TEI, with the exception of a specific root node, which will be called 
<Doc> and replaces the TEI element <text>. We adopt the sub-elements 
<front>, <body> and <back>, which are specified in TEI as follows: 
" <front> contains any prefatory matter (headers, title page, prefaces, 
dedications, etc. ) found at the start of a document, before the main body. 
" <body> contains the whole body of a single unitary text, excluding any 
front or back matter. 
9 <back> contains any appendixes, etc. following the main part of a text. 
The <front> might include a title, abstract or summary of the documents 
contents. In our framework's setting, metadata for author 
information, 
creation/release time, the document source, the date of download etc. are 
kept by 
using the Dublin Core Metadata Initiative as presented 
in general descriptive 
metadata section above. Other elements such as appendixes, notes or 
Chapter 5 Metadata model of the DocumentMiner Framework 
bibliography might be placed in the <back> section for some document types. 
However both front and back sections can be left out if not relevant. 
The main body of the documents (<body>) will be split into sections (<di-*v>). 
The TEI specifications allow for the attributes n, id and type to be applied to the 
div elements. While id is a global XML identifier which must be unique within 
the entire document, n provides a simpler way of numbering within a given 
section. Within any division, the <head> element might be used to identify a 
heading prefixed to the start of any textual division, at any level. 
Within each division, the paragraph is the fundamental organisational unit for all 
prose texts, being the smallest regular unit into which prose can be divided. The 
paragraph is marked using the <p> element. The TEI specifications do not 
provide for a way to annotate individual sentences, however they offer the <s> or 
<seg> ('segment') elements for any "consistent internal subdivision of 
paragraphs". We adopt for our framework the <s> element as a sentence marker. 
<div type="story"> 
<headtype="sub"> 
President pledges safeguards for 2,400 British 
troops in Bosnia 
</head> 
<head type="main"> 
Major agrees to enforced no-fly zone</head> 
<p>Greater Western intervention in the conflict in 
former Yugoslavia was pledged by President Bush ... </p></di. > 
Within the textual material, it might be possible to mark a phrase or passage 
attributed by the narrator or author to some agency external to 
the text with the 
<q> tag. 
The following elements arc provided for the encoding of 
lists, their constituent 
items, and the labels associated with them: 
er 5 Metadata model of the Document_lliner Framework 
" <list> contains any sequence of items organized as a list. The attribute 
type describes the form of the list. 
" <item> contains one component of a list. 
" <label> contains the label associated with an item in a list 
An example: 
On those remote pages it is written that animals are divided into 
<list> 
<item n="a">those that belong to the Emperor, </item> 
<item n="b">embalmed ones, </item> 
<item n="c">those that are trained, <litem> 
</list> 
We also find it useful to keep the TEI element note, for the encoding of 
discursive notes, either already present in the copy text or supplied by the 
encoder. 
Individual tokens are identified as <tok>. The final section of all documents will 
be marked by the element <Ann> (Annotations) where all of the lexical and 
semantic annotations are placed. 
5.3.2.2.2 Lexical Annotations 
Lexical Annotations are used to mark sequences of tokens, which can be of 
interest in our framework. One of the main purposes of these types of annotations 
is to instantiate templates representing facts involving these elements. They 
include (but are not limited to): 
" Named Entities in the classical MUC sense 
" New domain-specific Named Entities 
" Terms 
" Temporal Entities in the TimeML sense 
All of these entities will be tagged as <NAMEX> which carries 
the non-optional 
attributes id, type and tokId. The 
id is the tags unique identification and tokId 
lists the tokens that form the entity. The type might 
be one of: 
" Person 
" Organisation 
Chapter 5 Metadata model of the Document. lliner Framework 
" Location 
" Money 
" Percent 
" Quantity 
" Definition 
" Product 
" Role 
" Consumer 
" Pcycle 
" Bcycle 
" Term 
" Event 
" Timex3 
" Signal 
For simplicity reasons, all old MUC-style elements are replaced by NAMEX. 
However, while the name of the tag has changed from ENAMEX and NUMEX 
to NAMEX, the function of the tags has not changed. They can still be identified 
using the type attribute. The values of "person", "location" and "organisation" 
correspond to the old ENAMEX. The values of "money" and "percent" 
correspond to the old NUMEX. So it is always possible (with a trivial 
transformation) to recreate entities in the original MUC style. This might be 
useful for a number of reasons, for instance if we wanted to test the IE system 
against the MUC evaluation suites and know how it is doing compared to the 
competition. 
Each type is described below, with examples provided. As mentioned above for 
the examples we will utilise the xml look like format in order to be more reader 
friendly. Additionally for the same reason the tag <tok> and the <NAMEX> 
attribute "tokId" are ignored. 
Person 
The NAMEX entities of type "person" correspond to the traditional MUC-style 
named entities of type person (<ENAMEX type="person">), which are used to 
annotate names of persons, following the MUC conventions. 
They should include 
(if available) first name, middle name or 
initials, surname. Titles such as "Mr. " 
and role names such as "President" are not considered part of a person name. 
rater 5 Metadata model of the DocumentMiner Framework 
However, appositives such as "Jr. ", "Sr. ", and "III" are considered part of a 
person name. 
<NAMEX id="nl" type="person"> Henry Blair. Jr. </NAMEX> 
Organisation 
The NAMEX entities of type "organisation" correspond to the traditional MUC- 
style named entities of type organisation (<ENAMEX type="organisation">), 
which are used to annotate organisations. Various types of proper names that are 
to be tagged as organisation include stock exchanges, multinational 
organisations, political parties, orchestras, unions, non-generic governmental 
entity names such as "Congress" or "Chamber of Deputies", sports teams and 
armies (unless designated only by country names, which are tagged as 
LOCATION). Corporate designators such as "Co. " are considered part of an 
organisation name: 
<NAMEX id="n2" type="organisation"> Bridgestone Sports Co. 
</NAMEX> 
Location 
Examples of place-related strings that are tagged as LOCATION include named 
heavenly bodies, continents, countries, provinces, counties, cities, regions, 
districts, towns, villages, neighbourhoods, airports, highways, street names, street 
addresses, oceans, seas, straits, bays, channels, sounds, rivers, islands, lakes, 
national parks, mountains, fictional or mythical locations, and monumental 
structures, such as the Eiffel Tower and Washington Monument, that were built 
primarily as monuments. They correspond to the traditional MUC-style named 
entities of type location (<ENAMEX type="location">). 
If the name of the location refers to the organisation or business that is based 
there and not its location or facilities, then it is still marked as a LOCATION. 
<NAMEX id="pl" ty, pe="location"> Rome </N. AN1EX> announced vet another tax 
amnesty. 
The phrase "of <place-name>" following an organisation name may or may not 
be part of the organisation name proper. 
The MUC guidelines state that. (1) If 
Chapter 5 Metadata model of the DocumentMiner Framework 
there is a corporate designator, it marks the end of the organisation name: (2) if 
there is no corporate designator, the "of <place-name>" is part of the organisation 
name. Examples: 
<NAMEX i d="p3" type= "organ i sation">Hyundai of Korea, Inc. </NAMEX> 
<NAMEX id="p2" type="organisation"> Hyundai, Inc. </ENAMEX> of 
<NAMEX id="p4" type="location"> Korea</ENAMEX> 
Compound expressions in which place names are separated by a comma are to be 
tagged as separate instances of LOCATION. 
<NAMEX TYPE="LOCATION">Washington</NAM EX>, 
<NAMEX TYPE="LOCATION"> D. C. </NAMEX> 
According to the MUC guidelines, the adjectival forms of location names (e. g. 
American universities) should not be tagged as locations. Designators that are 
integrally associated with a place name are to be tagged as part of the name. For 
example, include in the tagged string the word "River" in the name of a river, 
"Mountain" in the name of a mountain, "City" in the name of a city, etc., if such 
words are contained in the string. 
<NAMEX TYPE=" LOCATION">Mississippi River</NAMEX> 
Historic-time modifiers ("former", "present-day") and directional modifiers 
("north", "south", "east", "west", "upper", "lower", and combinations thereof) 
should be tagged only when they are intrinsic parts of a 
location's official name. 
as in "Upper Volta" or "North Dakota". 
Do not include them in tagged 
expressions when used as ad hoc modifiers that are readily separable 
from the 
name: 
former <NAMEX TYPE=" LOCATION"> 
Soviet Union</NAMEX> 
Money 
The NAMEX entities of type "money" corresponds 
to the traditional MUC-style 
numerical entities of type money 
(<NUMEX type="money">). The entire string 
Chapter 5 Metadata model of the DocumentAfiner Framework 
expressing the monetary (or percentage) value is to be tagged, including the 
currency specification. The word "minus, " or the minus sign. should be included. 
<NAMEX id ="nl" type="money">$1.7 million</NAMEX> 
Percent 
This is similar to the one above, only the value of the type attribute changes. 
<NAMEX id ="n2" type="percent"> 88% </NAMEX> 
<NAMEX TYPE="percent">minus 15 percent</NAMEX> 
Quantity 
Any numerical quantity (except those that are captured by the "money" and 
"percent" types) might be tagged using the type "quantity". Any unit of measure 
further specifying the quantity (e. g. "mg" in the example below) should be 
included. This tag does not correspond to any pre-existing specification. 
The human body contains about 
<NAMEX id="n9" type="quantity"> 
50 mg 
</NAM EX> 
<NAMEX id="nlO" type="quantity"> 
per kg body weight 
</NAMEX> 
Definition 
This annotation is used to tag chunks that describe Named 
Entities. These will 
be useful for automatically classifying Named 
Entities but will create noise 
during partial analysis of the text. 
Charles Brown, 
<NAMEX id="n1" type="definition"> 
the world famous breeder of beagles 
</NAMEX> 
has launched a new range of Snoopy Kcnnel(TM). 
1,;  
Chapter 5 Metadata model of the Document, lliner Framework 
Product 
This type covers any entity produced by any other entity. This includes products 
from commercial organisations as well as products from research institutions. 
Charles Brown, 
<NAMEX id="nl" type="definition"> 
the world famous breeder of beagles 
</NAMEX>, 
has launched a new range of 
<NAMEX id="n2" type="product"> 
Snoopy Kennel(TM) 
</NAMEX>. 
Title 
This type of annotation refers to any title that can be assigned to a person, such as 
"Mr. ", "Dr. '', "Prof. ", etc. 
<NAMEX id="nl" type="title"> Dr. </NAMEX> 
<NAMEX id="n2" type="person"> 
Torben Svejgaard 
</NAMEX>, 
Role is used to annotate indications of a person's position in an organisational 
hierarchy. 
<NAMEX id="n2" type="person"> 
Torben Svejgaard 
</NAMEX>, 
<NAMEX id="n5" type="role"> 
president of emulsifiers 
</NAM EX> 
<NAMEX id="n4" type="definition"> 
Danish ingredients giant 
</NAM EX> 
<NAMEX id="n3" type="organisation"> 
Dansico 
Chapter 5 Metadata model of the DocumentMiner Framework 
</NAM EX> 
Consumer 
Identifies an individual consumer or consumer group, which is the target of a 
product which is simply a recognized interest group. 
on some packaging to help 
<NAMEX id="n6" type="consumer"> 
health-conscious consumers 
</NAMEX>identify how quickly... 
Pcycle 
Any word or phrase that indicates a stage in the products cycle of 1i fe is tagged as 
a <NAMEX> of type "pcycle". From "research" to "development", from 
"testing" to "certification", from "launch" to "marketing" and eventual 
"discontinuation" of a product. 
<NAMEX id="n2" type="definition"> 
Juice manufacturer 
</NAM EX> 
"Wild About Fruit" (WAF) 
<NAMEX id="9" type="pcycle"> 
markets 
</NAMEX> 
one of the first 
<NAMEX id="n5" type="product"> 
products 
</NAM EX> 
to use the symbol in Coles supermarkets. 
Bcycle 
Acts in much the same way as pcycle but indicates the stages 
in the life cycle of 
an organisation. 
<NAMEX id="n5" type="organ isation"> 
Easy) et 
</NAMEX> 
Chapter 5 Metadata model of the DocumentMiner Framework 
intends to 
<NAMEX id="10" type="bcycle"> 
expand 
</NAMEX> 
its share of the European market 
A term is a means of referring to a concept of a special subject language; it can 
be a single word form, a multiword form or a phrase. It is important that it has a 
"special reference": the term is restricted to refer to its concept of the special 
domain. The act of defining fixes the special reference of a term to a concept. 
Thus, it makes no sense to talk of a term not having a definition. A concept is 
described by defining it (using other certain specialised linguistic forms (terms) 
and ordinary words), by relating it to other concepts, and by assigning a linguistic 
form (term) to it. 
In general, if we can detect with some confidence that some form or expression 
in a text is a term, this helps the IE experts in several ways: 
1. can avoid trying to analyse it is a named entity 
2. if it is a terminological verb, it may help predict a named entity as an 
agent or patient, for example it can (if it is formed from several words) 
help us with problems of scope and ambiguity (by resolving these or 
doing away with the need to further analyse) 
More generally, especially if we are interested in fact extraction from densely 
terminological texts with few named entities apart from perhaps names of 
authors, names of laboratories, and probably many instances of amounts and 
measures, then we would need to rely much more on prior identification of terms 
in the texts, especially where these are made up of several word forms. 
A term can have many variants e. g. singular, plural forms of a noun. 
Thus we 
should perhaps more correctly refer to a term 
form, at least when dealing with 
text. Among variants one can also include acronyms and reduced 
forms. 
Therefore, term variants, referring to the same concept in a special 
domain: they 
are all terms (or term forms). This 
is what makes term extraction an interesting 
Chapter S Metadata model of the DocumentMiner Framework 
activity, as the variety of term forms referring to some concept has got to be dealt 
with. 
The term best representing its concept is called preferred. Non-preferred terms 
are synonyms and point at the preferred term. Synonym's relationship between a 
preferred term and several non-preferred terms represent the fact that several 
terms describe the same concept. Related term relationship applies to sets of 
terms that may be related to each other through occurrences. For any particular 
term, the relationship captures how strongly the term is related (inter-term co- 
occurrence) to a set of other terms. Terms may be arranged into a hierarchy of 
more generic and more specific entries as Broader/Narrower type of relationship 
i. e. "Greece" with "Country". This relationship allows the development of 
Glossaries. 
A terminologist will structure the concepts of a special domain in a relational 
knowledge structure, and assign terms to them. A term can thus refer to e. g. a 
root concept of a hierarchy, to some concept on an intermediate level or to a leaf 
concept. 
Attributes used are the unique id number and an optional domain. 
Men in the developed world have about 
<NAMEX id="n2" type="quantity"> 
500-100 mg 
</NAM EX> 
of iron stored as 
<NAMEX id="n3" type="term"> 
ferritin 
</NAMEX> 
<NAMEX id="n4" type="term"> 
haemosiderin 
</NAMEX> 
Chapter 5 Metadata model of the Document-%finer Framework 
EVENT 
The realisation of an event mainly occurs through verbs and nominalizations. 
Where TENSE and Aspect are determinable from the verbal form, the class is 
dependent on how the expression functions in the document. 
The event belongs to one of the following classes: 
" PERCEPTION -physical perception of another event (watch, hear, 
behold) 
" REPORTING - reported speech (said) 
" ASPECTUAL - aspectual predication 
" STATE - circumstances in which something obtains or holds true. 
" I_STATE - states referring to alternative or possible worlds (believe. 
hope) 
" I_ACTION - Intentional Action (attempt, investigate, delay) 
" MODAL - Modal Verbs 
" OCCURRENCE - all other events. 
The TENSE for all but nominalisations and non-finites and adjectives are 
unsurprisingly: 
  PRESENT 
  FUTURE 
  IRREALIS 
The Aspect of an event is one of: 
" PERFECTIVE 
" PROGRESSIVE 
In complex verb forms only the verbal head is tagged. 
Dyax Corp. has 
<EVENT eid="l" class= "OCCURRENCE" tense="PRESENT" 
aspect="PERFECTIVE"> 
received 
</EVENT> 
<EVENT eid="2" class= "REPORTING" tense="PAST" aspect="NONE"> 
</EVENT> 
Henry Blair. 
TIMEX3 
This tag is used for explicit time references, classified by the type attribute into 
DATE, TIME or DURATION. Additionally, the expression can be further 
qualified by its function in the document: 
  TemporalFunction attribute holds binary value where, 
o True indicates the text string does not contain all of the 
information needed to completely resolve the temporal reference 
("last night"). 
o False indicates a self contained time expression "last night, 2nd 
March 2003". 
  AnchorTimelD is the id number of the time expression to which the tag is 
anchored. 
  AnchorEventlD is the id number of the event to which the tag is 
anchored. 
FunetionlnDocument determines document specific temporal expressions beyond 
DocCreationTime (when the document was released, how long it is relevant for. 
etc. ). As stated above for these temporal expressions , e make use of the 
Dublin 
Core metadata, so this function is not necessary. 
The valueFromFunction is not 
relevant for our framework and is also ignored. 
Chapter 5 Metadata model of the DocumentMiner Framework 
Mary left on Thursday and John arrived the day after. 
Mary left on 
<TIMEX3 tid="tl" type="DATE" temporalFunction ="true" 
anchorTimeID="t0"> 
Thursday 
</TI M EX3> 
and John arrived 
<TIMEX3 tid="t2" type="DATE" temporalFunction = "true" 
anchorTimelD="t1 "> 
the day 
<ITI M EX3> 
after. 
SIGNAL 
This tag marks explicit relations (first, present, start of, almost, this) between two 
temporal entities or the modality of an event or that a verb refers to more than 
one event. Lexical items that trigger <SIGNAL> include temporal prepositions 
(on, in, at), temporal conjunctions (before, after) and modals (may, should). 
<SIGNAL sid="sl"> 
might 
</SIGNAL> 
teach Monday. 
All passengers died 
<SIGNAL sid="s2"> 
</SIGNAL> 
the plane crashed into the mountain 
5.3.2.2.3 Conceptual Annotations 
Conceptual annotations are used to tie the lexical entities together. 
They involve 
two tags: 
9 <Entity> draws together all co referent 
lexical tags 
Chapter 5 Metadata model of the Document_1liner Framework 
" <Relation> defining how these entities are related 
Additionally in conceptual annotations we include two tags (MAKEINSTANCE. 
LINK) adopted from TimeML. As in <Namex>, the <Entity> draws together all 
co references to a single entity (multiple tags pointing to the same concept). On 
the other hand, TimeML uses MAKEINSTANCE to indicate a single tag 
representing multiple events. 
Entity element is best thought of as identifying co reference within documents. 
It can be applied to any of the NAMEX types. Each Entity carries a unique 
identification number, a type (the same as the NAMEX tags it gathers together), a 
mnemonic representation, and the ids of all the NAMEX tags it includes. For 
example, a document containing two NAMEX tags of type person: 
<NAMEX id="nl" type="person"> 
</NAMEX> 
<NAMEX id="n8" type="person"> 
George W. Bush 
</NAMEX> 
Warrants the Entity: 
<Entity id="pet" type="person" mnem="bush" refid="nl n8"/> 
Relation elements tie these Entities together specifying the relation 
between the 
two. The tag carries five attributes: 
"a unique id 
" type 
" source 
" target 
" evidence 
The value of source and target 
is the id of an Entity. The value of the tipe 
attribute defines the relation 
between the source and the target. The attribute 
Chapter 5 Metadata model of the Document finer Framework 
evidence (optional) identifies the NAMEX (if any) which justifies the relation. it 
might also point directly to tokens which are taken as a justification. 
The value of the type attribute can be one of (but not limited, always depend on 
the domain): 
" rolePlayed 
" worksFor 
"i sLocated 
" definedAs 
" productOf 
" productStatus 
" amountOf 
rolePlayed 
Links a NAMEX of type role to NAMEX of type person (people with their 
roles). 
<NAMEX id="n2" type="person"> 
Torben Svejgaard 
</NAM EX>, 
<NAMEX id="n5" type="role"> 
president of emulsifiers 
</NAMEX> 
<NAMEX id="n4" type="definition"> 
Danish ingredients giant 
</NAM EX> 
<NAMEX id="n3" type="organisation"> 
Danisco 
</NAM EX> 
<Entity id=" pet" type=" person" refld="n2"/> 
<Entity id=" pet" type=" role" refid="n5"/> 
<Relation id="prl" type="rolePlayed" source="n2" 
target="n5"/> 
Chapter 5 Metadata model of the DocumentMiner Framework 
"Torben Svejgaard", a person identified by the NAMEX tag n2 is represented as 
the Entity pel along with any other references to this person in the document. 
The role "president of emulsifiers" identified as NAMEX n5 creates the Entity 
pe2. The Relation then states, that "Torben Svejgaard" is the source and 
"president of emulsifiers" is the target in a rolePlayed relation. The final Relation 
could be glossed as "Torben Svejgaard fills the role of "president of emulsifiers". 
worksFor 
This Relation links an employee and an employer. In other words links an Entity 
of type person and an Entity of type organisation. In the previous example this 
would add the Entity tag for the organisation, and the Relation linking the person 
and the organisation. 
<Entity id="pe3" type="organisation" refid="n3"/> 
<Relation id="pr2" type="worksFor" source="pe2" target="pe3"/> 
This Relation states that pe2 "Torben Svejgaard" is the source and pe3 "Danisco" 
is the target in a worksFor relation. 
definedAs 
This relation links any NAMEX entity to a definitional chunk. In the current 
example the organisation is defined as "Danish ingredients giant" adding an 
Entity for this definition and a Relation joining it with the organization. 
<Entity id="pe4" type="definition" refid="n4"/> 
<Relation id="pr3" type="definedAs" source="pe3" target="pe4"/> 
So "Danisco", represented as the entity pe3, 
is "defined. As" the target pe4. 
"Danish ingredients giant". 
Chapter 5 Metadata model of the DocumentMiner Framework 
producedby 
The producedby relation is used to identify which Entity of type organisation or 
person produces an Entity of type product. 
has responded to the recent debate in the Danish Press about the 
<NAMEX id="n6" type="organisation"> 
company 
</NAMEX> 
's fat replace -- 
<NAMEX id="n7" type="product"> 
Salatrim 
</NAM EX> 
<Entity id="pe3" type="organisation" refid="n3 n6"/> 
<Entity id="pe5" type="product" refid="n7"/> 
<Relation id="pr4" type="definedAs" source="pe5" target="pe3"/> 
In this example the "company" a NAMEX of type organisation refers to 
"Danisco" from the previous examples. As such it does not introduce a new 
Entity but is added to pe3 because "company" and "Danisco" are the same. The 
product "Salatrim" does introduce a new Entity pe5 and the Relation pr4 states 
that this product is the source and the entity pe3 is the target in a producedBy 
relation. "Salatrim" is produced by "Danisco" (the company). 
isLocated 
This relates any type of entity to locations. 
To encourage 
<NAMEX id="n9" type="location"> 
</NAMEX> 
<NAMEX id="nl0" type="consumer"> 
consumers 
</NAMEX> 
to increase and improve their physical activity 
Chapter 5 Metadata model of the Document: MMiner Framework 
<Entity id="pe7" type="location" refid="n9"> 
<Entity id="pe8" type="consumer" refid="n10" 
<Relation id="pr4" type="isLocated" source="n10" target="n9"/> 
The two NAMEX tags introduce the Entities pe7 "US" and pe8 "consumers". 
The Relation then makes explicit that the "consumers" are located at the "US". 
productStatus 
This type relation is used to link products to specific stages in its life cycle. 
has announced that its 
<NAMEX id="nll" type="pcycle"> 
patented 
</NAMEX> 
<NAMEX id="n12" type="product"> 
Potassium Hygrogen Glucarate</NAMEX> 
has achieved 
<NAMEX id="n13" type="pcycle"> 
GRAS (generally recognised as safe status) 
</NAM EX> 
<Entity id="pel 1" type="pcycle"> 
<Entity id="pel2" type="prod uct"> 
<Entity id="pel3" type="pcycle"> 
<Relation id="pr9" type="productStatus" source="pelt" 
target="pel 1 "/> 
<Relation id="pr10" type="productStatus" source="pe12" 
target="pel 3"I> 
In this example there is one product identified as the 
NAMEN tag n 12 and the 
Entity pel?. The text declares two facts concerning 
its product cycle, that it is 
"patented" (n] I and pel 1) and that it has achieved 
"GRAS status" (n 13 and 
Chapter 5 Metadata model of the DocumentMiner Framework 
pe l 3). The Relation pr9 identifies the product as the source and "patented" as the 
target whereas Relation prl 0 identifies "GRAS status" as the target. 
amountOf 
The amountOf relation identifies percentages (NAMEX of type percentage) and 
quantities (NAMEX of type quantity) with the entities they quantify (any 
NAMEX type). 
The average amount of 
<NAMEX id="n14" type="term"> 
iron stores 
</NAMEX> 
in women can be estimated to be around 
<NAMEX id="nl5" type="quantity"> 
150mg 
</NAM EX> 
<Entity id="pel4" type="term"> 
<Entity id="pe15" type="quantity"> 
<Relation id="pr10" type="amountOf' source="pe15" target="pe14"/> 
This amountOf relation links the two Entities, pe15 ("150mg") as the source and 
pe 14 ("iron stores") as the target. 
MAKEINSTANCE 
There is a distinction between event "tokens" and event "instances" motivated by 
predicates that represent more than one event. Accordingly, each event creates a 
MAKEINSTANCE tag that subsumes no text. Either, one tag for each realised 
event or a single tag with the number of events expressed as the value of the 
cardinality attribute. 
The tag is introduced and the event or signal to which it refers 
is determined by 
the attributes eventID or signalID. 
16,, -, 
Chapter 5 Metadata model of the DocumentMiner Framework 
The following examples use simplified event tags. 
<EVENT eid="el "> 
taught 
</EVENT> 
on Monday. 
<MAKEINSTANCE eiid="eil" eventlD="el"> 
<EVENT eid="el "> 
taught 
</EVENT> 
on Monday and Tuesday. 
<MAKEINSTANCE eiid="eil" eventlD="e1"> 
<MAKEINSTANCE eiid="ei2" eventlD="el"> 
<LINK> is used to identify the implicit relations. The type of the temporal 
relation (relType) can be one of the following: 
  BEFORE 
  AFTER 
  INCLUDES 
  IS INCLUDED 
  SIMULTANEOUS 
  IAFTER (immediately 
after) 
  IBEFORE 
before) 
  INITIATES 
  CULMINATES 
  TERMINATES 
  CONTINUES 
(immediately 
The LINK is divided into three categories. TL1NK marks temporal relations 
between events or between an event and a time. The attribute relType classifies 
the relation. SLINK is a subordination link that is used for contexts 
involving 
negation, modality, evidential, and factives. It interacts with 
four specific event 
types (REPORTING, MODAL, I_STATE and I_ACTION). ALINK is used to 
indicate an aspectual relation between two events. 
The boat 
Chapter S Metadata model of the DocumentMiner Framework 
<EVENT eid="e1" class="ASPECTUAL" tense="PAST" aspect="NONE"> 
began 
</EVENT> 
<MAKEINSTANCE eiid="eil" eventlD="el"/> 
<SIGNAL sid="sl"> 
</SIGNAL> 
<EVENT eid="e2" class="OCCURRENCE" tense="NONE" aspect= "NONE"> 
</EVENT> 
<ALINK eventlnstancelD="eil" signallD="sl" relatedToEvent="e2" 
relType="INITIATES"/> 
5.3.2.2.4 Templates 
As we have stated above information extraction has been defined as the 
extraction of information from text in the form of text strings which are placed 
into slots labeled to indicate the kind of information that can fill them. 
The approach proposed is to consider such templates as a "view" (in the sense of 
the database domain) over the knowledge base. Different entities can be tied 
together into a template-like structure using an entity or an event as a core object 
of the template and a set of relations. 
An example of a template used to describe an event: 
<sentence id="sl "> 
<tok id="tl ">Dyax</tok> 
<tok id="t2">buys</tok> 
<tok id="t3">enzymes</tok> 
<tok id="t4">for </tok> 
<tok id="t5">$100000</tok> 
<tok id="t6">. </tok> 
</sentence> 
<NAMEX id="nl" type="organisation" tokld="tl">Dyax</NAMEX> 
<NAMEX id="n2" type="product" tokld="t3">enzymes</NAMEX> 
<NAMEX id="n3" type="money" tokld="t4">$100000</NAMEX> 
<EVENT id="el" tokld="t2">buys</EVENT> 
Now going to the conceptual level we get to the 
following: 
Chapter 5 Metadata model of the DocumentMiner Framework 
<Entity id="e1" type="buying" refld="e1"/> 
<Entity id="e2" type="company" refld="n1"/> 
<Entity id="e3" type="product" refld="n2"/> 
<Entity id="e4" type="price" refld="n3"/> 
If we want to paraphrase the above we could say that el is a specific act of 
buying (which is a subtype of event), e2 is a company (Dyax), e3 is a product 
(enzymes) and e4 is a price ($100000). 
Then we can tie these entities together as follows: 
<Relation id="r1" type="buyer" source="el" target="e2" evidence="s1"/> 
<Relation id="r2" type="bought" source="el" target="e3" evidence="s1"/> 
<Relation id="r3" type="amount_paid" source="el" target="e4" 
evidence="sl "/> 
These relations can be paraphrased as saying: in the event 'el' the buyer is the 
object 'e2' (Dyax), the object bought is the object 'e3' (enzymes) and the amount 
paid is the object 'e4' ($100000). We could further add spatial and temporal 
properties to the event 'e l' if those were expressed in the original text. 
Additional classes adopted from the CWM include: 
BusinessDomain (or KnowledgeDomain) 
This represents a business (or Knowledge) domain. It is an area of knowledge or 
activity characterised by a set of concepts and terminology understood by 
practitioners in that area. It is described by the name of the domain. The 
relationship with the taxonomy class identifies the taxonomies owned by the 
BusinessDomain. 
Concept 
This represents a business idea or notion. In order to represent concepts we use 
Terms depending always in our business domain. Topic's of interest for the 
knowledge workers are modelled as concepts. Each domain expert chooses terms 
to relate to these concepts and most of the times the chosen terms are subset of 
the terms that refer to the same concept. The relationship with taxonomy class 
identifies the taxonomy that owns a concept. Related concept identifies the 
related concepts. 
Glossar' 
Chapter 5 Metadata model of the DocumentMiner Framework 
This represents a collection of terms that are related through implicit/explicit 
relationships. 
The relationship with Taxonomy class identifies the taxonomies that the glossary 
is derived from. The relationship with Term class identifies the terms that are 
owned by the Glossary. 
Taxonomy 
This represents a collection of concepts that form an ontology. The relationship 
with the BusinessDomain identifies the BusinessDomain that owns the 
taxonomy. With the Concept class identifies the concepts that are owned by the 
taxonomy. 
Chapter S Metadata model of the DocumentMiner Framework 
Document 
front 
is split 
div sections 
p_paragraphs 
s_sentence 
tok_tokens NAMEX lexical 
type of entities 
tokid 
EVENT_NAMEX TIMEX3_NAMEX 
class 
type of time 
tense 
TemploralFunction 
AnchorTimelD 
aspect AnchorEventlD 
Glossary Taxonomy 
denved owns 0.. 1 
' o.. 1 
concepts owned 
synonym wider term - 
-- related concept Term 
prcfcrredTcrm -- narrower term --- -- 
Concept 
represents- 
concept 
Entity Relation 
id id 
type of entities 
type of relation 
source 
refid 
target 
evidence 
Conceptual 
Figure 24 Extracted information metadata, logical model 
BusinessDomain 
5.3.3 Collecting and storing metadata 
The Dublin Core is the foundation for content metadata. Ideally we collect 
metadata along with the content of the documents of interest. The 
Dublin Core 
metadata can be included in documents using either the 
Dublin Core Structured 
Values encoding or the XML-based RDF [61 ] format. 
Chapter 5 Metadata model of the Document_tliner Framework 
The Dublin Core Structured Values (DCSV) allows metadata to be embedded 
within an HTML document using META tags. The DCSV standard is 
recommended by Dublin Core initiative for serialising metadata as a text string. It 
was designed to allow multiple occurrences of values and to minimise the use of 
restricted characters in HTML. DCSV includes two types of strings, labels and 
values. Labels are the name of the metadata element, such as DC. Creator. Values 
are simply the data associated with the element. 
While this representation has the advantage of being human readable and easily 
implemented within HTML, the Resource Descriptor Format (RDF) uses an 
XML-based encoding to overcome the limitations of HTML-based encoding 
schemes. 
The RDF is a standard developed by the World Wide Web Consortium to store 
resource metadata. RDF was developed, partly, with the goal of providing a 
complete method for representing the Dublin Core as this was primarily 
concerned with the semantics of metadata. The RDF is more of a syntax 
specification for implementing the semantics defined by others, such as the 
Dublin Core. Like the Dublin Core, the RDF is based upon attribute value 
pairings. 
Both the DCSV and RDF formats of the Dublin Core can be stored along with 
the documents. Additionally the metadata that we propose for the extracted 
information are easily mapped into a database schema. In DocumentMiner 
framework we follow the approach of extracting the metadata and keeping them 
in a relational database. There are several reasons of doing that: 
" Other types of metadata, such as process or technical metadata are 
kept in 
the relational database. For all types of metadata, easy management 
facilitated by a single metadata repository. 
" For both DCSV and RDF format, the metadata would need 
to be parsed 
each time they are needed. 
AHN iTe, 1 
Chapter 5 Metadata model of the DocumentMiner Framework 
" When stored in a relational database, the metadata can be indexed as one 
would index other data. 
0 Integration with other applications that utilise relational databases as their 
repositories is possible. 
5.4 Conclusions 
In this chapter we conclude the presentation of the DocumentMiner metadata 
model. The DocumentMiner conceptual framework, as discussed in chapter three 
is complemented by the DocumentMiner metadata model. The metadata model is 
extended into main areas that include basic linguistic, information extraction and 
other metadata necessary for the management of information through the text 
mining process. Moreover, this support covers both model and domain 
knowledge elements. 
Metadata is essential part of our proposed framework due to the database 
oriented architecture. Our model includes concepts derived from various existing 
standards such as the Dublin Core, Common Warehouse Metamodel, Message 
Understanding Conference (MUC7), TimeML, Text Encoding Initiative TEI etc. 
Each one assists the development of the DocumentMiner's model. 
A major issue of our work is the incorporation of information extraction metadata 
along with the other types of metadata. This is actually, the bridge between 
linguistic processing and the data mining components of our framework. 
Another important matter is the incorporation of time. TimeML seemed to be 
fully compatible with our metadata scheme (for extracted information) as both 
have a natural separation between lexical annotations and conceptual annotations. 
For example, on the level of lexical annotations, TimeML marks: 
EVENTS, 
SIGNALS, TIMEX3. On the level of conceptual annotations TimeML marks 
MAKEINSTANCE, LINK. 
Chapter 5 Metadata model of the DocumentMiner Framework 
In the next chapter we will describe our innovative use of temporal association 
rules in text. 
6. Analysis- Association in Text over Time 
6.1 Introduction 
Association rules describe how often two facts happen together. For instance an 
association rule that could exist is: "most companies from the IT sector tend to 
collaborate with companies from the biology sector that do research in the area of 
genetically modified food products". 
In this chapter we present our method of mining association rules in text 
collections under our proposed text mining framework. Association rules 
discovery is one of the mining operations that could be applied and helps us to 
demonstrate the advantages of our framework. Our approach utilise an algorithm 
[52] based on decision trees and expand the notion of association rules by 
incorporating time and ontologies to the relationships discovered. The concepts 
of temporal confidence and temporal support are introduced. Additionally we 
propose the use of a domain specific ontology to drive the discovery of the 
association rules on the conceptual levels that the analyst desires. 
6.1.1 Conventional Definition of Association Rules 
An association rule is a rule, which implies certain association relationships 
among a set of objects. Usually this set of objects is stored in a database and in 
text mining potential objects include terms, concepts that refer to company 
names, persons, locations etc. The formal definition of this 
kind of rules is the 
following. [49] 
Chapter 6 Temporal Associations in Text 
Let I= {i1, i2, 
im If be a set of items. Let DB be a database of transactions, 
where each transaction T consists of a set of items such that TcI. Given an 
itemset XcI, a transaction T contains X only and only if XcT. An association 
rule is an implication of the form X=Y holds in DB with confidence c if c' o of 
transactions in DB that contain X also contain Y. The association rule XY has 
support s in DB if s% of transactions in DB contain XuY. The task of mining 
association rules is to find all the associations rules whose support is larger than a 
minimum support threshold and whose confidence is larger than a minimum 
confidence threshold. 
6.1.2 Association Rules and Decision Trees 
The problem of discovering associations from data was introduced by Agrawal et 
al. [49]. It was followed by successive refinements, generalisations and 
improvements. Among these we can find improved algorithms for the discovery 
of frequent itemsets, generalised and quantitative associations rules, and new 
measures for various types of data. Most of the algorithms, which are based on 
Apriori, vary on the method of generating the candidate itemsets. 
Previous work about data mining that includes temporal aspects is most of the 
times related to the sequence of events' analysis. Usually the objective is to 
discover regularities in the occurrence of certain events and temporal 
relationships between the different events. 
The algorithm [52] for mining association rules that we utilise and expand 
based on decision trees. A decision tree is a collection of If-Then rules that are 
expressed by its individual paths. This is a semantic characteristic of 
decision 
trees and our method for finding associations rules 
is based on it. Since the 
method we are using is based on decision trees, 
it assumes the selection (hy the 
user) of a set of attributes that must appear 
in the "Then" part of the rules. This is 
actually the main difference between the algorithm we work with and 
the well 
known Apriori algorithm [51]. In Apriori no attribute 
is distinguished and 
moreover any attribute can occur either 
in the "If' or the "Then" part of the rule. 
Chapter 6 Temporal Associations in Text 
Additionally an important issue of the algorithm we are using is the generation of 
multiple decision trees. After the user selects the desired attribute for the "Then" 
part of the rule a set of decisions trees can be constructed based on this attribute. 
For each of these trees, each path from the root to a node represents one or more 
rules of the form IF (conditions) THEN (target attribute). 
Decision tree algorithm uses a subset of the available attributes in the test 
conditions of the internal nodes. Therefore, the rules that are extracted from a 
decision tree concern only a subset of the available attributes. As a consequence 
useful and informative rules may exist from the remaining attributes and these 
rules cannot be extracted from a single tree. This is the reason for generating 
several trees in order to extract more association rules from the available data. 
Each of these decision trees have to be trained on a subset of the available 
attributes in order to find rules that concern them. 
The process of generating a set of decision trees is as follows. Initially a set of 
combinations of the available attributes is generated. Due to the huge number of 
the combinations only a subset of all possible attribute combinations is generated 
and examined. The user defines the maximum size k allowed for a combination 
of attributes. For each of the generated combinations of attributes a decision tree 
is constructed. From each of these trees, new rules can be extracted and from 
these a rule is considered new if it refers to all the attributes of the examined 
combination of attributes. If a rule refers to less attributes. it is not considered as 
new, because it should be discovered from a smaller combination of attributes. 
Following this approach it is ensured that a rule does not appear multiple times. 
6.2 Textual Association and Temporal Rules Definition 
The general goal of association extraction, given a set of 
data items, is to identify 
relationships between attributes and items such as the presence of one pattern 
implies the presence of another pattern. These relations may 
be associations 
between attributes within the same data 
item ('Out of the shoppers who Nought 
Chapter 6 Temporal Associations in Text 
milk, 64% also purchased bread') or associations between different data items 
('Every time a certain stock drops 5%, a certain other stock raises 13c between 
2 and 6 weeks later') [50] 
Using formalism similar to the one provided by Agrawal et al [49], association 
rules in text can be defined as follows: 
Let 1= {i 1, i2,.., im } be a set of textual items (analogous to data item). A textual 
item varies from a single word to a multiword term i. e. person's name, location, 
company's name etc. Let FDB (Facts Database) be a database of facts (analogous 
to transactions), where each fact F consists of a set of textual items such 
that FeI. We use the term fact to refer to a set of textual items that refer to a 
specific piece of information that the user is interested. For example, the 
statement that "Sibilo acquired Oracle" is a fact and consists of the textual items: 
Sibilo (Company name), Acquire (Event) and Oracle (Company name). Given a 
textual itemset XeI, a fact F contains X only and only if XcF. An 
association rule is an implication of the form X=Y, where XeI, 
YeI and Xn Y= 0. The association rule X =>Y holds in FDB with 
confidence c (supporting by the rule facts divided by the facts that X is true) and 
has support s (supporting by the rule facts divided by the total facts). The task of 
mining association rules is to find all the association rules whose support 
is larger 
than a minimum support threshold and whose confidence is 
larger than a 
minimum confidence threshold. 
A rule that has a very high confidence is often very 
important, because it provides 
an accurate prediction on the association of the 
items in the rule. The support of a 
rule is also important, since it indicates how 
frequent the rule is in the facts 
database. Rules that have very small support are often uninteresting, since they- 
do not describe significantly large populations. 
An example of such associations may 
be the following: 
The prese'nce of Companj' A is associated with 
the presence off' Technology Area 
B (. c uppor'1 S, confi dc'"ic(e c) 
1"-"o 
Chapter 6 Temporal Associations in Text 
Each set of textual items (each fact) is treated individually in order to discover 
associations. However, in cases where fact histories exist, temporal patterns can 
be discovered. 
As we have mentioned before our method for finding association rules is based 
on a decision tree. It presumes the selection and distinction of set of attributes 
that must appear in the `THEN' part of the rules. The target attribute is selected 
and discriminated from the others. The target attribute is the one for which there 
is interest on finding associations rules and moreover must appear in the 'THEN' 
part of the rule. In case there is interest in finding rules with more than one 
attribute in the consequent of the rules, than the target attribute is constructed 
from the join of the selected and discriminated attributes. 
For specific temporal dimension, the mining of a data set at different temporal 
intervals results into a series of sets of association rules. Each set of association 
rules is the result of the mining process in one of the examined temporal 
intervals. The strong association rules of one temporal interval apply on the other 
temporal intervals with the same, lower or higher support and confidence. It 
seems that the rule evolution in a temporal dimension is demonstrated by the 
fluctuation of its support and confidence in a series of temporal intervals and thus 
we introduce the terms temporal confidence and temporal support. As a 
consequence it is named as temporal association rule an association rule 
accompanied by the fluctuation of support and confidence 
in a series of temporal 
periods. 
In order to discover temporal association rules, our approach 
focuses on a 
specific time slice that separates into continuous temporal periods of 
equal 
length. These temporal periods are the key in order to find the 
fluctuations of 
rule's support and confidence. The temporal associations are 
in the form of 
In period t the presence of Company .1 and 
Company B is associated with the 
presence of Technolo , Area C (supports and confidence 
c over time) 
111-111 
Chapter 6 Temporal Associations in Text 
An example of associations over time: 
IF Company IS CombiChem THEN Collaboration Subject IS Drug screening 
  Support 
0 Confidence 
Figure 25 Example of associations over time 
As a consequence of the above discovered pattern we can see that the 
involvement of the company CombiChem in the area of drug screening was high 
in the beginning of the year. 
6.2.1 Use of temporal association rule as a meta-rule 
As we have defined the temporal association rules it is clear that they can be used 
in order to derive meta-rules that concern the "birth", "death" and "resurrection" 
of discovered patterns. Actually these concepts describe the stability or change of 
a rule over the selected temporal dimension. 
As "birth" of an association rule is defined its first appearance in a temporal 
interval with support and confidence above the predefined minimum thresholds. 
On the other hand the first appearance in a temporal interval with support and 
confidence below the predefined minimum thresholds is considered as its 
"death". As "resurrection" of an association rule is its re-birth after a number of 
temporal intervals for which it was "dead". 
One sub-product of the idea is the following: If we exclude the temporal periods 
that the rule was "dead" then the total support and confidence which 
is computed 
from the support and confidence of each temporal period will 
be higher. Due to 
the higher total support and confidence the significance of the rule will 
Jan Feb Mar Apr May Jun Jul Aug Sept Oct Nov Dec 
Chapter 6 Temporal Associations in Text 
increased. In contrast if we had not excluded the periods that the rule was "dead" 
then with small support and confidence we may had lost the rule which could be 
significant for a specific temporal period. 
IF Company IS CombiChem THEN Collaboration Subject IS Drug screening 
[For month: March] 
6.2.2 Ontology driven association discovery 
The most important reasons, for involving ontologies in the discovery of 
association rules, is that; by utilising ontologies the generalised data set becomes 
more abstract and meaningful. The user is able to direct the analysis at the 
required conceptual levels and extract valuable knowledge that satisfy his needs. 
By using generalisation rules based on ontologies, values of the attributes are 
generalised at multiple concept levels. As a result of this process, a set of 
attributes is generated for every generalised attribute of the original data set. The 
generated attributes are instances of the same attribute but in different concept 
levels. Each of these generated attributes is the generalisation of the attribute of 
the previous concept level and its values are the basis for further grouping and 
generalisation in order to form the attribute of the next higher concept level. 
Template Slot Generalised Template Slot 
Company Market sector that the company belongs 
i. e. Pharmacopeia i. e. Drug sector 
Collaboration subject Generalised collaboration subject 
i. e. Collection of molecules i. c. Micro molecules sector 
Table 6 Examples of ontology driven generalisation 
After applying ontology driven -, cneralisations. 
association rules can he 
discovered. The derived association rules include both the 
initial and the 
generalised attributed. Often in text mining 
facts does not appear so frequent in 
Chapter 6 Temporal Associations in Text 
the document collections that we keep. Generalisation of objects such as 
companies, products, etc. into market sector, technology area etc. accordingly 
provides a means of producing frequent facts and thus detects useful patterns. 
6.2.3 Steps of the mining process for association rules 
At this stage, textual data sets already extracted from the documents are stored in 
the database. In section 4.2.5 the step of "structured textual data preparation " 
has been described and is the necessary before moving into the mining step. It is 
important to mention at this point that we have included the generalisation task as 
a preparation task in section 4.2.5, but in the case of the association rules we may, 
have to repeat this task under the mining step. The reason is that under 
preparation step, generalisation is useful in order to produce more meaningful 
textual data sets. For example, attributes that have a large number of distinct 
values. On the other hand under the mining step defining generalisations is 
actually a way to drive the discovery on the desired conceptual levels. 
Having completed the "structured textual data preparation" step we are ready to 
move into the mining step and except the general tasks that we have described in 
section 4.2.6 "modelling-apply mining algorithm " the following tasks are 
necessary for applying our approach for association rules discovery: 
According to the specific mining scenario 
1. User selects target attribute. Target attribute is the one for which there 
interest on finding association rules. 
2. User defines the conceptual level for the attributes (including the target 
attribute) 
An additional task appears in case of the temporal association rules. 
3. User must select a specific temporal 
dimension. In order to discover 
temporal rules the algorithm utilise specific time slices that separates 
continuous temporal periods of equal 
length. These temporal periods are 
the most crucial point in order to 
find the temporal support and 
confidence. After the selection of the 
temporal attribute the user should 
define the length of the temporal periods that are going 
to he examined. 
Chapter 6 Temporal Associations in Text 
For example this could be into minutes, hours, days etc. Finally the 
starting and ending points are defined. 
6.3 Conclusions 
In this chapter, with the application of discovering temporal associations under 
DOCUMENTMINER framework, we conclude the presentation of our approach 
elements. We have utilised an existing algorithm which we have modified and 
applied in a text mining scenario. 
We have expanded the notion of association rules by incorporating time and 
ontologies to the associations discovered. The concepts of temporal confidence 
and support are introduced. 
In the following chapter that concern the evaluation of the proposed framework 
we will present adequate number of examples that will demonstrate the ideas that 
appeared in the current chapter. 
7. Evaluation 
7.1 Introduction 
After discussing the main concepts of DocumentMiner's framework in chapter 
three and the presentation of major issues in chapter four, five and six we can 
now illustrate their application in a real situation. So far, we have only discussed 
the role of the various concepts and the DocumentMiner overall architecture. We 
have also discussed about the text mining process of our framework, the 
metadata model that supports our database oriented architecture and the 
application of associations (and temporal rules) into text. In this chapter, the 
discussion is extended to a specific domain. 
Therefore, it is necessary to demonstrate the remaining parts of our framework 
and provide an overall view and evaluation. The presentation concludes with a 
discussion about the DocumentMiner framework and environment. 
7.2 The Evaluation Procedure 
The overall evaluation was achieved with the application of the 
Document\tincr 
framework to a case study in order to investigate the applicability of our 
approach, its advantages and disadvantages and 
finally investigate possible issues 
that may rise during its application. 
The top-level theoretical overview of our 
framework was presented in chapter 
three. This discussion was abstract as all descriptions took place 
in meta-level 
realm. In order to instantiate these concepts and 
hence evaluate our frame%\ ork a 
Chapter 7 
Evaluation 
case study is deployed and it will be presented in the next sections. The domain 
chosen is competitive intelligence upon texts that describe collaborations in the 
biotechnology and pharmaceutical industry. This domain is generic enough to 
include knowledge that spans over the main concepts that define our framework. 
In general, as it was explained in chapter four the top-level tasks for applying the 
DocumentMiner approach to a specific domain are: 
1. Problem understanding and definition 
2. Collecting and understanding unstructured textual data 
3. Unstructured textual data preparation 
4. Converting unstructured textual data to a structured data format 
5. Structured textual data preparation 
6. Modelling-Apply mining algorithm 
7. Evaluation 
8. Deployment 
Additional step that is necessary but not included in the current study is creating 
or reusing an ontology which will drive text mining operations. Also some steps 
that are included in the process are not supported directly by the use of the 
DocumentMiner environment but we mention them as they are necessary for the 
overall text mining process. For example top-level steps that are included in this 
category are number one, seven and eight. 
Once the information of interest is described, extracted and stored from the 
source documents then it can be referenced and analysed throughout the whole 
text mining process. 
From the evaluation point of view we will answer to the following questions: 
9 Is it possible to integrate the DocumentMiner framework with other existing 
text mining operations? 
" How easy was for the knowledge worker to acquire the required 
information. 
" Can we reason about the problem domain 
knowledge? 
" How easy can we adapt the framework 
into new domains? 
Chapter 7 Evaluation 
" How do we examine the quality of the information extracted from the 
documents? 
" How the DocumentMiner environment assists the knowledge discovery- 
tasks? 
In order to answer those questions we will proceed with the investigation of the 
framework application to our case study. In this scenario there will be a series of 
assumptions and certain facets of the domain will be projected in order to 
demonstrate aspects of our approach. 
7.3 The First Case Study 
7.3.1 Competitive Intelligence 
For any modem organisation one of the most important issues nowadays, along 
with knowing its strengths/weaknesses and understanding its customers, is about 
knowing its competitors. Competitive intelligence is defined as "a systematic 
program for gathering and analysing information about your competitors' 
activities and general business trends to further your own company's goals" [55]. 
The major difference between business intelligence (a well known term) and 
competitive intelligence is that BI deals with internal to the organisation sources 
and Cl deals with external. The external sources may vary from news agents, 
public records, annual reports, speeches to competitor's marketing material, and 
other external information sources. 
Another difference is that usually information sources for business intelligence 
are in structured form or in the worst case on some semi-structured form. On the 
other hand CI most of the times utilise textual information which is considered 
unstructured if we exclude the grammatical structure that define the natural 
language texts. Internal data for BI operations are well defined and there are 
specific processes of transforming them before use. The external 
data for CI 
operations comes from a variety of sources with 
different formats, different 
languages, and not most of the times the same vocabulary. 
Thus the initial pre- 
processing transformation for this type of data 
is more difficult and time 
Chapter 7 Evaluation 
consuming. Additionally the role of the user in Cl is more important as he will 
need access to both the processed and the original texts to verify and understand 
the main concepts described by the analysed text. 
In order for someone to make strategic decisions he needs to understand 
competitors, markets, and external factors, such as political and global (or local) 
economic events and this is the area of competitive intelligence for which text 
mining is a valuable tool due to the unstructured (just grammatical) format of the 
information source. 
The core of the CI value proposition lies on the selection of the relevant data, the 
structuring of this information into coherent chunks and drawing inferences on 
the current status, trends and the probability that specific events will occur in the 
future which are useful to the consumers of Cl products and services. Both the 
primary material on which the inferences are based and the inferences themselves 
are a combination of qualitative and "factual" data and information. 
Qualitative information includes various analyses or personal viewpoints of the 
strategic technology, commercialisation, IPR, financing and other business 
related issues of the sector that are found in source materials as well as the 
analyses that are produced by company's CI investigators. These analyses are 
subjective and reflect the understanding and views of various 
industry actors. 
Qualitative information represents value addition since it contains actionable 
interpretations of major issues of the sector as distilled on the basis of the various 
actors' expertise. Example of qualitative information used 
in Cl could be 
discussion points of researchers in scientific articles. The 
drawback of qualitative 
information is that it incorporates the beliefs and intentions of the issuer of that 
information and it is up to the Cl investigator the way 
he will weight the quality 
of each piece of information in order to draw any conclusions 
from it. 
Factual information includes items such as names of a company's management 
team, customer lists of companies, company patent, stock prices, 
business events 
Chapter 7 Evaluation 
that have been announced and much more. Factual information can thus include 
not only numerical values but also non-numerical textual data. 
The difference between factual and qualitative information is that factual 
information is considered to be true whereas qualitative information contains 
views of actors and are thus subject to a degree of believability. While this is not 
normally the case, problems of accuracy can sometimes arise with factual 
information as well. 
7.3.2 Case Study Overview 
The real world case study scenario is concerned with a company 
(www. biovista. com/) that specialises in corporate intelligence (Including 
competitive intelligence) products and services for the biotechnology and 
pharmaceutical industry. These products and services are targeted mainly to 
managers in charge of business development and/or investment strategic level 
issues who wish to make decisions on industry and company developments or 
simply to monitor these on a regular basis. The case study will be a minimal 
scenario, just to demonstrate the approach main concepts and hopefully end up 
with an overall evaluation. 
The web and information providers can supply a wealth of information about 
both company's competitors and market of interest. In particular our case study 
will be applied in a number of documents that describe new alliances and 
partnerships of competitors (Appendix 1; Combinatorics IRCD (Descriptions of 
corporate activities: 1998-1999 YTD)). In an intelligent way we will try to 
discover business trends of competitors or the market in general. Competitors 
moving into markets or the appearance of a new market are of paramount 
importance. This type of analysis provides knowledge of competitors and 
markets so the business analyst is able to support the strategic plans of the 
organisation. 
7.3.3 The process 
Chapter 7 Evaluation 
Before we begun the process of gathering information on competitors we had to 
define some organising principles or else we could find ourselves searching. 
retrieving, and loading documents that do not meet our requirements. 
The first task we have to accomplish in order to collect the relevant documents is 
to define the target competitors. In some industries this is a simple matter i. e. 
automobile. In several other industries such as pharmaceutical is quite difficult. 
For example, while developing and testing new drugs is a complex process, 
which is often done by large companies, smaller companies are often formed to 
develop and market a single drug or use a single technology, such as genetic, in 
order to improve the manufacturing process for some class of drugs. This 
example shows that we should not only choose large competitor companies for a 
large target company but we should include and smaller companies. In fact 
smaller companies in a specific sector may indicate a new dynamic market 
opportunities. 
Some of the companies we have targeted are the following: 
Combichem 
Chromagen 
CEREP 
Sanofi 
Orchid Biocomputer 
Advanced Bioanalytical Services 
Pharmacopeia 
Pharmacia & Upjohn 
Rhone-Poulenc Rorer 
Novartis Crop Protection AG 
ArQule 
Chemrx 
Signal Pharmaceutical 
Figure 26 Sample competitors 
Chapter 7 Evaluation 
The documents we have gathered from business press releases on the internet 
concern the above companies and in particular discuss collaborations among 
them on specific scientific issues. In the following figure we present two of the 
sample documents. 
arch 15,1999 
DmbiChem and Chromagen enter into drug screening agreement 
ombiChem, Inc. (Nasdaq NMS: CCHM) and Chromagen, Inc. entered into a new 
; say development and drug screening agreement. Under the agreement, 
hromagen will develop biological assays and screen CombiChem compounds for 
ultiple targets pursued by CombiChem in its internal drug discovery program. 
hromagen will receive research support and a percentage of future payments 
, ceived 
by CombiChem upon outlicensing of drug candidates to pharmaceutical 
)mpanies. 
ebruary 20,1999 
EREP and Sanofi Expand Collaboration with Additional Targets. 
EREP S. A. (Nouveau Marche: CEREP) and Sanofi expanded their original R&D 
)Ilaboration of 1997 to include the screening of two additional targets provided by 
anofi. The screening will be done using CEREP's proprietary integrated drug 
scovery technology platform. Sanofi will provide CEREP with additional research 
Inds. The collaboration, which began in 1997, has two stages. Initially, CEREP is 
asigning and producing a new library of leads to be used by Sanofi in its in-house 
&D programs. In the second stage, the companies will pursue together the 
scovery and optimization of leads using CEREP's integrated platform technology. 
t the time of CEREP's initial public offering in February 1998, Sanofi also 
)mmitted FF 22.5 million in research funding as part of an equity stake in CEREP. 
Figure 27 Sample collected documents 
The need for the analyst is to identify trends; competitors' movements into new 
scientific areas, potential alliances of competitors and the relationships 
between 
industry specific sectors and technological areas. It is important to mention that 
these relationships over time would add great value for the analyst. 
These are 
types of business problems that initially drive the need for text mining. 
Chapter 7 El Llluation 
hat are the competitors doing? 
Are they moving in new technological 
areas? 
Are they signing new collaborations? 
How have they expanded their product 
line? 
Figure 28 Business problems 
Unlike the above business goal that states the objectives in business terminology 
that the analyst want to achieve, now it is time to define the text mining goal. A 
text mining goal is an operational definition of the goal. In other words the text 
mining goal states project objectives in technical terms. For example in our case 
study one of the business goal might be "Should we move in new technological 
areas? " One of the sub-questions to answer is "Are competitors moving in 
technological areas and which? " The corresponding text mining goal might be 
"Discover, according to the collaborations they have signed the last year our 
competitors, relationships with new technological areas" As a consequence the 
business competition planning could be supported by a text mining operation 
such as trends analysis. The text mining analyst will include the description of 
the intended output of the project that enables the achievement of the business 
objectives. Additionally to the text mining overall goal it is of paramount 
importance an initial specification of the pieces of information (Name entities, 
facts etc. ) that we indeed to extract from the documents in order to feed the 
mining process. These initial specifications will greatly support the assessment of 
linguistic tools and resources. 
The methodology we describe is a general approach that can be applied to text 
databases of varying complexity. In our approach the process of 
detecting 
patterns within and across text documents (Text Mining) 
depends mainly upon 
information extraction techniques. By identifying key entities 
in a text along with 
the temporal characteristics, one can find relationships 
between terms and 
identity unknown links ('hidden') between related topics. 
Chapter 7 Evaluation 
The overall architecture of our system is shown in the following figure. First the 
IE module elaborates input text in order to extract and fill the predefined 
template. Then the generalisation module performs the necessary generalisation 
upon specific slots of the template. Finally the filled templates feed the 
association discovery module in order to extract the rules. In the following 
paragraphs we will present a sample of the results with some terms replaced due 
to confidentiality of the case study. 
Templates 
Generalisation 
v Text Information of Templates I' Associations over Time slots Time 
G. nerosed Templates 
Figure 29 Architecture supporting the Case study 
Information extraction is the technology that will extract from the text the 
required information pieces and as a consequence we have to define them. We 
consider that each fact (or event or template according to IE) defines a 
"transaction" that creates the associations between textual items. In analogy with 
a traditional data mining example, each fact is the corresponding market basket 
and the textual items (template slots according to IE) assigned on it constitute the 
items in the basket. Consequently our general goal of association extraction over 
time, given a set of facts, is to identify relationships between textual items and 
events such as the presence of one pattern implies the presence of another pattern 
over time. The IE tasks involved are part of the fourth step (Converting 
unstructured textual data to a structured data format) of the 
TM process we have 
proposed in previous chapter. For the purpose of the current case study we are 
interested on extracting a specific fact that concern 
Collaborations among 
companies on a specific technology subject and at some point 
in time. The 
ter 7 
Evaluation 
following figures give an example of a source document and the output of the EE 
subsystem with the information pieces that we are interested on extracting. 
January 19,1999 
Pharmacopeia and Pharmacia & Upjohn form research 
collaboration 
The 10th of January 1999 Pharmacopeia, Inc. (Nasdaq: PCOP) 
signed a new multi-year, multi-target research collaboration with Tharmacia & Upjohn Inc. Under the terms of the agreement, Pharmacopeia will screen its multi-million compound sample 
collection of diverse, small molecules against numerous targets 
chosen by Pharmacia & Upjohn each year. In addition to annual 
guaranteed payments, Pharmacia & Upjohn will make further 
payments based on the success of the screening programs. 
optimization of active compounds identified by Pharmacopeia 
may be performed either by Pharmacia & Upjohn or by 
Pharmacopeia, for additional consideration. Financial terms of 
he agreement were not disclosed. 
Figure 30 Sample source document 
Company 1: Pharmacopeia 
Company 2: Pharmacia & Upjohn 
Event-Collaboration Type: Research collaboration 
Collaboration Subject: Screen collection of molecules 
Announced Date: January 19,1999 
Valid Date : January 10,1999 
Figure 31 Fact extracted (template) 
The above fact of interest that concern collaborations among companies is 
defined with the following features: First of all the two companies that 
participate. Then the collaboration ti pe and the collaboration subject that 
presents useful information of the relationship among the two companies. The 
collaboration subject usually includes technology terms of significance to the 
bio-knowledge worker. These technology terms point to technology 
areas/markets and they are utilised in order to represent the current or future 
directions of company activities. The last two features involve time. The 
announced date is the date that the fact was announced or published and the lw/icl 
date is the actual date that the fact happened or is active. All these 
features have 
been defined in cooperation between the text mining analyst and the 
domain 
Chapter 7 Evaluation 
expert. The information derived by the IE system from the text is stored in the 
database according to the schema presented in chapter 5 
<NAMEX id="n1" type="organisation" tokld="t1">Pharmacopeia </NAMEX> 
<NAMEX id="n2" type="organisation" tokld="t3">Pharmacia & 
Upjohn</NAMEX> 
<NAMEX id="n3" type="event", class="occurrence", tense="past" 
tokld="t10">signed research collaboration</NAMEX> 
<NAMEX id="n4" type="term" tokld="t20">Screen collection of 
molecules</NAM EX> 
<NAMEX id="n5" type="Date" temporalFunction="false">10th of January 
1999</NAMEX> 
Going to the conceptual level: 
<Entity id="el" type="organisation" mnem=" Pharmacopeia" refid="n1"/> 
<Entity id="e2" type="organisation" mnem=" Pharmacia & Upjohn" refid="n2"/> 
<Entity id="e3" type="event" mnem=" Research collaboration" refid="n3"/> 
<Entity id="e4" type="term" mnem=" screen molecules" refid="n4"/> 
<Entity id="e5" type="date" mnem=" 10-Jan-1999" refid="n5"/> 
If we want to paraphrase the above we could say that el and e2 are companies 
(Pharmacopeia and Pharmacia & Upjohn), e3 is a specific act of a research 
collaboration (which is a subtype of event), e4 is a term (screen molecules) and e4 
is a date (10-Jan- 1999). 
Then these entities are tied together as follows: 
<Relation id="r1" type="collaborate" source="e3" target="el" I> 
<Relation id="r2" type=" collaborate" source="e3" target="e2" I> 
<Relation id=" r3" type=" collaborate _subject" 
source="e3" target="e4"/> 
<Relation id=" r4" type=" collaborate _date" 
source="e3" target="e5"/> 
These relations can be paraphrased as saying: in the event 'e3' the 
first 
collaborated company is the object 'el' (Pharmacopeia), the second collaborated 
company is the object e2' (Pharmacia & Upjohn), the subject of the collaboration 
the object 'e4' (screen molecules) and the date of collaboration 
is the object 'c5' 
(10-Jan-1999). 
While all the above prepare the IE output a "v 
icw' over the database tichcma \ ill 
result the facts (or template according, to IF) table. 
Chapter 7 Evaluation 
FactlD Companyl Company2 1 coT, 1pe J collabSubiect L DateAn FDateVa 
1 ; Pharmacopeia Pharmacia & Research Screen 19'01 '' 1999 10 01 1999 
Upjohn  molecules 
2 ICombiChem Chromagen Research drug screening 115/03/19991: 15103/1999 
3 RCombiChem Ono Commercial drug discovery 04/01/1999 10'04 2000 
4CombiChem Roche Research 'novel inhibitors 13/05/1998 05/06/1998 
Bioscience 
5 Novartis Crop Pharmacopeia Commercial library 16/12/1998 17/12/1998 
Protection screening 
6 Pharmacopeia Schering A. G Commercial  multi-target 27/10/19981 27/10/1998 
 screening 
7 Bayer AG Novalon Research anti-infective 29/9/1998 29/9/1998 
Pharmaceutical drug screening 
Corporation 
8 - SmithKline j LJL I Commercial high throughput 9/9/1998' 9/9/1998 
Beecham BioSystems screening 
9 Oxford Cambridge Research Leads for 30/6/1998 30/6/1998 
Molecular Combinatorial metabolic 
Group PLC Ltd disorders 
Corporation Agrochemicals screening 
11 Bayer AG Symyx Commercial combinatorial ( 30/3/19981 30/3/1998 
Technologies materials 
(science 
12 Inspire NeoGenesis Commercial leads for 25/3/1998 25/3/1998 
Pharmaceuticals respiratory 
diseases 
13 IDBS Afferent Commercial combinatorial 25/2/1998 25/2/1998 
Systems ( chemistry 
software 
14 Ontogeny Genzyme Commercial gene expression 25/2/1998 25/2/1998 
Molecular 
Oncology 
15 Acacia American Commercial lead selection 20/1/1998 20/1/1998 
Biosciences Cyanamid and 
Company optimization 
Table 7 Collaboration facts 
By using generalisation rules, values of the template slots are generalised at 
multiple concept levels. As a result of this process, a set of slots 
is generated for 
every generalised slot of the original set. 
The generated attributes are instances of 
the same slots but in different concept 
level. This facilitates the discovery of 
association patterns in various conceptual 
levels. 
Chapter 7 Evaluation 
Bioscience Industry 
Bioscience Sector A 
CombiChem 
Oxford Molecular Group PLC 
Alanex Corporation 
Acacia Biosciences 
Schering A. G 
NeoGenesis 
Afferent Systems 
Bioscience Sector B 
Pharmacia & Upjohn 
Chromagen 
Roche Bioscience 
SmithKline Beecham 
Ontogeny 
Genzyme Molecular Oncology 
American Cyanamid Company 
Bioscience Sector C 
Pharmacopeia 
Novartis Crop Protection 
Novalon Pharmaceutical Corporation 
LJL BioSystems 
Cambridge Combinatorial Ltd 
Bioscience Sector D 
Bayer AG 
Inspire Pharmaceuticals 
Zeneca Agrochemicals 
Chapter 7 
Symyx Technologies 
Evaluation 
Template Slot Generalised Template Slot 
Company Bioscience sector that the company belongs 
i. e. Pharmacopeia Bioscience sector C i. e. Molecules sector 
i. e. CombiChem Bioscience sector A i. e. Drug sector 
Collaboration subject Generalised collaboration subject 
i. e. Collection of molecules i. e. Micro molecules sector 
Table 8 Generalised attributes 
So far, the relevant document where collected, linguistically processed (by IE 
module, in order to extract the necessary information) and generalised. The facts 
extracted and the generalised attributes are stored in the database. Few examples 
are shown in the following tables. 
Generalised Fact table... (1/2) 
FactID Companyl GenCompanyl Company2 GenCompany2 
1 Pharmacopeia Bioscience Pharmacia & Upjohn Bioscience 
Sector C  Sector B 
2 CombiChem Bioscience Chromagen Bioscience 
Sector A Sector B 
3 CombiChem Bioscience Ono Bioscience 
Sector A Sector B 
4 CombiChem Bioscience Roche Bioscience Bioscience 
Sector A (Sector B 
5 Novartis Crop Bioscience Pharmacopeia Bioscience 
Protection Sector C Sector C 
6 Pharmacopeia Bioscience Schering A. G Bioscience 
Sector C Sector A 
7 Bayer AG Bioscience Novalon Bioscience 
Sector D Pharmaceutical Sector C 
Corporation 
8 SmithKline Beecham Bioscience LJL BioSystems IBioscience 
Sector B 'Sector C 
Chapter 7 Evaluation 
Generalised Fact table... (1/2) 
FactiD Companyl GenCompany1 Company2 GenCompany2 
9  Oxford Molecular Bioscience (Cambridge Bioscience 
Group PLC Sector A Combinatorial Ltd Sector C 
10 lanex Corporation Bioscience jZeneca Bioscience 
Sector A grochemicals 'Sector D 
11 Bayer AG Bioscience ; Symyx Technologies Bioscience 
Sector D (Sector D 
12 Inspire Bioscience NeoGenesis Bioscience 
Pharmaceuticals Sector D Sector A 
13 IDBS Bioscience Afferent Systems Bioscience 
Sector D Sector A 
14 Ontogeny Bioscience Genzyme Molecular Bioscience 
Sector B Oncology Sector B 
15 Acacia Biosciences Bioscience American Cyanamid Bioscience 
Sector A Company Sector B 
Table 9 Collaborations table (1/2) 
Gen Fact table (212) 
CollabType CollabSubject GenCollabSubject E DateAn DateVal 
Research Screen molecules TechAreaB 19/01/1999 10/01/1999 
Research drug screening TechAreaA 15/03/1999 15/03/1999 
Commercial drug discovery TechAreaA 04/01/1999 110/04/2000 
Research novel inhibitors TechAreaB 13/05/1998 05/06/1998 
Commercial library screening 
TechAreaC 
echAreaC 
16/12/1998117/12/1 998 
27/10/1998 27/10/1998 
Research anti-infective drug screening TechAreaC 29/09/1998 29/09/1998 
[ TechAreaC 09/09/1998 09/09/1998 
Research Leads for metabolic disorders TechAreaB 30/06/1998 30/06/1998 
Commercial agrochemical screening 
J TechAreaC 18/06/1998 18/06/1998 
 Commercial combinatorial materials 
science 
TechAreaA 30/03/1998 30/03/1998 
Commercial leads for respiratory diseases e 25/03/1998 
25/03/1998 
Commercial combinatorial chemistry echAreaA 1 
25/02/1998 25/02/1998; 
Chapter 7 Evaluation 
... Gen Fact table (2/2) 
CollabType CollabSubject GenCollabSubject DateAn DateVal 
software ii -7- 
I_____________ 
Commercial gene expression JTechAreaA 125/02/1998125/02/1998 
commercial Ilead selection and optimization ITechAreaB 
Table 10 Collaborations table (2/2) 
1/1998120/01M 
Once we have extracted the information required from the documents we are able 
to perform temporal data mining algorithms in order to discover valuable 
temporal patterns. The general goal of association extraction, given a set of data 
items, is to identify relationships between attributes and items such as the 
presence of one pattern implies the presence of another pattern. 
The task of mining association rules is to find all the association rules whose 
support is larger than a minimum support threshold and whose confidence is 
larger than a minimum confidence threshold. An example of such associations 
may be the following: The presence of Company A is associated with the 
presence of Technology Area B (support s, confidence c) 
Our initial text mining goal is to "Discover relationships between our competitors 
and new technological areas, according to the collaborations they have signed the 
last year. " The associations extracted with respect to specific queries expressed 
by potential knowledge workers. 
Query: "find all associations between a set of companies or the 
bioscience sector that the companies belong and any technology area" 
Result: [Bioscience Sector A, Bayer AG] --) Combinational chemistry 
software 
According to this mining scenario the following tasks performed: 
  User selects target attribute. Target attribute 
is the one for which there is 
interest on finding association rules. In our case we select 
Chapter 7 Evaluation 
collaboration subject and the generalised collaboration subject. which 
represent the technology area that a specific biotechnolo`-v belongs. 
  User selects the attributes, generalised or not, that will appear in the left 
part of the rule. We select the attribute company I and the generalisation 
of the company 1 that represent the bioscience sector that the company 
belongs. 
We have chosen to present the most significant relations according to the 
measures of confidence and support. Additionally, we present relations that 
appear to be particularly important to the domain expert. This is the case in many 
mining applications of association rules. Associations that have low confidence 
and support may denote great knowledge to the domain expert. 
IF Company IS Bayer AG THEN Collaboration Subject IS anti-infective drug screening 
IF Company IS CombiChem THEN Collaboration Subject IS drug discovery 
IF Company IS Acacia Biosciences THEN Collaboration Subject IS lead selection and optimization 
IF Company IS Alanex Corporation THEN Collaboration Subject IS agrochemical screening 
IF Company IS IDBS THEN Collaboration Subject IS combinatorial chemistry software 
IF Company IS Inspire Pharmaceuticals THEN Collaboration Subject IS leads for respiratory diseases 
IF Company IS Pharmacopeia THEN Collaboration Subject IS Screen molecules 
IF Company IS Novartis Crop Protection THEN Collaboration Subject IS library screening 
IF Company IS Ontogeny THEN Collaboration Subject IS gene expression 
IF Company IS Oxford Molecular Group PLC THEN Collaboration Subject IS Leads for metabolic disorders 
IF GenCompany IS Bioscience Sector A THEN Collaboration Subject IS Leads for metabolic disorders 
IF GenCompany IS Bioscience Sector A THEN Collaboration Subject IS agrochemical screening 
IF GenCompany IS Bioscience Sector B THEN Collaboration Subject IS gene expression 
IF GenCompany IS Bioscience Sector B THEN Collaboration Subject IS high throughput screening 
IF GenCompany IS Bioscience Sector C THEN Collaboration Subject IS Screen molecules 
IF GenCompany IS Bioscience Sector C THEN Collaboration Subject IS mufti-target screening 
IF Company IS Acacia Biosciences AND GenCompany Bioscience Sector A, Bioscience Sector B THEN 
Collaboration Subject IS Screen molecules 
IF Company IS CombiChem AND GenCompany Bioscience Sector A, Bioscience Sector B THEN 
Collaboration Subject IS drug discovery 
IF Company IS Alanex Corporation AND GenCompany Bioscience Sector A THEN Collaboration Subject IS 
agrochemical screening 
IF GenCompany IS Bioscience Sector C THEN GenCollaboration Subject IS TechAreaB 
IF GenCompany IS Bioscience Sector C THEN GenCollaboration Subject IS TechAreaC 
IF GenCompany IS Bioscience Sector BTHEN GenCollaboration Subject IS 
TechAreaA 
IF GenCompany IS Bioscience Sector BTHEN GenCollaboration Subject IS 
TechAreaC 
IF Company IS Pharmacopeia AND GenCompany Bioscience Sector A, Bioscience 
Sector C THEN 
GenCollaboration Subject IS TechAreaB 
IF Company IS Pharmacopeia AND GenCompany Bioscience 
Sector A, Bioscience Sector C THEN 
GenCollaboration Subject IS TechAreaB 
Chapter 7 Evaluation 
Although, the above results provide valuable information to the analyst, of the 
interaction between companies or bioscience sectors and technology areas, the 
real added value of our approach derives from the incorporation of time. 
Each fact (collaborations) is treated individually in order to discover associations. 
However, in cases where fact histories exist, temporal patterns can be discovered. 
In our case study two temporal dimensions exist. The date that the fact 
announced and the actual date that the fact took place. For these specific 
temporal dimension, the mining of the textual data set at different temporal 
periods results into a series of sets of association rules. Each set of association 
rules is the result of the mining process in one of the examined temporal periods. 
The strong association rules of one temporal period apply on the other temporal 
periods with the same, lower or higher support and confidence. It seems that the 
rule evolution in a temporal dimension is demonstrated by the fluctuation of its 
support and confidence in a series of temporal periods. After processing the result 
is of type: 
In period t the presence of Company A and Company B is associated with the 
presence of Technology Area C (support s and confidence c over time) 
Query: "find all associations between a set of companies or the 
bioscience sector that the company belongs and any technology area for 
the period t" 
Result: [Bayer AG] 4 Combinational chemistry software [Rule stronger 
in March] 
Except the tasks demonstrated above an additional task appears in case of the 
temporal association rules. User must select a specific temporal 
dimension. In 
order to discover temporal rules the algorithm utilise specific time slices 
separates into continuous temporal periods of equal 
length. After the selection of 
the temporal attribute the user should define the 
length of the temporal periods 
that are going to be examined. For example this could 
be into minutes, hours, 
Chapter 7 Evaluation 
days etc. Finally the starting and ending points are defined. In our case we have 
selected the dimension of the date that the fact actually happened. Because of the 
nature of the news stories we had to deal, live news, also the date that the fact 
was announced could provide valuable information. For the length of the 
temporal periods that are going to be examined we have selected months. 
Significant results are presented: 
IF Company IS CombiChem THEN GenCollabSubject IS TechAreaA 
[Rule stronger at the beginning of the year and especially March] 
  Support 
O Confidence 
Figure 32 Fact: CombiChem - TechAreaA 
IF GenCompany IS Bioscience Sector A THEN GenCollabSubject IS TechAreaA 
[Rule stronger at the end of the year and especially in November] 
Figure 33 Bioscience Sector A -- TechAreaA 
IF GenCompany IS Bioscience Sector B THEN Collaboration 
Subject IS gene expression 
[Rule stronger at the med and end of the year and especially 
in November] 
Jan Feb Mar Apr May Jun Jul Aug Sept Oct Nov Dec 
Jan Feb Mar Apr May Jun Jul Aug Sept Oct Nov Dec 
Chapter 7 Evaluation 
Figure 34 Bioscience Sector B --) Gene expression 
IF Company IS SmithKline Beecham THEN Collaboration Subject IS high throughput screening 
[Rule stronger in November] 
IF GenCompany IS Bioscience Sector C THEN GenCollabSubject IS TechAreaC 
[Rule stronger in November, December] 
IF Company IS Inspire Pharmaceuticals, IDBS THEN GenCollabSubject IS TechAreaA 
[Rule stronger at the end of the year] 
IF GenCompany IS Bioscience Sector B, Oxford Molecular Group PLC THEN Collaboration Subject IS 
metabolic disorders 
[Rule stronger in September and October] 
IF GenCompany IS Bioscience Sector B, Bioscience Sector C THEN GenCollabSubject IS TechAreaB 
[Rule stronger in September, October, November] 
IF Company IS Inspire Pharmaceuticals, IDBS, Bayer AG THEN GenCollabSubject IS TechAreaC 
[Rule stronger at the med of the year] 
The significance of the above type of results is obvious for the domain analyst. 
The analyst is able to detect movements of specific companies or bioscience 
sectors into technology areas over time. For a particular period of time the 
interest for specific technologies can be discovered and as such competitors. 
Jan Feb Mar Apr May Jun Jul Aug Sept Oct Nov Dec 
Chapter 7 Evaluation 
7.4 The Second Case Study MetaOn 
7.4.1 Overview 
In this section we present an overview of the MetaOn system. The core target of 
MetaOn is to construct and integrate semantically rich metadata extracted from 
documents, images and linguistic resources, to facilitate intelligent search and 
analysis. The MetaOn framework (DocumentMiner framework) involves 
ontology-based information extraction and data mining, semi-automatic 
construction of domain specific ontologies, content-based image indexing and 
retrieval, and metadata management. 
MetaOn project aimed to develop theory, techniques and a prototype system for 
efficient metadata management, flexible querying and knowledge discovery and 
delivery, based on image and text collections. Our approach is applied to a 
number of such multimedia data coming from The Foundation of the llellcnic 
World (FHW) that concerns Hellenic history. In the context of the project FHW 
provides its digital content that is available on the servers of the Cultural Center 
(www. e-history. gr). 
The methodology we describe is a generic approach that can be applied to text 
and image databases of varying complexity. In our approach the process of 
detecting patterns within and across text documents depends mainly upon 
information extraction techniques. By identifying key entities in a text along With 
the image characteristics, one can draw relationships among them. 
The overall architecture of our system is illustrated in Fig. 35. First the document 
and image collector performs basic processing including format transformation 
etc. Then in parallel the text and image processing modules elaborates the 
input 
textiimage in order to extract the key features. The process of text processing 
supported by the ontology. 
Chapter 7 Evaluation 
WP2 WP7 omr. y oorm, n soe Ram k- c ul I 
laogp ca-. n c Pa sus 
pamft C4, 
--------- _" 
----- - _---- I- 
WP5, WP6. Metadata Management 
WP3. i- 
Textprocesmrg 
12 Metadata(NLP malachte. 
Image metadata) 
Data Cc ector I 
(Documents & Images) 
(Defer 
au. o 
base) 
----- Cksteng old aooabon 
Image processing 
Greek & English Lwx 
Resances 
------------ 1 
Search Meths 
Free-text 
Query Image Query 
Search Assistant 
Ontology Cluster Seard Sugoest Rehtee Supoes Retalec 
based Queries Results Sears path Seartr -ems 
u J JJ 
Figure 35 The MetaOn framework 
A metadata repository is being utilized by our approach to store data that describe 
the information extracted from the documents and images. More specifically the 
role of the repository is to provide a neutral medium and unified representation 
forming the basis for analysis. Additionally provides a more solid basis as it 
allows large scale applications in text and images. 
The MetaOn system provides the following search methods: 
  Free-text query. 
  Query Image. 
  Ontology based queries. 
Ontology based queries allow the user by navigating in the ontology to directly 
retrieve the documents based on the ontology based index. Additionally the user 
is able to populate a free-text query with terms selected for the ontology. 
An important component of the MetaOn system is the Search Assistant. 
Search Assistant provides additional information to the user 
in order to support 
Chapter 7 Evaluation 
the search process. The information provided comes from the data mining 
operations that are applied upon the metadata. It includes: 
" Cluster search results. Results retrieved by free-text query are clustered in 
order to be presented as groups to user. In many cases this operation 
detects different thematic domains in the results and helps the user to 
retrieve faster the information needed. 
  Suggest related search paths. The web site ontology is introduced to 
express the semantics of the web site and the notion of pageset is used to 
model the behaviour of the web site visitors and extract usage patterns. 
This subsystem assists users in their browsing tasks by enhancing the 
searching capabilities of the web site by suggesting paths of similar user 
behaviour. 
  Suggest related search terms. Terms are suggested according to their 
semantic relation with the terms that user types in free-text query. The 
related terms are discovered via the term association component from the 
actual documents. 
7.5 Conclusions 
In this chapter, with the application of the DocumentMiner framework to the 
competitive intelligence scenario and the overview of the MetaOn project, we 
conclude the presentation of our approach elements. This was achieved with the 
demonstration of the instantiation of the components that were analysed in 
chapters three, four, five and six. More specifically, we examined how it is 
possible to apply the DocumentMiner concepts into managing and analysing the 
knowledge of a particular domain. This application was realised into two 
dimensions. 
The first dimension was concerned with the representation of the textual items 
extracted from text. These textual items are actually the result of the information 
extraction process IE and the need for representing them adequately 
in order to 
feed the mining components is of paramount importance. Another reason 
such representation is the integration with other applications 
in the domain, 
which is feasible now in the database level. Moreover, the establishment %% 
ontologies assisted text mining analysts with the reasoning about 
interdependencies among, these textual metadata. 
Chapter 7 Evaluation 
The second dimension was related to the mining components of the 
DocumentMiner environment which addressees the practical implications of our 
framework. The DocumentMiner framework assists the domain analysts and the 
text mining analysts into defining, managing, and reasoning about problem 
domain information. 
Evaluation results include for the analysts: 
  Being able to access and analyse unstructured data. 
  Able to store high level information for future use by various tools 
because compatibility is particularly easy due to metadata storage. 
  Able to apply mining algorithms to text via the metadata database 
intermediate. 
  Able to detect movements of companies into new territories 
  Able to discover rules that describe their market area over time. 
8. Conclusions & Future Work 
8.1 The DocumentMiner Approach 
In order to justify the contribution of this thesis in the domain of knowledge 
discovery in text, we investigated and studied in depth the current trend and the 
past achievements of commercial attempts and we came to a literature review of 
the approaches that have been introduced. The outcome of this review was that 
all of the approaches followed till now, insufficiently dealt with deep 
understanding of text, did not covered the temporal issues, no support of 
metadata and as a consequence pour architectures not compatible with other 
approaches etc. Additionally, for clearly related technologies such as information 
extraction, data mining and ontologies there was no common framework 
supporting their interaction. The challenge for us was to define the notions and 
provide solutions that will satisfy many of the arising research issues for 
knowledge discovery in text research area. 
Text Mining uses unstructured textual information and examines it in attempt to 
discover structure and implicit meanings "hidden" within the text [14]. % lain 
objective is the support of the knowledge discovery process in large document 
collections (Web or conventional storage). 
At the time we had started our research in this particular area. information 
extraction, data mining, databases and ontolo ies  ere research areas with little 
interaction in applications that dealt with text. In 2000 wti e had introduced (to the 
Chapter 8 Conclusion 
best of our knowledge) the combination of information extraction and data 
mining [14]. Later the same year Nahm and Mooney [20] also experimented in 
the same approach. 
Our proposed framework is mostly concerned with the task of discovering 
knowledge in text. Our contributions can be summarised on the following issues: 
 A unified database oriented temporal text mining framework for 
supporting knowledge discovery tasks. 
  Integration of EE and DM in order to discover useful patterns from text. 
  Incorporation of the time issue in the discovery process. 
  Incorporation of background knowledge via the use of ontologies in the 
discovery process. 
 A proposed methodology for conducting the Knowledge Discovery in 
Text process. 
  Metadata model for the text repository that transforms it into a document 
warehouse. 
  We demonstrated the usefulness of the proposed framework via a nov cl 
ontology driven technique for discovering temporal association in tcxt. 
DocumentMiner framework is dealing with huge amount of textual information 
and is analogous to data warehousing as method for dealing with 
large volumes 
of numeric data. In DocumentMiner we view this 
information essentially 
covering the following main aspects: 
" Information about the textual sources. 
  Information about the content of the documents: textual metadala 
  Information about the background 
knowledge mainh' in ontologies: the 
business structure, its processes, actors, goals, market sectors. 
products. ctc. 
  Information about the analv. s'is sto c 
Chapter 8 Conclusion 
In DocumentMiner Ontology models differ slightly from traditional conceptual 
models. This is because Ontologies are driven from the need to describe the 
elements that exist in the text mining process in a generic and explicit manner, so 
future references have to commit to these conceptualisations. Within the 
framework the role of the ontology is double. First the ontology drives the 
linguistic pre-processing upon the documents, by providing the necessary 
background information. The second important role of ontologies in 
DocumentMiner is to be utilised by the user during the analysis (mining) step in 
order to specify the conceptual levels of the discovered patterns. 
The role of information extraction in our framework is important. The goal of 
this component is to distil the necessary information for efficient management 
and mining and depends mainly upon information extraction techniques. By 
identifying key entities in a text, one can find relationships between terms and 
identify unknown links ('hidden') between related topics. In other words is the 
process of identifying essential pieces of information within a text, mapping them 
to standard forms and extracting them for use in later processing. The identified 
essential information is kept in the database according to our metadata model. 
Metadata is essential part of the DocumentMiner framework. Although we could 
randomly define the document metadata we have chosen to reuse and adapt few 
of the existing standards as a basis. The main reason for this is the rising 
awareness for the need for metadata on the web and all business intelligence 
applications in organisations. Such applications interact to each other and a 
common ground is paramount. Our model includes concepts derived 
various existing standards such as the Dublin Core [59], Common 
Warehouse 
Metamodel [60], Message Understanding Conference (MUC7) [57], TimeML 
[58], Text Encoding Initiative TEI [56] etc. Each one assists the development of 
the DocumentMiner's model. 
In order to demonstrate the usefulness of our proposed text mining 
framework we 
apply a method of mining association rules 
in text collections. Our approach 
utilises an algorithm [52] based on decision trees and expands 
the notion of 
Chapter 8 Conclusion 
association rules by incorporating time and ontologies to the relationships 
discovered. 
Additionally we propose a model to support the Knowledge Discovery in Text 
process. DocumentMiner's process is iterative and involves a number of steps, 
beginning with the understanding and definition of business problem and ending 
with the analysis of results and a strategy for using the results to gain competitive 
advantage. It is described in terms of a hierarchical process model consisting of 
sets of tasks described at different levels of abstraction (steps, tasks etc. ). At the 
top level, our text mining process is organised into eight steps; each step consists 
of several generic tasks. We use the term generic for the second level tasks in 
order to indicate that these tasks are general enough to cover all possible text 
mining situations, which of course comply with our proposed framework. In 
other words these generic tasks should cover both the whole process of text 
mining and all possible applications of our approach. 
Concluding, the main idea behind the DocumentMiner project was to propose 
new ideas in order to address some of the main issues-problems in business 
intelligence and deals with vast amounts of text. Our research established new 
directions regarding the text mining process and resulted to a theoretical 
framework. 
The power of our framework is demonstrated with our research prototype. Whilst 
much of the work presented here is a starting point for future research, 
it is 
expected that the approach proposed by the DocumentMiner project will 
contribute significantly to the improvement of the Knowledge 
Discovery in Text 
process by providing a supporting framework and environment. 
8.2 Future Work 
The future work proposed is classified into two main 
directions. The first has to 
do with the theoretical aspect concerning the metadata model, 
temporal issues 
Chapter 8 Conclusion 
etc. The second part is more practical and is about the implementation of the 
environment itself. 
KDT and TM is an emerging research area, where the need for neNN successful 
applications is of paramount importance. According to our survey presented in 
chapter 2 many research issues arise. For some issues there was an attempt to be 
addressed by the proposed framework. A promising new direction is the 
integration of different techniques in order to accomplish the text mining tasks. 
Currently available tools adopt a single technique or a limited set of techniques to 
carry out text analysis. From our experience we conclude that there is no best 
technique, therefore the issue is not which technique is better than the other, but 
which technique is suitable for the problem to be solved. In the future, though 
tools will have to provide a wide range of different techniques. Users should be 
able to choose and easily apply the techniques that are suitable for their analysis 
scenario. The target for our framework is to include a wide variety of techniques. 
There is a strong commercial desire to utilise data mining techniques on the 
masses of textual data. [44] A problem with many knowledge workers is that 
may not know exactly what they are looking for, from the textual database. and 
mining is incredibly effective at retrieving interesting facts from a database. A 
closer and easier tie of data mining techniques with linguistic and information 
extraction techniques, is what needed to be done in the near future. Concerning 
our framework this integration (of DM and IE) is taking place on the database 
level via the metadata model, which should be up to date. Advances on metadata 
research should be taken into account and extend the current model. For example 
we are aware of an EU project named PANDA, which 
defines an overall 
environment to represent and efficiently manage different types of patterns 
derived from the data mining phase. A possible future work concerns the 
extension of our metadata model in order to 
include the PANDA model. 
Additionally we need to extend the model for advanced temporal and spatial 
Issues. 
Chapter X Conclusion 
An issue that may restrict the use of our framework (as any other framework) is 
Multilinguality. Tools should be able to perform multilingual access upon 
document collection. This is a requirement to be resolved as the information that 
the user is looking may rely on document in various languages. 
Domain knowledge and the way to support the linguistic pre-processing plus the 
discovery tasks is an interesting topic to further investigate. Richer Ontologies to 
be utilised and their automatic construction seems to be the future direction. 
There are many challenges for text mining technology but the major one. as 
emerged from the current investigation, will be the development of tools which 
will support highly automated procedures in order to extract accurate and useful 
information from textual resources. 
References 
[1] Autonomy, http: //www. autonomy. com/autonomy v3/. October-' 001. 
[2] Berners-Lee Tim, Hendler James and Lassila Ora. "The Semantic b'l'eb" 
Scientific American May 2001. 
[3] ClearForest http: //www. clearforest. com/index. asp, November 2001. 
[4] dtSearch http: //www. dtsearch. com/dtsofware. html#anchor4l2454, 
November 2001. 
[5] Fayyad U. and Piatetsky-Shapiro G., "From Data Mining to Knowledge 
Discovery: An Overview", Advances in knowledge Discov-er and Data 
Mining, Fayyad U., Piatetsky-Shapiro G. pg 1-34. 
[6] Feldman R. & Dagan I. (1995), "Knowledge discovery in textual databascs 
(KDT)". In proceedings of the first International Conference on knowledge 
Discovery and Data Mining (KDD-95), Montreal, Canada, August 20-21, 
AAAI Press, 112-117. 
[7] Feldman R., Fresko M., Hirsh H, et al., "Knowledge Management: A Text 
Mining Approach", in proc. of the 2"d Int. Conf. on practical Aspects of 
Knowledge Management (PAKM98 ), Zurich, Switzcrland, 1998, pg 9 1-10 
[8] Goebel M& Gruenwald L., "A Survey of Data Mining and Knowledge 
Discovery Software tools", ACM SIGKDD, June 1999, pg 20-33 
[9] Hans-Georg Stork and Franco Mastroddi, Summary report "Semantic Wcb 
Technologies-a New Action Line in the European Commission", 
http: //www. cordis. lu/ist/ka3/iaf/swt_presentations semwebarticle. htm, 
[10] Hearst Marti A., "Untangling Text Data Mining", Proceedings of ACL '99: 
the 37 h Annual Meeting of the Association for computational 
Linguistics. 
University of Maryland, June 20-26.1999, (Invited paper) 
[11] Hehenberger M., Coupet P., "Text Mining applied to patent . 
\naly sis", 
Annual Meeting of American Intellectual Property 
Law Association 
(AIPLA). Airlington. October 1998. 
2l ( 
References 
[12] IBM, Text Mining Technology, "Turning Information into Knowledge". a 
White Paper, TKACH - IBM Business Intelligence Solutions CD. EUA. 
[13] IBM Intelligent Miner for Text, 
http: //www-4. ibm. com/software/data/iminer/fortext', September 2001 
[14] Karanikas H., Tjortjis C. and Theodoulidis B.. "An approach to Text 
Mining using Information Extraction", Proc. PKDD 2000 Knowledge 
Management: Theory and Applications, 12 Sept 2000. pg 165-178 
[ 15] Kodratoff Y., "About Knowledge Discovery in Texts: A Definition and an 
Example" Unpublished Paper, 2000, Available Online 
http: //www. Iri. fr/ia/articles/yk/2000/kodratoffupb. pdf 
[ 16] Kosala R. and Blockeel H., 'Web Mining Research: A Survcv", ACM 
SIGKDD Volume 2, Issue 1 (June 2000), pg 1-15 
[17] Lent B. Agrawal R. and Srikant R., "Discovering Trends in Tcxt 
Databases", Proc. of the 3rd Int'l Conference on Knowledge Discovery in 
Databases and Data Mining, Newport Beach, California, August 1997. 
[ 18] Meidan A., "WizDoc: A concept-based search engine that retrives relevant 
sections in text. http: //www. wizsoft. com, i", October 2001. 
[19] Merrill Lynch, e-Business Analytics, In-depth Report, 20 November 2000 
[20] Nahm U. Y., Mooney R. J., "Using Information Extraction to Aid the 
Discovery of prediction Rules from Text", In Proceedings of the Sixth 
International Conference on Knowledge Discovery and Data Mining (KDD- 
2000) Workshop on Text Mining, Boston, MA, Aug. 2000. 
[21] Oracle Text, "Application Developer's Guide". Release 9.0.1. June 2001, 
Part No. A90122-01 
[22] Oracle Text, http: //technet. oracle. com/products'text. content. html, 
December 2001 
[23] Pedersen T. and Bruce R., "Unsupervised Text Mining", Technical Report 
97-CSE-9, Department of Computer Science and Engineering. Southern 
Methodist University. June 1997 
[24] Rajman M., Besancon R., "Text Mining: Natural Language techniques and 
Text Mining applications", Artificial Intelligence Laboratory. Computer 
References 
Science Department, Swiss Federal Institute of Technology. IFIP 1997. 
Published by Chapman & Hall. 
[25] Rijsbergen C. J. van, "Information Retrieval". Butterworths, 1979 
[26] SemioMap, http: //www. semio. com/, October 2001. 
[27] Sergei Ananyan, "Text Mining-Applications and Technologies". 1legaputcr 
Intelligence, www. megaputer. com 
[28] Sullivan D., "Document Warehousing and Text Mining", Wiley Computer 
Publishing 2001. 
[29] Tan Ah-H., "Text Mining: The state of the art and the challenges", in 
proceedings of the Pacific Asia Conf on Knowledge Discovery and Data 
Mining PAKDD'99 workshop on Knowledge Discovery from Advanced 
Databases. 
[30] Texis, Thunderstone http: //www. thunderstone. com/texis'sitc pages. 
December 2001 
[31 ] TextAnalyst, http: //www. megaputer. com/products/ta-"index. php3, 
December 2001 
[32] TextSmart, http: //www. spss. com/textsmart/, October 2001. 
[33] Wacholder N. and Ravin Y., "Disambiguation of Proper Names in Texts", 
Applied Natural Language Conference (April 3,1997) 
[34] Wilks Yorick (1997), "Information Extraction as a Core Language 
Technology", International Summer School, SCIE-97 
[35] WizDoc, "WizDoc for Web Sites", User's Manual. 
[36] WizDoc, http: //www. wizsoft. com/. October 2001. 
[37] Workshop - Semantic Web Technologies 
22-23 November 2000, 
Luxembourg 
http: //www. cordis. lu/ist/ka3/iaf/swt_presentations swwstoc. htm, December 
[38] Sullivan Dan, "Enterprise Content Management: Information Extraction in 
Enterprise Content Management", DM Review May 2002. 
[ i9] Grishman R. (1997). "Information Extraction: Techniques and Challenges". 
International Summer School, SCIE-97. 
References 
[40] Gaizauskas R. and Humphreys K. (1997). "Concepticons vs. Lexicons: An 
Architecture for Multilinqual Information Extraction", International 
Summer School, SCIE-97. 
[41] Wilks Yorick (1997), "Information Extraction as a Core Language 
Technology", International Summer School, SCIE-97 
[42] InFact, Insightful, http: //www. insightful. com/products/infact', September 
2002. 
[43] LexiQuest, SPSS, http: //www. spss. com/spssbi/lexiquest'. September 2002. 
[44] Dixon M. (1997), "An Overview of Document Mining Technolo`g`-" 
[45] Grobelnik M., Mladenic D., Milic-Frayling N.. "Text Mining as Integration 
of Several Related Research Areas: Report on KDD'2000 Workshop on 
Text Mining", Web site for KDD-2000 Workshop on Text Mining: 
http: //www. cs. cmu. edu/-dunja/WshKDD200O. html 
[46] Merkl D. and A Min Tjoa, "Data Mining in large ftee text document 
archives", In Proceedings of the International Symposium on Cooperative 
Database Systems for Advanced Applications (CODAS'96), Kyoto, Japan, 
December 5-7 1996. 
[47] Feldman R., et al., "A domain Independent Environment for Creating 
Information Extraction Modules", Proceedings of the tenth international 
conference on Information and knowledge management, Atlanta, Georgia, 
USA, 2001 pg 586 - 588 
[48] CRISP-DM 1.0, "Step-by-step data mining guide", SPSS 1999-2000, 
www. crisp-dm. org 
[49] Agrawal R., Imielinski T., Swami A., "Mining Association Rules between 
Sets of Items in Large Databases", In Proceedings of the ACM SIGMOD 
Conf. on Management of Data, May 1993, Washington DC pp. 207-216. 
[50] Goebel M& Gruenwald L., "A Survey of Data Mining and Knowledge 
Discovery Software tools", ACM SIGKDD, June 1999 
[51 ] Rakesh Agrawal and R. Srikant "Mining Quantitativ c Association Rules in 
Large Relational Tables" In Proc. of the ACM SIGMOD Conference on 
Management of Data, pages 1-12, Montreal. Canada. June 1996. 
References 
[52] Koundourakis G. "EnVisioner: A Data Mining Framework Based On 
Decision Trees. " PhD Thesis. UMIST. 2002. 
[53] Fikes R., M. Cutkosky, T. Gruber, & J. Van Baalen, "Knowledge Sharing 
Technology Project Overview" Stanford University. Knowledge Systems 
Laboratory, Technical Report KSL 91-71, November 1991. 
[54] Appelt D., Israel D., "Introduction to Information Extraction Technology ". 
Tutorial IJCAI-99, pg 1-13. 
[55] Kahaner L., "Competitive Intelligence: How to Gather, Analyze, and (i c 
Information to Move Your Business to the Top ", 1996 
[56] TEI, "The Text Encoding Initiative", http: //www. tei-c. or(,, '. chapter 6 and 7. 
[57] MUC7, Nancy Chinchor, MUC-7 Named Entity Task Definition, Version 
3.5919979www. itl. nist. gov/iaul/894.02/related_prOJects/muc proceedings/ne 
_task. 
[58] TimeML Annotation Guideline 1.00 (internal version 0.4.0). James 
Pustejovsky, Roser Sauri, Andrea Setzer, Rob Giazauskas and Bob Ingria. 
July 21,2002 
http: //www. cs. brandeis. edu//o7Ejamesp/arda/timc documentation Annotati 
onGuideline-vO. 4.0. pdf 
[59] Dublin Core Metadata Initiative, 2002, http: //dubIincorc. org/ 
[60] Common Warehouse Metamodel (CWM) Specifications, Version 1.0,2 
February 2001. 
[61] Resource Description Framework RDF, 2002, http: //www. NA, 3. org/ 
[62] Open Information Model, "Knowledge Management Model, Knowledge 
Descriptions", July 15,1999 
Appendix 1 Descriptions of corporate 
activities: 1998-1999 (YTD) 
March 15,1999 
CombiChem and Chromagen enter into drug screening agreement 
CombiChem, Inc. (Nasdaq NMS: CCHM) and Chromagen, Inc. entered into a new assay development and drug screening agreement. Under the agreement, Chromagen will develop biological assays and screen CombiChem compounds for multiple targets pursued by CombiChem 
in its internal drug discovery program. Chromagen will receive research support and a percentage 
of future payments received by CombiChem upon outlicensing of drug candidates to 
pharmaceutical companies. 
February 20,1999 
CEREP and Sanofi Expand Collaboration with Additional Targets. 
CEREP S. A. (Nouveau Marche: CEREP) and Sanofi expanded their original R&D collaboration 
of 1997 to include the screening of two additional targets provided by Sanofi. The screening will 
be done using CEREP's proprietary integrated drug discovery technology platform. Sanofi will 
provide CEREP with additional research funds. The collaboration, which began in 1997, has two 
stages. Initially, CEREP is designing and producing a new library of leads to be used by Sanofi in 
its in-house R&D programs. In the second stage, the companies will pursue together the discovery 
and optimization of leads using CEREP's integrated platform technology. At the time of 
CEREP's initial public offering in February 1998, Sanofi also committed FF 22.5 million in 
research funding as part of an equity stake in CEREP. 
February 3,1999 
Orchid Biocomputer signs deal with Advanced Bioanalytical Services on microfluidic chip 
technology development that combines mass spectrometry 
Orchid Biocomputer entered into a five-year collaborative research and marketing agreement with 
Advanced BioAnalytical Services, Inc. (ABS) to develop novel microfluidic chip technologies. 
By combining the proprietary strengths of Orchid's microfluidics program and ABS' miniaturized 
electrospray liquid chromatography/mass spectrometry (LC/MS) programs in a single system, the 
resulting product is intended to sharply increase efficiency and reduce the costs associated with 
chemical synthesis and analysis of pharmaceutically relevant compounds. Use of the new arrays 
will potentially allow for the characterization and quantitative analysis of 10,000 compounds in 
only five days while using less than 1/50th of the sample material currently required. This 
technology also has tremendous potential in SNP (single nucleotide polymorphism) analysis as 
part of the pharmacogenetics programs at Orchid. MS and chip interfaces will drive down the cost 
of primer extension for SNP genotyping, which maintains Orchid's leadership in this area. Under 
the terms of agreement, Orchid gains exclusive rights to the jointly developed technology for the 
integration of synthesis and analysis. Orchid will pay an undisclosed sum to ABS upon the 
achievement of specific milestones over the course of the collaboration. Upon successful 
development of the new biochip products, the companies will share collaboration and product 
revenues. The commercialization strategy will be led by Orchid. 
January 19,1999 
Pharmacopeia and Pharmacia & Upjohn form research collaboration 
Pharmacopeia, Inc. (Nasdaq: PCOP) signed a new multi-year, multi-target research collaboration 
with Pharmacia & Upjohn Inc. Under the terms of the agreement, Pharmacopeia will screen its 
multi-million compound sample collection of diverse, small molecules against numerous targets 
chosen by Pharmacia & Upjohn each year. In addition to annual guaranteed payments, 
Pharmacia & Upjohn will make further payments based on the success of the screening programs. 
Appendix 1 
Optimization of active compounds identified by Pharmacopeia may be performed either by 
Pharmacia & Upjohn or by Pharmacopeia, for additional consideration. Financial terms of the 
agreement were not disclosed. 
January 7,1999 
Axys Pharmaceuticals' Advanced Technologies Division Enters Combinatorial Chemistry 
Agreement With Rhone-Poulenc Rorer 
Axys Pharmaceuticals' (NASDAQ: AXPH) Advanced Technoloqies Division entered into an 
aqreement with Rhone-Poulenc Rorer ("RPR"), a subsidiary of Rhone-Poulenc S. A. (NYSE: RP) 
to provide a generic compound screening library consisting of multiple small molecule synthetic 
organic compound libraries created usinq Axys' combinatorial chemistry technologies. These 
compounds will add diversity to RPR's screening library and are desinned to enhance RPR's drug 
discovery efforts. The two year agreement calls for Axys to provide RPR with structurally diverse 
compounds as well as enabling technologies for recreating the generic compound library. 
Financial terms of the agreement were not disclosed. This new agreement marks Axys' second 
collaboration with RPR signed within the past month. In December 1998, Axys and RPR entered 
a collaboration potentially worth more than $80 million to Axys for the discovery and 
development of small molecule therapeutics that inhibit cathepsin S, a human cysteine protease 
associated with certain inflammatory diseases, including arthritis, asthma, atherosclerosis and a 
variety of autoimmune diseases. 
January 4,1999 
IRORI and Ontogen sign deal for exclusive rights to Ontogen's radiofrequency (RF) tagging 
patents 
IRORI, a subsidiary of Discovery Partners International and a provider of drug discovery 
technologies, and Ontogen Corporation, a chemical drug discovery and development company, 
signed a deal covering Ontogen Corporation's two patents relating to the RF tagging of chemical 
libraries. The assignment agreement gives IRORI exclusive rights to United States Patent 
#5,770,455, "Methods and Apparatus for Synthesizing Labeled Combinatorial Chemistry 
Libraries", as well as one other allowed and pending parent patent application. Ontogen retains 
commercial rights to use the technologies internally. Financial terms of the deal were not 
disclosed. The innovative RF technology relates to labeling individual library members of a 
combinatorial synthesis library with unique identification tags that facilitate elucidation of the 
structure of the individual library members synthesized. This identifier tag is a microchip that is 
pre-encoded or encodable with information that is related back to a detector when the identifier is 
pulsed with electromagnetic radiation. Using this technology, IRORI will further develop library 
synthesis systems designed to accelerate the drug discovery process. IRORI's own technology, 
called AccuTagTM, uses IRORI's novel "directed sorting" process and RF labeled microreactors 
to synthesize discrete compounds using the split-and-pool approach. It provides researchers with 
the most economic means to synthesize large numbers of discrete compounds in quantities large 
enough to by useful for analytical testing and biological screenings. IRORI has become the 
leading supplier of combinatorial chemistry systems using the "directed sorting" synthesis 
technique. 
January 4,1999 
CombiChem enters into drug discovery collaboration with Ono 
CombiChem, Inc. (Nasdaq NMS: CCHM) signed a drug discovery collaboration with Ono 
Pharmaceutical Co., Ltd., Osaka, Japan. Under the terms of the collaboration, CombiChem will 
work with Ono to generate lead drug candidates for a target of 
interest to Ono in exchange for a 
project initiation fee, research support and milestone payments. 
Ono has exclusive global rights to 
develop and market or sub-license products resulting from the collaboration. 
CombiChem will be 
entitled to royalties on sales of products resulting from the collaboration. 
The target of interest to 
Ono was selected from a group of targets which were screened using 
CombiChem's Universal 
Informer Library (UIL). Computational analysis of the screening data by CombiChem 
led to 
mutual agreement on the choice of target. 
Appendix 1 
December 16,1998 
Novartis Crop Protection and Pharmacopeia form new collaboration 
Novartis Crop Protection AG and Pharmacopeia, Inc. (Nasdaq: PCOP) signed a collaboration 
agreement effective January 1,1999, focused on library screening for agro-chemical compounds. 
Under the terms of this new collaboration, Pharmacopeia will use its patented ECLiPSTM' 
technology to provide collections of small molecule compounds for screening by Novartis Crop 
Protection, the world's leading supplier of crop protection products. Pharmacopeia will receive 
payments from Novartis Crop Protection for each library provided. Pharmacopeia will also be 
entitled to milestone and royalty payments as compounds progress through development and 
commercialization. Further financial terms will not be disclosed. 
December 16,1998 
ArQule, Inc. enters into four year collaboration with R. W. Johnson Pharmaceutical Research 
Institute for the discovery of novel pharmaceuticals 
ArQule, Inc. (Nasdaq: ARQL) entered into a collaboration with R. W. Johnson Pharmaceutical 
Research Institute ("PRI"), a Johnson & Johnson affiliate, to discover drug candidates. Under 
terms of the agreement, PRI will receive a four year subscription to ArQule's Mapping ArrayTM 
program to discover new lead compounds for a variety of therapeutic areas. ArQule will begin 
shipping the Mapping ArraysTM to PRI in December 1998. PRI will make annual subscription 
payments to ArQule over the term of the collaboration for delivery of 1.1 million compounds. 
PRI will also receive a non-exclusive license to the process of designing and manufacturing 
arrays of logically ordered chemical compounds, as claimed in ArQule's U. S. Patent No. 
5,712,171, entitled "Method of Generating a Plurality of Compounds in a Spatially Arranged 
Array. " In addition, ArQule will be entitled to payments for milestones and royalties from sales 
of any products emanating from this collaboration. 
December 7,1998 
Chemrx, and Signal Pharmaceuticals enter into chemistry collaboration agreement 
ChemRx, a wholly owned subsidiary of Discovery Partners International, and Signal 
Pharmaceuticals signed a chemistry-based development agreement. Under the terms of 
agreement, ChemRx will develop multiple classes of compounds for Signal's drug discovery and 
lead optimization libraries. Financial terms of the agreement were not disclosed. ChemRx will 
develop the chemistries on a fee-for-service basis, and will provide the compounds to Signal on 
an exclusive basis and with no milestone or. royalty obligations. ChemRx scientists use the 
combinatorial chemistry tools developed by their sister company, IRORI, to rapidly convert 
synthesis methods into chemical libraries for pharmaceutical lead discovery or 
for optimization 
of new drug leads. ChemRx (a Discovery Partners International company) 
is a chemistry services 
company that leverages the internal medicinal chemistry resources of 
its pharmaceutical partners 
by providing primary lead-discovery chemical libraries and lead-optimization services 
for drug 
discovery and development. It provides its partners with lead 
discovery libraries (25,000 + 
compounds) and focused libraries (1,000 - 10,000 compounds) on a 
fee-for-service basis, with 
all rights to the compounds assigned to its partners. The 
Company also provides lead 
optimization and scale-up services on a cost-plus basis as 
its partners discover new drug leads 
from the compound libraries it generates. Discovery 
Partners International is an umbrella 
company focused on providing the best platforms, services and 
information available to augment 
the internal drug discovery efforts of pharmaceutical and 
biopharmaceutical companies. 
Subsidiaries include IRORI, a combinatorial chemistry instrument company, and 
ChemRx, a 
provider of chemistry services. The company is 
headquartered in La Jolla, California. 
November 25,1998 
CEREP and ExonHit Therapeutics Enter into 
Agreement on Toxicity Assessments, Target 
Validation and Drug Discovery. 
CEREP S. A., (Nouveau marchd: CEREP) and ExonHit 
Therapeutics (Paris, France) signed a 
collaborative agreement focusing on the 
development of new methods for toxicity assessments, 
target validation and drug discovery over a two-year period. 
CEREP will make an equity 
Appendix 1 
investment of FF3 million (USD 0.54 million) in ExonHit and will fund R&D up to FF 13 million (USD 2.32 million), based on the achievement of specific milestones. Central to the agreement is 
ExonHit's proprietary technology DATAS (Differential Analysis of Transcripts Alternatively 
Spliced). DATAS permits the detection of qualitative changes in gene expression at a key 
regulation process known as RNA splicing. This key stage in gene expression can be altered by 
genetic and epigenetic events. DATAS ultimately provides a cost-effective, expedient method of identifying elements in pathophysiological conditions by using miniaturized assays. CEREP and 
ExonHit will work together to develop new methods of gene profiling for toxicity assessments. 
CEREP will have a non-exclusive license to all new assays developed via DATAS and will be 
entitled to royalties on sales of assays to third parties. Under the terms of the Agreement, the two 
companies will also collaborate in target identification and validation in the field of 
neurodegenerative diseases. CEREP will finance research and acquire exclusive worldwide rights 
for two genes or coding sequences identified by ExonHit's DATAS technology. CEREP will 
independently undertake the discovery and development of chemical entities against selected 
targets, while ExonHit will retain rights to gene therapy applications. Finally, CEREP will make 
available its fully integrated drug discovery platform for the discovery and development of 
chemical entities against specific proprietary targets identified by ExonHit on the basis of shared 
costs and revenues. 
27 October 1998 
Pharmacopeia announces new collaboration with Schering A. G. 
Pharmacopeia, Inc. signed a new, multi-target screening agreement with Schering AG, Germany. 
Under the terms of this new agreement, Pharmacopeia will screen its multi-million compound 
sample collection of diverse, small molecules against multiple targets chosen by Schering AG. 
Chemical optimization of active compounds may be performed by either Pharmacopeia or 
Schering AG. In return, Schering AG will make payments to Pharmacopeia of up to $64 million, 
excluding royalties, over the next several years for research and development funding and 
milestones. Pharmacopeia signed an agreement with Berlex Laboratories, Inc., an affiliate of 
Schering AG, in February, 1995. The collaboration, which focused on specific targets in the field 
of inflammation, had a potential value, excluding royalties, of approximately $19 million, some of 
which has already been received by the Company. In January, 1997, Berlex decided to pursue an 
in vivo evaluation of a candidate compound identified by Pharmacopeia and, as a result, made a 
milestone payment to Pharmacopeia. This compound is being pursued by Berlex and is moving 
towards formal clinical development. In August, 1998, Berlex provided Pharmacopeia with 
additional funding to pursue chemical optimization on additional lead compounds identified by 
Pharmacopeia. 
21 October 1998 
Trega announces that Chromaxome is granted two U. S. patents in combinatorial biology 
ChromaXome Corporation, a wholly-owned subsidiary of Trega, received two US patents, giving 
it patent protection for a wide variety of combinatorial biology technologies. The two patents 
(U. S. Patent Nos. 5,783,431 and 5,824,485) relate to processes by which DNA from donor 
organisms is isolated and manipulated to generate and express novel biosynthetic pathways. The 
two related patents, both entitled "Methods for Generating and Screening Novel Metabolic 
Pathways, " provide composition and method claims for combinatorial biology processes, library 
formats, vector/expression host systems, high throughput screening formats and library donor 
material, including environmental samples, for use in natural products discovery. 
5 October 1998 
Biogen becomes Trega's first chem. folioT"' customer. Biogen to Also Obtain Non-exclusive 
License to Trega's Chemical-Synthesis Technologies 
Trega Biosciences, Inc. signed a two-year non-exclusive agreement to supply Biogen, Inc. with 
small-molecule, combinatorial chemistry libraries through its 
Chem. FolioT*'' program. In addition, 
Trega has agreed to provide Biogen with non-exclusive, worldwide 
licenses to certain of its 
chemical-synthesis technology patents to facilitate the creation of compounds 
by means of solid- 
phase synthesis. Under the terms of the agreement, Trega will receive guaranteed 
funding of S7.5 
Imr, I -N' 
Appendix 1 
million over two years from Biogen for supplying combinatorial chemistry libraries. The 
Company will also receive from Biogen an undisclosed license fee for use of Trega's proprietary 
Tea-Bag and Positional Scan technologies, as well as potential milestone payments for 
compounds developed using these technologies. 
30 September 1998 
Eli Lilly Selects Nonlinear Dynamics Inc, to Develop Drug Discovery Software 
Eli Lilly & Co. launched an 18-month software development program with Ann Arbor-based 
Nonlinear Dynamics Inc. (NDI) designed to help speed drug discovery processes at Lilly. A 
team of six NDI research scientists headed by Dr. Laurel Harmon will work with cheminfonmatics 
specialists at Lilly to develop data mining software to help reduce the time and expense required 
to identify high-potential compounds for further drug development and preclinical evaluation. 
Pilot testing of NDI's preproduction software is scheduled to begin later this year. Today only one 
out of thousands of compounds investigated by the pharmaceutical industry is eventually 
marketed. The process itself can take 15 years or more. The new software is designed to give 
Lilly the ability to evaluate hundreds of thousands of additional compounds per year. The first 
working prototypes usually are delivered ten weeks after receipt of order, and Phase Two 
software for Lilly will be available by December, and production software will be online within 
12 months. The software programs are based on nonlinear methods developed over the past 10 
years in the nation's defense and aerospace laboratories. Computer-based nonlinear analysis has 
the ability to identify high-potential compounds significantly faster than any commercial or 
proprietary system currently available. 
29 September 1998 
Bayer AG and Novalon Pharmaceutical Corporation to collaborate on anti-infective drug 
screening. 
The companies entered into a research collaboration to screen anti-infective drug discovery 
targets identified by Bayer using Novalon's proprietary BioKeyTM assay technology. Under the 
terms of the collaboration, Novalon identifies BioKeys, formats and delivers BioKey assays, and 
conveys a license for their use in anti- infective drug discovery. Bayer selects targets, conducts 
high throughput screening, and provides Novalon with research funding, along with milestone 
payments and royalties for products identified through the collaboration. Novalon's BioKey 
technology uses structured libraries of over 20 billion unique biopolymers to identify surrogate 
ligands, known as BioKeys, that bind specifically to the functional domains of target proteins. 
These BioKeys are labeled to test for small molecule drugs that displace BioKeys. 
14 September 1998 
Orchid Biocomputer acquires Molecular Tool 
Orchid Biocomputer, Inc. acquired Molecular Tool, a Baltimore-based research and 
development 
subsidiary of GeneScreen, Inc. The acquisition enables Orchid to combine 
its ultra high- 
throughput microfluidics technology with Molecular Tool's proprietary single nucleotide 
polymorphism (SNP) analytic chemistry and genetic bit analysis 
(GBA) capabilities, providing 
faster, more flexible tools to analyze correlations between SNPs and specific genotypes, 
diseases 
and therapeutics. Under the terms of the deal, Orchid will receive 
full assignment to Molecular 
Tool's proprietary GBA and other patented technologies 
in the SNP area. Orchid will acquire 
equipment and know-how. Each current Molecular Tool employee 
has accepted positions at 
Orchid. Molecular Tool's operations and facilities will be transferred to 
Orchid's headquarters in 
Princeton during the next 12 months. 
9 September 1998 
Smithkline Beecham enters into advanced technology access program with 
LJL Biosystems 
LJL BioSystems, Inc. (LJL) entered into a collaboration with SmithKline 
Beecham (SB) for early 
access to LJL's advanced technologies 
for high throughput screening (HTS). Under the 
agreement, SmithKline Beecham will obtain early access 
to LJL's next generation HTS platform. 
called FLAReTM. FLAReT"' will employ patented or patent 
applied for instrumentation, methods 
Appendix 1 
and proprietary chemistries for drug discovery assays based on the measurement of fluorescence 
lifetime. Terms of the agreement were not disclosed. This HTS technology platform is part of 
LJL's Total Solution approach to addressing today's drug discovery challenges. LJL intends to 
market the FLARe technology platform as an integrated instrument-reagent system for accelerated 
drug discovery. Prior to release of FLARe system to the general market, LJL plans to work with 
select collaborators, such as SmithKline Beecham, who have comparable technical expertise, 
immediate need for this leading edge technology, and the ability to fund development. 
11 August 1998 
Pharmacopeia announces expansion of programs with Berlex and Organon to pursue chemical 
optimization. 
Pharmacopeia, Inc. received additional funding from Berlex Laboratories, Inc. and N. V. Organon 
(Organon), in order to pursue chemical optimization on additional lead compounds identified by 
Pharmacopeia in collaborations with each of these companies. Under the terms of its original 
agreement with Berlex signed in February, 1995, Pharmacopeia agreed to use its proprietary 
combinatorial chemistry, high-throughput screening and informatics capabilities to identify lead 
compounds against certain inflammation targets. A series of highly potent and selective 
compounds were identified and optimized. A product candidate is progressing towards formal 
clinical development. A second compound, active against a different target, has recently been 
identified by Pharmacopeia. Berlex has agreed to provide Pharmacopeia with additional funding 
to pursue chemical optimization to improve potency and selectivity of this lead compound. As 
this compound progresses through development, Pharmacopeia will be entitled to milestone 
payments, and if commercialized, royalties on net sales. 
16 July 1998 
Aurora Biosciences Corp. and Cytovia collaborate on cell-based assays. 
The companies will collaborate so as to couple their strengths in cell-based assays for drug 
discovery. Drawing on Aurora's current high throughput screening, compound library and 
informatics capabilities in combination with Cytovia's proprietary fluorogenic protease substrates 
and live-cell screening technology, the companies will conduct screening programs to identify 
new drug leads for cancer and degenerative diseases. The agreement also anticipates the use of 
Aurora's Ultra-high Throughput Screening System ("UHTSS") which is planned to come on line 
in 1999. Under the agreement, Cytovia will receive rights to develop and commercialize any new 
drug leads identified as a result of the collaboration. Aurora will receive fees from Cytovia for 
compound access and screening services, as well as payments for achievement of development 
milestones and royalties on product sales. In addition, Aurora will make an equity investment 
Cytovia. 
15 July 1998 
Trega Biosciences, Inc. obtains milestone payments from ONO Pharmaceuticals. 
Trega identified several lead compounds in the first phase of research and development agreement 
with Ono Pharmaceuticals Co., Ltd., Osaka, Japan, triggering a milestone payment 
to Trega to 
fund the next phase of the program. Trega, which screened its own combinatorial chemistry 
libraries for active compounds, will now optimize lead compounds and conduct pre-clinical 
testing of orally-active small molecules for the treatment of certain 
inflammatory diseases 
mediated by the melanocortin-1 (MC-1) receptor pathway through cytokine regulation. 
14 July 1998 
Axys Pharmaceuticals, Inc. and Signal Pharmaceuticals Inc. collaborate 
for the accelerated 
discovery of compounds that interact with specific cell signaling pathways. 
The companies entered into a collaboration 
for the accelerated discovery of compounds that 
interact with specific cell signaling pathways. 
Signal has developed proprietary assays for these 
signaling pathways which Signal will use to screen small molecules 
derived from Axys' Advanced 
Technologies division's compound libraries. The agreement provides 
that Axys will receive from 
Signal an upfront payment, as well as other payments upon 
the achievement of certain research 
Appendix 1 
and development milestones. The specific molecular targets and financial terms of the agreement 
were not disclosed. 
JOHN RYLAN D, 
UNIVERS ' 
L1 PARY 
