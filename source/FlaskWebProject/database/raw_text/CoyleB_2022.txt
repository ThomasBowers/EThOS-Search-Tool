coversheet
This thesis has been submitted in fulfilment of the requirements for a postgraduate degree 
(e.g. PhD, MPhil, DClinPsychol) at the University of Edinburgh. Please note the following 
terms and conditions of use: 
This work is protected by copyright and other intellectual property rights, which are 
retained by the thesis author, unless otherwise stated. 
A copy can be downloaded for personal non-commercial research or study, without 
prior permission or charge. 
This thesis cannot be reproduced or quoted extensively from without first obtaining 
permission in writing from the author. 
The content must not be changed in any way or sold commercially in any format or 
medium without the formal permission of the author. 
When referring to this work, full bibliographic details including the author, title, 
awarding institution and date of the thesis must be given. 
Machine learning applications for noisy
intermediate-scale quantum computers
Brian Coyle
N I V E R
D I N B
Doctor of Philosophy
Laboratory for Foundations of Computer Science
School of Informatics
University of Edinburgh
Abstract
Quantum machine learning (QML) has proven to be a fruitful area in which to
search for applications of quantum computers. This is particularly true for those
available in the near term, so called noisy intermediate-scale quantum (NISQ)
devices. In this Thesis, we develop and study QML algorithms in three applica-
tion areas. We focus our attention towards heuristic algorithms of a variational
(meaning hybrid quantum-classical) nature, using parameterised quantum circuits
as the underlying quantum machine learning model. The variational nature of
these models makes them especially suited for NISQ computers. We order these
applications in terms of the increasing complexity of the data presented to them.
Firstly, we study a variational quantum classifier in supervised machine learning,
and focus on how (classical) data, feature vectors, may be encoded in such models
in a way that is robust to the inherent noise on NISQ computers. We provide
a framework for studying the robustness of these classification models, prove
theoretical results relative to some common noise channels, and demonstrate
extensive numerical results reinforcing these findings.
Secondly, we move to a variational generative model called the Born machine,
where the data becomes a (classical or quantum) probability distribution. Now, the
problem falls into the category of unsupervised machine learning. Here, we develop
new training methods for the Born machine which outperform the previous state
of the art, discuss the possibility of quantum advantage in generative modelling,
and perform a systematic comparison of the Born machine relative to a classical
competitor, the restricted Boltzmann machine. We also demonstrate the largest
scale implementation (28 qubits) of such a model on real quantum hardware to
date, using the Rigetti superconducting platform.
Finally, for our third QML application, the data becomes purely quantum in
nature. We focus on the problem of approximately cloning quantum states, an
important primitive in the foundations of quantum mechanics. For this, we develop
a variational quantum algorithm which can learn to clone such states, and show
how this algorithm can be used to improve quantum cloning fidelities on NISQ
hardware. Interestingly, this application can be viewed as either supervised or
unsupervised in nature. Furthermore, we demonstrate how this can algorithm
can be used to discover novel implementable attacks on quantum cryptographic
protocols, focusing on quantum coin flipping and key distribution as examples. For
the algorithm, we derive differentiable cost functions, prove theoretical guarantees
such as faithfulness, and incorporate state of the art methods such as quantum
architecture search.
Acknowledgements
First and foremost, I want to thank my supervisor, Elham Kashefi for unending
support and the opportunity to take my research in directions almost perpendicular
to my original intended path. I would also like to thank my extensive team of co-
supervisors, Vincent Danos, Tony Kennedy and Ajitha Rajan, for keeping me on
the rails for the last 4 years. Of particular importance are my PhD examiners, Raul
Garcia-Patron Sanchez and Iordanis Kerenidis, for agreeing to give me a PhD at
the end of the road.
Edinburgh is a fantastic city, but made even more so by the people there
who made the PhD what it was. The journey began with those in the CDT in
Pervasive Parallelism, from the Informatics forum to the Bayes centre and back;
Bruce Collie, Jack Turner, Pablo Andrs-Martnez (a pleasant surprise at having
someone else studying quantum computing in a classical computer science doctoral
training centre), Mattia Bradascio, Nicolai Oswald, Martin Kristien, Maximiliana
Behnke, Aleksandr Maramzin and Margus Lind.
Next, of particular importance, the quantum team - First, the quomrade:
Daniel Mills, who knows how much he contributed. Matty Hoban, my first quan-
tum mentor and Andru Gheorghiu, who gave me my first lesson on complexity
theory (among others), and generously donated the title of my first PhD paper.
Petros Wallden, the ever present rock of the quantum group. Ellen Derbyshire,
who the journey started with. Mina Doosti, the renaissance woman, Alex Cojo-
caru the master cryptographer. The postdocs who taught me so much about life,
the universe and everything; Niraj Kumar, Atul Mantri, Rawad Mehzer, Mahshid
Delavar Theodoros Kapourniotis and Kaushik Chakraborty. Finally, those in the
larger quantum group at the University of Edinburgh: James Mills, Chris Heunen,
Myrto Arapinis, Ieva epaite, Nuiok Dicaire, Meisam Tarabkhah, Nishant Jain,
Parth Padia, Lakshika Rathi, Patric Fulop and Jonas Landman.
Our group was fortunate enough to span the wisdom of two countries. The
second half of this wisdom belonged to those in Paris, where I was fortunate
enough to spend 3 months at the start of my PhD and interact with many amaz-
ing academics including: Pierre-Emmmanuel Emariau, Dominik Leichtle, Ulysse
Chabaud, Armando Angrisani, Lo Colisson, Luka Music, Shane Mansfield, Tom
Douce, Anu Unnikrishnan, Marc Kaplan, Rhea Parekh, Natansh Mathur, Shrad-
dha Singh, Mathieu Bozzio, Fred Groshans, Harold Ollivier, Damian Markham,
Eleni Diamanti.
Then, my detour into quantum industry where I learned from fantastic re-
searchers like Stasja Stanisic, Lana Mineh, Charles Derby, Raul Santos, Joel
Klassen and Ashley Montanaro at Phasecraft, and Mattia Fiorentini, Michael
Lubasch, Matthias Rosenkranz, David Amaro, Kirill Plekhanov, Carlo Modica,
Chiara Leadbeater and Louis Sharrock at Cambridge Quantum.
Thanks to my collaborators and friends, Ryan LaRose, Max Henderson, Justin
Chan (who taught me how to software engineer) Alexei Kondratyev, Graham Enos,
Mark Hodson, and Marco Paini.
A special thanks goes to those who read and gave feedback on my Thesis;
Dan Mills, Ellen Derbyshire, Atul Mantri, Elham Kashefi and especially Marcello
Benedetti, a quantum machine learning pioneer who I was fortunate enough to
collaborate with on two papers while at CQC.
To those friends and family outside of academia, Kyle, Conor, Mark, Alan,
Fionn, Kieran, Stephen, Duncan, Carmel, Marie, Laura, Ann & Isabel and all of
the Coyles too abundant to name. To my Scottish family; Sandra & Pat, Mark
& Dot, Kenny & Ann, Lucy, Fern & Michael, Ronan, Amy & Greg, and last but
not least; Dudley the dog.
Finally, the one who deserves the most thanks is Kathryn, without whose
endless support, none of this would have been possible.
To finish, the coronavirus pandemic from 2019-present, an entity which does
not deserve thanks, but certainly deserves acknowledgement.
Declaration
I declare that this thesis was composed by myself, that the work contained herein
is my own except where explicitly stated otherwise in the text, and that this work
has not been submitted for any other degree or professional qualification except
as specified.
(Brian Coyle)
To my parents.
Lay summary
In modern times, almost everyone on the planet has access to some form of classical
computer. For most, this will be a simple smartphone, but what we refer to as classical
computers also encompass everything up to the largest supercomputers on the planet.
The computational capacities of these two extremes are vastly different, but they all obey
the same laws under the hood. In contrast, quantum computers allow us to access a
fundamentally different computational paradigm, by manipulating quantum information
directly. Given this difference, we believe that there exists an unbridgeable gap between
what quantum and classical computers can do. Given this, in the long run we have
a handful of quantum algorithms which can capitalise on this distinction and have real
impact on problems which we cannot hope to solve using purely classical means.
However, actually building and scaling quantum computers is a significant engineering
challenge, albeit one towards which tremendous progress is being made. As such, rather
than being a clear cut advantage between classical and quantum devices promised by
the theory, we currently have a rat race between the most powerful of both examples
competing with each other. This is primarily due to the physical imperfections in the
quantum devices and their small sizes, which classical computers can exploit to effectively
nullify any theoretical advantages.
Nonetheless, small and error-prone quantum computers do exist and in the coming
years they will only improve and grow in size. Therefore, the question arises; what should
we do with them? This Thesis attempts to address exactly this question and looks to
the field of machine learning to find answers. Machine learning is the ability of computers
to learn for themselves without being explicitly programmed (intelligent machines), and
using it we can, for example, recognise patterns in data invisible to the human eye. In its
own right, machine learning is ubiquitous in our lives, for example, the recommendation
system algorithms which underpin many social networks are machine learning based in
nature. Given this ubiquity, it is not surprising that any potential for quantum computing
having an impact in this area has created excitement.
In this vein, the Thesis investigates and develops three potential machine learning-
based models and algorithms, suitable as applications for the quantum computers avail-
able now. The first is the problem of classifying data using a quantum model. In this
application, we study the effect of quantum errors on such models, and whether clever
model design could be used to make the models more stable against such errors.
The second, is using a quantum model to generate synthetic data. Here, we discuss
questions of whether such models may provide and advantage over classical counterparts.
We also provide new methods to make the quantum model learn better and more efficiently
and run large scale experiments on a real quantum computer.
Finally, the third application has no obvious classical counterpart, in that we show
how to train a quantum computer to learn how to clone, or make copies of, quantum
states. We show an application of the algorithm we develop in the field of quantum
cryptography, by demonstrating how the algorithm can learn to attack certain quantum
protocols. Finally, we discuss the possibility of using the algorithm to discover things
about the foundations of quantum mechanics.
Publications and manuscripts
During the period of time in which the work of this Thesis was completed I have
been a part of the following articles:
Included in this Thesis
The contents of this thesis are based on the following publications and one un-
published manuscript:
1. Robust Data Encodings for Quantum Classifiers, [LC20]
Ryan LaRose and Brian Coyle.
Publication: Physical Review A 102, 032420 (2020).
Preprint: ArXiv: 2003.01695.
2. The Born Supremacy: Quantum Advantage and
Training of an Ising Born Machine. [CMDK20]
Brian Coyle, Daniel Mills, Vincent Danos and Elham Kashefi.
Publication: npj Quantum Information 6, 60 (2020).
Preprint: ArXiv: 1904.02214.
3. Quantum versus Classical Generative Modelling in Finance. [CHL+21]
Brian Coyle, Maxwell Henderson, Justin Chan Jin Le, Niraj Kumar, Marco
Paini and Elham Kashefi.
Publication: Quantum Science and Technology, Volume 6, Number 2 (2021).
Preprint: ArXiv: 2008.00691.
4. Variational Quantum Cloning: Improving Practicality
for Quantum Cryptanalysis. [CDKK20]
Brian Coyle, Mina Doosti, Elham Kashefi and Niraj Kumar.
Preprint: ArXiv: 2012.11424.
https://link.aps.org/doi/10.1103/PhysRevA.102.032420
https://arxiv.org/abs/2003.01695
https://www.nature.com/articles/s41534-020-00288-9
https://arxiv.org/abs/1904.02214
https://iopscience.iop.org/article/10.1088/2058-9565/abd3db/meta
https://arxiv.org/abs/2008.00691
https://arxiv.org/abs/2012.11424
Excluded from this Thesis
I also coauthored the following publications which are excluded from this Thesis:
5. Certified Randomness From Steering Using Sequential Measurements. [CKH19]
Brian Coyle, Elham Kashefi and Matty Hoban.
Publication: Cryptography 2019, 3(4), 27 (2019).
Preprint: ArXiv: 2008.00705.
6. A Continuous Variable Born Machine. [CK20]
Ieva epaite, Brian Coyle and Elham Kashefi.
Preprint: ArXiv: 2011.00904.
7. Graph neural network initialisation of quantum approximate optimisa-
tion. [JCKK21]
Nishant Jain, Brian Coyle, Elham Kashefi and Niraj Kumar.
Preprint: ArXiv: 2111.03016.
8. Variational inference with a quantum computer. [BCF+21]
Marcello Benedetti, Brian Coyle, Mattia Fiorentini, Michael Lubasch, Matthias
Rosenkranz.
Publication: Phys. Rev. Applied 16, 044057
Preprint: ArXiv: 2103.06720.
9. f -divergences and cost function locality in generative modelling with
quantum circuits. [LSCB21]
Chiara Leadbeater, Louis Sharrock, Brian Coyle and Marcello Benedetti.
Publication: Entropy 2021, 23(10), 1281.
Preprint: ArXiv: 2110.04253.
https://doi.org/10.3390/cryptography3040027
https://arxiv.org/abs/2008.00705
https://arxiv.org/abs/2011.00904
https://arxiv.org/abs/2111.03016
https://link.aps.org/doi/10.1103/PhysRevApplied.16.044057
https://arxiv.org/abs/2103.06720
https://www.mdpi.com/1099-4300/23/10/1281
https://arxiv.org/abs/2110.04253
Table of Contents
1 Introduction & background 1
1.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.2 Quantum computing for machine learning . . . . . . . . . . . . . 3
1.3 Machine learning for quantum computing . . . . . . . . . . . . . 4
1.4 Thesis overview . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
2 Preliminaries I: Quantum information 9
2.1 Quantum computing . . . . . . . . . . . . . . . . . . . . . . . . 9
2.1.1 Quantum states . . . . . . . . . . . . . . . . . . . . . . . 10
2.1.2 Quantum operations . . . . . . . . . . . . . . . . . . . . 12
2.1.3 Quantum gates . . . . . . . . . . . . . . . . . . . . . . . 14
2.1.4 Quantum measurements . . . . . . . . . . . . . . . . . . 17
2.1.5 Quantum noise . . . . . . . . . . . . . . . . . . . . . . . 19
2.1.6 Quantum hardware . . . . . . . . . . . . . . . . . . . . . 22
2.1.7 Distance measures . . . . . . . . . . . . . . . . . . . . . 24
2.2 Quantum cloning . . . . . . . . . . . . . . . . . . . . . . . . . . 32
2.2.1 Beyond the no-cloning theorem . . . . . . . . . . . . . . 33
2.2.2 Approximate cloning . . . . . . . . . . . . . . . . . . . . 34
2.2.3 Cloning of fixed overlap states . . . . . . . . . . . . . . . 37
3 Preliminaries II: Machine learning 39
3.1 Machine learning . . . . . . . . . . . . . . . . . . . . . . . . . . 39
3.2 Neural networks . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
3.2.1 Feedforward neural networks . . . . . . . . . . . . . . . . 43
3.2.2 The Boltzmann machine . . . . . . . . . . . . . . . . . . 43
3.3 Machine learning tasks . . . . . . . . . . . . . . . . . . . . . . . 44
3.3.1 Classification as supervised learning . . . . . . . . . . . . 44
3.3.2 Generative modelling as unsupervised learning . . . . . . . 46
3.4 Training a neural network . . . . . . . . . . . . . . . . . . . . . . 47
3.4.1 Computing gradients . . . . . . . . . . . . . . . . . . . . 49
3.5 Kernel methods . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
3.6 Learning theory . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
4 Preliminaries III: Quantum machine learning 59
4.1 Variational quantum algorithms . . . . . . . . . . . . . . . . . . . 62
4.1.1 Cost functions . . . . . . . . . . . . . . . . . . . . . . . 63
4.1.2 Anstze . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
4.1.3 Cost function optimisation . . . . . . . . . . . . . . . . . 69
4.1.4 VQA inputs and outputs . . . . . . . . . . . . . . . . . . 72
4.2 Quantum kernel methods . . . . . . . . . . . . . . . . . . . . . . 78
4.3 Quantum learning theory . . . . . . . . . . . . . . . . . . . . . . 80
5 Robust data encodings for quantum classifiers 83
5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
5.2 Quantum classifiers . . . . . . . . . . . . . . . . . . . . . . . . . 84
5.2.1 Data encodings . . . . . . . . . . . . . . . . . . . . . . . 87
5.2.2 Robust data encodings . . . . . . . . . . . . . . . . . . . 89
5.3 Analytic results . . . . . . . . . . . . . . . . . . . . . . . . . . . 92
5.3.1 Classes of learnable decision boundaries . . . . . . . . . . 93
5.3.2 Characterisation of robust points . . . . . . . . . . . . . . 94
5.3.3 Robustness results . . . . . . . . . . . . . . . . . . . . . 96
5.3.4 Modifications for finite sampling . . . . . . . . . . . . . . 104
5.3.5 Existence of robust encodings . . . . . . . . . . . . . . . 107
5.3.6 Lower bounds on partial robustness . . . . . . . . . . . . 108
5.4 Numerical results . . . . . . . . . . . . . . . . . . . . . . . . . . 110
5.4.1 Decision boundaries and implementations . . . . . . . . . 110
5.4.2 Robust sets for partially robust encodings . . . . . . . . . 113
5.4.3 An encoding learning algorithm . . . . . . . . . . . . . . . 114
5.4.4 Fidelity bounds on partial robustness . . . . . . . . . . . . 117
5.5 Discussion and conclusions . . . . . . . . . . . . . . . . . . . . . 119
5.5.1 Subsequent work . . . . . . . . . . . . . . . . . . . . . . 120
6 Generative modelling with quantum circuit Born machines 123
6.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
6.2 Quantum advantage in generative modelling . . . . . . . . . . . . 124
6.2.1 The Ising Born machine . . . . . . . . . . . . . . . . . . 125
6.2.2 The supremacy of quantum learning . . . . . . . . . . . . 126
6.2.3 A learning advantage via quantum computational supremacy129
6.3 Training a quantum circuit Born machine . . . . . . . . . . . . . 132
6.3.1 Training with the Stein discrepancy . . . . . . . . . . . . 134
6.3.2 Computing the Stein score function . . . . . . . . . . . . 138
6.3.3 Training with the Sinkhorn divergence . . . . . . . . . . . 141
6.4 Numerical Results . . . . . . . . . . . . . . . . . . . . . . . . . . 149
6.4.1 The data . . . . . . . . . . . . . . . . . . . . . . . . . . 149
6.4.2 Comparison between training methods . . . . . . . . . . . 152
6.4.3 Quantum versus classical generative modelling in finance . 157
6.4.4 Weak quantum compilation with a QCBM . . . . . . . . 167
6.5 Discussion and conclusion . . . . . . . . . . . . . . . . . . . . . . 167
6.5.1 Subsequent work . . . . . . . . . . . . . . . . . . . . . . 169
7 Practical quantum cryptanalysis by variational quantum cloning 171
7.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171
7.2 Variational quantum cloning: cost functions and gradients . . . . 173
7.2.1 Cost functions . . . . . . . . . . . . . . . . . . . . . . . 173
7.2.2 Cost function gradients . . . . . . . . . . . . . . . . . . . 176
7.2.3 Asymmetric cloning . . . . . . . . . . . . . . . . . . . . . 177
7.2.4 Cost function guarantees . . . . . . . . . . . . . . . . . . 178
7.2.5 Sample complexity of the algorithm . . . . . . . . . . . . 189
7.3 Variational quantum cryptanalysis . . . . . . . . . . . . . . . . . 191
7.3.1 Quantum key distribution and cloning attacks . . . . . . . 191
7.4 Quantum coin flipping and cloning attacks . . . . . . . . . . . . . 193
7.4.1 Quantum coin flipping . . . . . . . . . . . . . . . . . . . 193
7.4.2 2-state coin flipping protocol (P1) . . . . . . . . . . . . . 194
7.4.3 4-state coin flipping protocol (P2) . . . . . . . . . . . . . 198
7.5 VarQlone numerics . . . . . . . . . . . . . . . . . . . . . . . . . 203
7.5.1 Fixed-structure anstze . . . . . . . . . . . . . . . . . . . 204
7.5.2 Variable-structure anstze . . . . . . . . . . . . . . . . . 208
7.5.3 State-dependent cloning . . . . . . . . . . . . . . . . . . 212
7.5.4 Training sample complexity . . . . . . . . . . . . . . . . . 217
7.5.5 Local cost function comparison . . . . . . . . . . . . . . 217
7.6 VarQlone learned circuits . . . . . . . . . . . . . . . . . . . . . . 218
7.6.1 Ancilla-free phase-covariant cloning . . . . . . . . . . . . 218
7.6.2 State-dependent cloning circuits . . . . . . . . . . . . . . 219
7.7 Discussion and conclusions . . . . . . . . . . . . . . . . . . . . . 219
8 Conclusion 221
A Proofs and derivations 223
A.1 Proofs for Chapter 5 . . . . . . . . . . . . . . . . . . . . . . . . 224
A.1.1 Proofs of Theorem 9 and Theorem 10 . . . . . . . . . . . 224
A.1.2 Proof of Theorem 14 . . . . . . . . . . . . . . . . . . . . 225
A.2 Proofs for Chapter 7 . . . . . . . . . . . . . . . . . . . . . . . . 227
A.2.1 Proof of gradients for VarQlone . . . . . . . . . . . . . . 227
A.2.2 Proof of Theorem 19 . . . . . . . . . . . . . . . . . . . . 228
A.2.3 Proof of Theorem 20 . . . . . . . . . . . . . . . . . . . . 229
A.2.4 Proof of Theorem 26 . . . . . . . . . . . . . . . . . . . . 231
A.2.5 Proof of Theorem 30 . . . . . . . . . . . . . . . . . . . . 232
A.2.6 Proof of Theorem 31 . . . . . . . . . . . . . . . . . . . . 233
A.2.7 Proof of Theorem 34 . . . . . . . . . . . . . . . . . . . . 234
Bibliography 237
List of Figures
2.1 Skeleton design of the Aspen-7/Aspen-8/Aspen-9 32 qubit chip
series of Rigetti. . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
2.2 Select sublattices from the Aspen-7 and Aspen-8 chips. . . . . . 23
2.3 The SWAP test. . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
2.4 Ideal cloning circuit for universal and phase-covariant cloning. . . 37
3.1 Examples of neural network architectures. . . . . . . . . . . . . . 42
3.2 Two canonical neural networks. . . . . . . . . . . . . . . . . . . . 42
3.3 A feedforward neural network trained on the MNIST training dataset 45
3.4 A restricted Boltzmann machine learning to generate MNIST digits. 46
3.5 An example 6 visible node restricted Boltzmann machine . . . . . 52
3.6 The support vector machine and feature maps . . . . . . . . . . . 54
4.1 Cartoon illustration of a variational quantum algorithm. . . . . . . 63
4.2 Examples of VQA anstze. . . . . . . . . . . . . . . . . . . . . . 66
5.1 A binary quantum classifier. . . . . . . . . . . . . . . . . . . . . . 84
5.2 A visual representation of classical data encoding for a single qubit. 87
5.3 Cartoon illustration of robust points for a single qubit classifier . . 90
5.4 Misclassification percentage as a result of Pauli noise. . . . . . . . 97
5.5 Misclassification percentage as a result of measurement noise. . . 104
5.6 Examples of learnable decision boundaries for a single qubit classifier.110
5.7 Three single qubit (two dimensional) datasets. . . . . . . . . . . 111
5.8 Circuit diagrams for the PQC anstze used for classification. . . . 112
5.9 Partial robustness for the dense angle encoding. . . . . . . . . . . 113
5.10 Partial robustness for the amplitude encoding. . . . . . . . . . . . 114
5.11 Cartoon illustration of the encoding learning algorithm. . . . . . . 115
5.12 Minimum cost achieved from applying the encoding learning algo-
rithm to three example datasets. . . . . . . . . . . . . . . . . . . 117
5.13 Learnability vs. robustness on the vertical dataset using the parametrised
dense angle encoding. . . . . . . . . . . . . . . . . . . . . . . . . 118
5.14 Fidelity bounds on robustness . . . . . . . . . . . . . . . . . . . . 119
6.1 Data generated from FX spot prices of the currency pairs. . . . . 151
6.2 Comparison of Born machine training methods for 3 qubits, MMD
Sinkhorn and Stein training with exact and spectral score functions.155
6.3 Comparison of Born machine training methods for 4 qubits, MMD
Sinkhorn and Stein training with exact and spectral score functions.156
6.4 MMD vs. Sinkhorn for 3 qubits on QVM versus QPU. . . . . . . 156
6.5 MMD vs. Sinkhorn for 4 qubits on QVM versus QPU. . . . . . . 157
6.6 Hardware efficient circuit for the Aspen-7-4Q-C device. . . . . . 158
6.7 Hardware efficient circuits for 6, 8, 12 qubit Born machine ansatz. 159
6.8 2 currency pairs at 2, 3, 4 bits and 6 bits of precision. . . . . . . 162
6.9 QQ plots of the marginal distributions of 2 currency pairs at 6 bits
of precision. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163
6.10 3 currency pairs at 2 bits and 4 bits of precision. . . . . . . . . . 164
6.11 4 currency pairs at 2 bits and 3 bits of precision. . . . . . . . . . 164
6.12 Meyer-Wallach entangling capability for a random choice of pa-
rameters and the trained parameters in the same circuit. . . . . . 165
6.13 4 currency pairs at 2 bits of precision with a 28 qubit QCBM. . . 166
6.14 Differing numbers of layers in the QCBM. . . . . . . . . . . . . . 166
6.15 Differing numbers of hidden nodes in the RBM. . . . . . . . . . . 167
6.16 Weight training on the Boltzmann machine along with the node
biases. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
6.17 Compilation of a p= 1 QAOA-QCIBM circuit to a IQP circuit with
two and three qubits. . . . . . . . . . . . . . . . . . . . . . . . . 168
7.1 Illustration of VarQlone for M N cloning. . . . . . . . . . . . . 174
7.2 The SWAP test circuit illustrated for 1 2 cloning. . . . . . . . . 190
7.3 Cartoon overview of VarQlone in a cryptographic attack. . . . . . 191
7.4 Learning the parameters of the ideal cloning circuit. . . . . . . . . 205
7.5 Local VarQlone cost minimised on a training set of random phase-
covariant states. . . . . . . . . . . . . . . . . . . . . . . . . . . . 205
7.6 Variable-structure ansatz details for VarQlone. . . . . . . . . . . . 209
7.7 Variational quantum cloning implemented on phase-covariant states
using three qubits. . . . . . . . . . . . . . . . . . . . . . . . . . . 210
7.8 Overview of cloning-based attack on the protocol of Mayers et.
al., plus corresponding numerical results for VarQlone. . . . . . . 213
7.9 Cloning attacks and numerical results for the protocol of Aharonov
et. al.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215
7.10 VarQlone for 1 3 and 2 4 cloning with the states of the in the
coin flipping protocol of Aharonov et al., and the effect of gate
pool connectivity. . . . . . . . . . . . . . . . . . . . . . . . . . . 216
7.11 Numerical sample complexity of VarQlone. . . . . . . . . . . . . . 217
7.12 Comparison between the local and squared cost functions for 2 4
cloning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218
7.13 Circuits to clone phase-covariant states, without ancilla for 2 qubits.219
7.14 Circuit learned by VarQlone in to clone states in the protocol of
Mayers et. al.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219
7.15 Circuits learned by VarQlone to clone states from the protocol of
Aharonov et. al. for 1 2, 1 3 and 2 4 cloning. . . . . . . . 220
Introduction & background
The Doctor: . . . Its a bit dodgy, this process. You never know what
youre going to end up with.
 Doctor Who, series 1, episode 13
1.1 Introduction
Quantum machine learning (QML) is nascent, and has the potential to dramati-
cally impact the lives of every human in ways unbeknownst to them. This is
perhaps not surprising, given the nature of its parent fields. Both machine learn-
ing (ML) and quantum computing (QC) have the potential, individually, to reach
into and impact our lives in myriad ways. On one hand, for classical machine
learning, this potential is partially realised, and ML is now commonplace in the
products we consume and the services provided to us. This is due to many rea-
sons, including the abundance of big data given to machine learning algorithms,
and the availability of big compute in the development of specialised hardware
for performing intensive machine learning calculations, like tensor/graphics pro-
cessing units (TPUs/GPUs). On the other hand, we have quantum technologies
(including, but not limited to, quantum computing, quantum information, quan-
tum cryptography and quantum sensing and metrology), whose potential is (to
date) mostly unrealised, but theoretically strong.
One key variable in modern times is the existence (and rapid development) of
small scale quantum computers. These devices utilise many features of quantum
mechanics which are desirable but, due to their small size, are burdened by the
unwelcome aspects also. One primary example is the destruction of quantum in-
formation via outside interaction, which is difficult to eliminate. Fortunately, once
we have quantum computers of a suitable scale, these undesired decoherences
can be corrected via mechanisms from quantum error correction theory. At the
time of writing, however, large enough self-correcting quantum computers are
at least (optimistically) 10 years away. In the meantime, we would like to use the
small systems we have for any interesting purpose whatsoever.
2 1. Introduction & background
As we have already hinted at, an area which has emerged as a promising
source of applications is in machine learning. However, one may still ask the very
pertinent question why should we have any reason to expect quantum computers
to help machine learning? Of course it is understandable why one may want them
to; machine learning is everywhere and is already (and will continue to) changing
our world, and the way we interact with it in many ways. If quantum computers
can aid or accelerate machine learning solutions, one could imagine a plethora of
scientific and business use-cases, with real world impact.
One commonly expressed answer is that both, in many cases, can be reduced
a core element of multiplying large matrices; performing linear algebra in high
dimensional spaces. Such operations are crucial for machine learning, hence the
development of specialised (classical) hardware to do just these operations very
efficiently. Quantum computers do these operations very naturally. Such an argu-
ment is not dissimilar to the original motivation for quantum computers, proposed
by Feynman and others, in the simulation of complex physical systems. Indeed,
tools from quantum simulation provided some of the early proofs of exponential
speedups that quantum computers could (potentially) deliver to machine learning
problems. These proofs kicked off the field of quantum machine learning proper.
However, in contrast with quantum simulation, it becomes significantly trickier to
be sure of a guaranteed advantage in machine learning using quantum techniques.
In many cases, it is very likely that classical machine learning may perform (almost)
equally as well, and in these cases, one would be very justified to ask why should
we bother building an expensive new technology? Indeed, recent dequantisations
of quantum algorithms have done exactly this, and reduced dramatically our hope
for large speedups in ML problems, except perhaps in edge cases.
However, fortunately for this Thesis, the nature and flavour of research quan-
tum machine learning has drastically shifted the last few years, coinciding with
the simultaneous development of the small quantum computers mentioned above.
With direct access to quantum hardware the question somewhat changed from
How can quantum computers deliver speedups to machine learning problems?
to How can we use the quantum computers we have now for machine learning
problems?. Such a mindset change has delivered an explosion of engagement
and excitement to a field which was previously dominated almost exclusively by
provable computer science. Now, anyone with an internet connection may con-
duct quantum machine learning research. Of course, the theoretical aspects of
quantum machine learning, and the development of provable algorithms is still as
important as ever. In terms of motivation and content, this Thesis sits some-
where in the intersection. In particular, we aim to bring provable guarantees and
theoretical justifications to a more experimental plug-and-play approach.
In order to set the scene for this Thesis, and to hopefully aid reading, we provide
two introductory viewpoints. The first, is for machine learning practitioners to
grasp some useful intuition about what quantum technologies have to offer. The
second is for the reverse scenario; to give quantum scientists a brief glimpse into
how quantum technologies may impact machine learning.
1.2. Quantum computing for machine learning 3
1.2 Quantum computing for machine learning
Modern computers and information technology are embedded in almost every as-
pect of our daily lives. Given this ubiquity, it is difficult to look back in time and
imagine an era in which computers were no more than highly specialised pieces
of equipment, used only for tedious calculations and research purposes. Even
Thomas Watson, chairman of IBM, famously said in 1943;  I think theres a
world market for maybe five computers. It is not hard, however, to observe the
modern development of quantum computers and draw parallels between the two
eras. Similar to the clunky computers of the mid-1900s, quantum devices of the
current day are only accessible via a few large industrial bodies, fill entire rooms in
some cases, lack large computational capacity and are used primarily for research
purposes. Unlike the pioneers of the past, modern quantum engineers have the
luxury of a precursor, and can use this information to guide and accelerate de-
velopment of quantum technologies. Unlike Watson, we almost have the reverse
problem; not imagining the scope of potential use cases for the novel technol-
ogy, but actually tempering the hype surrounding it, with claims1 that quantum
computing will revolutionise every aspect of our lives, from finance to engineering
to medicine. As such, the cautious quantum algorithm developer must not only
design the algorithm, but also provide evidence that no classical algorithm could
achieve the same thing; a so-called quantum advantage. This is part of the
reason why quantum algorithm development is difficult.
A common misconception is that this advantage2 is due to the fact that quan-
tum computers can try all solutions in parallel  (since the fundamental building
block of quantum logic, the qubit, can exist in a superposition of all possible states
simultaneously), and hence arrive at the problem solution exponentially faster than
is possible using purely classical logic. In reality, superposition is only one aspect
of quantum advantage (other sources include interference, non-locality, and con-
textuality) and, in practice, careful manipulation3 of quantum systems is required
to realise such advantages and build quantum algorithms. The famous algorithms
that kindled early interest in quantum computation, such as Shors celebrated
factoring algorithm, and Grovers algorithm to accelerate search, do exactly this -
using the natural abilities of quantum computers to deal with complex problems.
It is also commonly stated that quantum computers are a natural solution
to the demise of Moores law, as the rapid decrease of available real estate on
integrated circuits precipitates the emergence of quantum effects, which are inten-
tionally suppressed by chip manufacturers. It is perhaps less likely that quantum
computers will replace modern classical computers, but instead will become an-
other piece of specialised hardware used for specific problems only. It is not likely
1Whether these are true or not is unknown.
2Here we use the term advantage loosely to mean quantum doing something better than
classical in some capacity. In fact, the nature and manifestation of quantum advantage is a
subtle, deep and sometimes controversial point [RWJ+14, AAB+19, Cho19]. We solidify what we
mean by quantum advantage later at the relevant points in this Thesis.
3Both in theory and in practice.
4 1. Introduction & background
we will have quantum mobile phones any time in the (near) future. With this
perspective in mind, it is more natural to fit quantum computers into the current
machine learning ecosphere, supplementing the specialised computing devices that
we use currently for machine learning tasks, such as GPUs.
Looking at the similar development track between classical and quantum com-
putational hardware, one may also draw parallels between early machine learning,
and the development of its quantum counterpart. As mentioned above, much of
the success of modern machine learning and deep learning is due to the access to
hardware. Similarly, access to (albeit small scale) quantum computers has changed
the manner in which much of quantum machine learning research is conducted.
The physical demonstrations of quantum computational supremacy beginning
in 2019 indicated that we are now at a transition period, as fully programmable
devices exist which can perform tasks out of the reach of even the largest super-
computer on the planet. These tasks are not yet useful in any practical sense,
but are specifically designed to play to the natural strengths of the devices on
which they are implemented. As mentioned above, access to these devices en-
ables a new, more experimental type of quantum machine learning research. In
many cases, we have to sacrifice provable guarantees for the algorithms we run
on near term quantum devices, since they are primarily heuristic in nature (not
very dissimilar to modern machine learning in fact), but in return we gain extreme
flexibility in algorithm design and we can simply try them and see.
1.3 Machine learning for quantum computing
According to Arthur Samuel, a pioneer credited with the popularisation of the
name machine learning, the field of machine learning is the field of study that
gives computers the ability to learn without being explicitly programmed . This is
perhaps a slight misconception, since modern computers, despite being incredibly
successful at playing complex games such as chess, or Go4, are not considered
to be intelligent, or even learn in the same manner that humans do. In a more
practical definition, machine learning models and algorithms reproduce and impor-
tantly, generalise from observations, or data. The human brain has a remarkable
capacity for learning and generalisation, and ML algorithms have only been able to
emulate this by focusing on very specialised scenarios, with limited cross-domain
applicability.
Nevertheless, machine learning has the potential to be (and already is) ex-
tremely impactful in our lives. For example, aiding doctors in reducing false
positive and negative cancer diagnoses5, or reducing road traffic accidents with
autonomous vehicles by removing a major cause of accidents; human error6.
Just as machine learning is ubiquitous in our daily lives, it has also become an
extremely useful tool in many aspects of quantum science and technology. For
4AlphaGo: The story so far.
5How AI is improving cancer diagnostics.
6How autonomous vehicles could save over 350K lives in the US and millions worldwide.
https://deepmind.com/research/case-studies/alphago-the-story-so-far
https://www.nature.com/articles/d41586-020-00847-2
https://www.zdnet.com/article/how-autonomous-vehicles-could-save-over-350k-lives-in-the-us-and-millions-worldwide/
1.3. Machine learning for quantum computing 5
example, it is used to calibrate and stabilise quantum experiments, which was an
instrumental piece in the quantum computational supremacy experiment discussed
in the previous section. It is has also been used as a tool for foundational research.
For example, in sifting through large amounts of data produced by the large hadron
collider in CERN, looking for patterns in which new particles many be lurking.
Machine learning has also been used successfully in the representation of quantum
states [CT17] or even in the discovery of new experiments entirely [KMF+16].
While all of these are certainly impactful and exciting applications, there are
a number of problems with modern machine and deep learning. The first is the
extreme expense required to train huge models, for example the recent demon-
stration of the impressive natural language processing model, GPT-37, reportedly
cost an estimated $12 million dollars to train. Secondly, deep learning is very
hungry for high-quality data, the lack of which in many situations can lead to poor
results. Manually collecting and labelling the data required for supervised learning
is an expensive an time consuming task, having to be done manually by humans
in many cases. It can also be very difficult to interpret modern machine learning
models, in particular large neural networks, which arrive at problem solutions via
the complex interaction between their billions of parameters. How an individual
parameter correlates with the network output is almost incomprehensible to hu-
man interpretation. This latter limitation is extremely important in areas where
machine learning is used in sensitive issues, for example policy decisions or medical
diagnoses. We want to know why the model is doing what it is doing.
It is largely hoped that quantum computers may be able to help with at least
some of these problems. For example, the speedups promised by quantum machine
learning algorithms (those based on high dimensional linear algebra) are claimed
to be more interpretable also than classical counterparts, as the ability to run
them many more times can give insights into the decisions being made by them.
It is also hoped that quantum devices may be able to aid with small data, big
compute problems8, which fits nicely into situations where large amounts of data
are not accessible, for example in diagnosing patients with rare medical conditions,
of which there may only be a handful of examples. As mentioned in the previous
section, it is clear that the nature of a quantum advantage in machine learning is
also subtle, and to date it is largely unknown how quantum computers may help
the field. Regardless, it also clear is that the rewards are great for finding such a
thing, which makes it a very exciting goal to strive for.
7GPT-3 stands for the third iteration of a generative pre-trained transformer Language Models
are Few-Shot Learners.
8The small data part is perhaps more of a necessity ; in order to run a QML algorithm the data
must be first loaded onto the quantum computer which may be tricky and time consuming for
large datasets.
https://arxiv.org/abs/2005.14165
https://arxiv.org/abs/2005.14165
6 1. Introduction & background
1.4 Thesis overview
Before diving into background material, let us provide a brief summary of the
contributions from the primary chapters of the Thesis. Each chapter provides
one application and model and the chapters are ordered relative to an increasing
complexity of the data presented to the application in question.
 Chapter 5 focuses on the use of the variational quantum algorithm (VQA)
and the parametrised quantum circuit (PQC) for the supervised learning
task of classification. Here, our aim is to study the means in which data
can be encoded into PQCs in a way to be robust to some of the noise
sources present in NISQ computers. We find that by focusing on the use
of the quantum device for an application specific task, we can gain some
noise robustness simply by careful construction of the classifier model. By
robustness in this context, we mean the preservation of classification results
before and after the noise channel is applied. We find that data encodings
which preserve classification will always exist, and discuss the trade offs
in finding suitable encodings in practice. We provide several theoretical
results and extensive numerics to supplement this question. The work of
this chapter was based on a collaboration with Ryan LaRose from Michigan
State University, and resulted in the publication Physical Review A 102,
032420 (2020) - Robust data encodings for quantum classifiers.
 Chapter 6 is concerned with a PQC for the purpose of generative modelling,
which falls into the category of unsupervised learning. The specification of
the PQC is referred to as a Born machine, since the statistics it generates
originate directly from Borns rule of quantum mechanics. We study sev-
eral aspects of the application of this model to the problem of generative
modelling. Firstly, an argument about provable quantum advantage with a
Born machine is presented. We then describe new training methods for the
Born machine. Finally, we provide extensive numerics on three datasets.
Here, we begin by demonstrating the effectiveness of the training meth-
ods. We next provide an example of a real world use case with a financial
dataset; a Born machine as a market generator, and compare against the
restricted Boltzmann machine for this problem. We finally turn to a quan-
tum dataset, and propose the use of the Born machine as a weak method
of quantum compilation. The discussions of quantum advantage, and the
training methods for the Born machine (plus related numerics) were the
result of a collaboration between Daniel Mills, Vincent Danos and Elham
Kashefi from the University of Edinburgh. This resulted in the publication
npj QI 6, 60 - The Born Supremacy: Quantum Advantage and Training of
an Ising Born Machine.. The part of this chapter containing the numerics
for the Born machine relating to the financial dataset, and the comparison
with the restricted Boltzmann machine were the result of a collaboration
with Niraj Kumar and Elham Kashefi from the University of Edinburgh, and
Max Henderson, Justin Chan and Marco Paini from Rigetti computing. This
https://link.aps.org/doi/10.1103/PhysRevA.102.032420
https://link.aps.org/doi/10.1103/PhysRevA.102.032420
https://www.nature.com/articles/s41534-020-00288-9
https://www.nature.com/articles/s41534-020-00288-9
1.4. Thesis overview 7
resulted in the publication QST, 6(2) - Quantum versus Classical Generative
Modelling in Finance.
 Finally, Chapter 7 introduces our third application, the use of a PQC in
a quantum foundations problem, resulting in a new variational algorithm
for the approximate cloning of quantum states, VarQlone. For this algo-
rithm, we prove notions of faithfulness and derive gradients for the cost
functions we propose. We also discuss the existence of barren plateaus in
the algorithm. As a new research direction, we propose variational quan-
tum cryptanalysis; the merging of quantum cryptography with quantum
machine learning, and demonstrate the applicability of VarQlone in this con-
text. Concretely, we study quantum protocols whose security reduces to
quantum cloning (specifically quantum key distribution, and quantum coin
flipping), and show how VarQlone can be used to discover new attacks on
these protocols which are directly implementable, given only a specification
of the available resources from a particular quantum device. For quantum
coin flipping, we also provide new theoretical analyses of two example pro-
tocols, into which VarQlone can be inserted. This chapter is the result of
a collaboration with Mina Doosti, Niraj Kumar and Elham Kashefi from the
University of Edinburgh and resulted in the preprint ArXiv: 2012.11424 -
Variational Quantum Cloning: Improving Practicality for Quantum Crypt-
analysis.
https://iopscience.iop.org/article/10.1088/2058-9565/abd3db/meta
https://iopscience.iop.org/article/10.1088/2058-9565/abd3db/meta
https://arxiv.org/abs/2012.11424
https://arxiv.org/abs/2012.11424
https://arxiv.org/abs/2012.11424
Preliminaries I: Quantum
information
2.1 Quantum computing
Quantum computing is really easy" when you take the physics out of
 Prof. Scott Aaronson
In the year 2000, David DiVincenzo proposed seven ingredients, known as the
DiVincenzo Criteria [DI00], required to construct a physical quantum computer.
The first five are the following (the final two refer to quantum communication
and are less relevant for our purposes):
The DiVincenzo criteria:
1. A scalable physical system with well characterised qubits. (Quantum
bits).
2. The ability to initialise the state of the qubits to a simple fiducial
state. (Quantum state preparation).
3. A universal set of quantum gates (Quantum operations).
4. A qubit specific measurement capability (Quantum measurement).
5. Long relevant decoherence times (Quantum noise).
These criteria refer to the physical implementation of each ingredient, and
so are relevant for quantum physicists and engineers who directly work with the
quantum hardware and qubits. The physical platforms in which these criteria can
be realised have many forms, and qubits have been realised in many (competing)
technologies. Ions, superconducting circuits or photons are among the most ubiq-
uitous (at the time of writing) mediums in which qubits are realised. We remark
a refinement and generalisation of DiVincenzos criteria have been proposed by
10 2. Preliminaries I: Quantum information
Ladd, Jelezko, Laflamme, Nakamura, Monroe and OBrien (LJLNMO) [LJL+10].
The LJLNMO criteria are only three: scalability, universal logic and correctability,
which allow for the possibility for quantum computation to be performed with more
general building blocks than qubits (e.g. qudits or continuous variable systems)
and allow alternative logic operations than quantum gates (e.g. adiabatic quan-
tum evolution [FGGS00] or measurement-based quantum computation [RB01]),
among other generalisations. In this Thesis, we abstract away the physical imple-
mentations or the LJLNMO generalisations. This abstraction is the mathematical
model of quantum computation, and in the following sections, we describe the rel-
evant ingredients of it, which match closely with the requirements in DiVincenzos
criteria.
2.1.1 Quantum states
A fundamental object in quantum mechanics is the quantum state. This quantum
state resides in a Hilbert space, H, whose dimension is denoted d . A Hilbert space
is a vector space equipped with an inner product, and is complete with respect to
the norm defined by the inner product. A qubit is a quantum state with dimension
d = 2. We can define a basis for the corresponding Hilbert space, whose elements
are vectors, |0, |1  H:
(2.1)
The notation | is known as Dirac notation and is called a ket. Since H is a
vector space, we can define a dual space, H, and states in the dual space are
known as bras, denoted |. A vector in the dual space is obtained by taking
the complex conjugate transpose of |: | := |. Since H is a vector space,
linear combinations of these two vectors also reside in H: | := |0+|1  H.
However, in order for these states to be valid quantum states, we impose the
restriction that | must be a vector with norm 1, which implies that ||2+ ||2 =
1. States in the general form of | are referred to be in superposition, since a
measurement of this qubit will reveal one of the two possible states, |0, |1 with
some probability. We return to this point in Sec. 2.1.4. This probability is defined
by the amplitudes, ,, of the state which can be complex numbers in general,
,  C.
A vector representation of a general qubit in superposition can be written as:
|= |0+|1= 
(2.2)
The qubit is a fundamental building block of quantum logic, and is named to draw
parallels between the classical logic unit, the bit. Unlike the qubit, a classical bit
has only definite states it can reside in, i.e. a bit, b, can only be on (b = 1) or
off (b = 0), with no intermediate possibilities. Since H is a Hilbert space, it is
equipped with an inner product defined between two states, |=
, |=
2.1. Quantum computing 11
|=
 
= +  (2.3)
The vectors, |0, |1 are of course not a unique choice for a qubit basis. Any
spanning set of linearly independent vectors will suffice to build a basis, but those
in Eq. (2.1) are usually called the computational basis. Two other important
bases are {|+, |} and {|+i, | i}, given by:
|+ :=
|0+ |1), | :=
(|0 |1) (2.4)
|+i :=
(|0+i|1), | i :=
(|0 i|1) (2.5)
These three sets of states, {|0/1}, {|+/} and {|+i/ i} are the eigenstates of
the Pauli matrices, which we shall introduce shortly.
2.1.1.1 Mixed states
The formalism described above is actually not sufficient to capture the full gen-
erality of a possible quantum state. Specifically, the state presented in Eq. (2.2)
is an example of a pure quantum state - any correlations present in this state
are fundamentally quantum1. In general, we may have a quantum state, which
also contains some classical randomness, or uncertainty. For example, we could
imagine instead of having a single (pure) quantum state, |, we may have an
ensemble of N (pure) states, {|i}Ni=1. Furthermore, we may have some proba-
bility distribution, {pi}Ni=1 over the elements of this ensemble. From this, we can
construct a mixed quantum state:
pi |ii | (2.6)
which is the effective state we would generate if we chose to prepare one of the
pure states, |ii |2, with probability pi . The density matrix formalism allows
us to model our uncertainty about which (pure) state the system is actually in.
Formally speaking, a density matrix is an operator on the Hilbert space:  :HH
and we denote the space of density matrices3 to be S(H), which is a convex set.
One important property of density matrices is that they have trace 1; Tr() = 1,
which ensures probability conservation.
2.1.1.2 More qubits
One qubit, however, is not usually sufficient to do anything interesting - the
evolution of a single two level quantum system can be easily simulated classically
1This will be an important distinction in Chapter 6.
2The notation | | is nothing more than the outer product of the state, |, with itself.
Since | is a column vector, and | is a row vector, | | is a matrix.
3We later use the shorthand notation Sn to represent the space of n-qubit density matrices.
12 2. Preliminaries I: Quantum information
by multiplying 22 matrices. The power of quantum computing comes into being
when multiple quantum systems are added, and it turns out quantumly4 (unlike
with classical systems) a many body quantum system is worth more than the sum
of its parts.
The most common formalism used in quantum computing to describe multiple
systems is the tensor product model. Formally, a tensor product between two
Hilbert spaces, H1 and H2 is denoted by H1,2 :=H1H2, which is also a vector
space. A basis ofH1,2 is formed by taking the tensor product of the basis elements
of the component Hilbert spaces.
However, not all states in H1,2 can be written in the form 1 2 for some
1  S(H1),2  S(H2). Such states are by definition entangled. More precisely:
Definition 1 (Entangled & separable states).
Any state 1,2 in S(H1,2) which can be written as:
1,2 =
2 (2.7)
for some {pi},i pi = 1 and for some states {i1} in S(H1) and {
2} in
S(H2) is separable and any state which is not separable is entangled.
A special case of separability occurs when there is just one pi , in which the
state is called a product state.
Using the tensor product and density matrix formalism, we can also describe
individual subsystems of a multi-qubit state. This is achieved using the partial
trace and reduced density operators. Let us take 1,2 from above. We can
recover the subsystem 1 by taking the partial trace over subsystem 2 ([NC10])
1 := Tr2 (1,2) (2.8)
defined by:
Tr2 (|11 | |22 |) := |11 |Tr (|22 |) = |11 |2 |2 (2.9)
where |1, |1  H1, |1, |2  H2. This action is referred to as tracing out
the subsystem 2.
2.1.2 Quantum operations
A general quantum operation is known as a channel, which maps quantum states
to quantum states in two different Hilbert spaces:
E : S(H1)S(H2), E(1) = 2 (2.10)
There are multiple ways to think about the interpretation of such channels [NC10].
For example, as an interaction between the quantum system and some environ-
ment. Alternatively, in a more mathematical sense using the operator-sum formal-
ism. Finally, we could build an interpretation from physically motivated principles
4We use this widely adopted phrase to mean anything done via quantum mechanical means.
2.1. Quantum computing 13
or axioms we would expect quantum processes to obey. It turns out that these
three viewpoints are equivalent, and they may each have their own utility in a
particular scenario [NC10]. For mathematical usefulness, we primarily use the
operator-sum formalism for the remainder of this Thesis.
In this formalism, the channel E , can be represented as:
 := E() =
(2.11)
where the action on  is specified by k operation elements (or Kraus opera-
tors [KBDW83]) {Ek} which can be specified as complex-valued matrices. In this
form, the operator E can be proven to be completely positive (CP) and we also
require a completeness relation on the operators:
Ek  1 (2.12)
In order to ensure the conservation of probabilities through E . Furthermore, E
becomes a completely positive trace preserving (CPTP) map if we have an equality
in Eq. (2.12). The relationship to the trace of the input and output quantum
states can be seen as follows:
Tr() = Tr
 Tr (1) = Tr() (2.13)
Beginning with definitions of quantum channels in this way allows in the following
sections to look at special cases of the above to tease out the parts relevant to
us. An example of trace decrease in the system  is where information is lost to
an environment (see below) as a result of a measurement.
However, for the remainder of this Thesis, we are concerned only with CPTP
maps, and unitary operations, which make up a core element of quantum com-
putation. As mentioned above, we can also envisage a quantum operation via an
interaction with an environment Hilbert space, denoted HE. However, in ac-
cordance with quantum mechanics, this interaction must be unitary, and can be
described by a unitary matrix, U. A unitary matrix, is a complex, square matrix
defined by the property,
U1 = U (2.14)
In order to implement a general channel, E , we can imagine a quantum state in
the environment Hilbert space, E, and then a unitary operation acting on both
quantum states. The action of the channel on the target, , is recovered by
tracing out the environment subsystem:
E() = TrE
UEU
(2.15)
A special case of the above is when we have no environment, and the unitary acts
on the target system directly. If we choose k = 1 and E1 = U from Eq. (2.11) we
 = UU (2.16)
14 2. Preliminaries I: Quantum information
In this case, the system, , is closed. A further simplification occurs if  is a pure
state, in which case we can represent the transition simply as:
|= U| (2.17)
A neat (classical) comparison with unitary evolution is the analogue of stochastic
matrices acting on probability vectors. The fundamental difference is that in
quantum mechanics, the probability vector is replaced with an amplitude vector,
whose elements may be complex.
Unitary evolution is therefore the key driving element in quantum mechanics,
and indeed in quantum computation, where the state | is the output of a
quantum processor, on input |. Given this, the next relevant question is how can
we actually implement such unitaries on quantum devices to drive computation,
especially given the apparent exponential complexity of the problem5. It turns out
that this can be done in terms of simple single and two-qubit quantum operations
on quantum processors, and we shall discuss this in the next section.
2.1.3 Quantum gates
In classical computation, the circuit model is a useful computational model in
which large operations are built by composing smaller ingredients (gates) in a
circuit acting on bit registers. In quantum computation, the analogue is, perhaps
not surprisingly, the quantum circuit model6. Here, we require the ability to
generate and implement the unitary transformations discussed in the previous
section. It turns out that we can do so with the help of a universal set of quantum
gates. Quantum gates are the logic operations which act on quantum information,
analogous to AND, OR and NOT gates in the classical circuit model.
Definition 2 (Universal set of quantum gates [NC10]).
A set of quantum gates, G = {Gi}, is said to be universal for quantum
computation if any unitary operation may be approximated to an arbitrary
accuracy using a quantum circuit involving only those gates.
There are many candidates for universal gate sets, each of which has advan-
tages and disadvantages. For practicality of implementation7, it is also sufficient
5For example, a unitary acting on an n qubit quantum state (containing 2n complex amplitudes)
has dimension 2n2n, which is quite a big matrix.
6Just as with classical computation, the circuit model is not the unique way to describe com-
putation. Quantum computers could alternatively be driven by adiabatic evolution [FGGS00],
measurement-based quantum evolution [RB01] which are equivalent to the circuit model from a
complexity point of view. In this Thesis, we only require the circuit model so we neglect further
discussion of alternatives.
7We do not take into account the efficiency of implementing an arbitrary unitary in terms
of this universal set. Many unitaries may require exponentially many operations from the set to
be implemented [NC10]. We therefore hope that at least some of the unitaries which can be
implemented using polynomially many gates (i.e. those that we can implement on a quantum
computer) are useful in solving problems of interest.
2.1. Quantum computing 15
to restrict to a discrete set of gates and the price we pay for this is an error in
the approximation of the unitary8.
We first list some useful single and two qubit quantum gates, and then com-
ment on their universality. Firstly, we have the canonical Pauli matrices9:
, Y =
(2.18)
Each of these gates induces the following transition on the computational basis
states:
X|0= |1,Y|0=i|1,Z|0= |0 (2.19)
X|1= |0,Y|1= i|0,Z|1=|1 (2.20)
The X gate is the only one has some classical analogue, also being known as the
bit flip gate (or NOT gate). It flips the computational basis state 0 to a 1
state and vice versa. Also to be noted is the effect of the Pauli-Z gate (or PHASE
gate), which only adds a phase (of 1) to the |1 state. The strangest action is
that of the Pauli-Y gate, which flips the computational basis state but also adds
an imaginary () phase.
Of use for our purposes, are the following operations generated by these ma-
trices, which are intuitively rotations around the corresponding axes of the Bloch
sphere10:
Rx() := e
i 2X = cos
1 i sin
i sin
i cos
) ) ,
Rz() := e
i 2Z = cos
1 i sin
Ry () := e
i 2Y = cos
1 i sin
) ) .
(2.21)
The final distinct relevant gate is the Hadamard gate:
(2.22)
which translates between the Pauli-X and Pauli-Z basis (i.e. transforming eigen-
values from one basis to the other):
H|0= |+=
(|0+ |1) H|1= |=
(|0 |1) (2.23)
8In order to exactly build an arbitrary unitary, the gate set is required to be infinite in size (e.g.
to contain all single qubit operations, of which there are infinitely many).
9The specific representation of these unitary matrices is basis dependent. In this Thesis we
assume that everything is relative to the computational (or Pauli-Z) basis - i.e. those matrices
which are diagonal have computational basis states as eigenvalues.
10The Bloch sphere is a convenient illustrative tool to represent single qubit states, which
completely breaks down if we introduce multiple qubits. Nevertheless, we use it extensively in
Chapter 5.
16 2. Preliminaries I: Quantum information
Finally, we can list some useful two qubit gates. The two most common are the
controlled-Z (CZ) and the controlled-X (CX) gates, defined in matrix representa-
tion as:
CX0,1 :=
1 0 0 0
0 1 0 0
0 0 0 1
0 0 1 0
 = q0 
(2.24)
CZ0,1 :=
1 0 0 0
0 1 0 0
0 0 1 0
0 0 0 1
 = q0 
(2.25)
In the above we also include the quantum circuit representation of these gates
acting on quantum registers or wires, qi . The circuit representation of the
single qubit gates above is simply given by their label (e.g. Z,H,Rx()) in a box
acting on a particular qubit wire. As the name of these two qubit gates suggests,
they are controlled, meaning they require a control qubit and a target qubit.
The reason for this nomenclature can be more easily seen if the gates are written
in the following form:
CU0,1 = |00 |011+ |11 |0U1 (2.26)
Where U is an arbitrary single11 qubit unitary. If we choose, U  Z, it can be
checked that the above reduces to the CZ gate in Eq. (2.25). This representation
is useful since it can be seen that if the control qubit (qubit q0) is in the state
|0, the identity operation will be applied to the target qubit (qubit q1), and if
it is in the state |1, the desired operation will be applied12. Hence, the gate is
controlled on the control qubit being 1. Similarly, CX flips the qubit q1 if qubit
q0 is in the |1 state. For this reason, the CX gate is also commonly called the
CNOT gate (controlled-NOT), to draw comparison with the classical analogue
(the NOT gate). The subscripts in CU0,1 are used to keep track of which qubit
is the control, and which is the target, but we can usually drop these when the
ordering is clear from context. Finally, let us introduce another very useful two
qubit gate, the SWAP gate:
SWAP :=
1 0 0 0
0 0 1 0
0 1 0 0
0 0 0 1
 = q0 
= q0  
(2.27)
This gate swaps the two states |01  |10 and therefore has the effect of
swapping the quantum information between two registers, q0,q113. This gate is
11In fact, there is no need for this restriction.
12Based on the projectors into these states, |00 |, |11 |. See Sec. 2.1.4 for further discussion
about projectors.
13Since the other two possible states, |00, |11, are symmetric in q0 and q1.
2.1. Quantum computing 17
especially useful in near term quantum computers, where qubits (upon which we
want to apply a joint operation) may not be directly connected to each other.
Therefore, the SWAP gate serves to route qubits around a quantum chip to bring
them close enough to interact.
Now, we are somewhat in a position to discuss the universality of some of
these gates. As mentioned above, it is sufficient to restrict our attention to a
discrete set of universal gates. One of the most common such sets is:
Gdiscrete = {H,CNOT,S,T} (2.28)
The T and S are just rotations around the Pauli-Z axis, i.e. Rz() for particular
choices of the angle , up to a global phase. We say two unitaries, U,V are
equivalent up to a global phase if we can write U= eiV, for some global phase,
. Such global phases are typically unimportant, since they do not manifest in a
physical way (they cancel out via complex conjugation), and so we do not observe
them. The choice of  which makes the T and S is given by T = /8,S = /4.
The above choice is not unique for a universal gate set, and there are others one
could choose.
One of the holy grails of quantum computation is the Solovay-Kitaev theorem,
which says that given an arbitrary single qubit gate, there exists a sequence of
single qubit gates from the above set Eq. (2.28) that can approximate the original
unitary up to a precision . Most importantly, this sequence has only polyloga-
rithmic in 1, implying that the discreteness is not a problem in practice. This
theorem is essential for building scalable quantum computers, but it is not relevant
for the remainder of this Thesis so we conclude our discussion of universality with
it. For further details on these topics, see the excellent overview given in [NC10]
or the original14 paper of [Kit97].
2.1.4 Quantum measurements
In the previous section, we discussed unitary evolution, a special case of a quantum
operation in which there is only one operator, E1, in Eq. (2.11). Let us discuss
another extreme, which will cover the case of quantum measurement. Measure-
ments in quantum mechanics are crucial ingredients as they are the mechanism
by which (classical) information can be extracted from a quantum system, and so
are necessary in extracting answers from quantum computations.
In contrast to unitary evolution, one can lose information as a result of measur-
ing a quantum system. This is because after a measurement, the quantum state
of a system will irreversibly change and any information which was not extracted
by the measurement will disappear, except in some special cases.
A quantum measurement is defined by a collection of measurement operators,
{Mm}, where m denotes the outcome of the measurement. For example, if the
measurement is binary, only two possible values for m can be observed, m  {0,1}.
The probabilities of observing these outcomes is given by Borns rule:
14Solovay proved and announced the result via a mailing list in 1995, so is unpublished. Kitaev
proved the result independently.
18 2. Preliminaries I: Quantum information
Borns rule of quantum mechanics:
An outcome, m, from a measurement on a quantum system, , occurs with
a probability given by:
p(m) = Tr(MmM
m) (2.29)
Furthermore, if  is a pure state, and if Mm describes the special case of a
projective measurement (meaning the operators, Mm, obey MmMm = m,mMm),
we have:
p(m) = Tr(MmM
m) = Tr(Mm| |M
m) = Tr(|M
mMm|) (2.30)
=  |mm |= |m ||2 (2.31)
In this case {Mm = |mm |} are called projectors15, which project onto
the states, |m. These states are the eigenvalues of an observable, O that can
be written as a spectral decomposition in terms of these eigenvectors, and their
eigenvalues, (the outcomes m):
m|mm | (2.32)
The result of the quantum state after a measurement is:
|  |m :=
Mm|
|MmMm|
(2.33)
and so a measurement can be interpreted as taking the state | and replacing it
with the state |m with a probability given by Eq. (2.30). However, if we do not
care about the state of a quantum system after a single measurement, there is
a useful mathematical formalism which we can use. The positive operator value
measure formalism (POVM) describes more general measurements and here we
define POVM elements as Em =M
mMm. We can see from Eq. (2.29) that the
outcome probabilities p(m) can now be completely described in terms of the op-
erators Em16. A simple example of a measurement is the so-called computational
basis measurement, or a measurement in the Pauli-Z basis. Here, the measure-
ment operators, Mm, are given by the outer products of the computational basis
states, M0 := |00|,M1 := |11 | (it is simple to check that this measurement is
projective as well). The corresponding observable for this measurement is exactly
the Pauli-Z operator, hence the name:
Z=+1|00 |1|11 |=+1
(2.34)
The measurement of observables will be of crucial interest later in this Thesis,
as it is one of the key ingredients in variational quantum algorithms, which we
15In general, a projector can be any operator, P , such that P 2 = 1.
16See [NC10] for further discussion about the usefulness of the POVM formalism.
2.1. Quantum computing 19
introduce in Sec. 4.1. In particular, we care about the expectation values, and
variance of the observables which are defined in the usual way with respect to a
specific state, |:
E[O] := O := |O|= Tr (| |O) (2.35)
Var[O] := O2O2 (2.36)
As a simple example, take the expectation of the Pauli-Z observable on the state
|+= 1/
2(|0+ |1). Computing Z+ gives:
Z+ = +|Z|+=
(0|Z|0+ 0|Z|1+ 1|Z|0+ 1|Z|1) = 1/2(11) = 0
since Z has no off-diagonal terms. Furthermore, since Z2 = 1, we can also simply
compute Var[O] = 1.
2.1.5 Quantum noise
The final special case of a quantum operator (Eq. (2.15)) that is of interest to us
is quantum noise. Noise in a quantum system is often detrimental to useful quan-
tum computation as (in one form) it can cause decoherence (see Divencenzos
criteria). Roughly speaking this is the loss of information from the quantum state
via interaction with its environment [Sch05], i.e. the system E in Eq. (2.15).
The preservation of information in a quantum system is crucial for the operation
of quantum algorithms, and as such the fields of quantum error-correction (QEC)
[Bru19] and fault-tolerance has evolved to find useful ways to correct the errors
that occur in a physical quantum computer. Fundamental theoretical results such
as the fault-tolerant threshold theorem ([KLZ98, Kit03, ABO08]) are important
to believe in the scalability of quantum computers. This theorem roughly states
that quantum errors can be corrected arbitrarily as long as the physical error rate
is below a certain threshold, pth, and implies that errors can be corrected faster
than they accumulate.
In the long term, building error-corrected quantum computers is a universally
accepted end goal, since we have algorithms with provable quantum speedup17
which can be run on them. However, for now (and for the foreseeable future),
the only devices we have available do not have the ability to perform QEC. Such
devices in the current era have been dubbed noisy intermediate-scale quantum
(NISQ) [Pre18] computers and they have on the order of 101-103 noisy qubits.
These devices cannot implement QEC and fault-tolerant encoding of quantum
information simply as a question of resources, the required amount is significantly
higher than allowed by these meager numbers. The usual suspects for the re-
source hungry nature of QEC and fault tolerance, include but are not limited to,
ancilla qubits for error detection, magic state factories [GF19] and the require-
ment for many physical qubits per logical qubit. For example, a recent estimate
17To name the few obvious candidates: Shor factoring ([Sho94]), Grover search ([Gro96]), and
the HHL algorithm ([HHL09]) among others [Mon16].
20 2. Preliminaries I: Quantum information
for factoring 2048 bit numbers using Shors algorithm puts the required number
of qubits at  20 million ([GE21]), which is a factor of 200,000 more than we
have available now. Deriving and building applications for such devices is not only
an important goal in its own right, but can also be very useful for benchmarking
the hardware [MSSD21]. Since the central theme of this Thesis is to find such
applications, we will not discuss further the expansive topic of quantum error cor-
rection, and instead focus our attention towards applications which can be run in
the presence of quantum noise and on small-scale quantum devices.
Finally, we note that quantum noise on a device may be either passive or ma-
licious (or a mixture of both). The former type of noise is that which occurs
naturally in quantum devices - via interaction with outside sources, information is
lost in a somewhat random fashion (the E in E really means an environment).
In contrast, with malicious (or adversarial) noise the computation is being de-
liberately corrupted by a malicious adversary in order to gain information about
the parties performing the computations. Here E can refer to an eavesdropper,
Eve, who is actively manipulating the ancillary state, E
18. When we use the term
noise in this Thesis we mean the former, passive situation.
Let us begin by introducing some common simple noise channels which we use
(primarily in Chapter 5). These models are generally not representative of the
true noise on a given NISQ machine19 in isolation, but they are extremely useful
as a starting point, and are frequently used in theoretical studies.
In the following for clarity, we use the notation  to represent an n-qubit
quantum state, and  for a single qubit state. Recall in the operator-sum formalism
(Eq. (2.11)) a quantum channel can be written as:
 7 E() :=
(2.37)
Choosing the Kraus operators in Eq. (2.37) gives us the noise channels of interest
to us. The first specific instance of which is the Pauli channel.
Definition 3 (Pauli noise channel).
The Pauli channel maps a single qubit state  to EPp () defined by
EPp () := p1+pXXX+pY YY+pZZZ (2.38)
where p1+pX+pY +pZ = 1, p := (pX ,pY ,pZ).
Two special cases of the Pauli channel are the bit-flip and phase-flip (dephas-
ing) channel.
18The fields of quantum verification [GKK19] and quantum cryptography [PPA+20] have
emerged to deal with such adversarial noise.
19Real devices may have noise which is correlated across qubits and may be time-dependent,
meaning that a single quantum device may even have different performances across different
days of the week As such modelling, characterising and mitigating quantum errors is extremely
challenging in practice and is an active area of research.
2.1. Quantum computing 21
Definition 4 (Bit-flip noise channel).
The bit-flip channel maps a single qubit state  to EBFp () defined by
EBFp () := (1p)+pXX (2.39)
where 0 p  1.
While a bit-flip channel flips the computational basis state with probability p,
the phase-flip channel introduces a relative phase with probability p.
Definition 5 (Dephasing noise channel).
The phase-flip (dephasing) channel maps a single qubit state  to Edephp ()
defined by
Edephp () := (1p)+pZZ (2.40)
where 0 p  1.
The final special case of the Pauli channel is the depolarising channel which
occurs when each Pauli is equiprobable pX = pY = pZ = p and p1 = 13p. This
channel can be equivalently thought of as replacing the state  by the maximally
mixed state 1
20 with probability p.
Definition 6 (Depolarising noise channel).
The depolarising channel maps a single qubit state  to Edepop () defined
Edepop () := (1p)+p
(2.41)
where 0 p  1.
The d = 2n-dimensional generalisation of Definition 6 is straightforward.
Definition 7 (Global depolarising noise channel).
The global depolarising channel maps an n-qubit state  to EGDp () defined
EGDp () := (1p)+p
(2.42)
where 0 p 1, d =2n, and 1 :=1d is the d-dimensional identity operation.
Finally, we consider amplitude damping noise which models decay to the |0
state (for example, in a physical implementation this could be the decay from an
excited state to the ground state via spontaneous emission of a photon).
20A maximally mixed state is one which is fully random, contains no useful information and so
is mostly useless for computation.
22 2. Preliminaries I: Quantum information
Definition 8 (The amplitude damping channel).
The amplitude damping channel maps a single qubit state  to EADp ()
defined by
EADp () :=
00+p11
1p01
1p10 (1p)11
(2.43)
where 0 p  1.
One could also consider these channels interacting in parallel on an n qubit
state, via a decomposable quantum channel. For example, a two qubit (sepa-
rable) quantum state,  = 12, could be acted on by a bit flip channel indi-
vidually on each qubit with different magnitudes; E() = EBFp1 E
(12) =
EBFp1 (1)E
(2). As mentioned above, these noise channels are not com-
pletely representative of real quantum noise. Realistic quantum noise is gen-
erally time dependent, gate dependent and includes multi-qubit effects such as
crosstalk [SPR+20], which are correlated and non-local (meaning operating on
qubits which may not be spatially close). Defining, detecting and mitigating this
multitude of errors is one of the major challenges in building useful quantum pro-
cessors.
2.1.6 Quantum hardware
Now that we have some idea about quantum noise and how it affects realistic
computations, let us discuss some real quantum hardware, on which all of the
noise sources described in Sec. 2.1.5 are present.
At the time of writing, there are many competing candidates for the physical
manifestation of quantum computation which satisfy the Divencenzo criteria, or
the LJLNMO criteria. The primary drivers are companies specialising in quan-
tum computation (startups), or companies which have large quantum computing
efforts with roadmaps to build large fault tolerant (universal) devices. For ex-
ample, Google, IBM and Rigetti are some of the main developers of quantum
computers built from superconducting qubits21. On the other hand, companies
such as Xanadu, PsiQuantum and QuiX are developing quantum computers based
on photons22. Others such as IonQ, Honeywell and Universal Quantum are devel-
oping ion-trap quantum computers23, while Pasqal focuses on neutral atoms. Of
course, this list is absolutely non-exhaustive and not to mention countless smaller
endeavours, plus the efforts of governmental and university labs and programmes
around the world. Each of these different technologies have their advantages and
disadvantages, and it is not clear which is the most scalable. Only time will tell.
Later in this thesis, we develop and study algorithms which are particularly
suited to the near-term devices, like those held by the companies listed above.
In order to determine the performance of our variational quantum algorithms and
21See [HWFZ20, KSB+20] for an overview of recent advances in superconducting hardware.
22See [ABB+21, BKL+21a] for example.
23See [LWF+17, WBD+19, PDF+21].
https://quantumai.google/hardware
https://www.ibm.com/quantum-computing/systems
https://www.rigetti.com/
https://www.xanadu.ai/hardware
https://psiquantum.com/
https://www.quix.nl/technology/
https://ionq.com/technology
https://www.honeywell.com/us/en/company/quantum/quantum-computer
https://universalquantum.com/
https://pasqal.io/technology/
2.1. Quantum computing 23
Figure 2.1: Skeleton design of the Aspen-7/Aspen-8/Aspen-9 32 qubit chip series of Rigetti.
Note that while 32 qubits are shown here, not all qubits are actually usable due to defects (for
example the Aspen-7 has only 28 usable qubits, while the Aspen-9 has 31. Furthermore, not
all connections shown above are directly accessible on the chip itself. The Aspen-4 chip has the
same connectivity structure, but only has 16 available qubits. We specify the exact connectivity
we use in this Thesis when relevant.
Figure 2.2: Select sublattices from the Aspen-7 and Aspen-8 chips, corresponding to different
problem sizes. Figure shows the (a) Aspen-7-4Q-C, (b) Aspen-7-6Q-C, (c) Aspen-7-8Q-C, (d)
10 qubit Aspen-8, (d) 12 qubit Aspen-8 and (f) Aspen-7-28Q-A sublattices.
quantum machine learning models in reality, we must implement them on the
actual hardware. For this Thesis, we primarily focus on the Aspen series of Rigetti
quantum chips. We refer to these as quantum processing units (QPUs) and we call
a simulator of such hardware a quantum virtual machine (QVM)24. As mentioned
above, these qubits are superconducting in nature, which is the same technology
that demonstrated quantum computational supremacy in 2019 [AAB+19]25. At
the time of writing, the current iteration of the chip series is the Aspen-9, but
we use primarily slightly older versions in this thesis - the Aspen-4, Aspen-7 and
Aspen-8 models.
In Fig. 2.1, we show the full ideal connectivity26 of the Aspen quantum chip
24Most hardware providers have their nomenclature for quantum simulators/hardware;
QPU/QVM is that given to those of Rigetti.
25Followed shortly by a demonstration on a photonic platform [ZWD+20].
26This sparse connectivity is one of the disadvantages of superconducting technologies - in
comparison to trapped ion qubits, which may have all-to-all native connectivity, [WBD+19].
24 2. Preliminaries I: Quantum information
series, while in Fig. 2.2, some select sublattices of the chip are shown (we use
these specific sublattices in Sec. 6.4.3). As we alluded to above, on these de-
vices, we have native gatesets, which are the basic operations that non-native
quantum programs must be compiled into27. The native single qubit gates are
Rz(),Rx(/2) and for an entangling two qubit gate, we can choose either CZ
(Eq. (2.25)) or the XY gate [ADJ+20]. For this Thesis, we use only the CZ gate,
but one may prefer to use instead the XY gate depending on the application. We
can also get the Ry gate via the following decomposition, which is almost native:
Ry () = Rx
Rz()Rx
(2.44)
Now, the state of the art (SOTA) in quantum hardware is constantly evolving
and therefore this section will be quickly out of date. That said, let us highlight
some specifications given on the Rigetti website (https://www.rigetti.com/)
for the Aspen-9 chip for illustration. Firstly, we have single and two qubit gate
fidelities of 99.8% and 99% respectively. We also have the qubit lifetimes which
are referred to as T1 and T2 times. The T1 time is related to the probability of
a qubit decaying from the |1 state to the |0 state as a function of time. For
example with a simplified model, one may have Pr(|1) = et/T1, so the larger T1
is, the slower the decay will be. This corresponds to the ability to create and
maintain superposition states,  |0+ |1 for as long as necessary. As such, the
T1 time is related to the amplitude damping channel in Definition 8. In contrast,
the T2 time can be modelled as a dephasing quantum noise channel in Definition 5
and related to the time taken for the state |+  |0+ |1 to acquire a relative
phase. For the Aspen-9 chip, the median estimated values for these quantities
are: T1 = 27s,T2 = 19s. For comparison, the median time to operate a one
qubit gate on this chip is T1Q = 48ns, while a two qubit gate requires about
T2Q = 168ns.
One final point to note - in this Thesis we also run experiments on simulated
quantum hardware (via QVMs), in which case we can extract exact (noiseless)
calculations, and have an arbitrary connectivity between qubits. For small scale
problems, using both simulators and real hardware is an essential part of quantum
algorithm research, particularly NISQ algorithms.
2.1.7 Distance measures
We extensively use measures of distance in this Thesis. Having concise methods to
characterise closeness of objects is of fundamental importance, and it is central to
applications. For example, when verifying correctness of the output of a quantum
computation, we need some measure to tell us how well we are doing, and whether
the computation is performing as we expect. Furthermore, as we revisit when
discussing machine learning in Sec. 3.1, distance measures usually provide a cost
27A native gateset is distinct from a universal gate set Definition 2. Even if one has decomposed
a quantum algorithm into a universal gateset, these universal gates must be further decomposed
into the native gates, or even further to pulse-level operations [ADJ+20, AKE+20].
https://www.rigetti.com/
2.1. Quantum computing 25
function which we use to improve the output of a learning algorithm. In this latter
example in particular, the strengths and weaknesses of certain distance measures
determines the effectiveness of the learning procedure. We go to some detail
here in order to impress the importance of this on the reader. Let us begin with
classical concepts first, before moving to quantum measures.
Firstly, for completeness, the definition of a metric:
Definition 9 (Metrics).
A metric, d, on a set, X , is a function:
d : X X  [0,) (2.45)
with the following properties:
 Faithfulness: d(x,y) = 0  x  y
 Symmetry: d(x,y) = d(y ,x)
 Triangle Inequality: d(x,y) d(x,z)+d(z,y)
For all x,y ,z  X .
There is usually no unique choice for a metric, and common choices for Rn
are the 1 and 2 metrics28:
d1(x ,y) := ||x y ||1 :=
|xi  y1| 1 metric (2.46)
d2(x ,y) := ||x y ||2 :=
(xi  y1)2 2 metric (2.47)
The latter is also called the Euclidean distance, and these can be generalised
to p metrics in an obvious way. Here x := [x1, . . . ,xn]T ,y := [y1, . . . ,yn]T are n
dimensional real vectors. Throughout this Thesis, we use x to denote a vector,
and x to denote a scalar value.
2.1.7.1 Probability distance measures
We are also interested in metrics over probability spaces, describing similarity be-
tween probability distributions. For this Thesis, we assume the sample space under
which the distribution is defined is finite (more generally countable)29. Let us de-
fine the space of distributions as D, defined over a sample space, X . Then, for a
particular distribution, D  D, we can use p(x) to denote the probability density
function, or probability mass function, of the outcome x  X . The support of D
28We may also refer to these interchangeably as norms; for example the 2 norm, ||x ||2 which
measures the (Euclidean) length of a vector. Many distance measures can be induced from their
corresponding norm.
29This will always be the case in binary quantum computation, where the space is typically the
space of binary strings, {0,1}n, which we label (technically incorrectly) as the Boolean hypercube.
26 2. Preliminaries I: Quantum information
is the subset of X for which p(x)> 0. Since we assume X is countable, we can
define a vector according to p for each outcome, so D := [p(x1), . . . ,p(xn)]T 30.
When viewed in this manner, the 1 metric above (Eq. (2.46)) also defines a
metric over the probability simplex, and becomes the total variation metric, also
known as the statistical distance. We define it between two distributions, p,q, as:
dTV = TV(p,q) :=
|p(x)q(x)| (2.48)
If p(x) is the outcome distribution from a quantum circuit, x will be a bitstring of
length n, and so the sum in Eq. (2.48) will have exponentially (2n) many terms.
We revisit this point in detail later in the Thesis.
A second relevant distance measure is the Kullback-Leibler (KL) divergence
([KL51]), defined as:
dKL(p,q) := KL(p,q) :=
p(x) log
p(x) logp(x)
p(x) logq(x)
=: H(p)XE(p,q)
(2.49)
H(p) is the entropy of the distribution, p(x), and XE(p,q) is the cross entropy of
p and q, which can be seen as the expectation value of logq(x) under p. Note
that this is not a metric, since it is not symmetric in its arguments31. However, it
has an important history in machine learning and is useful in information theory.
Importantly, it provides an upper bound on the total variation distance via Pinksers
Inequality:
TV(p,q)
KL(p,q) (2.50)
The next important measure we use in this Thesis is the maximum mean
discrepancy (MMD). To introduce this metric, we require a quantity known as a
feature map and a kernel. Kernel methods are important techniques in machine
learning, and in order to do them justice we defer a more lengthy introduction
to Sec. 3.5 in Chapter 3. For now, it is sufficient to think of a feature map as a
non-linear embedding function applied to the sample space to map into a high
dimensional Hilbert space,  : X  H. Then from this, a kernel can simply be
defined as the inner product between two feature vectors, in this Hilbert space,
(x ,y) := (x),(y)H.
Let us also define the mean embedding, p  H which weights the feature
map according to a distribution, p, as follows:
p := Ep((x , )) = Ep[(x)] =
(x)p(x) (2.51)
We can use this mean embedding (which essentially takes an average with respect
to p) then as the root of a distribution comparison measure, which becomes the
30We use the notation D and p(x) interchangeably in this Thesis.
31Switching the arguments in Eq. (2.49) results in the reverse KL divergence.
2.1. Quantum computing 27
maximum mean discrepancy32. The MMD is defined via the difference between
two mean embeddings of two distributions [BGR+06, GBR+07] to be compared:
dMMD(p,q) := ||pq||H (2.52)
We use the notation dMMD here since the object we refer to as the MMD later
in this Thesis is actually the square of the dMMD
MMD(p,q) := dMMD(p,q)
2 = ||Ep[(x)]Eq[(x)]||
H (2.53)
The square in Eq. (2.53) can be expanded to reveal the more computationally
useful form of the MMD:
MMD(p,q) = E
((x ,y))+ E
((x ,y)) 2E
((x ,y)) (2.54)
In this form, the computation of the MMD exploits the so-called kernel trick 
(where  is a kernel defined in Theorem 2), which relieves us of the necessity in
evaluating the feature maps themselves at every point, (x), but only the inner
products between pairs of points (i.e. computing the kernels), which is designed
to be simpler. The same kernel trick is also applied in the support vector machine.
In order to estimate Eq. (2.54), we can draw N independent samples from p,
x := {x1, . . . ,xN}  p, M samples from q, y = {y1, . . . ,yM}  q and compute the
following unbiased quantity ([SFG+09]):
(pN ,qM) := dMMD(pN ,qM)
N(N1)
i =j
(x i ,x j)+
M(M1)
i =j
(y i ,y j)
(x i ,y j) (2.55)
A crucial property of this MMD estimator is the fast convergence rate in probability
it enjoys to its true value (where we denote pN to be the empirical estimate of p
with N samples, and likewise for q)34:
|dMMD(pN ,qM)dMMD(p,q)|Op,q
N1/2+M1/2
(2.56)
This quadratic convergence rate is highly desirable35, since it does not depend on
the dimension of the space from which the samples are drawn. This is due to a
very important property of the MMD (which we generalise later) - it is written
solely in terms of expectation values over the distributions. In contrast, both
the TV (Eq. (2.48)) and KL divergence (Eq. (2.49)) require the evaluation of
32Note, this mean embedding was generalised to use quantum feature maps in [KMS19].
33dMMD itself is the metric on the space of probability distributions.
34The notation Op indicates convergence in probability. A sequence of random variables, {Xn}
is said to converge in probability towards a random variable, X if > 0, lim
Pr(|XnX|> ) = 0.
35This convergence rate means that if we require the estimator to be  close to the true value,
we require O
samples from the distributions (assuming N =M for simplicity).
28 2. Preliminaries I: Quantum information
the probabilities themselves (since they cannot be written in terms of expectation
values of both p,q). The consequence is that if any outcome, x, in (say p) has
exponentially low probability (e.g. p(x) =O(1/2n)), it will require exponentially
many samples to estimate the specific probability of x.
Let us now discuss the final relevant distribution measure relevant for this The-
sis. To introduce this, we move onto the rich subject of optimal transport. The
optimal transport problem was formalised by Gaspard Monge in 1781 and asks
how one can transport one mass (the distribution, p) into another (the distribu-
tion, q), in an efficient manner. We do not attempt to provide a comprehensive
overview of this expansive topic (see [Vil09, PC19] for a starting point), but only
highlight the parts of it which are relevant for our purposes.
In the discrete case, the optimal transport (OT) distance is given by:
OTc(p,q) := min
UU(p,q)
(x ,y)XY
c(x ,y)U(x ,y) (2.57)
where p,q are the marginal distributions of U (called a coupling), i.e. U(p,q) is
the space of joint distributions over X Y such that:
U(x ,y) = q(y), 
U(x ,y) = p(x), (2.58)
in the discrete case. c(x ,y) is the cost of transporting an individual point,
x , to another point y . If we take the optimal transport cost, to be a metric
on the sample space, X Y, i.e. c(x ,y) = (x ,y)36 we get the Wasserstein
metric, which turns out to be equivalent to the Kantorovich metric due to the
Kantorovich-Rubinstein theorem [Dud02]:
dW(p,q) := min
UU(p,q)
(x ,y)XY
(x ,y)U(x ,y) (2.59)
Unfortunately, unlike the MMD with its quadratic sample complexity, the optimal
transport metric has exponential sample complexity, O(N1/k) [SFG+09], where
k is the dimension of the underlying space, X (again assuming k > 2):
W(pN ,qM)d
W(p,q)|Op,q
N1/k +M1/k
(2.60)
All of the probability measures we have introduced above actually can be cat-
egorised into two distinct families. The KL divergence is an example of an f -
divergence, whereas the MMD and the Kantorovich metric (OT) fall into the
category of integral probability metrics (IPMs). Interestingly, the TV distance is
the only measure which is both and f -divergence and an IPM.
f -divergences are a family parametrised by a function, f , as follows [AS66,
Csi67, SFG+09]:
36Note, we confusingly switch between d and  here - both are metrics, but we use  to be a
metric on a vector space while d becomes a metric on a probability space.
2.1. Quantum computing 29
Definition 10 (f -divergence).
Let f be a convex function on (0,) with f (1) = 0. The f -divergence df
between two distributions, p,q defined on X is:
df (p||q) :=
p(x) (2.61)
The choice of the function, f can have a variety of effects on the properties
of the divergence, and in order to recover the two special cases mentioned above,
we can take f (t) = t logt for the KL divergence, and f (t) = |t  1| to give the
TV distance.
In contrast, IPMs are defined as follows [SFG+09]:
Definition 11 (Integral probability metrics).
Let F be a class of real-valued bounded measurable functions on Z. The
IPM, dF , between two distributions, p,q defined on Z is:
dF(p,q) := sup

(x)p(x)
(x)q(x)
= sup
Ep []Eq []
 (2.62)
We use the definitions assuming X is discrete, but the generalisation to general
measurable spaces, M, is straightforward by replacing the sums with integrals,
see [SFG+09]. For IPMs, their individual properties are given by the choice of the
function family, F . For example, we can recover the above Wasserstein, MMD
and TV distances by taking the function families as follows:
MMDFMMD := { H : ||||H  1} dMMD (2.63)
TVFTV := { : ||||  1} (2.64)
Wasserstein/OTFW := { : ||||L  1} (2.65)
||  ||H, ||  ||, ||  ||L in Eq. (2.65) are the norm in the Hilbert space, H, the
infinity norm (or max/sup norm) and the Lipschitz semi-norm respectively. The
latter two are defined by |||| = sup{|(x)| : x  X} and ||||L := sup{|(x)
(y)|/(x ,y) : x = y  X}, where  is a metric on X .
2.1.7.2 Quantum distance measures
Not surprisingly, we can generalise the definitions of distribution measures into
the quantum realm, defining metrics which operate on quantum states in H. The
first one which can be considered is the trace distance, which is a generalisation
of TV to quantum states ,:
dTr(,) :=
Tr|| (2.66)
Here, |A| :=
AA is defined as the positive square root of a matrix A. The trace
distance can be easily shown to reduce to the total variation distance if the two
30 2. Preliminaries I: Quantum information
states, , commute [NC10]. Eq. (2.66) can also be written in terms of the
absolute values of the eigenvalues of the matrix . Just as the TV is a strong
distance measure on distributions, dTr is a strong distance measure on quantum
states.
One can also generalise the 2 norm to quantum states, where is becomes the
Hilbert-Schmidt distance:
dHS(,) := Tr
()2
(2.67)
We also have the following interesting relationship between dHS and dTr [CCC19]:
dHS  d
Tr  rdHS, r :=
rank()rank()
rank()+ rank()
(2.68)
The next important measure is the fidelity [Joz94] between two quantum states,
defined as37:
F (,) :=
(2.69)
The fidelity is symmetric, perhaps surprisingly, but it is not a metric on the space
of quantum states. One can be defined from it however called the Bures angle38
(or quantum angle), defined by:
dBA = arccos
F (,)
(2.70)
This metric has a nice intuition as being the angle between quantum states in
H. If , are pure states, the Bures angle corresponds to the Fubini-Study dis-
tance [ZS05]. The fidelity can be simplified in some special cases. Firstly, lets
consider if  = | | is a pure state, then the fidelity becomes:
F (,) := || (2.71)
which is also the overlap between  and . Furthermore, if = || is also pure,
we get:
F (,) := | ||2 (2.72)
As with the trace distance, the fidelity also generalises its classical counterpart.
However the classical version is not relevant for this Thesis so we exclude it.
As a final remark, we note that in general (i.e. for two general mixed states) it
is likely both the fidelity and the trace distance are exponentially hard to compute.
This is due to the following relationship between the two:
F (,) dTr(,)
1F (,), (2.73)
37A version of the fidelity sometimes appears with a square root [NC10], but we stick to the
squared version in this Thesis.
38The Bures angle is not the only way one can define a metric using the fidelity.
2.1. Quantum computing 31
and the fact that the trace distance defines the complexity class quantum sta-
tistical zero knowledge (QZSK)39. The difficulty in computing fidelity is related
to the fact that the expression Eq. (2.69) contains non-integer powers of 
and . Instead, to get estimates of the fidelity one can compute bounds on it
(see [CPCC20]). These sub- and super-fidelity bounds are made of quantities
which are integer powers of the states, For example, the purity of a quantum
state, Tr
, or the overlap between two states, Tr () can be efficiently com-
puted By efficient here we mean using polynomially many copies of the states,
we can compute an estimate to a polynomial precision and exponential confidence.
This can be done via the SWAP test in the next section.
We have given analogues of the classical TV distance, but not surprisingly
we can generalise the other classical distance measures to the quantum case also.
Above, we defined the classical entropy of a probability distribution (also called the
Shannon entropy). The analogous version for quantum states is the von Neumann
entropy, defined for a quantum state  as:
S() :=Tr [ log] (2.74)
Not surprisingly, this definition is used to define the quantum relative entropy
between two quantum states, ,, which is given by:
S(||) :=Tr [ log]S() (2.75)
By staring at Eq. (2.75) for a few seconds, we can see it looks suspiciously like the
classical relative entropy, or the KL divergence. Indeed, in the case where  and 
commute, it becomes exactly this, as with the TV and trace distance. Finally, we
mention that one can also consider quantum generalisations of the Wasserstein
distance between quantum states (see [DPMTL20], among others), but again
we do not directly use them in this Thesis so we leave further investigation to the
interested reader.
2.1.7.3 The SWAP test
In the previous section, we described and discussed some features of the quantum
fidelity Eq. (2.69), and in particular how it reduces to the overlap between two
quantum states when one of the states is pure. Here, we discuss a very important
algorithmic subroutine which is commonly used in quantum information, called the
SWAP test, introduced by [BCWdW01]. This is a method to extract overlaps (and
hence fidelities) using an ancillary qubit, and a CSWAP gate (see Eq. (2.26)).
We can directly relate the probability of the ancilla qubit, |anc being in the
state |0 after the transformation depicted in Fig. 2.3 to the overlap between two
39This class has a complete problem in deciding whether two quantum states are close or far in
trace distance, which is an exponentially hard problem and not believed to be solvable by quantum
computers in general (it is believed the containment BQP QZSK is strict). It generalises SZK,
which is the classical analogue - deciding whether two probability distributions are close or far in
total variation distance.
32 2. Preliminaries I: Quantum information
Figure 2.3: The SWAP test. The ancillary qubit is measured and the corresponding probability of
getting outcome |0 gives the overlap between  and |, ||.
input states, , |, one of which may be a mixed state [BCWdW01]:
||= 12Pr[|anc= |0] (2.76)
A related subroutine to the SWAP test is called the Hadamard test, and can
be used to extract real and imaginary parts of quantum expectation values. This
test has many uses in quantum computation, but related to the topics of this
Thesis, notably for extracting gradients with respect to quantum circuit parame-
ters. Specifically, the Hadamard test can be used as an alternative method to the
parameter-shift rule [BC21], which we present in Chapter 4.
2.2 Quantum cloning
A fundamental primitive in quantum theory is the famous impossibility result: the
no-cloning theorem40, stated as follows:
Theorem 1: (The no-cloning theorem)
For an arbitrary state, |, in a Hilbert space H, there exists no unitary, U,
operating on HH (e.g. the state | tensored with some reference state
|R  H) with the following behaviour:
U (| |R) = ei(,R)| |, (2.77)
where  is some global phase which may depend on the input states, |, |R.
A simple proof of the above theorem can be seen by observing the action of
the unitary, U, on two different states, |, |. Assume without loss of generality
that we set = 0. Then, take the inner product of U acting on each of | and
|. On one hand, we have:
(| R|)
(| |R) = (| R|)(| |R) since UU = 1
=  |R |R=  |
40The theorem was proven originally by [Par70] in 1970, and independently rediscovered by
[WZ82] and [Die82] in 1982.
2.2. Quantum cloning 33
where we use the assumption that |R is normalised, hence R |R = 1. On the
other hand, we have:(
| R|U
(U| |R) = | || | since U|/ |R= |/ |/
= | ||2
Putting the two together implies:
 |= | ||2 (2.78)
The derived condition is only true if | and | are either identical (in which case
 | = 1), or they are orthogonal (so  | = 0). Clearly, this does not cover
all possibilities and we arrive at a contradiction, so quantum cloning is forbidden.
While this seems like a strange consequence of quantum theory, it is actually
not so different from the classical world. Indeed, there also exists a no-cloning
theorem for classical probability distributions. While this is outside the scope of
this Thesis, some intuition can be gained by imagining a single flip of a coin.
From a single coin toss, we cannot know what the full probability distribution over
possible coin outcomes can be, we only observe the outcome from the singular
coin toss. The coin could be fair (i.e. both heads and tails occurring with equal
probability) or biased (i.e. some skew towards one outcome or the other). The
true answer is hidden from us unless we flip the coin multiple times. Analogously,
since quantum theory is a generalisation of classical probability theory, we cannot
know the contents of an arbitrary quantum state with only a single copy of it.
Clearly, if we had the ability to clone quantum states perfectly, a single copy
would be sufficient to learn it completely (since we could make many clones, and
measure in all possible bases to reconstruct the classical description of the state).
This impossibility is manifested in the no-cloning theorem.
2.2.1 Beyond the no-cloning theorem
Fortunately (for this Thesis at least), the no-cloning theorem is far from the
end of the story. It turns out there some hidden assumptions in the theorem as
presented above. Specifically, we require the cloning machine to be perfect41
and deterministic42. Both of these assumptions can be relaxed, and doing so has
resulted in a rich field of study.
The first work to address these assumptions was [BH96], where the perfect-
ness requirement was relaxed. Here the idea of approximate cloning was born,
meaning that the output clones do not have to be exactly the same as the input
state, but only somehow close. This closeness is typically measured in terms of
the fidelity Eq. (2.69) which we revisit in greater detail in Sec. 2.2.2.1.
The second assumption (determinism) was relaxed originally by [DG97, DG98].
This resulted in probabilistic quantum cloning, where the unitary is able to pre-
pare perfect clones of the input state, but is only able to succeed with a certain
41The output of the cloning unitary must be exactly the same state as the input: | |.
42The unitary must be successful with probability one.
34 2. Preliminaries I: Quantum information
probability. Probabilistic quantum cloning is less relevant for this Thesis so we
neglect further discussion of it in order to focus on the approximate version.
2.2.2 Approximate cloning
2.2.2.1 Approximate cloning preliminaries & notation
Even once one has decided to focus on the approximate version of quantum
cloning, further subdivisions are still possible. To keep track of these, let us
begin with some notation.
The three most common qualities when studying quantum cloning machines
(QCMs) are:
Universality. Locality. Symmetry.
These properties manifest themselves in the comparison metric which is used
to compare the clones outputted from the QCM, relative to the ideal input states.
Almost exclusively in the literature, the fidelity (Eq. (2.69)) is the comparison
metric of choice, and we revisit this shortly.
Universality
This refers to the family of states which QCMs are designed for, S H (H is
the full Hilbert space), as this has a significant effect on their performance. Based
on this, QCMs are typically subdivided into two categories, universal (UQCM),
and state-dependent (SDQCM) quantum cloning machines. In the former case
all states must be cloned equally well (S =H). In the latter, the cloning machine
will be tailored to the family of states fed into it, so S H.
Locality
Locality refers to whether a QCM optimises a local comparison measure (i.e.
check the quality of individual output clones - a one-particle test criterion ([Wer98,
SIGA05]) or a global one (i.e. check the quality of the global output state from
the QCM - an all-particle test criterion).
As in the example above, the state to be cloned, |A43, will be inputted into
the QCM with some blank state (say |0) to carry the clone, plus possibly some
ancillary state |R to aid with the cloning44. For local cloners, the clones will be
43We suggestively use the notation A, to make the connection to quantum cryptographic
protocols later in this Thesis. In this context, it refers to the mythical Alice of Alice & Bob -
the canonical protagonists in cryptographic protocols. The first reference to Alice and Bob dates
back to the seminal RSA paper [RSA78].
44This will behave as an environment as in Sec. 2.1.2 in order to implement general quantum
operations.
2.2. Quantum cloning 35
the reduced density matrices of the output states from the first two registers:
1 = Tr2R
U| | |00 | |RR |U
2 = Tr1R
U| | |00 | |RR |U
) (2.79)
In contrast, for global cloners, we care about the full output state of the QCM,
only neglecting the ancilla state, |R:
1,2 = TrR
U| | |00| |RR |U
(2.80)
Symmetry
Symmetry in a QCM compares one clone relative to the other. Symmetric
QCMs require each clone outputted from the QCM to be the same relative
to the comparison measure, which practically means that both clones will have
the same local fidelity (see below). However, in some scenarios we may wish for
asymmetric output for the clones (meaning they have different fidelities). This
property obviously only applies to local QCMs.
Next, let us revisit the fidelity as the comparison metric of choice, in the
context of these criteria. When dealing with local cloners, we define the local
fidelity, F jL := FL(
j ,A), j  {1,2}. This compares the ideal input state, A :=
| |A45, to the output clones, j .
In contrast, the global fidelity compares the entire output state of the QCM
to a product state of input copies, FG(
1,2,AA). It may seem at first like the
most obvious choice to study is the local fidelity, however the global fidelity is a
relevant quantity for some cryptographic protocols. We revisit this explicitly later
in the Thesis in Chapter 7.
From one to many
In the above, we only assumed a single state was given, and two clones were
required. As a generalisation, we can instead receive M > 1 copies of the input
state. In this case, the task is to produce N >M output clones46. As such, the
input state to the cloning unitary is (we assume the ancillary register, |R, may
have an arbitrary dimension):
|MA |0
(NM)|R (2.81)
This scenario is referred to as MN cloning ([GM97]), and the standard sce-
nario corresponds to 1 247. Now returning to the symmetry criterion concretely,
45For the purposes of this Thesis, the input state will always be a pure state. A notion of
cloning can also be defined for mixed input states, where it is referred to as broadcasting. The
no-broadcasting theorem ([BCF+96]) generalises the no-cloning theorem in this context.
46A note on notation: we primarily use m,n in this Thesis to denote the number of qubits, and
N,M to be typically a number of samples (as, for example, in Sec. 2.1.7.1) or a vector dimension.
In this section, and in Chapter 7, we switch to M,N indicating the number of qubits, for historical
consistence.
47It is interesting to note that in the limit M  , an optimal cloning machine becomes
equivalent to a quantum state estimation machine [SIGA05] for universal cloning.
36 2. Preliminaries I: Quantum information
a symmetric M N cloning machine will require:
L = F
L , j,k  {1, . . .N}. (2.82)
Now that we have our notations in place, let us move onto some concrete examples
in the next section.
2.2.2.2 State-dependent approximate cloning
The earliest result [BH96] in approximate cloning was that a universal symmetric
cloning machine for qubits can be designed to achieve an optimal cloning fidelity of
5/6 0.8333, which is notably higher than trivial copying strategies48 [SIGA05].
In other words, if |A is an arbitrary single qubit (pure) state on the Bloch sphere,
there exists a UQCM which can achieve the local fidelity FU,jL,opt = 5/6, j  {1,2}.
This is not the end of the story however, and much higher fidelities can be achieved
using a state-dependent QCM. Roughly speaking, higher cloning fidelities corre-
spond to extra degrees of freedom which are known to the cloner. The two most
common examples are phase-covariant states and fixed-overlap states, which
we describe next.
Phase-Covariant Cloning
Phase-covariant ([BCMDM00]) states are those states which are, roughly
speaking, confined to a particular two dimensional plane in the Bloch sphere.
The canonical choice is to restrict to the X - Y plane which supports states of the
form:
|xy ()=
|0+e i|1
(2.83)
A SDQCM for these states can be constructed ([BCMDM00]) with FPC,jL,opt  0.85
which notably is higher than the fidelity achievable with a UQCM.
These states are relevant since they are used in BB84 QKD protocols and also
in universal blind quantum computation (UBQC)49 protocols ([BB14, BFK09]).
Interestingly, the cloning of phase-covariant states can be accomplished in an
economical manner, meaning without needing an ancilla register, |R ([NG99]).
However, as noted in [SG01, SIGA05], removing the ancilla has consequences for
quantum cryptography and protocol attacks, which we revisit later in this Thesis.
An alternative parameterisation of phase-covariant states are those in the X -
Z plane:
|xz()= cos()|0+sin()|1 (2.84)
48Examples of trivial cloning machines are where one simply measures the state, and prepares
two copies according to the measurement result, or so-called trivial amplification where one simply
prepares a random state as the second clone, and the first is allowed to pass through undisturbed.
These trivial strategies can achieve an average fidelity of 75% ([SIGA05]), so the reader should
keep this number in mind, particularly in Chapter 7.
49Similar acronyms become confusing.
2.2. Quantum cloning 37
The optimal cloning fidelity here is the same as with the states (Eq. (2.83)),
since in both cases, one degree of freedom is revealed.
To round off this discussion, let us present an explicit quantum circuit which im-
plements the above cloning transformations. A unified circuit ([BBHB97, FWJ+14,
FMWW01]) for all above cases (universal, X-Y and X-Z phase-covariant cloning)
can be seen in Fig. 2.4. The parameters of the circuit,  = {1,2,3}, are
chosen depending on the family of states to be cloned ([BBHB97, FMWW01,
FWJ+14]).
Explicitly, we have suitable angles for universal cloning:
U1 = 
, U2 =arcsin
(2.85)
whereas for phase-covariant cloning of X-Y states, we require:
1 = 
3 = arcsin
 0.477,
2 =arcsin
0.261 (2.86)
Figure 2.4: Ideal cloning circuit for universal and phase-covariant cloning. The Preparation
circuit prepares the registers to receive the cloned states, while the Cloning circuit transfers in-
formation. Notice, the output registers which contain the two clones of |A are the last and
penultimate registers, while the ancilla, R appears at the first register.
2.2.3 Cloning of fixed overlap states
An alternative family of well known states are those which are not defined by their
inhabitancy of a particular plane of the Bloch sphere, but instead by their overlap.
In this case, we usually care about only a finite set of states50. For our purposes,
we usually assume we have only two51 states in this set, typically parametrised as
follows:
|1= cos|0+sin|1
|2= sin|0+cos|1
(2.87)
50As opposed to phase-covariant states, which have a continuous parameterisation.
51We can also consider this type of cloning with four input states instead, which are usually the
two given in Eq. (2.87) plus their orthogonal counterparts. We use both examples in Chapter 7
of this Thesis.
38 2. Preliminaries I: Quantum information
which have a fixed overlap, s := 1 |2= sin2. We refer to this set of states as
fixed-overlap states52. Interestingly fixed-overlap cloning was one of the original
scenarios studied in the realm of approximate cloning [BDE+98], but was diffi-
cult to tackle analytically so was somewhat sidelined. Furthermore, fixed-overlap
cloning has been used to demonstrate advantage related to quantum contextual-
ity [LS20b].
Let us now turn to the optimal achievable cloning fidelities for these states. It
was shown early on ([BDE+98]) that the optimal local cloning fidelity for 1 2
cloning53 is given by:
L,opt =
(1+ s)
33s+
12s+9s2
1+2s+3s2+(1 s)
12s+9s2, j  {1,2} (2.88)
It can be shown that the minimum value for this expression is achieved when s = 1
and gives F FO,jL,opt 0.987, which is much better than the symmetric phase-covariant
cloner (recall FPC,jL,opt  0.85.
To complement this, optimal global fidelity of cloning the two states in Eq.
(2.87) is given by:
F FOG,opt(M,N) =
1+ sM+N+
1 s2M
1 s2N
(2.89)
At this point, it is necessary to draw attention to a certain fact which we will
encounter in great detail later in this Thesis. Namely, it can be shown that the
SDQCM which achieves this optimal global fidelity, does not actually saturate
the optimal local fidelity (i.e. the individual clones do not have a fidelity given
by Eq. (2.88)). Instead, if we take the QCM which achieves an optimal value
of Eq. (2.89), and compute what the local fidelity of each clone would be, we
get [BDE+98]:
L, (M,N) =
1+ sM
1+ sN
1+ s2+2sN
1 sM
1 sN
1+ s22sN
1 s2M
1 s2N
1 s2
(2.90)
which (taking M =1,N =2) is actually a lower bound for the optimal local fidelity,
L,opt in Eq. (2.88). As such, optimising either the local or global fidelities to
find the optimal values will lead to different answers, in general54. We return to
this issue in Chapter 7.
52Confusingly, cloning states with this property is historically referred to as state-dependent,
but as we discussed above state-dependent cloning is a more general scenario where the cloner
generically depends on the input state properties.
53This messy expression gives hints for the tricky analytical nature of this problem, even in the
simplest scenario.
54This is not always the case however, for universal and phase-covariant cloning, local and global
optimisation are equivalent.
Preliminaries II: machine learning
3.1 Machine learning
Machine intelligence is the last invention that humanity will ever need
to make.
 Nick Bostrom, TED, 2015
Having introduced most of the prerequisites in quantum computation, it is
time to turn our attention to second pillar of this Thesis, the expansive field of
machine learning (ML). There are many ways of describing what this field is, and
what it aims to do, but roughly speaking, machine learning is an arm of artificial
intelligence (AI) which aims to train machines to learn to solve some problem.
Most commonly, they (computers) learn from data and solve problems without
being explicitly programmed to do so. As such, the algorithms developed by
machine learning practitioners are usually data-driven, and in many cases, their
success is dependent on the quality of data available to them. The methods and
techniques used in ML algorithms draw heavily on the field of statistics and the
type of machine learning we use here can also be referred to as statistical learning.
For an excellent and gentle introduction to this topic, see [JWHT13a].
Machine learning algorithms are canonically split into three main categories1:
Supervised
learning.
Unsupervised
learning.
Reinforcement
learning.
Each of these subfields are large, and we do not attempt to provide a com-
prehensive overview here. In this immediate section, we are also only referring to
classical tasks, i.e. those where the underlying data is classical (vectors, matrices,
etc.). We generalise this when discussing quantum machine learning in Chapter 4.
The typical differences between these learning types is that supervised learning
learns with labels (we give a more precise definition in Sec. 3.3.1), unsupervised
learning learns without labels, and reinforcement learning learns from experience or
1Although, by modern standards, these categories are oversimplified. For example, there exists
a plethora of intermediate categories, and merging of the ideas in each one. A major example is
semi-supervised learning.
40 3. Preliminaries II: Machine learning
via interaction. To avoid unnecessary tangents, we focus in this Thesis on the two
particular examples which we use explicitly, one from supervised learning, and the
other from unsupervised learning. We also take the viewpoint that (un)supervised
learning is learning with(out) access to the correct answer (or more technically,
the ground truth) for given problem instances, rather than with(out) labels. The
reason is that with this modification, the definitions also then encompass the
contents of Chapter 7 of this Thesis.
However, for now, let us focus on the two following (classical) tasks:
1. Classification (Supervised learning).
2. Generative modelling (Unsupervised learning).
Before diving into these applications, let us first introduce some terminology.
Firstly, we have a machine learning model. A model may be deterministic or
probabilistic, and may be supervised or unsupervised in nature. In the supervised
setting, a model is usually a proxy to represent a function, f (x), defined on some
space x  X . A generative model, on the other hand, is a proxy for a probability
distribution, p. One of the most widespread classical machine learning models are
neural networks, which we discuss in Sec. 3.2 which can be used to solve both
of the problems above. At the time of writing, the closest2 counterpart in the
quantum realm to a neural network is the parametrised quantum circuit (PQC),
which we introduce in Sec. 4.1. These models usually belong to a model family,
M; for example a family of functions,M :=F := {f } or probability distributions,
M :=D := {p}3. It is also common to parametrise the model family,MM.
A machine learning algorithm then optimises these parameters in order to learn,
which corresponds to finding the optimal candidate in the model family to solve
the problem of interest. As we use neural networks as our benchmark model, we
view gradient descent as our canonical machine learning algorithm, since we use
it almost exclusively in this Thesis.
The parameters, , define the model, but there is another type of parameter
which one must also be concerned with. These are hyperparameters, which we
define as follows:
Definition 12 (Hyperparameters).
In a machine learning algorithm, a hyperparameter is a parameter used to
control the learning procedure. It is distinct from a model parameter in that
it is outside the model, and usually associated to the learning algorithm
itself.
Next, we have the concept of (labelled) data.
2We use the term closest here lightly - there is still discussion in the community about what
should be considered the quantum analogue of a neural network. For a (now outdated) discussion
of this topic, see [SSP14].
3This is actually slightly misleading since supervised learning problems also are defined with a
distribution over the input domain.
3.2. Neural networks 41
Definition 13 (A dataset).
A dataset is a collection of datapoints, {x i}i , from some input domain, X .
Each element of a labelled dataset will also have a corresponding label, y i ,
coming from some domain, Y, for each datapoint, x i . We may also have a
probability distribution, D  D defined over X , so x i D.
The above definition of data is intentionally flexible, as in general datapoints
may be binary, real or complex-valued scalars (x i  {0,1}/R/C), vectors (x i :=
z i = [z1, . . .zn]
T Rn/Cn) or even quantum states in a Hilbert space, x i := i H.
In this Thesis, data will take the form of each of the above. Generally speaking,
the difference between supervised and unsupervised learning is the access to, or
lack of, the labels, y i .
Definition 14 (Train/test/validation data).
Before a dataset is used in a ML algorithm, it is split into a training set,
a testing set, and a validation set. The training set is used to train the
model, the test set acts as unseen data and tests the models generalisation
capability and the validation set is used to tweak hyperparameters of the
ML model.
3.2 Neural networks
One of the main workhorses of modern machine learning is the neural network
(NN). In its most basic form, a NN is a collection of nodes of neurons4 which are
connected in a graph structure. This graph structure is one of the defining fea-
tures of NNs. If the graph has no loops (acyclic), it is referred to as a feedforward
neural network, while NNs with loops are often called recurrent. There exists a
plethora of examples of NN architectures, some examples are given in Fig. 3.1.
Neural networks are usually built by stacking layers of simpler ingredients, and
deep neural networks (i.e. those with several layers) are the primary cause of the
deep learning revolution ([Sch15, GBC16]), and have seen great success in solving
difficult real life tasks of practical interest. Each layer consists of trainable param-
eters which are optimised to solve the task in hand. In particular, two we wish to
highlight are the simple feedforward neural network, and the Boltzmann machine,
shown in Fig. 3.2. As observed, the key difference between the two is the graph
structure; the Boltzmann machine contains loops whilst the feedforward network
is acyclic. The parameters in both models consist of trainable weights, W , and
biases, b (or self-loops). We focus on these two examples, because they are canon-
ical examples of classical models for the two primary (classical) ML tasks in which
we are interested in this Thesis. Specifically, the feedforward network is used for
4The inspirations for artificial neural networks came from neurons in the brain. This was
formalised mathematically by McCulloch & Pitts in 1943 in the development of an artificial neuron
([MP43]).
42 3. Preliminaries II: Machine learning
Figure 3.1: Examples of neural network architectures. Image adapted from the mostly
complete chart of neural networks from https://www.asimovinstitute.org/neural-network-
zoo/.
classification tasks, and the Boltzmann machine is used for generative modelling
of probability distributions. We explicitly use the Boltzmann machine in Chapter 6.
Figure 3.2: Two canonical neural networks (NN). (a) A feedforward NN with an input, hidden
and output layer. (b) a fully connected Boltzmann machine with visible (blue) and hidden (green)
units.
https://www.asimovinstitute.org/neural-network-zoo/
https://www.asimovinstitute.org/neural-network-zoo/
3.2. Neural networks 43
3.2.1 Feedforward neural networks
Let us begin with the feedforward NN. This consists of an input layer, into which
the data is inserted, a hidden layer which increases the representational power of
the model, and a final output layer. These layers are connected by the weights,
which form a matrix. For example, in Fig. 3.2(a), W13 is the weight connecting
vertices v1 in the input layer and v3 in the hidden layer. Putting these weights
together forms the matrix W IH. Each node in Fig. 3.2(a) may also have its own
bias, b. These weights and biases are the trainable parameters of the network,
= {W IH,WHO,bI,bH}. Given an input data vector, x , the output of this network
is given by:
f(x) := y
out(x) := (WHOy1(x)+bH)
y1(x) := (W IHx +bI)
(3.1)
The output vector can be a vector of probabilities, for example (as we shall see in
Sec. 3.3.1) the probabilities that the input sample belongs to each of the desired
classes. The function, , is crucially important in a neural network as it provides
a non-linearity to the model5, called an activation6 function. Common choices for
 are the sigmoid, tanh or the ReLU (recitified linear unit) activation functions:
sigmoid(x) =
1+ex
tanh(x) = tanh(x) ReLU(x) = max{0,x} (3.2)
These functions provide differentiable alternatives to the more obvious binary step
function which could be used as an activation function.
3.2.2 The Boltzmann machine
The Boltzmann machine is an example of a physics inspired model known as an
energy based model. This means it is completely defined by an energy function,
which in the case of the Boltzmann machine is given as an Ising model:
E(z) :=
Wi jzizj +
(3.3)
The variables zi  {+1,1}7 are the values that the nodes, vi in Fig. 3.2(b) can
take. Each configuration of the spins will have its associated energy, and the
5Without this non-linear function after each layer in deep neural networks, the model would be
entirely linear, and a deep network would be equivalent to a single layer model. Implementing such
a non-linearity to design quantum neural networks is an active area of research, and is problematic
due to the naturally linear nature of quantum mechanics.
6Coming from the activation of a biological neuron when sufficient input signal is applied.
7This is inherited from the notation of atomic spins in the Ising model of ferromagnetism,
named after Ernst Ising. It is sometimes convenient to associate these variables to be 0 or 1
instead. The weights between nodes Wi j are also called interaction terms, and denoted Ji j , and
the biases, bk , are called local (external) magnetic field strengths and denoted hk for historic
reasons.
44 3. Preliminaries II: Machine learning
model is defined by the distribution over these energy values. In other words,
the probability of a given energy (a given configuration of the n variables, z =
[z1, . . . ,zn]
T ) is given by the Boltzmann distribution:
p(z) :=
eE(z)
, (3.4)
where Z := z eE(z) is the partition function, a normalisation constant which is
intractable to compute in general, due to the exponentially many terms in the
sum over 2n possible configurations of z . We use the notation, zi , to denote a
specific realisation of the value of node i and we use vi to label the node itself.
As observed in Fig. 3.2(b), some of the nodes in the Boltzmann machine may
be hidden also8 which are used to increase the representational power of the
model, and as such the data is associated only to the remaining visible nodes.
In Sec. 3.3.2, we will see how the Boltzmann machine can be used for the second
of our tasks: generative modelling. In this context, the parameters,  = {Wi j ,bk}
of the Boltzmann machine will be used to fit the probability distribution of the
model to a data distribution.
3.3 Machine learning tasks
3.3.1 Classification as supervised learning
As mentioned above, we will use classification as the primary example of supervised
learning. Before diving into the special case, let us define the general problem in
this domain [SP18].
Definition 15 (Supervised learning problem).
Given an input domain, X , and an output domain, Y, a training data set:
D = {(x1,y1), . . . ,(xM ,yM)} (3.5)
of training pairs, (xm,ym)X Y with m= 1, . . . ,M of training inputs, xm
and target outputs ym as well as a new unclassified input, x  X , guess or
predict the corresponding output y  Y.
Two of the most common examples of supervised learning problems are regres-
sion and classification. Roughly speaking, the difference is in the nature of the
variables or labels, yi . In regression problems, the labels are continuous (or quan-
titative), whereas in classification, the labels are discrete (meaning categorical or
qualitative) ([JWHT13b]). There are a plethora of methods to solve these prob-
lems, and in this thesis we use a quantum computer to do so (for the classification
version)
8Hidden nodes are one feature which generalise the Boltzmann machine from its simpler coun-
terpart, the Hopfield network, named after John Hopfield for his 1982 paper [Hop82]. The Hopfield
network is also a deterministic model.
3.3. Machine learning tasks 45
The most basic form of classification is binary classification, where the labels
are only two possible values i.e. y i  {0,1}, so each data point, x i , is assigned a
value of 0 or 1. The goal of the classifier is to learn this function, f , mapping
the data to their corresponding labels. This function may come from a particular
function family, F , which defines our model class. As discussed at the start of this
section, we can parametrise this function family, f  f. If we use a feedforward
neural network for this task the parameters correspond to the weights and biases
of the network. In Sec. 3.6 we will introduce a slightly more formal discussion for
completeness.
A very common baseline example (in the classical world) of such a classifica-
tion task is the classification of digits, using the famous MNIST database, which
consists of 60,000 training images and 10,000 test images of handwritten digits
from 0 to 9. The goal is to learn the mapping from image to digit label, so to
classify all images in the test set correctly.
Training machine learning models has become very accessible in recent years
with the development of deep learning software, particularly in Python, such as
TensorFlow [ABC+16], Pytorch [PGM+19] and Keras [Co15]. Using Tensorflow,
we can very easily load the above MNIST dataset, and train a feedforward NN (as
in Fig. 3.2(a)) to classify images. Fig. 3.3 shows the results of 15 digits from the
trained model. Under each image, we see what the model labelled the test digit as,
and the corresponding probability of doing so. The particular trained model had
100% confidence about each image, except the digit 5 in the first row and third
column, which has a 4% chance of being the digit 6, according to the model. From
this example, one can imagine how neural networks have become so powerful, and
widely used - modern architectures can have billions of parameters [TGR15] and
are applied on much more challenging tasks than digit recognition. As quantum
computers scale in size, we hope they will be able to tackle similar problems.
596%(5) 0123456789
6100%(6) 0123456789
1100%(1) 0123456789
01234567890100%(0)
01234567894100%(4)
01234567899100%(9)
9100%(9) 0123456789
01234567897100%(7)
0100%(0) 0123456789
0100%(0) 0123456789
9100%(9) 0123456789
4100%(4) 01234567892100%(2) 0123456789
01234567891100%(1)
1100%(1) 0123456789
Figure 3.3: A feedforward neural network trained on the MNIST training dataset, classifying
example images in the test dataset. The boxes to the right of each digit show the probability
assigned to each possible label. We see all of these digits were correctly labelled, since the trained
model assigns a high probability weight to the correct label. The model in question is a simple
neural network consisting of an input layer, a hidden layer with 128 nodes, and a final output
layer with 10 nodes, where each node in the output represents a digit, {0, . . . ,9}. This figure was
produced using an example tutorial in Tensorflow [ABC+16].
46 3. Preliminaries II: Machine learning
3.3.2 Generative modelling as unsupervised learning
Chapter 6 of this Thesis will be focused on using quantum computers for generative
modelling, which as mentioned is our primary example of unsupervised learning.
Unlike the supervised learning models discussed in the previous section, gen-
erative models can be significantly more complex, since they have the ability to
also generate new data observations, sometimes called synthetic data genera-
tion. While the former deals with parametrised functions, generative models use
parametrised probability distributions, p, which again come from some distribu-
tion model family, D. The distribution, p may be continuous, in which case p is
the probability density function (PDF) over the sample space, X . In this Thesis
however, we exclusively use discrete distributions over X , so p is a probability
mass function (PMF). In particular, we will take X = {0,1}n.
In the generative modelling problem, we are required to learn some data
distribution, which we denote  over some space Y. We may be given direct
access to the probabilities, (y),y  Y, but more commonly we will only have
a dataset of M vectors sampled according to , {y i}Mi=1,y  . We further
assume  is a discrete distribution over {0,1}n, so for efficiency we only have
M = O (poly(n)) samples. Given our model, p, the goal is then to fit p to ,
using the parameters, . If we were to use a Boltzmann machine for this task, we
would fit the weights and biases in the energy function Eq. (3.3) and train the
model distribution, p(z). In Fig. 3.4, we demonstrate generated results using a
restricted Boltzmann machine on the same MNIST dataset as in Fig. 3.3. In the
next section, we provide some further detail on how to train such neural network
models. This will be especially relevant for the contents of this Thesis, since
the methods we use to train our quantum models incorporate these exact same
techniques (with suitable modifications).
Figure 3.4: A restricted Boltzmann machine learning to generate MNIST digits. From left to
right, we see improving performance (in terms of resolution), indicating that the model is learning
to generate digits which look similar to those in the test set (the rightmost image). In the RBM,
every (visible) spin can by used to represent each pixel being black or white (up or down).
3.4. Training a neural network 47
3.4 Training a neural network
There are a multitude of methods to train machine learning models, such as the
neural networks we presented in the previous section. For particular models and
algorithms, such as the support vector machine (SVM) the training procedure is a
(relatively) simple convex optimisation problem, provided one can easily compute
the underlying kernel matrix (e.g. the one defined in Theorem 2 in Sec. 2.1.7.1).
Unfortunately, for neural networks, the problem is not so simple. This is because
the loss or cost9 function landscape may become highly non-convex due the choice
of our parameterisation, . In Sec. 7.2.1 we will discuss desirable properties of
cost functions in more detail, since they apply to both quantum and classical
machine learning models. For now, let us simply assume that a cost function is
some function of our model parameters, C(), and that it gives us a method to
measure how well we are doing in the training process. The goal of training is to
minimise10 C(), i.e. solve  = argminC(). In many common examples, the
cost can be defined as an expectation over the dataset which, in the limit M,
approaches the expectation over the full distribution from which each datapoint
is drawn, x i D:
C() = ExDCx()
Ci()  
(3.6)
Ci() is the term evaluated using a single sample from the dataset, x i , and Remp
is known as the empirical risk. Minimising this quantity is known as empirical risk
minimisation, and is commonly used as a proxy to minimise our true objective, C.
The cost function defines the cost landscape as a function of , and the
training procedure is to find the absolute (global) minimum of C(). However,
as we mentioned, this problem is non-convex, meaning many sub-optimal local
minima exist and may also be found. The goal of a training algorithm is usually
to efficiently traverse this landscape to find the global minimum.
Let us now focus on NNs specifically. Methods to train NNs can be roughly
split into two categories - gradient-free and gradient-based. Gradient-free meth-
ods are also called zeroth-order methods, since they optimise C using only infor-
mation about C itself. A non-exhaustive list of gradient-free optimisation methods
include Bayesian optimisation ([Moc89]), evolutionary methods ([BS02]) or sim-
ulated annealing ([LA87, KGV83]), see [RS13] for an overview. Gradient-free
methods are useful in cases where perhaps the gradients of C are not available, or
difficult to compute.
9We use these terms interchangeably here; some may consider the cost function to be the loss
function, but with the addition of regularisation.
10We can assume this WLOG since if one may want to maximise C(), we can convert to a
minimisation problem by defining C =C and minimising C.
48 3. Preliminaries II: Machine learning
In cases where we can compute gradients, we can use alternatively gradient-
based optimisation. As the name suggests, these methods use first-order gradient
methods to optimise C. One may also consider training algorithms which use
second-order gradient methods require evaluation of also the Jacobian or Hessian
of C. For this Thesis, first-order gradient based methods will suffice.
The main algorithm we use is stochastic gradient-descent (SGD):
Definition 16 (Stochastic gradient descent).
Given a single sample from a dataset, x i , stochastic gradient descent up-
dates the parameters, , as:
(j+1) (j)C(, j,) = (j)Ci() (3.7)
where j defines an epoch,  is a learning rate and  denotes the gradient
vector with respect to each parameter, k .
The number of epochs is typically a predefined number, and one epoch consti-
tutes one complete pass over the training data. The learning rate, , controls the
speed of training and both of these are hyperparameters (see Definition 12) of
the learning algorithm. While SGD is very fast, it may suffer from high variance
since only a single training sample is used to estimate the true gradient. Instead,
one can consider mini-batch gradient descent, where a certain number of samples
are taken (given by a batch size, b) from the training dataset to estimate C()
for each update. Taking the batch size to be the same as the training data set
size (b = 1) recovers SGD as in Definition 16, while taking b =M recovers regu-
lar gradient descent, where the entire dataset is used in each parameter update.
Clearly, choosing the batch size also heavily affects the speed of training, and so
it is another hyperparameter.
Notice that we have written the update rule for SGD in Eq. (3.7) in two
different ways, one including a term C(, j,), and then a specification of this
expression which is that used specifically for SGD, i.e. an update using a term
Ci(). The former expression is written in this form to allow more general
update rules. For example, one may consider that the update depends on the
epoch, j , i.e. time dependence, or a more complicated expression in terms of the
parameters, .
In this Thesis, we primarily use an update rule known as Adam, which uses an
adaptive learning rate, , and incorporates the notion of momentum to SGD.
3.4. Training a neural network 49
Definition 17 (Adam update rule ([KB15])).
The adaptive momentum estimation (Adam) update rule to the parameters
is given by the following:
(j+1) (j)Adam C(, j,) = (j)
init
v j +
mj , (3.8)
mj =
1j1
, v j =
1j2
, (3.9)
mj = 1mj1+(11)(j)C(), (3.10)
v j = 1v j1+(12)[(j)C()]
2 (3.11)
where 1,2,,init are hyperparameters. 
1 ,k  {1,2} means k raised to
the power j , which results in a exponential decay.
The hyperparameters, 1,211, are heavily weighted towards a value of 1,
which means that random fluctuations in the gradient, if they are sufficient small,
will contribute less than the weighted sum of previous gradients. Therefore, the
optimisation is likely to keep moving in the same direction at each epoch (hence
the term momentum). The vector, v , keeps track of the second moment of
the gradient, and the terms mj , v j are computed in order to correct the tendency
of mj ,v j to be biased towards the zero vector, 0, since they are both initialised
at 0. One could also choose alternative update rules such as Adagrad [DHS11],
Adadelta [Zei12], NAdam [Doz16] or a plethora of others. In many cases, the
update rule is designed to encourage large values at the start of training, but de-
crease towards the end so we do not overshoot the minimum we are after. As we
shall see in Sec. 4.1, for training modern quantum machine learning models, one
may wish to include quantum awareness into such optimisers. To this end, ver-
sions of update rules have been proposed by incorporating quantum measurement
error [KACC20], or to partially overcome linear scaling of computing quantum
gradients [CMMS20, GZCW21] (see Eq. (4.13) in Sec. 4.1.3). We will not use
any of these more complicated methods in this Thesis, so we will not discuss them
further here. However, we remark that all of these methods could be applied and
tested on each of the applications we study in the subsequent chapters.
3.4.1 Computing gradients
Finally, let us remark on the computation of the gradient terms, C() for the
two classical models we have introduced above. For the feedforward NN, we do this
to illustrate a difference between the quantum models we discuss in Sec. 4.1 and
for the Boltzmann machine, we explicitly use it later in this Thesis in Chapter 6.
11The original Adam paper chooses 1 = 0.9, 2 = 0.999,  = 1108, which are the values
we use throughout this Thesis.
50 3. Preliminaries II: Machine learning
3.4.1.1 Feedforward neural networks
For training feedforward neural networks, a key breakthrough was the discovery
of the backpropagation algorithm ([RM87]), which is short for the backpropaga-
tion of errors in the neural network. Recall the structure of the simple example
feedforward NN in Fig. 3.2. We ultimately want to compute the gradient with
respect to each parameter, the weights, WHO,WIH plus the biases of each node,
bI,bH. Fortunately, the output vector, yout is a concatenation of simple functions
applied to the input. So the gradient can be computed using the simple chain
rule (take for example the gradient with respect to the input bias vector, bI with
a slight abuse of notation):
C(W,b)
C(W,b)
yout
yout
(3.12)
C(W,b)
yout
(WHOy1+bH)
(W IHx +bI)
(3.13)
If we use a sigmoidal activation function, y(z) := sigmoid(z) = 1/
1+ez
, the
derivative is simple: sigmoid(z)/z = sigmoid(z)(1sigmoid(z)). Furthermore, since
each layer before the activation function has a simple affine form, it can also be
trivially computed. We must also obviously assume that the cost function is a
differentiable function with respect to yout, but one can choose the 2 norm, Eq.
(2.47) as a simple example.
One crucial feature of the above derivation is the computational efficiency of
the calculation. By staring at Eq. (3.13) and beginning with the parameters at
the end or the network, one can see how each gradient evaluation reuses the
same computational steps. For example, to compute the gradient with respect
to every weight and bias, we must begin by computing C(W,b)/yout. Similarly, the
computation of every parameter gradient will require an evaluation of a gradient
for the layer immediately after it. Storing these intermediate computations as one
trains the network, rather than re-computing them for each parameter separately,
massively improves the complexity. This general principle underpins the field of
differentiable programming, and automatic differentiation. Most deep learning
libraries such as Tensorflow and PyTorch contain an autodiff function which can
perform this differentiation automatically for many functions. We highlight this
here to make the connection to the quantum case. We shall see in Sec. 4.1.3
how the parameter-shift rule, and the black box nature of quantum computers
prohibits these chain rule type of gradient computations (at least in the near term).
As such, for training quantum models we must pay the price of a linear scaling
in the number of parameters which may be a major bottleneck as problem sizes
scale. We will revisit this discussion in Sec. 4.1.3.
3.4.1.2 Boltzmann machine
Now, lets turn to the training of the Boltzmann machine. Recall that here we
are trying to match the visible variables of the network (see Fig. 3.2) to a data
3.4. Training a neural network 51
distribution , given some M samples from ,{y i}Mi=1,y i  (y). As with training
a feedforward neural network, we require a cost function, C(), to indicate how
well our model is training. Contrary to the previous example, now we must use
cost functions which deal with probability distributions, rather than vectors or
labels. Here is where the probability metrics we introduced in Sec. 2.1.7.1 will
become useful. For training Boltzmann machines, we use the KL divergence Eq.
(2.49), as this is one of the most common cost functions. Specifically, if we recall
the definition of the KL divergence between the Boltzmann distribution, p(v)
and the data,  in terms of the cross entropy:
KL(,p) =
(x) log(x)
(x) logp(x) (3.14)
Now the first term is the entropy of the data distribution, which does not depend
on our model parameters, hence we ignore it in the optimisation problem. As
such, our cost function then can be defined as:
C() :=
(x) logp(x)
logp(y i) (3.15)
which is the empirical negative log-likelihood. Minimising this expression, max-
imises the likelihood that the training data vectors, {y i} are observed from our
model distribution. Now, in this case, our trainable parameters are the weights
and self-loops in the Boltzmann machine graph (Fig. 3.2(b)). Due to the relatively
simple form of the probability distribution generated by a Boltzmann machine Eq.
(3.3), Eq. (3.4), we can compute the derivatives with respect to an edge of the
graph as (the edge i j connecting nodes i and j):
C()
Wi j
= v iv jv iv jp ,
C()
= v kv kp (3.16)
which essentially matches the expectation value of each node v k and the corre-
lations between pairs of nodes, v iv j between the model and data distributions.
One might expect that with these expressions, the training is straightforward.
Unfortunately, it is not so simple. In order to evaluate Eq. (3.16), we need to
be able to efficiently sample from our model distribution, p. A fully connected
Boltzmann machine is not trivial to sample from, unfortunately. In order to bypass
this problem, practitioners commonly restrict the connectivity to define restricted
Boltzmann machines (RBMs), which are defined with only a bipartite graph be-
tween visible and hidden nodes rather than a fully connected one. An example of
an RBM is given in Fig. 3.5.
For an RBM, there exists an algorithm to generate samples from p, which
is called k-step contrastive divergence [Hin02, CPH05, Hin12]. A Gibbs sampler
is an algorithm used to prepare a sample on the visible units as the end result of
a Markov chain Monte Carlo (MCMC)12 algorithm. Due to the bipartite nature
12A Markov chain MC algorithms are a family of probabilistic algorithms which enable sampling
from a desired distribution which is constructed as the equilibrium distribution of a Markov chain.
A Gibbs sampler is a special case which requires the conditional distributions of the target to be
exactly samplable, since these are used to update the elements of the sample.
52 3. Preliminaries II: Machine learning
Figure 3.5: An example 6 visible node restricted Boltzmann machine. The RBM has hidden
nodes and 12 parameters overall. For this illustration, we assume the weights between nodes, Wi j ,
are not trainable. Biases for visible and hidden nodes, bvi ,bhi correspond to the self-loop weights
in Fig. 3.2(b). We shall explicitly use this RBM structure in Chapter 6.
of the RBM graph, the visible and hidden nodes are conditionally independent
given each other. This means that all the nodes in each layer can be updated
simultaneously, which significantly improves the time required to generate a sam-
ple. In contrast, a fully connected Boltzmann machine would require each node
to be considered separately in the Gibbs sampling process. Running a Markov
chain for an infinite number of steps would generate an exact sample from the de-
sired distribution, however k-step contrastive divergence halts after k steps of the
chain, and produces and approximate sample. Hinton in his original work [Hin02]
observed that taking simply k = 1 was sufficient to generate good samples from
the RBM. This k = 1 contrastive divergence was the method used to generate
samples from, and train, the RBM in Fig. 3.413. We have discussed the above
method to generate samples from the RBM for completeness, however we shall
not actually use it directly in this Thesis. Instead, we use an alternative method
based on path-integral Monte Carlo (PIMC)14, which we revisit in Chapter 6.
3.5 Kernel methods
In Sec. 2.1.7.1 we briefly introduced feature maps and kernels, in terms of the
mathematical machinery required to use them in probability distance measures.
Let us now revisit kernel methods in their appropriate context and use in machine
learning more generally. We then discuss the extension of kernel methods to the
quantum world in Sec. 4.2.
Previously, we presented a feature maps and kernels as being simply functions,
and their inner products respectively. In reality, the definition of kernels and their
relationship to features maps is more of a subtle point which we address in the
following section. We build to these discussions by working through an exam-
ple in machine learning where the motivation to use kernels becomes apparent.
The example we choose to illustrate this is the canonical support vector machine
13This implementation uses code adapted from https://github.com/meownoid/tensorflow-
rbm, which also adds a momentum term to the gradient update (see C in Eq. (3.8)).
14In contrast to Markov chain Monte Carlo, the path-integral version computes a statistical
estimate of a function using the path-integral formulation of quantum mechanics.
https://github.com/meownoid/tensorflow-rbm
https://github.com/meownoid/tensorflow-rbm
3.5. Kernel methods 53
(SVM), which was one of the most favourable classification algorithms before the
advent of deep learning. Let us first introduce the basics of the SVM before dis-
cussing how kernels fit into the picture. The most basic version is the linear SVM.
Here, we have a labelled dataset {(x i ,y i),x i  X ,y i  {+1,1}}Mi=1, and the lin-
ear SVM classifies points by fitting a maximum margin15 hyperplane (a straight
line) in the sample space, X . The line parameters are a weight vector and a bias,
w ,b with the hyperplane defined as wT x b = 0. This line must be constructed
such that every point labelled yi = 1 has w
T x b  1 and those labelled yi =1
have wT xb 1 where equality defines the parallel hyperplanes to the original
line and describe the margin. Now, in order to actually find the maximum margin
hyperplane in the SVM, one solves the following optimisation problem:
w =min ||w ||2 (3.17)
subject to y i(wT x i b) 1 (3.18)
where the condition in Eq. (3.18) ensures that the predictions of the SVM (given
by y i = sgn
wT x i b
), match the true labels, y i .
Of course, a linear SVM will only work16 if the data is linearly separable in the
original space, X , which will not be the case in many scenarios. To remedy this,
the non-linear  SVM does not fit a hyperplane, but instead maps points x i  X
to the feature space, H. We can then solve a similar optimisation problem to
Eq. (3.17) but replacing the original vectors with their feature mapped versions,
(x i). As a result, the decision boundary generated in H will be still linear, but
when mapped back into X , will become non-linear, due to the non-linearity of .
However, we have not yet mentioned how the kernels (the inner products of
the feature vectors) fit into this picture. The answer lies in the dual formulation of
the optimisation problem Eq. (3.17). In the original linear SVM, this dual contains
terms involving only inner products of the datapoints, (x i)T x j . Therefore, when
replacing x i with their corresponding feature maps, (x i), and deriving the non-
linear version of the dual problem, we get inner products of the feature vectors,
(x i)T(x j) i.e. kernels17. This is referred to as the kernel trick, since by using
the dual formulation of the non-linear SVM, we are never actually required to
evaluate the actual feature maps themselves, only their overlaps.
The following theorem illustrates how kernels can be defined from feature
maps, and the proof that the inner product produces a valid kernel can be found
in several works (see e.g. [SK19]), so we omit it here. With this in mind, we realise
that there is actually no need to restrict to kernels which result from the inner
products of explicitly defined feature maps, which is a sufficient (see Theorem 2),
but not necessary condition for a kernel definition. More generally, a kernel can
15The margin is the area between the fitted line, and the closest data points, which are the
support vectors.
16Although one can soften the hard margin requirement described above by introducing a
parameter which controls the size of the margin.
17This is the logic behind quantum enhance SVMs, a classical kernel can be simply swapped
out for a quantum kernel (computed on a quantum device). For suitable feature maps, we may
hope to gain a quantum advantage.
54 3. Preliminaries II: Machine learning
be any symmetric function18,  : X X  R, which is positive definite, meaning
that the matrix it induces, called a Gram matrix, is positive semi-definite19.
Figure 3.6: The support vector machine and feature maps. In the case where the data is linearly
separable, datapoints are classified according to sgn(wx b). In the non-linearly separable case,
a feature map is used to extend the dimension of the data vectors (in this particular example),
x = (x,y) (x) = [x,y ,x2+y2]. Using the third dimension, the data becomes linearly separable
again in the RKHS, H. The kernel, , is related to the distances between these points in H
For completeness, we also need to make sure that the inner products of fea-
ture maps do actually produce valid kernels (in the sense of being symmetric and
positive definite. This fact is ensured by the following theorem, whose proof can
be found in several works (see e.g. [SK19]), so we omit it here:
Theorem 2: (Feature map kernel)
Let  :X H be a feature map. The inner product of two inputs, x ,y X ,
mapped to a feature space defines a kernel via:
(x ,y) := (x),(y)H (3.19)
Note that the Hilbert feature space, H, is usually called a reproducing kernel
Hilbert space (RKHS). The reproducing part arises because the inner product of
the kernel at a point in X , with a function, f  H, in some sense evaluates or
reproduces the function at that point: f ,(x , )= f (x) for f H,x  X . Now,
let use return to the use of kernels in a context more relevant for the contents
of this thesis, i.e. as introduced in distribution comparison measures such as the
MMD in Sec. 2.1.7.1. In this case also, we use the kernel trick to bypass difficult
computations, and the feature maps allow us to map the sample points into a
higher dimensional RHKS in which the distributions of interest may be more easily
compared.
Let us conclude this section by mentioning some common examples of kernels.
These include; the polynomial kernel (with degree d):
P (x ,y) := (x
T y +c)d , c > 0, (3.20)
18Given a kernel in this form, a corresponding feature map,  may be derivable from it, but it
will not be unique in general.
19A symmetric matrix, A, is positive definite, if x Rd/0,xTAx > 0. Positive semi-definiteness
also allows for xTAx = 0. More generally, if A is Hermitian, positive definiteness can be defined
by allowing x  C/0 and xT  x (the complex conjugate transpose).
3.6. Learning theory 55
The cosine kernel:
C(x ,y) := 
cos(x i y i), (3.21)
The Laplace kernel:
L(x ,y) := e
 ||xy ||2 (3.22)
Or the mixture of Gaussians kernel:
G(x ,y) :=
||xy ||2
2i (3.23)
where i are bandwidth (hyper)parameters. The choice of the kernel func-
tion/feature map (i.e. the choice of the RKHS) may allow different properties
to be compared. For a comprehensive review of techniques in kernel embeddings,
see [MFSS17]. As mentioned above, we discuss the generalisation of kernels to
the quantum scenario in Sec. 4.2.
3.6 Learning theory
To conclude our preliminary material on (classical) machine learning, let us move
from the completely practical in the training of neural networks, to the entirely
theoretical realm at the other end of the spectrum and discuss learning theory.
Learning theory is the theoretical framework which underpins machine learning,
and tells us what things (functions, distributions, quantum states, etc.) we can
learn efficiently, and which things we cannot. It turns out that there is a deep
relationship between learning theory and cryptography, since many cryptographic
primitives are based on objects which cannot be efficiently learned. A canonical
example is the learning with errors problem, which is key to the field of post-
quantum cryptography20. While this problem is now considered to be in the
cryptography domain, its origins actually were formed in the context of learning.
While this topic may seem far removed from the contents of this Thesis,
based on the topics introduced previously, we emphasise it is not. In fact, we
make a small contribution to quantum learning theory in Chapter 6 by discussing
the learnability of distributions in a quantum setting, and also make a connection
between quantum machine learning and quantum cryptography in Chapter 7.
For now, let us set the scene by introducing the most common and well studied
learning scenario, that of function learnability21. The primary question in this
subfield are of the following form: given a function family, F , can we efficiently
learn a representation of any function, f  F? While one may consider learning f
20Post-quantum cryptography is the study of primitives which are secure even in the presence
of quantum computers (hence in a post-quantum world). This was necessitated as a result of
the hackability of RSA-based cryptosystems with Shors algorithm. See e.g. [BL17] for a review
of this field.
21Computational learning theory is a large field, and we do not attempt to even mention all
possible extensions one may consider. For a (quite outdated now) overview of some topics in the
field, see [Ang92, KV94]
56 3. Preliminaries II: Machine learning
exactly, it is perhaps more common to instead consider learning approximately and
with high probability. This learning model is called probably approximately correct
(PAC) learning. Furthermore, we usually refer to F as a concept class instead
of a function family, and the requirement of a learning algorithm is to output a
candidate function, h (a hypothesis), to a given concept, f  F , which is not
too different from f on most possible inputs, x , from some space, X . For this
section, we use the notation of [AdW17a] and the more interested reader may
look further into this paper and the references therein. Let us begin by defining
what we mean by a learner:
Definition 18 (PAC function learning).
A learning algorithm, A, is an (,)-PAC learning for a concept class, F if:
For every f  F , and every distribution, D, given access to a PEX(c,D)
oracle, A outputs a hypothesis, h such that:
Pr(h(x) = f (x))  (3.24)
with probability at least 1 .
PEX(f ,D) in the above is a random example oracle, which when queried,
outputs a labelled example, (x , f (x)) where x is sampled from some distribution,
D : {0,1}n{0,1}. This model is suitable for the supervised learning problem we
discussed in Sec. 3.3.1, since the PEX oracle can be viewed as exactly providing
one datapoint in a labelled dataset, see Definition 13.
Besides a random example oracle, one may consider alternative query models
to the function family, including membership queries (where the learner provides
x and the oracle returns f (x)), or alternative learning models such as exact or
agnostic learning [AdW17b], or learning under specific distributions (rather than
every distribution as in Definition 18).
One of the key quantities which one may care about in learning theory is the
sample complexity of learning the function. This is simply how many labelled
examples does A require, in order to output a sufficiently good hypothesis. In
the classical domain (and indeed in the quantum domain also), this quantity,
M, is given by the so-called Vapnik Chervonenkis (VC) dimension, which is a
combinatorial quantity depending on the concept class, defined as:
Definition 19. (VC dimension [VC15].)
Fix a concept class, F , over {0,1}n. A set S = {s1, . . . , st}  {0,1}n is said
to be shattered  by a concept class, F if {[f (s1)    f (st)] : f F}= {0,1}t .
In other words, for every labelling,  {0,1}t , there exists a f F such that
[f (s1)    f (st)] = . The VC dimension of F , VCdim(F), is the size of a
largest S  {0,1}n that is shattered by C.
The result of [BEHW89, Han16] shows that M is exactly determined by
VCdim(F):
3.6. Learning theory 57
Theorem 3: ([BEHW89, Han16, AdW17a])
Let F be a concept class with VCdim(F). Then:
VCdim(F)1
log(1/)
(3.25)
examples are necessary and sufficient for an (,)-PAC learner for F .
With that, we conclude our discussion of function learning theory in the clas-
sical world - in the next section, Chapter 4, we will briefly introduce the quantum
generalisations of the above definitions, and in Chapter 6, we present the extension
of these definitions into the distribution (rather than function) learning setting.
Preliminaries III: Quantum machine
learning
In the previous two sections, we introduced ideas from quantum computing, and
machine learning independently. Now, let us merge some of them in quantum
machine learning (QML). The field of QML proper is only around 12 years old
at the time of writing, and was kicked off by the seminal paper of Harrow, Ha-
sidim and Lloyd (HHL) [HHL09]. This work introduced the HHL algorithm for
solving systems of linear equations and performing matrix algebra. This algo-
rithm promised (at the time) exponential speedups over classical algorithms for
problems involving high dimensional matrix arithmetic, and a flurry of results fol-
lowed for a variety of problems in supervised and unsupervised learning including
least-squares fitting [WBL12], clustering [LMR13], principal component analy-
sis [LMR14], support vector machines [RML14], dimensionality reduction [KL20]
or Gaussian process regression [ZFF19] to name a non-exhaustive list. However,
will this paper may have sparked excitement in the research community, researchers
have been considering for much longer how to quantize machine learning, and find
advantages.
Roughly speaking, quantum machine learning algorithms can be divided into
four categories, depending on the type of data on which they operate, and the
nature of the algorithm. The notation, ij, i, j  {C,Q} refers to the data type (i)
and the algorithm type (j) either of which may be quantum, Q, or classical, C in
nature:
1. CC classical data, processed by classical algorithms (e.g. image classifi-
cation/generation in Sec. 3.1 by neural networks).
2. CQ classical data encoded in quantum states or channels, processed by
quantum algorithms (e.g. the HHL algorithm acting on data vectors),
3. QC quantum data in a classical form, processed by classical algorithms
(e.g. using neural networks to analyse measurement statistics from a quan-
tum experiment),
60 4. Preliminaries III: Quantum machine learning
4. QQ quantum data, processed by quantum algorithms (e.g. feeding quan-
tum states from quantum experiments directly into quantum algorithms, for
example quantum neural networks).
In this Thesis, we touch on all of these sub-fields.
While the HHL algorithm drew tremendous attention to the field, foundational
ideas in QML have been around almost as long as quantum computing itself. For
example, the typography of QML algorithms discussed above was introduced early
on in 2006 by [ABG06]. Bshouty and Jackson initiated quantum learning theory
(the quantum analogue of that presented in Sec. 3.6) in 1995 by generalising
probably approximately correct (PAC) learning to the quantum setting [BJ95] (we
will return to this topic later in this Thesis). Finally, proposals for quantum neural
networks appeared as early as 1996 [BNSS96], the same year as the canonical
Grover search algorithm [Gro96]. Indeed, a well-accepted definition for a quantum
version of a neural network is still elusive to date [SSP14], with the non-linearity
(in Eq. (3.2) for example) being difficult to emulate quantumly. As mentioned
in Sec. 3.1, the most widely accepted model (at the current time) for a quantum
neural network is based on the parametrised quantum circuit (PQC) which is the
key component of this Thesis.
Since the development of HHL, QML development has experienced what could
be called two waves of progress. The first wave focused around coherent al-
gorithm development, primarily using methodologies pioneered by HHL (such as
the examples listed at the start of this chapter). The second wave was born with
the introduction of the PQC and the variational algorithm. The algorithms in the
former case are widely believed not to be implementable on quantum hardware
for many years to come, as they require large numbers of error corrected qubits
(hence the nomenclature coherent), and deep circuits to guarantee the speedups
they promise. In contrast, variational quantum algorithms1 (also dubbed modern
quantum algorithms [Bia20]) in the second wave are heuristic in nature, and as
such they do not usually have provable guarantees of runtime complexity. However,
they are readily implementable on the quantum computers available now, many of
which are accessible remotely through the quantum cloud [LaR19]. Early work in
QML algorithms almost exclusively focused around quantum speedups in runtime.
However, speed is not the only way to measure a quantum advantage over clas-
sical methods for machine learning tasks. Indeed, one may consider other avenues
along which to search for a quantum advantage (but not limited to: expressibility,
accuracy, interpretability, privacy and generalisability). The simultaneous rapid
development of quantum computing hardware since  2010 means that building
and experimenting with real quantum models is accessible to anyone with an in-
ternet connection, and as such current research has partially shifted away from
that of purely theoretical algorithm proofs to more experimental and exploratory
1Variational algorithms are actually used for more general purposes than simply machine learning
(meaning data-driven) applications. However, the core of both variational algorithms and heuristic
quantum machine learning algorithms reduces to the optimisation of some parametrised object
(typically a parametrised quantum circuit). Hence, we use both terms interchangeably in this
Thesis to refer to the same concept.
in nature. For completeness, we mention some examples of quantum machine
learning which do not quite fit into these categories, including speedups (however
not exponential) in training Boltzmann machines [WKGS15], or building quantum
models based on quantum annealing hardware, including the quantum Boltzmann
machine [KW17, AAR+18, BRGPO18, WVHR19, WW19]. One may also con-
sider advances in quantum reinforcement learning [DCLT08, DTB17, DLWT18,
LS20a, JTPN+21, JGM+21, SJD21]
For coherent algorithms in the first wave, a crucial ingredient is required. This
is the ability to access data in superposition. Specifically, these algorithms usually
require one to be able to prepare states of the form:
xi |i (4.1)
Where x = [x1, . . . ,xn]T  Rn, is a data vector2. This is the so-called amplitude
([SP18, ZFR+21]) encoding and the procedure to prepare such a state is called
a data-loading routine. We return to data encoding in Sec. 4.1.4 and Chap-
ter 5. Once amplitude encoded states are prepared, they are further processed
by the quantum algorithm. However, when considering the true runtime of such
algorithms, one must also factor in the cost of preparing such states. As stated
by [Aar15], if a routine to prepare Eq. (4.1) required nc steps (for some constant
c), then speedups are lost.
In principle, such data-loading issues can be sidestepped by the existence of
a quantum random access memory3 (QRAM) [GLM08], acting as follows on a
superposition of address registers, |i:
|i 
|i|xi (4.2)
However, constructing such a quantum memory which can be accessed in super-
position may be as hard as building a fully fault tolerant quantum computer. As
we hinted at above, there are also many other practical issues with these co-
herent QML algorithms (such as HHL) that need to be carefully addressed in
order determine whether speedups are actually feasible [Aar15]. For an overview
of technical details used in such linear system algorithms, see [DHM+18] which
also discusses constructions for QRAM objects.
Finally, we would be remiss if this Thesis included a section introducing quan-
tum machine learning without mentioning the dequantisations. In 2018, [Tan18a]
proposed a fully classical algorithm to kill the exponential speedup of one of
these coherent algorithms - the quantum recommendation system algorithm
of [KP16]. Previously, this quantum algorithm was one of the most promis-
ing candidates for an end-to-end exponential speedup over its classical coun-
terparts. The classical algorithm of Tang however demonstrated this was not
2Assuming the vectors are normalised such that ||x ||2 = 1.
3Similarly to a classical RAM, which returns the contents of a memory register when accessed.
62 4. Preliminaries III: Quantum machine learning
the case, and the advantage of [KP16] was reduced to only polynomial4 We
highlight this to demonstrate how subtle the nature of quantum advantage is in
QML. For those interested in further reading of dequantised quantum algorithms,
see [Tan18b, GLT18, CLW18, CGL+20, SM21] and also [ADBL20] for a study of
these algorithms in practice.
Before moving to the next section, we conclude by highlighting some reviews of
quantum machine learning including [Wit14, SSP15, BWP+17, DB18, BLSF19]
which discuss many of the topics we neglected here.
4.1 Variational quantum algorithms
This Thesis however is focused solely on algorithms in the second wave, the vari-
ational quantum algorithms (VQAs). As mentioned above, VQAs (also called
near-term quantum machine learning algorithms) are a powerful model for quan-
tum algorithm design on NISQ devices. Fig. 4.1 gives an overview of some of the
key ingredients involved. VQAs are characterised by a quantum component, and a
classical component, which work synergistically to solve the problem of interest. In
order to maximise the use of quantum coherence time (which is limited by the lack
of error correction in NISQ devices), the majority of the computation is carried out
by the classical device, which performs the task of optimising the parameters of
a quantum object (typically a parametrised quantum state) to solve the problem.
The quantum component may have some input, |in, and produces some out-
put state, out, which is parametrised by . In order to apply a general quantum
transformation, the quantum component may interact with an environment (de-
scribed by ancillary registers), and from the output state several observables may
be measured, Oi . These classical results are then passed into a classical computer
which computes some cost function of these observables and finds parameter
updates    (via gradient descent or otherwise) to drive the computation.
These approaches are clearly heuristic in nature, however some effort has been
made to study them from a theoretical point of view [MRBAG16, Bia20] and the
computational model they exhibit is actually also universal for quantum compu-
tation [Bia19] despite only using short depth quantum circuits. At this point, if
we replace the quantum device by a neural network5, the connection to machine
learning becomes clear. To introduce the relevant concepts in this section, we use
and refer to the following reviews, [CAB+21, BCLK+21, ECBY21]. Key concepts
involved in variational quantum algorithms, are:
4However, we remark that this is not so much of a blow as it seems, the constant factors in
the original algorithm of [Tan18a] indicated that the quantum algorithm could still be significantly
faster. The most recent analysis by [SM21] indicated that this speedup is only quadratic in reality
for a special case of the problem.
5In fact, a more general scenario is where some function is applied to the output measurements
from the quantum circuit before passing it to a classical optimiser. This function may be simple,
or may be the output of another classical neural network. We do not consider this possibility in
this Thesis, so we neglect it for simplicity.
4.1. Variational quantum algorithms 63
Figure 4.1: Cartoon illustration of a variational quantum algorithm.
 The cost function defining the problem.
 The ansatz used by the quantum computer.
 The optimisation of the cost function.
 The input to the VQA (some form of data encoding for classical data).
 The desired output - measurement readout, or quantum states.
In the subsequent sections, we introduce each of these to the degree of detail for
which they are relevant in this Thesis.
4.1.1 The cost function
Just as in classical machine learning, the problem to be solved can be reduced to
optimising a cost function, which is some function of the parametrised state:
C() (4.3)
The choice of cost function is crucial for the success of VQAs, and not surprisingly,
this fact originates from their machine learning origins. Concretely, four desirable
qualities for VQA cost functions are ([CAB+21]):
Faithfulness. Efficiently
computable.
Operational
meaning.
Trainable.
64 4. Preliminaries III: Quantum machine learning
1. Faithfulness: The minimum point  := argminC(), is the solution to the
problem of interest in the parameter space6.
2. Efficiently computable: It should be possible to estimate C to a reasonable
precision with polynomial resources (number of qubits, gates in the circuit,
etc.).
3. Operational meaning: Smaller cost values correspond to higher quality
solutions.
4. Trainable: The cost should be optimisable in an efficient way (efficiently
computable gradients, navigatable parameter space, etc.).
For many variational algorithms, the problem can be encoded as a ground
state optimisation problem, meaning that the problem parameters are encoded
into a Hamiltonian, H, and the solution is encoded in the ground state of this
Hamiltonian (the state with the minimum energy). In this case the optimisation
becomes:
()|H|() := E (4.4)
The goal is to find the eigenstate, |(), with the lowest energy, E. VQAs in
this form are known as variational quantum eigensolvers (VQE), which was one of
the original VQA proposals by [PMS+14] that pioneered the field. VQE caused a
great deal of excitement due to its applicability to quantum chemistry [MEAG+20]
problems and quantum simulation [CMMS20]. In these cases, the Hamiltonian,
H, is the one defined by the physical system to be simulated, and is the source of
the observables, Oi , to be measured. For example, in finding the ground state of
the Ising model Hamiltonian acting on N spins:
Hising :=
Ji jZi Zj +
bkZk , (4.5)
the observables to be measured are the correlations Zi Zj and the local ex-
pectations Zk7. Each of these observables is measured8, and the results are
collected (and weighted by the coefficients Ji j ,bi) to compute the energy of the
system when in the state, |.
Cost functions are highly problem dependent, those in the form of Eq. (4.4)
are among the simplest, but in the case of machine learning problems particularly,
the choice of cost function is complex and crucial to the success of the algorithm.
As such, one can generalise cost functions into the following form [CAB+21]:
C() =
fk(Tr[Ok
out]) (4.6)
6The parameter space is the multidimensional space described by the parameters, .
7O denotes the expectation value of the operator, O, with respect to the state, |:
|O|.
8Since all terms in this Hamiltonian commute, these statistics can all be collected using a single
measurement setting, by measuring every spin (qubit) in the computational basis.
4.1. Variational quantum algorithms 65
where {fk}k are a set of functions applied to the output. In this Thesis, cost
function choice will play an important role.
Local versus Global Cost Functions
Before moving on, let us highlight one important generic feature of VQA cost
functions; their locality. Namely, whether the observables involved in computing
C are either local or global. Roughly speaking, local observables are those con-
sisting of a function of terms which each can be computed by only measuring
a handful of qubits, whereas global cost functions require information about the
entire quantum state. For example, the cost function defined by VQE on the Ising
model Hamiltonian, Eq. (4.5) is 2-local since the terms in it require measuring no
more than 2 qubits at a time (the terms Zi Zj only operate on two qubits, i , j ,
and Zk only acts on qubit k). In the case of VQE problems, the locality of the
cost function directly relates to the locality of the Hamiltonian in question which
is a well-studied topic [KKR06], but it is a more general concept.
Specifically a k-local cost function observable can be written as ([CSV+21]):
O= c01+
ci O
i (4.7)
where each Oki are k-local operators, acting at most on k  [n] qubits. In contrast,
a global cost function has the form:
O= c01+
ci O1i  Oi2  Oin (4.8)
In both cases, N is the number of terms given by the problem of interest and
c0,ci are coefficients which can assumed to be real WLOG (e.g. corresponding
to {Ji j ,bk} in Eq. (4.5)).
Cost function locality is also important regarding operational meaning. Typ-
ically, from this point of view, global cost functions are usually more favourable
since they can be directly interpreted. In variational compilation [KLP+19], the
cost function compares the similarity of two global unitaries, and is zero when
the unitaries are identical. In contrast, local cost functions are perhaps less in-
terpretable, but can usually be used as a bound to optimise a global cost. Fur-
thermore, as we shall discuss in Sec. 4.1.3, local unitaries are usually easier to
optimise than their global counterparts ([CSV+21]). We return to this discussion
in Chapter 7.
4.1.2 Anstze
The second important ingredient in VQAs is the anstze, for the parametrised
state, i.e. the operation that performs the mapping |in out in Fig. 4.1. In
66 4. Preliminaries III: Quantum machine learning
VQAs, the ansatz9 is usually a unitary transformation:
|()= U()|in, out := |()() | (4.9)
This unitary, U(), is usually referred to as a parametrised quantum circuit (PQC),
although it need not be strictly a quantum circuit. For example, as we shall see
momentarily, it may be implemented using a direct Hamiltonian evolution. Two im-
Figure 4.2: Examples of VQA anstze. The general unitary transformation U() is usually divided
into L layers U() = UL(L)   U2(2)U1(1). Problem-inspired anstze typically encode problem
information - an example shown with two non-commuting Hamiltonian terms, H1,H2, which may
apply to the QAOA or as a Hamiltonian variational ansatz for VQE. Problem-agnostic anstze use
native gatesets and connectivities (for example, shown is an nearest neighbour connected ansatz
using CZ as the entangling gate, with Rx being the native single qubit parametrised gate). Finally,
variable structure anstze may be problem-inspired, as in the ADAPT-VQE algorithm [GEBM19],
or problem-agnostic with hardware native operations [CRSC21]. Note that the layer structure
is not strictly necessary in the definition of an ansatz, and each layer need not have the same
specific structure, but it provides a nice conceptual connection to neural network layers in machine
learning.
portant families of anstze are problem-inspired and problem-agnostic [CAB+21]
(see Fig. 4.2), where the former includes information from the problem speci-
fication in building the ansatz, whereas the latter is generic and applies to all
problems. A canonical example of a problem inspired ansatz is that used in the
quantum approximate optimisation algorithm (QAOA) [FGG14]. The QAOA is
a special case of the VQE algorithm, where the Hamiltonian, H, is restricted
to be diagonal in the computational basis (an example is the Ising Hamiltonian,
Hising in Eq. (4.5). The QAOA is applied primarily to (classical) combinatorial
9Historically, an ansatz in physics is an educated guess for a problem solution. This has
translated into quantum computing in preparing the trial state for a VQA.
4.1. Variational quantum algorithms 67
optimisation algorithms, such as MAXCUT on a graph. In its original form, the
QAOA applies p layers of alternating problem and driver Hamiltonians, starting
from a state which is not diagonal in the computational basis, for example the
state |in = |+
n on n qubits. If one translates an optimisation problem of
interest into finding the ground state of the Ising model Hamiltonian above, the
corresponding anstze would then be:
U() = U(,) =
eiiHdrivereiiHising, (4.10)
where Hdriver =
j=1Xj is the driver Hamiltonian for which the initial state, |+n
,is an eigenstate. Each layer in the ansatz has 2 free parameters, so the optimi-
sation requires finding 2p optimal parameters.
This type of ansatz has been generalised to the quantum alternating operator
ansatz10 [HWO+19] which can encode different symmetries of the problem and
may improve performance. A second type of ansatz is a further generalisation
of the QAOA ansatz, called the Hamiltonian variational ansatz, and is used when
the problem Hamiltonian is not diagonal, i.e. in VQE problems. In this case, the
Hamiltonian can be split into multiple non-commuting terms, Hk [CAB
+21],
U() =
eil ,kHk
, (4.11)
Again, the ansatz is applied for a number of layers. Both of these anstze derive
inspiration from the quantum adiabatic algorithm [FGGS00], and Trotterization
methods for quantum simulation, which are used for simulating the evolution of
a system. For the latter case in VQE, the initial state is usually also problem
inspired, for example the Hartree-Fock ground state, which is exactly solvable for
a given system.
The second class of anstze are the problem-agnostic, which usually aim to
reduce the circuit depth and number of gates required to implement them (in
contrast to the alternating operator/Hamiltonian variational anstze, which may
require significant depth to implement). Furthermore, the operations in agnostic
anstze are usually hardware-native. For example, to implement a hardware-native
ansatz on Rigetti [SCZ17] one may use the CZ or XY gates for entanglement cre-
ation, whereas for IBM quantum computers, one would choose a directed CNOT.
Using native gates dispenses with the need to perform extensive compilation and
so keeps the circuits shallow. Furthermore, only allowing the connectivity between
qubits which is directly accessible on the quantum device also is essential for high
performing results on NISQ hardware11.
10Which also offers the acronym QAOA, confusingly.
11Compilation may significantly increase the circuit depth in a NISQ circuit implementation. If
an algorithm requires a CNOT, but only CZ is available natively, two H gates will need to be added
to change the basis. Secondly, performing qubit routing to apply a transformation between two
qubits which are not directly connected is expensive - a single SWAP gate requires 3 CNOT gates
to implement.
68 4. Preliminaries III: Quantum machine learning
The most common problem-agnostic anstze are called hardware-efficient.
Two specific forms of hardware efficient anstze are used in this Thesis. The
first are fixed-structure, which contain a number of layers, where each layer has a
fixed structure of entangling and single qubit rotations12
The second class of hardware-efficient anstze are variable-structure anstze.
In these circuits, not only are the continuous gate rotations optimised over, but
also the gates in the circuit themselves. A fixed-structure ansatz may require
several layers to solve a given problem, in contrast variable-structure anstze are
capable of finding circuits with the absolute minimum number of quantum gates.
Formally, this corresponds to a more general optimisation problem than Eq. (4.3):
(,g) = argmin
,gG
C(,g) (4.12)
Here, G is a gateset pool, g corresponds to a certain sequence of gates in the
pool. Clearly, the problem now becomes a discrete optimisation problem as well
as a continuous one (we still need to optimise over the gate parameters, ).
The variable-structure approach was given proposed in [CSSC18], and varia-
tions on this idea have been given in many forms [OGB21, RHP+20, LFC+20,
PT21]. Most recently, these ideas have been given the broad classification of
quantum architecture search (QAS) [ZHZY20] to draw parallels with neural archi-
tecture search [YWC+19, LSY19], (NAS)13 in classical ML. We also mention that
the variable-structure approach is not only restricted to be used with a hardware-
efficient ansatz. Indeed, a variation has been used on the Hamiltonian variational
ansatz for VQE ([GEBM19, CSU+20]) and for QAOA [ZTB+20]. One interpreta-
tion of QAS is its use in discovering novel primitives (we apply it to the primitive of
quantum cloning in Chapter 7) for quantum algorithms, protocols or experiments;
QML for primitive discovery if you will. Interestingly, a parallel and related line of
work has also been progressing using purely classical machine learning in discover-
ing novel quantum protocols and experiments [KMF+16, MNK+18, WMDB20].
Finally, it is also crucial to mention that for machine learning applications,
good anstze design is still an open area of research. A primary reason for this
is the data-driven nature of ML which makes definition of a problem-inspired
ansatz difficult. The choice is perhaps more natural when examining quantum
machine learning for quantum data, where one can use a physically inspired ansatz
which may encode certain problem symmetries. For example, a proposal for a
quantum convolutional neural network [CCL19] (QCNN) uses a translationally
invariant structure which may be suitable for physical systems with this property,
or defining Hamiltonian-based models [VMN+19] as anstze which can learn to
represent mixed quantum states.
12For ease of implementation, when dealing with hardware efficient anstze in this Thesis, we
assume that two qubit unitaries are un-parametrised (meaning implemented with a fixed parame-
ter), and the trainable parameters, , are only contained in single qubit rotations. This is not a
necessary assumption in general.
13NAS is the analogous task for finding optimal structures in neural network architectures, for
example tuning the width and depth of the network.
4.1. Variational quantum algorithms 69
4.1.3 Cost function optimisation
Once a suitable cost function has been chosen, the question then becomes how one
can optimise it to solve the problem. In the above VQE example, this corresponds
to finding the optimal parameter setting, , as a proxy for optimising over the
set of possible quantum states to find the ground state of the Hamiltonian in
question.
As with neural network training, optimisation routines for cost functions can
be broadly grouped into two categories: gradient-free and gradient-based. Crite-
ria number four of defining useful cost functions indicates they must be efficiently
trainable (similarly again to the neural network training in Sec. 3.2. This means (in
the case of gradient-based optimisation) that they must have efficiently14 com-
putable gradients.
4.1.3.1 Gradients
Fortunately, for many VQAs, this is the case. Specifically, if the gates in the VQA
ansatz, U(), are of a particular form, the gradients of the cost function can be
computed analytically, using what is known as the parameter-shift rule [MNKF18,
SBG+19, BIS+20, SWM+20, HN21] which we alluded to previously:
The parameter-shift rule:
Given a cost function, C, which is evaluated using an ansatz (with L param-
eters) containing quantum operations of the form U(i) = e
i(i/2), where
2 = 1, the gradient of C with respect to a particular parameter i is given
C()
:= i C() =
C(+i )C(
, (4.13)
where i = (1,2, . . . ,i 
, . . . ,L)
Note also that the expression in Eq. (4.13) is exact, and the shift constant
/2 is large, in contrast to the infinitesimally small shifts and approximate nature
of methods such as finite-difference15. However, since C is estimated by running
a circuit on a quantum device, we cannot extract the exact value C without an
infinite number of measurement shots. As such, we will only have access to
estimates, C and C of the cost and its gradients.
Interpreting Eq. (4.13), we see that we can evaluate the gradient of a cost
with respect to any particular parameter by simply computing the cost twice in
a parameter-shifted fashion. As such, if we assume a complexity of T for eval-
uating C, then we will occur an overhead of 2LT to evaluate the gradients
for all L parameters in the ansatz, and so if the cost is efficient to compute, so
14Efficient here usually refers to with respect to the number of qubits, N, but also all other
relevant parameters of the problem.
15See [MBK21] for a comparison between parameter-shift and finite-difference methods.
70 4. Preliminaries III: Quantum machine learning
are its gradients. However, comparing this black-box type computation (since the
parameters must be evaluated independently) to the efficiency of the backprop-
agation algorithm we discussed in Sec. 3.4, where we can reuse computations
from one layer to another, we see this linear scaling may be an expensive16 price to
pay. Nonetheless, many quantum software libraries including TensorFlow Quan-
tum [BVM+20] by Google, or Pennylane [BIS+20] by Xanadu offer these gradient
computations incorporated to the standard deep learning libraries such as Ten-
sorFlow or PyTorch. One can circumvent this linear scaling by using a quantum
simulator to train quantum models, as direct access to the wavefunction allows
the implementation of autodiff methods. However, this is clearly not possible if
training a (near term) model on quantum hardware, and at the time of writing,
this linear scaling is the best we have.
A final point to note is the assumption that 2 = 1 in the above theorem.
This is not in fact necessary, and the theorem has been extended in many as-
pects, including stochastic versions for more general operations [Cro19] higher
order [MBK21] and natural gradients [SIKC20, KB20a], imaginary time evolu-
tion [MJE+19], and analytic methods ([OGB21, KB20b, CMMS20]) have been
proposed. This is an active area of research, and this Thesis will only require the
simplest forms of gradient descent using the parameter-shift rule in the form given
above, so we neglect further discussion of other methods.
16If we treat the cost function in the quantum and classical cases as an oracle, we are only
counting the number of evaluations which must be done for each parameter. A discussion of the
quantum/classical computability of the cost is a separate question.
4.1. Variational quantum algorithms 71
4.1.3.2 Barren plateaus
The problems mentioned relating to VQA optimisation are also common to ma-
chine learning, but the nature of VQAs also introduces unique problems. These
problems manifest in the cost function landscape and two phenomena are called
barren plateaus (BPs) and narrow gorges. Barren plateaus are exponentially flat
regions in the parameter space, in which gradient-based17 optimisation will fail,
since the gradient of the cost function is close to zero in all directions.
Definition 20 (Barren plateau (from [AHCC21])).
Consider a VQA cost function defined as in Eq. (4.6) with fk(x);= akx,ak 
R. The cost exhibits a barren plateau if   , the variance of its partial
derivative vanishes exponentially with n, the number of qubits:
Var[i C()] F (n), with F (n)  O(b
n) (4.14)
for some b > 1. The expectation values are taken with respect to the
parameters, : Var[i C()] = [i C()]
2a.
aFor proofs it is usually assumed that the expectation is computed over unitaries that
contain the parameters, and furthermore that these unitaries form exact ([MBS+18]) or
approximate ([HSCC21]) quantum t-designs [DCEL09] in order to make the integration
tractable.
This phenomenon was first observed by [MBS+18] which found that such
plateaus occurred in sufficiently deep random (and hardware-efficient) PQCs. This
was extended in [CSV+21] and found to occur with much shallower circuit depths
and be highly dependent on the locality of the cost function. Entanglement, a key
resource in quantum computation, was also proven to be detrimental to training
PQCs as too highly entangled systems were shown to exhibit also a BP [MKW21].
Of interest to note, is the ansatz dependent nature of barren plateaus - they are
rigorously proven to exist primarily for hardware efficient anstze, but have proven
to be absent in quantum convolutional neural networks [PCW+20] or tree tensor
network based anstze [ZG21, ZHLT20]. Furthermore, using problem-inspired
anstze such as the Hamiltonian variational ansatz has been shown numerically to
be useful for mitigating BPs [WZdS+20]. Finally, the barren plateaus referenced
above originate from the randomness in such circuit designs, and as such would
exist even if VQAs were implemented on fault-tolerant quantum devices. For near
term NISQ hardware unfortunately, an alternate BP rears its head. Dubbed noise-
induced barren plateaus [WFC+21], these arise simply due to hardware noise on
quantum devices, and may prove to be an alternative roadblock in the usefulness
of VQAs.
Hope, however, is not lost. While the discovery of BPs have proliferated, so
too have strategies to mitigate them, ranging from clever initialisation strate-
17This is not in fact limited to gradient methods, it has also been shown that gradient-free
methods which rely on cost function differences will also fail[ACC+20]
72 4. Preliminaries III: Quantum machine learning
gies [GWOB19] and correlating parameters [VC21] to using entanglement as a
resource [PNGY20].
Complementary to BPs, we also have the narrow gorge phenomenon. This
states that the problem solution occurs in a deep gorge with a small width which
shrinks exponentially quickly as the problem size increases. The narrow gorge
has been less well studied to date, so we will not focus on it in this Thesis, but
its appearance in a cost landscape was proven to be synonymous with a barren
plateau - one never occurs without the other [AHCC21]. This observation led to
the definition of quantum landscape theory [AHCC21], which aims to thoroughly
investigate parameter landscapes.
Given the above discussions, barren plateaus and landscape study is an active
area of study and highly interesting. This is particularly true given their impact on
discovering practical applications of quantum computers, which is the main goal
of this Thesis. We return to a discussion of BPs later for our specific use case.
4.1.4 VQA inputs and outputs
The nature of the inputs and outputs of a VQA are highly problem dependent. We
have touched on some of these in the above sections, but now let us highlight some
key examples in more detail. When using VQAs as machine learning applications
on classical data, the data must be encoded in quantum states to be processed
by the ansatz. Similarly, in these cases the outputs must also be classical, which
corresponds to performing some measurements on the quantum state. However,
for QML/VQA applications on quantum data, quantum states are directly provided
as training data {i}Mi=1 to the algorithm. Depending on the nature of the problem,
the output for quantum data may be still classical, but in some cases the VQA
may output quantum states themselves as the solution18. We shall encounter an
example later in this Thesis which exactly falls in this latter category.
In order to motivate key concepts in this area, we proceed by discussing three
examples, which will play central roles in the concepts used later in this Thesis.
Let us begin with one of the simplest possible:
 Data encoding for quantum classifiers.
A quantum classifier is one of the simplest applications one could consider for a
VQA/QNN. To be clear, here we refer to quantum classifiers for classical data. As
such, the problem statement is exactly the same as in Sec. 3.3.1. Quantum clas-
sifiers for quantum data are an extremely interesting area of study (see [CCL19]
for example) but outside the scope of this Thesis.
Given a classical dataset of N dimensional (real) vectors, D = {xm}Mm=1, we
must have some method to encode or embed the datapoints, xm RN into quan-
tum states for further processing in the quantum classifier. For the remainder of
18Of course, even in these cases some classical information is usually extracted for the purposes
of training the PQC. There are also proposals for quantum training, which is done more coherently
than is typical in VQAs [VPB18, KP20, LHF21]
4.1. Variational quantum algorithms 73
this Thesis, we primarily focus on data embeddings which encode only a data point
per quantum state19. Such an encoding process is achieved by a state preparation
operation, E, which acts as follows:
E : x  x (4.15)
In this Thesis, we assume an implementation of E as a unitary, S, acting on a
fixed initial (pure) state, so:
S(x)|0n = |(x) (4.16)
The function  is a feature map which maps from the original data space, X , into
a feature space (similar in nature to the feature map discussed for kernel methods
in Sec. 2.1.7.1). The resulting state, |(x) is therefore sometimes referred to as a
quantum feature vector. For S(x) to be useful as a data encoding, it should have
several desirable properties. First, S(x) should have a number of gates which
is at most polynomial in the number of qubits as well as the size of the dataset,
dimension of the data to be encoded, and all other relevant input parameters. For
machine learning applications, we want the family of state preparation unitaries
to have enough free parameters such that there is a unique quantum state x
for each feature vector x  i.e., such that the encoding function E is bijective.
Additionally, for NISQ applications, sub-polynomial depth is even more desirable,
and we want S(x) to be hardware-efficient.
We have already encountered such a state preparation routine earlier in this
Thesis in the form of the amplitude encoding (Eq. (4.1)). Data encoding methods
are of key interest to the success of QML algorithms in general, see [SP18] for an
overview of encoding strategies and techniques. For completeness, we list some
example encoding methods (taken from [SP18, SK19]).
Definition 21 (Basis encoding).
Given a feature vector x = [x1, ...,xN ]
T  {0,1}N , the basis encoding maps
x 7 EBasis as:
EBasis : x  {0,1}
N  |(x)= |x
= SBasis (x)|0
n = Xx0 XxN |0n
(4.17)
19As opposed to data encodings which consider also a superposition over datapoints in the
dataset, which may be required for some QML algorithms. An example is the amplitude encoding
given in Eq. (4.1).
74 4. Preliminaries III: Quantum machine learning
Definition 22 (Amplitude encoding).
Given a feature vector x = [x1, ...,xN ]
T CN , the basis encoding maps x 7
: x  Cn |(x)=
||x ||2
xi |i (4.18)
Here, we use n =O(log(N)) qubits to encode N-dimensional datapoints.
Definition 23 (Product (qubit) encoding).
Given a feature vector x = [x1, ...,xN ]
T RN , the basis encoding maps x 7
EProd
=(fi ,gi )
given by
EProd=(fi ,gi )ni=1
: x RN
[fi(xi)|0+gi(xi)|1] , |fi |2+ |gi |2=1, i (4.19)
The canonical example for a product encoding is choosing fi = cos(xi),gi =
sin(xi), i . In this case we have S(x) :=
i=1Ry (2xi), and the resulting state
|(x)=
[cos(xi)|0+sin(xi)|1] (4.20)
Finally, let us end this discussion with a reflection on the dimension, N, of the
datapoints. In order to actually use the NISQ computers available today for clas-
sification dataset, we are limited to relatively small datasets. For example, if one
tried to directly perform a classification task on the MNIST dataset (the example
in Sec. 3.3.1), each image consists of 2828= 784 pixels, which means N =784.
However, no (universal) quantum computer currently exists with 784 qubits, so
encoding using the product or basis encodings is not possible. In contrast, qubit ef-
ficient encodings such as the amplitude encoding would only require 10> log(784)
qubits, but may require a linear (relative to the length of vector to be encoded)
depth for the state preparation circuit, making it impractical to run on NISQ
devices with short coherence times. An alternative name for state preparation cir-
cuits are data loaders, and a family of such data loaders have been proposed with
q qubits, and depth D with overall complexity qD =O(N logN) [JDM+21], so it
is likely either one must sacrifice circuit depth or qubit number to load data. Alter-
native strategies for fitting larger datasets on NISQ computers are, for example,
using dimensionality reduction or feature extraction techniques such as principal
component analysis (PCA) [GBC+18] to extract the most relevant features.
In summary, loading classical data into quantum states in NISQ devices is
challenging in practice, but it is an active and interesting area of research, which
we contribute to in Chapter 5.
Next, let us examine VQA outputs taking now the following example:
4.1. Variational quantum algorithms 75
 Distribution extraction for quantum circuit Born Machines
A second key example of a VQA relevant to this Thesis (specifically in Chapter 6)
is the quantum circuit Born machine (QCBM), which we define here for conve-
nience20. Just as with the quantum classifier above, the most basic example of a
QCBM involves a unitary evolution by a PQC, U(), on a reference state, again,
usually taken to be |0n. The QCBM is used for the task of distribution learning
(as introduced in Sec. 3.3.2). Since quantum mechanics is naturally probabilistic,
a measurement of the prepared state of the QCBM in the computational basis,
produces a sample, z according to the distribution p:
|0n U()|0n Measure Z
 z  p(z) = |z |U()|0n|2 (4.21)
generated according to Borns rule of quantum mechanics (hence the name of
the model). The sample z is a binary string of length n, and when discussing the
QCBM, the primary object we are interested in is this distribution, p.
This output distribution (and in general output probability distributions from
quantum circuits) is intractable to compute, and so the QCBM is an example of an
implicit generative model [ML17, DG84]. An implicit model is one which is easy to
sample from, but whose corresponding probabilities are intractable to compute,
or we do not have access to. A popular example of an implicit model is the
generative adversarial network (GAN) which transforms a latent random variable
via a deterministic function to a random variable distributed according to the
distribution of choice. In the case of the QCBM, the ease of sampling is obvious;
a measurement of all qubits produces a sample. However, the corresponding
probabilities may be exponentially small, i.e. p(z) =O(1/2n) for n qubits. Since a
straightforward method to estimate probabilities is to simply construct an empirical
distribution by drawing S samples from the QCBM (preparing the same circuit
and measuring S times):
p(z) :=
(z z(s)), (4.22)
which counts the number of times a particular sample, z(s), occurs in the S
samples, {z(1),z(2), . . . ,z(S)}. If a particular sample, (say for example z() =
100010 . . .001  
) has a probability p(z
()) = 1/2n, then by only running the QCBM
circuit poly(n) times (as we would to keep the runtime efficient), we would never
observe the particular sample (unless we were incredibly lucky). This intuitive
argument explains why probability estimation of quantum circuits is exponentially21
20The original proposal [CCW18] for a Born machine was actually more general than in the
definition of the QCBM. Instead, [CCW18] simply proposed a parametrised quantum state as the
model, which could be prepared by arbitrary means. The specification to the quantum circuit
model came from the works of [LW18a, BGPP+19].
21Exponential here meaning in terms of sample complexity.
76 4. Preliminaries III: Quantum machine learning
hard in the worst case22.
In summary, when using VQAs for generative modelling, the outputs we care
about are the entire probability distribution of the quantum circuit, p(z). Since
we do not have access to p we must deal exclusively with samples, z . As such,
when dealing with these models, we must find powerful but efficient methods of
doing so in order to train them. This will be one of the primary contributions of
Chapter 6, where we return to a discussion of Born machines in greater detail.
Finally, we note that a number of extensions to the simple Born machine
model described above have been considered in the literature23. In its standard
form, a Born machine does not require an input (in contrast to the quantum
classifier). This is in contrast to comparable classical generative models such as
GANs [GPAM+14] which transform uniform input randomness into the desired
distribution. A Born machine, in contrast, is naturally probabilistic, due to the
inherent randomness in quantum mechanics. The first possible extension one
could consider for the Born machine is to also include classical input randomness,
or by including latent variables as an input space. For example, one could consider
a Born machine defined by a mixed state [RAG21, VMN+19, BCF+21]:
Born =
q(x)U()|xx |U() (4.23)
Where q(x) is a distribution over latent variables, x , which are encoded into input
states, |x. This classical distribution q(x) could also be parametrised, and
output from, say for example, another (classical) neural network [VMN+19]. In
this case, extracting samples from the machine requires the more general rule than
Eq. (4.21):
p(z) = Tr
|zz |Born
(4.24)
A second extension, is to allow a measurement in different bases. For example,
there is no requirement for the output strings, z , to be generated by computa-
tional basis measurements, Z. Measurement in alternate bases can be achieved
by appending gates to U() to rotate the basis. For example, by adding H to
each qubit, and measuring Z, this is equivalent to measuring in the Pauli-X basis.
With this in mind, a neat method to extend the output space of the QCBM was
proposed in [RTK+20] by measuring in both bases and concatenating the resulting
samples together24). To summarise this idea, the QCBM is measured once in the
Pauli-Z basis to produce an n-bit sample z1. The same state is then re-prepared
and measured in the Pauli-X basis which gives another n-bit sample, z2. The final
22For example, if we are required to fully characterise a QCBM distribution which has a large
number of these probabilities being exponentially small, we will not be able to observe a substantial
probability mass. These type of distributions are exactly those used to demonstrate quantum
computational supremacy [AAB+19], which we will discuss later in the Thesis. Of course, this
will not always be the case - quantum circuits can also produce distributions which are also very
concentrated and may have many probabilities being zero.
23Sometimes under different names, but the ideas could be adapted.
24In [RTK+20], the Born machine was actually used as a prior distribution for a GAN, and the
resulting model was shown to be able to generate high-quality MNIST digits (see Chapter 3.1
4.1. Variational quantum algorithms 77
sample is then the concatenation of these two: an 2n-bit sample z := z1||z2, and
the model is dubbed a basis-enhanced Born Machine. Such a scheme has a limited
expressibility since a basis-enhanced QCBM using n qubits cannot generate all the
possible distributions which are expressible using a 2n qubit (non basis-enhanced)
QCBM, but it is a useful method to possibly enhance the capabilities of quantum
generative models on NISQ hardware.
A final possible extension is to remove the QC from QCBM. It is plausi-
ble that advantages could be gained by studying alternative methods to prepare
the final state, | outside of the circuit model. For example, via a Hamilto-
nian evolution, measurement based quantum evolution or tensor network methods
(see [GZD18] for example).
Finally, let us conclude our discussion with purely quantum input and output.
We mentioned above that quantum data comes in the form of quantum states,
{i}Mi=1. In reality, what actually constitutes quantum data is slightly ambiguous.
Let us illustrates this ambiguity with some examples of VQAs which are designed
specifically with quantum inputs in mind:
Quantum
autoencoder
[ROAG17].
Variational
quantum
thermaliser
[VMN+19].
Quantum
convolutional
neural networks
[CCL19].
QVECTOR
[JRO+17].
Variational
quantum state
diagonalisation.
[LTOJ+19].
Variational
compilation &
unsampling.
[KLP+19, CMO+20]
We have presented a fairly broad selection of algorithms here; at one end of
the spectrum, we have the quantum autoencoder and the quantum convolutional
NN (QCNN). These can be considered specific quantum neural network models,
whose definition of quantum data is exactly that given above, a set of quantum
states, {i}Mi=1. The goal of the autoencoder is to compress quantum informa-
tion (as in its classical counterpart), and the QCNN is used to classify quantum
states based on their properties. In the middle, we have variational quantum state
diagonalisation and the variational quantum thermaliser which are the quantum
analogues of principal component analysis, and a generative model (for mixed
quantum states) respectively.
Finally, at the other end of the spectrum we have QVECTOR, variational
compilation (also [HSNF18, JB18]) and unsampling [CMO+20]. These tend to
fall more into the category of variational algorithm than QML models, and they
have a slightly different feel to the others. The reason for this is the goal of
these VQAs is to learn subroutines or primitives. In other words, the quantum
data in these cases is not strictly a set of quantum states, but instead we care
about the actual unitary in the PQC itself. For example, QVECTOR aims to
learn unitaries which can perform quantum error correction, and protect quantum
78 4. Preliminaries III: Quantum machine learning
memories, whereas compilation outputs a compiled version of an input unitary.
Finally, quantum unsampling ([CMO+20]) aims to learn the underlying unitary25
which produced certain quantum states. Now, hopefully it is clear that quantum
data can have a variety of meanings, depending on the context. In Chapter 7,
we present an example of a VQA which operates on quantum data; in this case
it is the unitary itself we are interested in so this application is similar to that
of variational compilation. To round off this section, let us describe one method
which is useful in dealing with quantum data in a VQA. Since the algorithms
of this nature require a classical feedback loop, even if the desired input-output
relationship is purely quantum, we must still extract classical information from
the output states from the VQA, if for no other purpose than to train the model.
In Sec. 2.1.7.3, we introduced the SWAP test, which could be used to estimate
overlaps between quantum datapoints. Alternatively, one could simply extract a
full description of the quantum state and use this as the classical signal. This
process is called quantum tomography, and in general is inefficient. Regardless, it
is extremely important. Let us illustrate tomography with a single qubit example.
Any qubit state, , can be decomposed into a linear combination of Pauli
matrices:
(1+ rxX+ ryY+ rzZ) (4.25)
Now, we can reconstruct , by estimating the Bloch vector, r := [rx , ry , rz ]T . This
can be done by estimating the expectation value of each of the Pauli observables,
Tr(A),A {X,Y,Z} and conglomerating the results. Of course, due to statistical
errors, the final state computed using an estimate, r of r , may not be a valid quan-
tum state, so methods such as direct inversion tomography, or maximum likelihood
tomography are used to project onto the best quantum state. (See [Sch16] for a
discussion of single qubit tomography methods, and [DPS03, BK10, GLF+10] and
references therein for a discussion of the more general scenario). For the purposes
of this Thesis, it will suffice to use an implementation of quantum tomography
from the forest-benchmarking library [GCH+19]. We use it particularly in Chap-
ter 7 where we adopt the technique of direct linear inversion.
4.2 Quantum kernel methods
In Sec. 2.1.7.1, we introduced kernel methods in the context of generative mod-
elling. However, we only briefly alluded to what is, arguably, their main use - in
support vector machines. Here, the feature maps (as in Theorem 2) are typi-
cally used to project data vectors into a higher dimensional space in which they
are linearly separable via a hyperplane, and so can be easily classified. Kernel
methods have also become popular in the quantum domain, beginning with the
works of [SK19, HCT+19] which noticed the connection between feature maps
and quantum states, since both live in Hilbert spaces, H. In particular, [SK19]
25Unitary learning in particular is an interesting problem in QML - quantum versions of the
no-free lunch theorem [WM97] have been proven in this context [PBO20, SCH+20].
4.2. Quantum kernel methods 79
proved the relevant formalism and [HCT+19] conjectured that a quantum advan-
tage could be gained if one used feature maps which correspond to classically
intractable kernels26, meaning those which could be efficiently evaluated by a
quantum computer, but not by any classical one.
Just as in the classical case, one may define a quantum RKHS, as follows:
Definition 24 (Quantum RKHS).
Let  : X H be a feature map over an input set X , giving rise to a real
kernel: (x ,y) = |(x) |(y)|2. The corresponding RKHS is therefore:
H := {f : X  R| f (x) = |w |(x)|2, x  X ,w H} (4.26)
For example, one may consider the state |(x) being prepared by a quantum
circuit, in the case of [HCT+19], one inspired by circuits in the family of instanta-
neous quantum polynomial time (IQP, [SB09]), which we return to in Chapter 6.
 : x  X n|(x) (4.27)
|(x) := U(x)|0
n = U(x)H
nU(x)H
n|0n (4.28)
U(x) := exp
S[n]
S(x)
(4.29)
so the resulting kernel is:
Q(x ,y) := |(x) |(y)|2 (4.30)
The unitary, U(x), is the key ingredient in the class IQP since it is diagonal in the
computational basis, and so when decomposed into single and two qubit unitaries,
can be implemented in a temporally unstructured manner (instantaneous). One
may also imagine simply extracting a kernel via a complex quantum amplitude
(rather than as a probability, as in Definition 24, which is real valued). We
mention this here to foreshadow a result in Sec. 6.3.1 in which an adaption of a
proof technique will allow for complex valued kernels.
To round off this discussion, let us mention some important follow up works in
the space of quantum kernels, which indicate their promise as a source of quantum
advantage. Firstly, [HCT+19, Sch21] demonstrated how quantum kernels were
actually equivalent to QNNs in the sense that the space occupied by quantum
models27 (e.g. the quantum classifiers discussed in Sec. 4.1.4 and which form the
focus of Chapter 5) is equivalent to the quantum RKHS from Definition 24. This
is perhaps surprising, since on the surface both methods appear quite different,
but these differences appear more in the practical aspects of dealing with both
26We return to a related discussion on classical intractablility and the relationship to quantum
advantage in Chapter 6.
27A quantum model is defined as one which outputs a function as f(x) = Tr ((x)E) where 
is a quantum encoding of the data vector, x and E is a suitable (POVM) measurement in the
notation of Sec. 2.1.4.
80 4. Preliminaries III: Quantum machine learning
methods, for example in trainability. Secondly, [HWGF+21] proposed parameteris-
ing and training the quantum feature maps in Eq. (4.27) to improve classification
(similar to the idea of [LSI+20] and that which we give in Chapter 5 for quantum
classifiers). Finally [HBM+21] (among others) noticed that since quantum mod-
els embedded in a form similar to Eq. (4.30) are simply quadratic functions in
the data, x , one could devise a classical algorithm which could completely repro-
duce the predictions of the model, given sufficient data28. This can be seen by
writing the functions be generated by a quantum model (with amplitude encoded
data, Eq. (4.1)), as the result of the measurement of an observable as:
f (x) = Tr ((x)E) =
xkk |
x l |l
kx l (4.31)
which is simply a quadratic function in the data point with n2 coefficients and can
be fit classically using n2/ data examples (as shown in [HBM+21])
This indicates that even though a kernel or quantum model may be intractable
to compute exactly, it may not actually outperform a classical model on the task
of interest (a true quantum advantage in machine learning). However, one may
expect that classical intractability is at least a prerequisite for quantum advantage,
since if one has the ability to efficiently simulate the model, solving the learning
task is also classically possible. We make a similar observation in the context of
generative modelling in Chapter 6.
In light of this, [HBM+21] proposes a projected quantum kernel which is a
function of the reduced density matrices of two encoded feature maps. With such
a kernel, and a measure known as the geometric difference, [HBM+21] was able to
also demonstrate a sizeable outperformance over a comparable classical machine
learning algorithm.
4.3 Quantum learning theory
To round off this chapter, let us circle back to the discussion of classical learning
theory presented in Sec. 3.6. The field of quantum learning theory is almost as old
as quantum computation itself, having been initiated by [BJ95]. In this work, the
quantum random example oracle (QPEX) was introduced (recall Definition 18),
which instead of outputting a single labelled example when queried, produces a
superposition of all possible labelled examples:
28The specific statement in [HBM+21] relates to the average prediction error of a quantum
model relative to a classical one, which is not too dissimilar to the risk, R, mentioned in Sec. 3.4.
4.3. Quantum learning theory 81
Definition 25 (Quantum random access oracle [BJ95]).
A quantum random access oracle, QPEX, when queried outputs a quantum
state of the following form:
QPEX | :=
D(x)|x , f (x) (4.32)
The work of [BJ95] demonstrated how, when given quantum access to such
examples, certain learning problems (for example the problem disjunctive normal
form (DNF) could be solved with exponentially fewer queries than classically pos-
sible. There is an important caveat however, in that this is achievable relative to
a specific distribution (the uniform distribution) and in general it is known that
such exponential reductions are not possible. The result of [SG04] demonstrates
how if there exists a quantum learning algorithm for a given problem, then there
also exists a classical algorithm for the same problem, which requires at most
polynomially more samples than its quantum counterpart29. So, we have another
no free lunch for quantum learning theory. This is perhaps not so surprising - we
also only have exponential speedups with a handful of quantum algorithms, so why
should there be any difference with learning theory problems?
To complement [SG04], the work of [AdW17b] proved the following theorem,
the quantum analogue Theorem 3:
Theorem 4: ([AdW17b, AdW17a])
Let F be a concept class with VCdim(F). Then, for every  
VCdim(F)1
log(1/)
(4.33)
examples are necessary and sufficient for an (,)-PAC learner for F .
This theorem illustrates that quantum provides no sample complexity advan-
tage when we require learning to be with respect to every distribution. In the
last few years, there have been a flurry of results in quantum learning theory, for
example, [AS05, Kot14, AGS19, ACL+20, AGY20, AGY20] and perhaps most
excitingly, the theoretical demonstration of an exponential speedup using support
vector machines [LAT21]. The latter result is somewhat contrived and uses an
engineered kernel in the spirit of Sec. 4.2 based on the discrete logarithm. As
such it is not as natural as one might like. However it is a very promising start.
In Chapter 6, we will discuss a work which proved a similar result in the distribu-
tion learning framework. For an excellent overview of topics in quantum learning
theory, we encourage the reader to see [AdW17a].
29The result of [AdW17b] subsequently proved optimal sample complexity bounds for quantum
learning.
Robust data encodings for quantum
classifiers
The Authors: . . . when todays intelligent systems fail, they often
fail spectacularly disgracefully without warning or explanation, leaving
a user staring at an incoherent output, wondering why the system did
what it did.
 Grad-CAM ([SCD+20])
5.1 Introduction
In Sec. 3.3.1 we introduced the problem of classification, one of the most common
problems for which classical neural networks are deployed. In Sec. 4.1.4, we then
described a notion of a quantum classifier, one based on a PQC which required
data vectors, x , to be encoded into quantum states after which a label is extracted
via a single qubit measurement. However, everything in this previous discussion
assumed access to a perfect quantum computer - one in which data uploading
and processing is achieved perfectly, with no errors in the process. Of course, this
assumption is not valid in the NISQ regime.
In Sec. 2.1.5, we introduced some example noise models which are present on
NISQ devices, and also discussed the overheads required to implement coherent
or fault-tolerant quantum algorithms, which put them out of the reach of current
devices. We also touched on the overhead required to detect and correct errors,
which are vital subroutines. Given this, the main question we ask in this chapter
is the following. Given an application specific mindset, is it possible to gain any
natural noise tolerance (or robustness) for free. The application we have in mind
is the variational quantum classifier, and we shall see in this chapter how even
posing this question raises interesting ideas.
As a spoiler, the answer we give to this question is: yes (but we may need to
be slightly pathological in our model designs).
Before we begin, let us outline the structure of the chapter. We begin by
presenting a more formal definition of the variational quantum classifier we con-
84 5. Robust data encodings for quantum classifiers
sider in Sec. 5.2. We elaborate on the definitions and examples of data encodings
from Sec. 4.1.4. After this, we present analytic results and proofs for robustness
in Sec. 5.3. We begin by showing that different encodings lead to different classes
of learnable decision boundaries, then characterise the set of robust points for
example quantum channels (those presented in Sec. 2.1.5). We state and prove
robustness results, and discuss the existence of robust encodings. Finally, we
prove a lower bound on the number of robust points in terms of fidelities between
noisy and ideal states. Lastly, we include several numerical results in Sec. 5.4 that
reinforce and extend our findings. Finally, we conclude this chapter with some
musings in Sec. 5.5.
5.2 Quantum classifiers
Let us begin by defining specifically what we mean by a quantum classifier. We
also in this chapter deal only with binary classifiers, where the label takes only
two possible values, y i  {0,1} for a corresponding datapoint (or feature vector),
x i  X 1. Let M denote the number of such datapoints in a dataset.
Figure 5.1: A common architecture for a binary quantum classifier that we study in this work.
The general circuit structure is shown in (a) and the structure for a single qubit is highlighted in
(b). In both, a feature vector x is encoded into a quantum state x via a state preparation
unitary Sx . The encoded state x then evolves to UxU =: x where U() is a unitary ansatz
with trainable parameters . A single qubit of the evolved state x is measured to yield a predicted
label y for the vector x .
Given this data, the goal (as discussed in Sec. 3.3.1) is to output a rule
f : X  {0,1} which accurately classifies the data and can predict the labels of
unseen data.
For quantum classification, outputting information (predictions) can be done
in a relatively straightforward manner. As several authors have noted [FN18,
SBSW20, SP18, GBC+18, PSCLGFL20], it is natural to use the measurement
1In practice, we typically have X = RN , but other sets  e.g., X = ZN or X  ZN2  are
possible, so we write X for generality.
5.2. Quantum classifiers 85
outcome of a single qubit, which produces a binary outcome, as a class prediction2.
We adopt this strategy in this chapter.
Informally, we define a (binary) quantum classifier as a procedure for encoding
data into a quantum circuit, processing it through trainable PQC, and outputting
a (binary) predicted label. Given a feature vector x  X , a concise description of
such a classifier can be written as:
Encoding
x 7 x (5.1)
Processing
x 7 x (5.2)
Prediction
x 7 y [x ] (5.3)
Several remarks are in order. First, a given data point x in the training set is
encoded in a quantum state x  Sn via a state preparation unitary Sx (introduced
in Sec. 4.1.4 and we elaborate in Sec. 5.2.1). We remark that each (unique) x
in the training set leads to a (unique) Sx , so the state preparation unitary can be
considered a parameterized family of unitary anstze.
For the processing step Eq. (5.2), several ansatz architectures have been pro-
posed in the literature, including quantum convolutional neural networks (men-
tioned in Sec. 4.1.4) [CCL19, HSPC20], strongly entangling anstze [SBSW20],
and more [SS16, GBC+18]. In this chapter, we allow for a general unitary evolu-
tion U() such that:
x = U()xU
() (5.4)
We remark that some classifier anstze involve intermediate measurements and
conditional processing (notably [CCL19]) and so do not immediately fit into Eq.
(5.4). Our techniques for showing robustness could be naturally extended to such
architectures, however, and so we consider Eq. (5.4) as a simple yet general
model. We primarily focus on data encodings and their properties in this chapter,
and not on other VQA features such as the choice of cost function. For this
reason we often suppress the trainable parameters  and write U for U().
Finally, the remaining step is to extract information from the state x to obtain
a predicted label. As mentioned, a natural method for doing this is to measure a
single qubit which yields a binary outcome 0 or 1 taken as the predicted label y .
Since measurements are probabilistic, we measure Nm times and take a majority
vote. That is, if 0 is measured N0 times and N0  Nm/2, we take 0 as the
class prediction, else 1. Generalising the finite statistics3, this condition can be
expressed analytically as
y [x ] =
0 if Tr[c0x ]
1 otherwise
, c0 := |00|c  |00|c 1c (5.5)
2An alternative strategy could be to use the parity (1)of outcome measurements from all
qubits, as proposed by [ASZ+21]. However, this is a global property and therefore may suffer from
barren plateau problems as introduced in Sec. 4.1.3
3We discuss details arising from finite statistics in Sec. 5.3.4. The results we prove can be
easily modified to incorporate finite statistics as shown in this section, but they are simpler to
state in terms of probabilities.
86 5. Robust data encodings for quantum classifiers
is the projector onto the ground state of the classification qubit, labelled c , and
the remaining qubits are labelled c . For brevity we often omit these labels when it
is clear from context. Throughout this chapter, we use y for predicted labels and
y for true labels, and we refer to Eq. (5.5) as the decision rule for the classifier.
Eq. (5.5) is not the only choice for such a decision rule. In particular, one could
choose a different weight  such that y = 0 if Tr[0x ]  as in [PSCLGFL20],
add a bias to the classifier as in [SBSW20], or measure the classification qubit in
a different basis. The latter shall be an example of a technique to gain robustness
to certain noise models later. Let us also remark that this decision boundary is
harsh, in the sense that it does not discriminate, in principle, between points
which have only Tr[c0x ] = 1/2+ versus those which have Tr[
0x ] = 1. As
such, one could also introduce a sequence of soft decision rules by defining the
model based on the distance of the single qubit probability4 to 1/2. Such decision
rules also allow the classifier to express a confidence in its choice for a given
label. Our techniques for showing robustness (Sec. 5.3.3) could be easily adapted
for such alternate decision boundaries.
The preceding discussion is summarised with the following formalisation:
Definition 26 ((Variational) quantum classifier).
A (binary) variational quantum classifier consists of three functions:
(i) An encoding function:
E : X  Sn, E(x) = x (5.6)
(ii) A function which evolves the state (possibly including ancillary qubits
and/or measurement):
U : C2
n2n  C2
m2m , U(x) = x (5.7)
(iii) A decision rule:
y : C2
m2m {0,1} (5.8)
Note that we have not included training data or a cost function in this def-
inition, so a quantum classifier can be considered a hypothesis family. We can
distinguish between the hypothesis family and the trained model  in which opti-
misation has been performed to minimise a cost function over a specified training
data set  by referring to the latter as the realised quantum classifier if it is
not clear from context. Let us now concentrate our focus to the primary object
of interest for the remainder of this chapter, step (i) in Definition 26, the data
encoding strategy.
4In practice, this is easily achieved by defining the decision rule based on the positivity, or
negativity of the expectation value of Z on the classification qubit, Zcx . We deal with the
probabilities rather than expectation values for simplicity, but the results could be extended in a
straightforward manner.
5.2. Quantum classifiers 87
Figure 5.2: A visual representation of data encoding Eq. (5.6) for a single qubit. On the left
is shown a set of randomly generated points {x i ,y i}Mi=1 normalised to lie within the unit square,
(x i = [x i1,x
T ), separated by a true decision boundary shown by the dashed black line. A data
encoding maps each x i  R2 to a point on the Bloch sphere x i  C
2. The dashed black line on
the Bloch sphere shows the initial decision boundary of the quantum classifier. During the training
phase, unitary parameters are adjusted to rotate the dashed black line to correctly classify as many
training points as possible. Different data encodings lead to different learnable decision boundaries
and different robustness properties.
5.2.1 Data encodings
As discussed in Sec. 4.1.4, a data encoding can be thought of as loading a data
point x  X from memory into a quantum state so that it can be processed by a
classifier. To present the work of this chapter, we primarily focus on the example
of a single qubit classifier. This is highly illustrative and allows us to visualise
some nice features of the model. We will also focus on variations of the qubit
encoding in Definition 23, i.e. data encodings whose state preparation unitary is
of the form:
S(x) =
Si (x). (5.9)
However, we remark that some of the results presented towards the end of the
chapter do in fact generalise to multi-qubit classifiers.
As a first step, let us generalise the product encoding of Eq. (4.20) to reduce
the number of qubits required by a factor of two. We do so by exploiting the
relative phase degree of freedom in a single qubit and define a dense angle5:
Definition 27. [Dense angle encoding]
Given a feature vector x = [x1, ...,xN ]
T  RN , the dense angle encoding
maps x 7 EDAE(x) as:
|(x) :=
N/2
cos(x2i1)|0+e2ix2i sin(x2i1)|1 (5.10)
5The qubit encoding in Eq. (4.20) is sometimes referred to as an angle encoding. Here we
make it dense by utilising both degrees of freedom in a single qubit.
88 5. Robust data encodings for quantum classifiers
For some of our analytic and numerical results, we highlight the dense angle
encoding for two-dimensional data x  R2 with a single qubit given by
|(x)DAE := cos(x1)|0+e2ix2 sin(x1)|1 (5.11)
which has density matrix
cos2x1 e
2ix2 cosx1 sinx1
e2ix2 cosx1 sinx1 sin
Although the angle encoding in Eq. (4.20) and dense angle encoding in Eq.
(5.10) use sinuosoids and exponentials, there is nothing special about these func-
tions (other than, perhaps, they appear in common parameterisations of qubits
and unitary matrices [NC10]). We can easily abstract these to a general class of
qubit encodings which use arbitrary functions6.
Definition 28. [General qubit encoding]
Given a feature vector x = [x1, ...,xN ]
T  RN , the general qubit encoding
maps x 7 EGQE(x) as:
|(x)GQE :=
N/2
fi(x2i1,x2i)|0+gi(x2i1,x2i)|1 (5.12)
where f ,g : RR C are such that |fi |2+ |gi |2 = 1 i .
We remark that a similar type of generalisation was used in [PSCLGFL20] with
a single qubit classifier that allowed for repeated application of an arbitrary state
preparation unitary. However, a key difference in our definition is the care that
needs to be taken to avoid information loss in the encoding - a single qubit only
has two degrees of freedom, while a general SU(2) unitary has three. Therefore,
while it is technically possible to use three sequential rotation gates as an encoding
unitary, each with a single feature encoded in the rotation angle, this is perhaps
not advisable. While this strategy would reduce the number of qubits required by
a factor of 3 over the product encoding of Eq. (5.9)), information in the feature
vectors will be lost since the input state is usually fixed at |0n.
We can also similarly generalise the amplitude encoding to allow for parame-
terisations of features (amplitudes).
6In fact, we already did this when introducing data encodings in Sec. 4.1.4. The main difference
here is a definition with approximately half the required number of qubits.
5.2. Quantum classifiers 89
Definition 29 (Generalised amplitude encoding).
For x  CN , the generalised amplitude encoding maps x 7 EGAE
={fi}Ni=1
={fi}Ni=1
: x  CN 
fi(x)|i (5.13)
with i |fi |2 = 1, i
The functions fi could only act on the i
th feature, e.g. fi(x) = sinxi , or could
be more complicated functions of several (or all) features. Note, we are not
commenting on the efficiency of preparing such a state here.
Thus far, we have formally defined a data encoding Eq. (5.12) and its role in
a quantum classifier (Definition 26), and we have given several examples. While
we have discussed different properties of state preparation circuits which imple-
ment data encodings (depth, overhead, etc.), we have not yet discussed the two
main properties of data encodings we consider in this chapter: learnability and
robustness. By learnability, we mean the expressive power [RPK+17] of a given
hypothesis family in its ability to find a set of parameters such that the realised
quantum classifier can (optimally) separate the data classes. In other words, learn-
ability measures the extent to which the quantum classifier can represent certain
functions. We show in Sec. 5.3.1 that different data encodings lead to different
classes of learnable decision boundaries. For robustness, we show that different
data encodings lead to different sets of robust points (to be defined) in Sec. 5.3.2
 Sec. 5.3.5. As a teaser for the statement made in the introduction of this
chapter, we find that encodings always exist which satisfy our definitions of ro-
bustness, but these may come at the expense of the learnability of our model, and
hence are pathological.
5.2.2 Robust data encodings
In this section, we define what we mean for a classifier to be robust against noise.
In the analytical and numerical results that we present in the following sections,
we focus on the simple noise models derived in Sec. 2.1.5.
Let us begin by introducing our definitions for robust points and robust data en-
codings of quantum classifiers. Informally, the intuition is as follows: the quantum
classifier with decision rule Eq. (5.5) requires only a coarse-grained measure-
ment to extract a predicted label. For example, with a single qubit classifier,
all points in the top hemisphere of the Bloch sphere are predicted to have la-
bel 0, while all points in the bottom hemisphere are predicted to have label 1.
The effect of noise is to shift and contract7 points within the Bloch sphere, but
certain points can get shifted such that they get assigned the same labels they
would without noise. This is the idea of robustness, represented schematically in
7For example, shifts would result from coherent noise sources, whereas contractions typically
arise as a result of stochastic noise.
90 5. Robust data encodings for quantum classifiers
(b)(a) (c)
Figure 5.3: Cartoon illustration of robust points for a single qubit classifier. In panel (a),
input training data points x i with classes yi  {yellow,blue} are mapped into quantum states x i
according to some encoding function E. The dashed line through the Bloch sphere indicates the
initial decision boundary. Points with a green outline are classified correctly, while points with a
red outline are misclassified. In (b), data points are processed by the PQC with optimal unitary
parameters (after minimising a cost function to find such parameters). For clarity, we keep data
points fixed and adjust the location of the decision boundary, which is now rotated to correctly
classify more points (fewer points with red outlines). In (c), a noise process E occurs which shifts
the location of the final processed points (or location of decision boundary), causing some points
to be misclassified. The set of points which maintain the same classification in (b) and (c) are the
robust points. Example points x1 and x2 are correctly classified in (a) and (b) then misclassified
in (c) due to the noise. Example point x3 is incorrectly classified in (a), correctly classified in (b)
after propagating through the PQC, and remains correctly classified in (c).
Fig. 5.3. For classification purposes, we only require that the point remain in the
same hemisphere in order to get the same predicted label.
Formally, we define a robust point as follows.
Definition 30 (Robust point).
Let E be a quantum channel, and consider a (binary) quantum classifier
with decision rule y as defined in Eq. (5.5). We say that the state x  Sn
encoding a data point x  X is a robust point of the quantum classifier if
and only if
y [E(x)] = y [x ] (5.14)
where x is the processed state via Eq. (5.7).
As mentioned, for the purpose of classification, Eq. (5.14) is a well-motivated
and reasonable definition of robustness. We remark that Eq. (5.14) is expressed
in terms of probability; in practice, additional measurements may be required to
reliably determine robustness. We discuss this point further in Sec. 5.3.4.
Further, we note that Eq. (5.14) assumes that noise occurs only after the
evolution x 7 x . While this may be a useful theoretical assumption, in practice
noise happens throughout a quantum circuit. We can therefore consider robust-
ness for an ideal data encoding as in Definition 30, or for a noisy data encoding
in which some noise process E1 occurs after encoding and another noise process
E2 occurs after evolution:
y [E2(U(E1(x)))] = y [x ] (5.15)
For our results, we primarily consider Eq. (5.14), although we show robustness
for Eq. (5.15) in some cases.
5.2. Quantum classifiers 91
Robust points (Eq. (5.14)) are related but not equivalent to (density operator)
fixed points of a quantum channel, and can be considered an application-specific
generalisation of fixed points. In Sec. 5.3.2, we characterise the set of robust
points for example channels, and in Sec. 5.3.5 we use this connection to prove
the existence of robust data encodings.
For classification, we are concerned with not just one data point, but rather a
set of points (e.g., the set X or training set Eq. (3.5)). We therefore define the
set of robust points, or robust set, in the following natural way.
Definition 31 (Robust set).
Consider a (binary) quantum classifier with encoding E : X  Sn and deci-
sion rule y as defined in Eq. (5.5). Let E be a quantum channel. The set
of robust points, or simply robust set, is
R(E ,E, y) := {x  X : y [E(x)] = y [x ]} (5.16)
where x is the processed state via Eq. (5.7) and x = E(x).
While the robust set generally depends on the encoding E, there are cases in
which R is independent of E. In this scenario, we say all encodings are robust to
this channel. Otherwise, the size of the robust set (i.e., number of robust points)
can vary based on the encoding, and we distinguish between two cases. If the
robust set is the set of all possible points, we say that the encoding is completely
robust to the given noise channel.
Definition 32 (Completely robust data encoding).
Consider a (binary) quantum classifier with encoding E and decision rule y
as defined in Eq. (5.5). Let x  X and let E be a quantum channel. We
say that E is a completely robust data encoding for the quantum classifier
if and only if
R(E ,E, y) = X (5.17)
We note that in practice (e.g. for numerical results), complete robustness is
determined relative to the training set Eq. (3.5). That is, we empirically observe
that E is a completely robust data encoding if and only if
R(E ,E, y) = {x i}Mi=1 (5.18)
Complete robustness can be a strong condition, so we also consider a partially
robust data encoding, defined as follows.
92 5. Robust data encodings for quantum classifiers
Definition 33 (Partially robust data encoding).
Consider a (binary) quantum classifier with encoding E and decision rule y
as defined in Eq. (5.5). Let x  X and let E be a quantum channel. We
say that E is a partially robust data encoding for the quantum classifier if
and only if
R(E ,E, y) X (5.19)
Similar to complete robustness, partial robustness is determined in practice
relative to the training set. For 0    1, we say that E is a -robust data
encoding if and only if
|R(E ,E, y)|= M (5.20)
where |  | denotes cardinality so that |R(E ,E, y)|  [M].
5.3 Analytic results
Using the definitions from Sec. 2.1.5, we now state and prove results about data
encodings8. First, we show that different encodings lead to different classes of
decision boundaries in Sec. 5.3.1. Next, we characterise the set of robust points
for example quantum channels in Sec. 5.3.2. In Sec. 5.3.3, we prove several
robustness results for different quantum channels, and in Sec. 5.3.5 we discuss the
existence of robust encodings as well as an observed tradeoff between learnability
and robustness. Finally, in Sec. 5.3.6, we prove an upper bound on the number of
robust points in terms of fidelities between noisy and noiseless states.
Before diving into these results, let us first derive some useful identities for a
single qubit classifier. Let  be a single qubit state with matrix elements i j , i.e.
00 01
10 11
Then we have:
11 10
01 00
YY =
11 10
01 00
00 01
10 11
Next, defining the projectors 0 := |00 | and 1 := |11 |, one can show
Tr[0XX] = Tr[1], Tr[0Y Y] = Tr[1] Tr[0ZZ] = Tr[0]
For any Hermitian matrix A= [Ai j ] and any unitary matrix U = [Ui j ], we have
Tr[0UAU
] = |U00|2A00+2[U00U01A10]+ |U01|
2A11. (5.21)
Similarly, one can show that
Tr[1UAU
] = |U10|2A00+2[U11U10A01]+ |U11|
2A11. (5.22)
8In actual fact, our results generally apply to the entire model as a whole, not just the data
encoding.
5.3. Analytic results 93
If we further assume the single qubit unitary, U(), has the decomposition:
Rz(21)Ry (22)Rz(23) (up to a global phase) [NC10], we get:
U() =
ei(13) cos2 ei(1+3) sin2
ei(13) sin2 e
i(1+3) cos2
(5.23)
Therefore, we get the various terms in Eq. (5.21), Eq. (5.22) t be:
|U00|2 = cos2(2), |U01|2 = |U10|2 = sin2(2), |U11|2 = cos2(2)
U00U01 =e
i23 cos(2)sin(2) =
e2i2 sin(22)
U11U10 = e
i23 cos(2)sin(2) =
e2i3 sin(22)
So the conditions become:
Tr[0UAU
] = |U00|2A00+2[U00U01A10]+ |U01|
= cos2 (2)A00+sin
2 (2)A11[e2i3 sin(2)A10]
Tr[1UAU
] = |U10|2A00+2[U11U10A01]+ |U11|
= sin2 (2)A00+cos
2 (2)A11+[e2i3 sin(22)A01]
5.3.1 Classes of learnable decision boundaries
In Sec. 4.1 and Sec. 5.2.1 we introduced several data encodings and discussed
differences in the state preparation circuits which realise them. Here, we show that
different encodings lead to different sets of decision boundaries for the quantum
classifier, thereby demonstrating that the success of the quantum classifier in
Definition 26 depends crucially on the data encoding Eq. (5.12)9.
The decision boundary according to the decision rule Eq. (5.5) is implicitly
defined by
Tr[0x ] =
(5.24)
Consider a single qubit encoding Eq. (5.12) so that:
f (x1,x2)
2 f (x1,x2)g(x1,x2)
f (x1,x2)g(x1,x2) |g(x1,x2)|2
where we assumed without loss of generality that f is real valued. Let the unitary
U be such that x = UxU and has matrix elements Ui j . Then, one can write the
decision boundary Eq. (5.24) as (see Eq. (5.21))
|U00|2f 2+2[U00U01f g]+ |U01|
2|g|2 =
(5.25)
9This fundamental importance of the data encoding in quantum classifiers has also been ob-
served and reinforced in previous and subsequent work to ours [LSI+20, GVT20, SSM21].
94 5. Robust data encodings for quantum classifiers
where we have let f := f (x1,x2) and g := g(x1,x2) for brevity. Eq. (5.25) implicitly
defines the decision boundary in terms of the data encoding f and g. The unitary
matrix elements Ui j then define the hypothesis family.
Eq. (5.25) can be solved numerically for different encodings, and we do so
in Sec. 5.4.1 (Fig. 5.6) to visualise decision boundaries for single qubit classi-
fiers. At present, we can proceed further analytically with a few inconsequential
assumptions to simplify the equations.
For the amplitude encoding, we have f (x1,x2)= x1 and g(x1,x2)= x2. Suppose
for simplicity that matrix elements U00 =: a and U01 =: b are real. Then, Eq.
(5.25) can be written
(ax1+bx2)
(5.26)
which defines a line x2= x2(x1) with slope a/b and intercept 1/
2b. Thus, a single
qubit classifier in Definition 26 which uses the amplitude encoding Eq. (4.1) can
learn decision boundaries that are straight lines.
Now consider the dense angle encoding Eq. (5.10) on a single qubit, for which
f (x1,x2) = cos(x1) and g(x1,x2) = e2ix2 sin(x1). Supposing again that matrix
elements U00  a and U01  b are real, we can write Eq. (5.25) as
a2 cos2x1+2abcosx1 sinx1 cos2x2+b
2 sin2x1 =
(5.27)
This can be rearranged to
cos2x2 =
12a2+(2a22b2)sin2x1
ab sin2x1
(5.28)
which defines a class of sinusoidal functions x2= x2(x1) (see Fig. 5.6 in Sec. 5.4.1).
The different decision boundaries defined by Eq. (5.26) and Eq. (5.28) em-
phasise the effect that encoding has on learnability. A realised classifier may have
poor performance due to its encoding, and switching the encoding may lead to
better results. We note that a similar phenomenon occurs in classical machine
learning  a standard example being that a dot product kernel cannot separate
data on a spiral, but a Gaussian kernel can. It may not be clear a priori what
encoding to use (similarly in classical machine learning with kernels), but different
properties of the data may lead to educated guesses. We note that [LSI+20]
consider training over hyperparameters to find good encodings, and we propose a
similar idea in Sec. 5.4.3 to find good robust encodings.
5.3.2 Characterisation of robust points
Given the model of a classifier, and the definitions of our noise channels we can
begin to discuss our robustness results against these channels. Intuitively, for a
given noise channel we wish to characterise the set of robust points (Definition 31).
If we then know what conditions our classifier will be deployed in (meaning the
noise model), we can then build the classifier to encode datapoints into this robust
set. As a result, these points will be protected. As a first step, we look instead at
5.3. Analytic results 95
the fixed points of a quantum noise channel E , defined as the states   Sn which
obey the following:
E() =  (5.29)
To proceed, we will first use some toy examples to build intuition and demonstrate
the relationship between robust points and fixed points. In Sec. 5.3.5, we formalise
this intuition. We remark that the characterisations similar to the ones in this
Section may be of independent interest from a purely theoretical perspective, as
robust points can be considered a type of generalised fixed point, or symmetry, of
quantum channels.
The pure states which are fixed points of the dephasing channel Eq. (2.40)
are 0 := |00| and 1 := |11|, and
= a0+b1 (5.30)
with a+b= 1 is the general mixed-state density operator fixed point. In contrast,
let us now consider the robust points of the same dephasing channel, which satisfy
y [Edephp ()] = y [] (5.31)
instead of Eq. (5.29). Certainly the state in Eq. (5.30) will satisfy Eq. (5.31) 
i.e., any fixed point is a robust point  but the set of robust points may be strictly
larger. To completely characterise the robust set, we seek the set of   S2 such
Tr[0] 1/2 = Tr[0Edephp ()] 1/2 (5.32)
Tr[0]< 1/2 = Tr[0Edephp ()]< 1/2. (5.33)
Using the simple properties of the trace and Pauli matrices from the beginning of
this section, we can write
Tr[0Edephp ()] = (1p)Tr[0]+pTr[0ZZ] = Tr[0].
Thus Eq. (5.32) and Eq. (5.33) are satisfied for all density operators S2. That
is, every data point x X is a robust point of the dephasing channel (independent
of the encoding) for the quantum classifier in Definition 26.
Consider now an amplitude damping channel Eq. (2.43) with p = 1, for which
the only fixed point is the pure state 0. By expanding the decision rule Eq.
(5.24) for a state under the amplitude damping channel as:
Tr[0EADp ()] = (1p)Tr[0]+p,
we see that a robust point  must satisfy Tr[0] = 1. That is, the only robust
point is 0, and in this case the set of robust points is identical to the set of fixed
points.
As expected from Eq. (5.14) and Eq. (5.29), these examples confirm that
F(E)R(E,E , y) (5.34)
where F(E) denotes the set of fixed points of E . In Sec. 5.3.5, we use this
connection to generalise the above discussion and prove the existence of robust
data encodings.
96 5. Robust data encodings for quantum classifiers
5.3.3 Robustness results
Regarding analytic results for robustness, we first demonstrate results for single
qubit classifiers with several channels in Sec. 5.3.3.1 and Sec. 5.3.3.2. Then,
we state and prove a robustness result for multi-qubit classifiers with (global)
depolarising noise in Sec. 5.3.3.3.
5.3.3.1 Single qubit classifier - Pauli noise
First, we consider when robustness can be achieved for a Pauli channel.
Theorem 5: Let EPp be a Pauli channel Eq. (2.38) and consider a quantum
classifier on data from the set X . Then, for any encoding E : X  S2, we
have complete robustness
R(EPp ,E, y) = X
if pX+pY  1/2. (Recall that p := [p1,pX ,pY ,pZ]).
Proof. The predicted label in the noisy case is identical to Eq. (5.5) with x
replaced by EPp (x). That is,
y [EPp (x)] =
0 if Tr[0EPp (x)] 1/2
1 otherwise
By definition Eq. (2.38), we have
Tr[0EPp (x)] = p1Tr[0x ]+pXTr[0XxX]+pY Tr[0YxY]+pZTr[0ZxZ]
(5.35)
Using the straightforward expressions given at the start of this section, we may
write Eq. (5.35) as:
Tr[0EPp (x)] = (p1+pZ)Tr[0x ]+(pX+pY )Tr[1x ]
By resolution of the identity, 1 = Tr[x ] = Tr[0x ]+Tr[1x ], we come to the
simplified expression:
Tr[0EPp (x)] = [12]Tr[0x ]+
where  := pX+pY .
Suppose the noiseless classification is y = 0 so that Tr[0x ]  1/2. Since
  1/2, we have
Tr[0EPp (x)] [12]
Hence, classification of data points with label y = 0 is robust for any encoding.
Suppose the noiseless classification is y =1 so that Tr[0x ]< 1/2. Since   1/2,
we have
Tr[0EPp (x)]< [12]
5.3. Analytic results 97
Hence, classification of data points with label y =1 is also robust for any encoding.
Returning to the condition, pX+pY  1/2, one can imagine a NISQ computer
in which either pX or pY were large enough such that this condition is not satisfied.
In this regard, we note two things. First, if this condition is not satisfied, then
not every encoding strategy will be robust to the Pauli channel in this model.
In particular, the set of robust points will now be dependent on the encoding
strategy. This is similar to the behaviour of the amplitude damping channel which
we illustrate in Sec. 5.4.
Second, the requirement pX+pY  1/2 appears because the decision rule uses
a measurement in the computational basis. We note that if we measure in the
Hadamard basis (see Theorem 6) then we get a modified robustness condition,
namely pY +pZ  1/2. A similar conclusion can be drawn from measurements in
the Pauli-Y basis. These results suggest that device-specific encoding strategies
and decision rules may be important for achieving robustness in practice on NISQ
computers.
To illustrate the results of Theorem 5, we focus on the dense angle encoding
(Definition 27), and a dataset for which the DAE can achieve nearly 100% ac-
curacy (specifically the vertical dataset - see Sec. 5.4). We then compute the
percentage which would be misclassified as a function of and Pauli noise param-
eters, pX , pY . The results are seen in Fig. 5.4. We note here, that for values
of pX + pY > 1/2, one has two strategies to achieve robustness. The first is to
adjust the measurement basis as per the discussion of the previous paragraph and
requires changing the model itself. Alternatively, one can apply an extra step of
post processing and relabel every output y = 0 to y = 1, and vice versa.
Figure 5.4: Misclassification percentage as a result of Pauli noise with strengths {pX ,pY ,pZ =
0}. As expected, classification is robust for pX + pY < 1/2 based on Theorem 5, and a sharp
transition occurs when this constraint is violated to give maximal misclassification. By classified
correctly in this context, we mean the fraction of points which are classified the same with and
without noise.
Three robustness results for different channels can be shown as corollaries
of Theorem 5 as follows.
98 5. Robust data encodings for quantum classifiers
1. By setting pX = pY = 0 in Theorem 5, it follows that R(E
p ,E, y) = X .
That is, all encodings are unconditionally robust to dephasing errors:
Corollary 1: Let Edephp be a dephasing channel Eq. (2.40), and con-
sider a quantum classifier on data from the set X . Then, for any
encoding E : X  S2,
R(Edephp ,E, y) = X
2. By setting pZ = pY =0 and using a modified decision rule which measures in
the Hadamard basis, denoted z , it follows that R(EBFp ,E, z) = X . That is,
all encodings are unconditionally robust to bit-flip errors (using a modified
decision rule). We first have the more general theorem, from which the
above statement follows immediately (+ := |++|):
Theorem 6: Consider a quantum classifier on data from the set X
with modified decision rule:
z [x ] =
0 if Tr[+x ] 1/2
1 otherwise
(5.36)
Then, for any E :X S2, with a Pauli channel EPp such that pY +pZ 
1/2, we have:
R(EPp ,E, z) = X
We can now specialise Theorem 6 to the case pZ = pY = 0:
Corollary 2: Consider a quantum classifier on data from the set X
with modified decision rule z defined in Eq. (5.36). Then, for any
encoding E : X  S2,
R(EBFp ,E, z) = X .
3. By setting pX = pY = pZ = p/4 and p1 := (1 3p/4), we then have that
R(Edepop ,E, y) = X . That is, all encodings are unconditionally robust to
depolarising noise:
Corollary 3: Let Edepop be a depolarising channel Eq. (2.41), and
consider a quantum classifier on data from the set X . Then, for any
encoding E : X  S2:
R(Edepop ,E, y) = X
Interestingly, we remark that fact (3) holds with measurements in any basis, not
5.3. Analytic results 99
just the computational basis. In Sec. 5.3.3.3, we generalise this result to multi-
qubit classifiers and as well as to noisy data encodings Eq. (5.15).
5.3.3.2 Single qubit classifier - amplitude damping
Let us now move to amplitude damping noise, for which the robust set R depends
on the encoding E. From the channel definition Eq. (2.43), it is straightforward
to see that:
Tr[0EADp (x)] = Tr[0x ]+pTr[1x ] (5.37)
Suppose first that the noiseless prediction is y = 0 so that Tr[0x ] 1/2. Then,
certainly Tr[0EADp (x)] 1/2 because p  0 and Tr[1x ] 0. Thus, the noisy
prediction is always identical to the noiseless prediction when the noiseless predic-
tion is y = 0. This can be understood intuitively because an amplitude damping
channel only increases the probability of the ground state. Now, suppose that the
noiseless prediction is y = 1. From Eq. (5.37), we require:
Tr[0EADp (x)] = Tr[0x ]+pTr[1x ]< 1/2
to achieve robustness. Using the resolution of the identity, Tr[1x ] = 1
Tr[0x ], we arrive at the condition:
Tr[1x ]>
2(1p)
(5.38)
Let x be given by the general qubit encoding Eq. (5.12) so that Eq. (5.38) can
be written (see Eq. (5.22))
|U10|2f 2+2[U11U10f g
]+ |U11|2|g|2 >
2(1p)
where Ui j denote the optimal unitary matrix elements. Formally, we then have:
Theorem 7: Consider a quantum classifier on data from the set X , and
let EADp denote the amplitude damping channel Eq. (2.43). Then, for any
qubit encoding E defined in Eq. (5.12) which satisfies
|U10|2f 2+2[U11U10f g
]+ |U11|2|g|2 >
2(1p)
, (5.39)
we have
R(EADp ,E, y) = X .
If E is not completely robust, the set of points x such that Eq. (5.39) holds
define the partially robust set.
We note that Eq. (5.39) depends on the optimal unitary U as well as the
encoding E. This is expected as the final state x has been processed by the
PQC. In practice, since we do not know the optimal unitary parameters a priori,
100 5. Robust data encodings for quantum classifiers
it remains a question of how large the (partially) robust set will be for a given
an encoding. To address this point, we discuss in Sec. 5.4.3 how training over
hyperparameters in the encoding function can help find the robust region even
after application of the unknown optimal unitary. Additionally, in the next Section
we discuss whether we can find an encoding which satisfies Eq. (5.39), or more
generally whether a robust encoding exists for a given channel.
Given the robustness condition Eq. (5.39) for the amplitude damping channel,
it is natural to ask whether such an encoding exists. In Sec. 5.3.5, we show the
answer is yes by demonstrating there always exists a robust encoding for any trace
preserving quantum operation. This encoding may be trivial, which leads to the
idea of a tradeoff between learnability and robustness.
5.3.3.3 Multi-qubit classifier
We now consider global depolarising noise on a multi-qubit classifier. It turns
out that any encoding is completely robust to this channel applied at any point
throughout the circuit. To clearly state the theorem, we introduce the following
notation. First, let
Epi () = pi+(1pi)
(5.40)
be shorthand for a global depolarising channel with probability pi . (Note pi and
1pi are intentionally reversed compared to Definition 7 to simplify the proof.)
Then, let
Ui Epi
x (5.41)
denote the state of the encoded point x after J applications of a global depolar-
ising channel and unitary channel. For instance, J = 1 corresponds to
U1 Ep1 x  U1(Ep1(x))
and m = 2 corresponds to
U2 Ep2 U1 Ep1 x  U2(Ep2(U1(Ep1(x))))
We remark that Ui can denote any unitary in the circuit.
With this notation, we state the theorem as follows.
Theorem 8: Consider a quantum classifier on data from the set X with
decision rule y defined in Eq. (5.36). Then, for any encoding E : X  Sn,
EGDp ,E, y
where EGDp denotes the composition of global depolarising noise acting at
any point in the circuit  i.e. such that the final state of the classifier is
given by Eq. (5.41).
5.3. Analytic results 101
To prove Theorem 8, we use the following lemma.
Lemma 1: The state in Eq. (5.41) can be written as (adapted
from [SKCC20])
piUJ   U1xU
1   U
(5.42)
where d = 2n is the dimension of the Hilbert space.
Proof. Using the definition of the global depolarising channel Eq. (5.40), it is
straightforward to evaluate (with 1 1d):
x = U1 Ep1 x = p1U1xU
1+(1p1)
Thus Eq. (5.42) is true for J = 1. Assume Eq. (5.42) holds for J = k . Then, for
k+1 we have
(k+1)
x = Uk+1 Epk+1  
x = pk+1Uk+1
+(1pk+1)
The last line can be simplified to arrive at
(k+1)
piUk+1   U1xU
1   U
(5.43)
which completes the proof.
We can now prove Theorem 8 as follows. Let l denote the total number of
alternating unitary gates with depolarising noise in the classifier circuit so that Eq.
(5.42) can be written
x = px +(1 p)
. (5.44)
Here, we have defined p :=li=1pi and noted that Ul   U1xU
1   U
= x is the
final state of the noiseless circuit before measuring. Eq. (5.44) is thus the final
state of the noisy circuit before measuring. Then:
Tr[0
x ] = pTr[0x ]+
(1 p)
(5.45)
using Tr[0Id ] = 2
d1. To prove robustness, suppose that y [x ] = 0 so that
Tr[0x ] 1/2. Then,
Tr[0
(1 p)
so that y [(l)x ] = 0. Similarly for the case y [x ] = 1, which completes the proof
of Theorem 8.
102 5. Robust data encodings for quantum classifiers
Thus, any encoding strategy exhibits complete robustness to global depolar-
ising noise. We remark again that our definition of robustness (Definition 30) is
in terms of probabilities, meaning that more measurements for sampling may be
required to reliably evaluate robustness. With this remark, we note an interesting
connection to explain a phenomenon observed in recent literature: the authors
of [GBC+18], found that classification accuracy decreased under the presence of
depolarising noise. Theorem 8 implies this was exclusively due to the finite shot
noise used to obtain the predicted label. We discuss errors due to finite sampling
in more detail in Sec. 5.3.4.
While global depolarising noise admits a clean robustness result for an arbitrary
d-dimensional circuit, general channels can lead to complicated equations which
are best handled numerically. We include several numerical results in Sec. 5.4,
and we discuss avenues for proving more analytical results with certain classes of
channels in future work in Sec. 5.5. To close the present discussion, we highlight
the special case of multi-qubit classifiers with factorisable noise, for which it is
straightforward to apply previous results proved in this section.
In particular, suppose that E : SnSn is a noise channel which factorises into
single qubit channels, e.g.
E = E1 En (5.46)
where Ei : S2  S2 for i  [n]. Without loss of generality, let the classification
qubit be the nth qubit. Then, if the processed state of the classification qubit is
robust to the channel En, the encoded state will be robust to the entire channel
E in Eq. (5.46). Specifically, we have the following (proof in Appendix A.1.1):
Theorem 9: If E is any noise channel which factorises into a single qubit
channel, and a multiqubit channel as follows:
E() = Ec(cx)Ec(
where WLOG Ec acts only on the classification qubit (cx = Trc(x)) after
encoding and unitary evolution, and Ec acts on all other qubits arbitrarily,
(cx = Trc(x)). Further assume the state meets the robust classification
requirements for the single qubit error channel Ec . Then the classifier will
be robust to E .
The above theorem is a simple consequence of causality in the circuit, only
errors which have to happen before the measurement can corrupt the outcome.
As such, outside of single qubit errors, we only need to consider errors before
the measurement which specifically involve the classification qubit. This result
also holds for general n 1 qubit channels which act on every qubit except the
classification qubit. Although this is relatively straightforward, the result could be
used as a building block to better understand more intricate robustness properties
of quantum classifiers.
5.3. Analytic results 103
5.3.3.4 Robustness to measurement noise
Just as the case of quantum compilation [SKCC20], we can deal with measurement
noise in the classifier:
Definition 34 (Measurement noise).
Measurement noise is defined as a modification of the standard POVM basis
elements, {0 = |00 |,1 = |11 |} by the channel Emeasp with assignment
matrix p for a single noiseless qubit:
0 = |00 |
Emeasp 0 = p00|00 |+p01|11|
1 = |11 |
Emeasp 1 = p10|00 |+p11|11 | (5.47)
p00 p01
p10 p11
where p00+p10=1,p10+p11=1, and hence pkl is the probability of getting
the k outcome given the input l . Furthermore, we assume that pkk > pkl
for k = l .
The definition for the general case of n qubit measurements can be found in
[SKCC20], but we shall not need it here, since we only require measuring a single
qubit to determine the decision function. More general classifiers which measure
multiple qubits (e.g. and then take a majority vote for the classification) could
also be considered, but these are outside the scope of this work. Now, we can
show the following result in a similar fashion to the above proofs (explicit proof
in Appendix A.1.1):
Theorem 10: Let Emeasp define measurement noise acting on the classifica-
tion qubit and consider a quantum classifier on data from the set X . Then,
for any encoding E : X  S2, we have complete robustness
R(Emeasp ,E, y) = X
if the measurement assignment probabilities satisfy p00 > p01,p11 > p10.
Just as above, we can replace the ideal state, x i with a noisy state, E(x i ),
where the operator accounts for other forms of noise, not including measurement
noise. We can see this allows us to take a model which is robust without mea-
surement noise, and upgrade it to one which is. However, we may be able to find
looser restrictions by considering different types of noise together, rather than in
this modular fashion.
To illustrate the results of Theorem 10 we focus on the dense angle encoding,
which can achieve nearly 100% accuracy on the vertical dataset. We then
compute the percentage which would be misclassified as a function the assignment
probabilities in the noisy projectors in Eq. (5.47). The results are seen in Fig. 5.5.
104 5. Robust data encodings for quantum classifiers
Figure 5.5: Misclassification percentage as a result of measurement noise as a function of
probabilities {p00,p11}. If either p00 or p11 is less than 1/2, then half the correctly classified points
will be misclassified, with the probability increasing as expected, with the number of misclassified
points increasing as the off diagonal terms, p01,p10  1, as expected from Theorem 10. By
classified correctly in this context, we mean the fraction of points which are classified the same
with and without noise.
Of course, all we have simply done here is include the uncertainty from mea-
surement in the definition of the robust point. If one were to do the same analysis
for the original definition of a robust point (Definition 30), we would have a ro-
bustness result only for those points which are sufficiently far outside the  region
(say for example those points, x , for which Tr[c0x ] 1/2+)
5.3.4 Modifications for finite sampling
In practical applications, we of course do not have access to exact probabilities
but rather samples from the underlying distribution. Specifically, we sample a
finite number N times from the state and get an estimate of the probabilities
in terms of recorded frequencies. In this section, we discuss implications of this
finite sampling error and show how to modify results from the previous sections
to account for finite sampling.
The first modification is to the decision rule Eq. (5.5). Here, we replace the
true probability px := Tr[c0x ] with an unbiased estimator px(N) obtained from
N samples. The modified decision rule is then
yN [x ] =
0 if px(N) 1/2
1 otherwise
. (5.48)
Suppose we find that px(N) 1/2. Then, by Hoeffdings inequality
Pr(|px(N)px | ) 1 (5.49)
we have:
5.3. Analytic results 105
|px  px(N)|  (5.50)
=  px 
  (5.51)
 px 
+ (5.52)
Therefore (taking the left hand inequality above), for any  > 0, we know that
px  1/2 with probability at least 1 (we refer to 1 as the confidence)
if we take:
N  Nc(,) :=
(22)
(5.53)
samples from px . This follows since each measurement of the observable gives a
Bernoulli random variable with expectation Tr[c0x ].
We note that it is in principle #P-hard to exactly determine the predicted class
label since points could be within any > 0 from the decision boundary. In practice
we do not expect this to be a limiting issue aside from pathological examples, but
it is important to note. One could introduce a penalty term into the cost function
such that the distance  between the boundary and the closest point is maximised
(as in support vector machines) to mitigate this effect.
Now, to begin the discussion of robustness in this context, we first note that
clearly, finite sampling effects can be treated as a form of noise. If we consider
N in Eq. (5.53), we recover the prediction of the ideal channel, yN[x ] =
y [x ]. Then, if N being finite (and fixed) is treated as a noise channel so, yN [x ] =
y [Esamp(x)], all points x for which y [x ] 1/2+ (assuming the label is 0) will
be robust against Esamp
Now, unfortunately those x  X for which 1/2 y [x ] 1/2+ may or may
not be in the robust set for the finite sampling channel, R(Esamp
,E, y).
Let Esamp
be the noise channel induced by a certain number of samples, N1,
such that a particular classifier does not have the entirety of the dataset in the
robust set, R(Esamp
,E, y) = X . In order to make a classifier for which all points
in the dataset are in the robust set, we simply need to increase the number of
samples to some sufficiently large value, N2 >N1, and we shall recover complete
robustness, i.e. we have R(Esamp
,E, y) = X . However, this comes at a caveat
that N2 may not be a polynomial function of the problem parameters. We leave
the investigation of the value N2 should be for a particular problem (as it will in
general be highly problem- and dataset-dependent) to future work.
Now, in the above discussions we have taken care of the cases ideal classifier
 noisy classifier and ideal classifier  finite sampling classifier. The final
case to deal with is finite sampling classifier  finite sampling noisy classifier.
Coupled with the discussion of the number of shots required for finite-sampling
robustness, this final step will allow us to uplift the robustness argument for any
other noise channel into the finite sampling regime, i.e. the case, ideal classifier
 finite sampling noisy classifier. To do so, we present a modified definition of
106 5. Robust data encodings for quantum classifiers
a robust point to account for errors due to finite sampling, specifically one which
incorporates the , parameters.
Definition 35 ((,)-Robust point).
Let 0< < 1/2, 0<  < 1, and E be a quantum channel. We say that the
state x  Sn encoding a data point x  X is an (,)-robust point of the
quantum classifier if and only if
yN [E(x)] = yN [x ] (5.54)
with confidence 1. Explicitly, Eq. (5.54) gives two conditions:
Tr[c0x ] 1/2 = Tr[
0E(x)] 1/2 (5.55)
Tr[c0x ]< 1/2+ = Tr[
0E(x)]< 1/2+ (5.56)
with confidence 1  . This is achieved using N  Nc(,) :=
1/(22) log1/ measurements in the decision rule Eq. (5.48).
Procedurally, we can think of this definition as follows. We are provided with
an 0 <  < 1/2 which corresponds to how close to the boundary we want to be
able to distinguish robust points (the farthest we can be from the boundary is
1/2), and a 0 <  < 1 which corresponds to how certain we want to be about
points within this region. Every point  which satisfies
|Tr[c0]1/2|<  (5.57)
is classified correctly with confidence 1 using at least Nc(,) measurements.
Here,  could be either x or E(x) as in Eq. (5.54).
For a specific example of how finite sampling considerations may be incorpo-
rated into the previous results in a natural way, consider Corollary (3) of Theorem 5
which states that a single qubit classifier is unconditionally robust to depolarising
noise. The modified statement of this theorem which accounts for finite sampling
is as follows.
Theorem 11: Let Edepop be a single qubit depolarising channel and consider
a quantum classifier on the set X . For any encoding E :X S2, let x  X
and , be such that
px(N(,))
. (5.58)
Then, x = E(x) is an (,)-robust point.
Proof. Given x  X and , such that
px(N(,)) 1/2, (5.59)
we have yN [x ] = 0 with confidence 1. This means that
Tr[c0x ] 1/2 (5.60)
5.3. Analytic results 107
with confidence 1. It follows that
Tr[c0E
p (x)] = p/2+(1p)Tr[
0x ]
 p/2+(1p)(1/2)
= 1/2+p
with confidence 1. The analogous result can be shown when px(N(,))< 1/2
which completes the proof.
The other preceding results could similarly be uplifted to incorporate finite
sampling in a similar manner.
5.3.5 Existence of robust encodings
In Sec. 5.3.2, we considered example channels and characterised their robust points
and fixed points. We found that the set of fixed points F(E) is always a subset
of the robust set R(E ,E, y) in Eq. (5.34). Here, we use this connection to
show that there always exists a robust encoding for a trace-preserving channel
E (regardless of optimal unitary parameters which may appear in the robustness
condition, e.g. Eq. (5.39)).
Theorem 12:
Any trace-preserving quantum operation has at least one density operator
fixed point Eq. (5.29).
Using this and the observation that F(E)R(E ,E, y), we have the following
existence theorem for robust encodings.
Theorem 13: Given a data point x X , a trace-preserving quantum channel
E , and decision rule y defined in Eq. (5.5), there exists an encoding E such
y [E(E(x))] = y [E(x)]. (5.61)
We note that the optimal unitary of the PQC affects the location of the
robust set, but not the existence. We emphasise that Theorem 13 is with respect
to a single data point x  X . As mentioned in Sec. 5.2.2, it is more relevant for
applications to consider the training set Eq. (3.5) or entire set X .
Appropriately, one can ask whether a completely robust encoding (Defini-
tion 32) exists for a given channel E . This answer also turns out to be yes,
but in a potentially trivial way.
In particular, suppose that there is a unique fixed point  of the channel E ,
e.g. depolarising noise or amplitude damping noise with p = 1. Then, consider
the encoding
E(x) = 
108 5. Robust data encodings for quantum classifiers
for all x  X . From a robustness perspective, this has the desirable property of
complete robustness. From a machine learning perspective, however, this has very
few desirable properties: all training data is mapped to the same point so that it
is impossible to successfully train a classifier10.
The previous example, while extreme, serves to illustrate the tradeoff between
learnability (expressive power) and robustness. By expressive power, we mean
the ability of the classifier (hypothesis family) to predict correct labels without
regard to noise. By robustness, we mean the property of retaining the same label
(without regard to correctness) in the presence of noise. These two properties
can be schematically connected as below:
y [x ]
Learnability
 y [x ]
Robustness y [E(x)]
The tradeoff we observe is that it is possible to maintain robustness by means
of a certain encodings, but these encodings generally reduce the expressive power
of the classifier. Succinctly, more robustness leads to less expressive power, and
vice versa. We discuss this point more in Sec. 5.4.3.
5.3.6 Lower bounds on partial robustness
In this section, we consider a slightly modified binary quantum classifier which
embeds the cost function in the circuit and computes the cost by measuring
expectation values. In contrast to the classifier in Definition 26, the output of this
circuit is thus the cost C instead of an individual predicted label y . Correspondingly,
the input to the circuit is all data points in the training set Eq. (3.5) (using a
mixed state encoding discussed below) instead of a single data point x . Such
a classifier was recently introduced by [CWV+19] and presents an interesting
framework to analyse in the context of noise. In the remainder of this section, we
prove a lower bound on the size of the robust set in terms of fidelities between
noisy and noiseless states.
Before precisely stating this theorem, we formally define the mixed state en-
coding and cost function of this modified classifier.
Definition 36 (Mixed state encoding).
Let {(x i ,yi)}Mi=1 be a dataset and E be an encoding. For each feature vector
x i , let
x i := E(x i)|yiyi |= x i |yiyi |. (5.62)
The mixed state encoding is then defined by
x i . (5.63)
10In principle, one can achieve an encoding which is completely robust and able to correctly
classify all data if there are at least two orthogonal fixed points in F(E). For example, if E is
the bit-flip channel, the encoding x i 7 |0+(1)yi |1 is both completely robust and completely
learnable (the optimal unitary is a Hadamard gate), but assumes the true labels yi are known.
5.3. Analytic results 109
We note that Eq. (5.63) may be realised by preparing one of the pure states Eq.
(5.62) with equal probability. We use the indicator cost function given by
1(yi(x i ) = yi). (5.64)
Here, the indicator 1 evaluates to the truth value of its argument  i.e., 1(yi =
yi) = 0 if yi = yi , else 1. This is clearly a special case of the general VQA cost in
Eq. (4.6) where f is the indicator, and the observables to be measured are the
0 projector on the classification qubit.
We now state and prove the following theorem which provides a lower bound
on the size of the robust set.
Theorem 14: Consider a quantum classifier using the mixed state encod-
ing Eq. (5.63) and indicator cost function Eq. (5.64). Assuming that a
noise channel E acts only on the encoded feature vectors x i (and not the
encoded labels |yiyi |), then
|R(E ,E, y)|M
1F (E(), )
(5.65)
where F is the fidelity between quantum states, Eq. (2.69).
This theorem is useful since, if one has an expression for the output fidelity of
a quantum circuit after a noise channel is applied, one can directly determine a
bound on how the classification results will be affected. In order to prove it, we
relate the difference between the noisy cost and the noiseless cost to the size of
the robust set.
We can also show the following tighter bound based on the average trace
distance between the individual encoded states (i.e., not using the mixed state
encoding) for the original cost function, C:
E C
1F (E(x i ), x i ). (5.66)
Proof.
E C := |CEC|= |Tr[D(E() )]|
|Tr(D
E(x i )|yiyi | x i |yiyi |
||D||||
E(x i ) x i
|yiyi |||1
1F (E(x i ), x i ) (5.67)
110 5. Robust data encodings for quantum classifiers
In Sec. 5.4.4, we use these inequalities to bound the size of the robust set for
several different encodings on an example implementation.
5.4 Numerical results
In this section, we present numerical evidence to reinforce the theoretical results
proved in Sec. 5.3 and build on the discussions. In Sec. 5.4.1, we show classes
of learnable decision boundaries for example encodings, building on the previous
discussion in Sec. 5.3.1. We then plot the robust sets for partially robust encodings
in Sec. 5.4.2 to visualise the differences that arise from different encodings. We
also generalise some encodings defined Sec. 5.2.1 to include hyperparameters and
study the effects. This leads us to attempt to train over these hyperparameters,
and we present an encoding learning algorithm in Sec. 5.4.3 to perform this task.
Finally, in Sec. 5.4.4 we compute lower bounds on the size of robust sets based on
Sec. 5.3.6. We note that we include code to reproduce all results in this section
at [Coy20]. For all numerical results in the following sections related to the single
qubit classifier, we use three simple datasets; the first is the moons dataset from
scikit-learn, [PVG+11], and two we denote vertical and diagonal. These
datasets can be seen in Fig. 5.7.
5.4.1 Decision boundaries and implementations
In Sec. 5.2.1, we defined an encoding Eq. (5.6) and gave several examples.
In Sec. 5.3.1, we showed that a classifier with the amplitude encoding Eq. (4.18)
can learn decision boundaries that are straight lines, while the same classifier with
the dense angle encoding Eq. (5.11) can learn sinusoidal decision boundaries. We
show this in Fig. 5.6, and we build on this discussion in the remainder of this
section.
Figure 5.6: Examples of learnable decision boundaries for a single qubit classifier. We show
the (a) dense angle encoding, (b) amplitude encoding, and (c) superdense angle encoding where
 =  and  = 2. Colours denote class labels. The PQC used here consisted of an arbitrary
single qubit rotation (see Fig. 5.8) with random parameters.
Fig. 5.6(c) shows a striped decision boundary learned by a superdense angle
encoding, defined below. The superdense encoding introduces a linear combination
of features into the qubit (angle) encoding Eq. (4.20) and is another special case
of the general qubit encoding Eq. (5.12).
5.4. Numerical results 111
Definition 37 (Superdense angle encoding (SDAE)).
Let x = [x1, ...,xN ]
T RN be a feature vector and , RN be parameters.
Then, the superdense angle encoding maps x 7 E(x) given by
N/2
cos(ix2i1+ix2i)|0+cos(ix2i1+ix2i)|1. (5.68)
For a single qubit, the SDAE is
|x := cos(x1+x2) |0+sin(x1+x2) |1.
We observe that = 0 recovers the qubit (angle) encoding Eq. (4.20) considered
by [SS16, SP18, CWV+19] and Eq. (5.68) encodes two features per qubit.
We note that Definition 37 includes hyperparameters  and . The reason for
this will become clear in Sec. 5.4.3 when we consider optimizing over encoding
hyperparameters to increase robustness. As previously mentioned, a similar idea
was investigated in [LSI+20] for the purpose of (in our notation) learnability.
Fig. 5.7, we illustrate the three single qubit datasets we employ in this chapter
to demonstrate our results, namely the vertical, diagonal and moons. The
former two are linearly separable, whereas the moons dataset is nonlinear.
0.0 0.2 0.4 0.6 0.8 1.0
0.0 0.2 0.4 0.6 0.8 1.0
0.0 0.2 0.4 0.6 0.8 1.0
Figure 5.7: Three single qubit (two dimensional) datasets which we use. (a) vertical, (b)
diagonal and (c) the moons dataset from scikit-learn [PVG+11] rotated by 90 with a noise
level of 0.05. 20% of each set is test data, indicated by the points circled with the opposite colour.
We choose the latter two due to the fact that the moons and vertical datasets can be well
classified by the dense angle encoding, while the diagonal dataset can be well classified by the
amplitude encoding, which can be seen by studying the decision boundaries generated in Fig. 5.6.
As a final example to explore the importance of encodings, we consider an
example implementation on a standard dataset using different encodings. The
dataset we consider is the Iris flower dataset [Fis36] in which each flower is de-
scribed by four features (x  R4). The original dataset includes three classes
(species of flower) but we only consider two for binary classification. A quantum
classifier using the qubit angle encoding Eq. (4.20) and a tree tensor network
(TTN) ansatz was considered in [GBC+18]. Using this encoding and PQC, the
authors were able to successfully classify all points in the dataset.
Since the angle encoding maps one feature into one qubit, a total of four
qubits was used for the example in [GBC+18]. Here, we consider encodings which
112 5. Robust data encodings for quantum classifiers
map two features into one qubit and thus require only two qubits. Descriptions
of the encodings, PQC anstze, and overall classification accuracy are shown in
Table 5.1.
Encoding PQC ansatz NP n Accuracy
Angle TTN 7 4 100%
Dense Angle U(4) 12 2 100%
Amplitude U(4) 12 2 100%
Superdense Angle U(4) 12 2 77.6%
Table 5.1: Classification accuracy achieved on the Iris dataset using different
encodings and PQCs in the quantum classifier. The top row is from [GBC+18]
and the remaining rows are from this work. The heading Np indicates number of
parameters in the PQC and n is the number of qubits in the classifier. The
accuracy is the overall performance using a train-test ratio of 80% on classes 0
and 2. (See [Coy20] for full implementation details).
Fig. 5.8 illustrates the specific decompositions for the single and two qubit
classifiers we utilise for the numerical results in this section. For the matrix rep-
resentation of the circuit shown in Fig. 5.8(a), see Eq. (5.23).
Figure 5.8: Circuit diagrams for the PQC anstze we use to obtain numerical results. (a)
Ansatz for the single qubit classifier. Note that any element of U(2) can be represented by this
ansatz [NC10]. (b) Ansatz for the two qubit classifier, with 12 parameters, {k}12k=1. The first
6 parameters, 1, . . . ,6, are contained in the first two single qubit unitaries, and 10, . . . ,12
are the parameters of the final single qubit gate. The parameters of the intermediate rotations
are defined by {1,2,3} := {27+,28,29}. This decomposition can realise any two qubit
unitary [VD04] up to global phase with the addition of a single qubit rotation on the bottom
qubit at the end of the circuit. We omit this rotation since we only measure the first qubit for
classification. As such, we reduce the number of trainable parameters () from 15 to 12.
As can be observed, we are able to achieve 100% accuracy using the amplitude
and dense angle encoding. For the SDAE, the accuracy drops. Because the SDAE
performs worse than other encodings, this implementation again highlights the
importance of encoding on learnability. Additionally, the fact that we can use
two qubits instead of four highlights the importance of encodings from a resource
perspective. Specifically, NISQ applications with fewer qubits are less error prone
due to fewer two-qubit gates, less crosstalk between qubits, and reduced readout
5.4. Numerical results 113
errors. The reduction in the number of qubits here due to data encoding parallels,
e.g., the reduction in the number of qubits in quantum chemistry applications due
to qubit tapering [BGMT17]. For QML, such a reduction is not always beneficial
as the encoding may require a significantly higher depth. For this implementation,
however, the dense angle encoding has the same depth as the angle encoding, so
the reduction in number of qubits is meaningful.
5.4.2 Robust sets for partially robust encodings
In Sec. 5.3.3, we proved conditions under which an encoding is robust to a given
error channel. Typically in practice, encodings may not completely satisfy such
robustness criteria, but will exhibit partial robustness  i.e., some number of
training points will be robust, but not all. In this section, we characterise such
robust sets for different partially robust encodings. We emphasise two points that
(i) the number of robust points is different for different encodings, and (ii) the
location of robust points is different for different encodings.
To illustrate the first point, we consider amplitude damping noise  which
has robustness condition Eq. (5.39)  for two different encodings: the dense
angle encoding and the amplitude encoding. For each, we use a dataset which
consists of 500 points in the unit square separated by a vertical decision boundary
at x1 = 0.5.
0.0 0.2 0.4 0.6 0.8 1.0
0.0 0.2 0.4 0.6 0.8 1.0
0.0 0.2 0.4 0.6 0.8 1.0
Figure 5.9: Partial robustness for the dense angle encoding. The dataset consists of 500
points in the unit square separated by a vertical decision boundary, and we use a train-test split of
80%. Red crosses/green circles indicate decision boundary location. Orange circles/blue crosses
indicate points labelled as each class. Misclassified points have opposite colour border. Panel (a)
shows the classifier test accuracy after optimizing the unitary without noise. Panel (b) shows the
reduced accuracy after amplitude damping noise of strength p = 0.4 is added. The robust set is
at the far left and far right of the unit square, explicitly shown in Panel (c). Here, blue circles
indicate the robust set and black crosses indicate its complement.
The results for the dense angle encoding are shown in Fig. 5.9. Without noise,
the classifier is able to reach an accuracy of  99% on the training set. When
the amplitude damping channel with strength p = 0.2 is added, the test accuracy
reduces to  78%. This encoding is thus partially robust, and the set of robust
points is shown explicitly in Fig. 5.9(c).
The results for the amplitude encoding are shown in Fig. 5.10. Here, the
classifier is only able to reach  82% test accuracy without noise. When the same
amplitude damping channel with strength p=0.4 is added, the test accuracy drops
114 5. Robust data encodings for quantum classifiers
0.0 0.2 0.4 0.6 0.8 1.0
0.0 0.2 0.4 0.6 0.8 1.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
0.0 0.2 0.4 0.6 0.8 1.0
Figure 5.10: Partial robustness for the amplitude encoding. The dataset consists of 500 points
in the unit square separated by a vertical decision boundary, and we use a train-test split of 80%.
Panel (a) shows classifier test accuracy after optimizing the unitary without noise. Panel (b) shows
the reduced accuracy after adding amplitude damping noise with strength p = 0.4. The robust
set is shown explicitly in Panel (c) where a blue circle indicates a robust point and a black cross
indicates a misclassified point. Panel (d) is the same as (b) but with decreased strength p = 0.2
of the amplitude damping channel. Test accuracy reduces from 81% to 60% in this case. Panel
(e) shows the robust set for (d).
to  43%. We consider also the effect of amplitude damping noise with strength
p = 0.2 in Fig. 5.10, for which the classifier achieves test accuracy  61%. The
robust set for both channels is also shown in Fig. 5.10.
5.4.3 An encoding learning algorithm
Given the emphasis that we have placed on the importance of finding suitable en-
codings, but the apparent difficulty in doing so, we introduce an encoding learning
algorithm to try and search for good encodings. The goal is crudely illustrated
in Fig. 5.11. As mentioned above, [LSI+20] trains over hyperparameters using
the re-uploading structure of [PSCLGFL20] to increase learnability. Here, the
encoding learning algorithm adapts to noise to increase robustness. We note the
distinction that in our implementations we train the unitary in a noiseless envi-
ronment and do not alter its parameters during the encoding learning algorithm.
By training the encoding we change E and hence also the model family. This
is actually desirable in this case since, as we have demonstrated, E directly im-
pacts the robustness of the model family. This could be incorporated alongside
training the unitary itself to deal with coherent noise, for example as seen in
[OBK+16, SKCC20].
The encoding learning algorithm is similar to the Rotoselect11 algorithm which
11We implement a similar idea in Chapter 7 based on quantum structure learning to search for
5.4. Numerical results 115
Figure 5.11: Cartoon illustration of the encoding learning algorithm with a single qubit
classifier. In (a), a preset encoding with no knowledge of the noise misclassifies a large number of
points. In (b), the encoding learning algorithm detects misclassifications and tries to adjust points
to achieve more robustness, attempting to encode into the robust set for the channel.
is used to learn circuit structure [OGB21]. For each function pair (fj ,gj) from
a discrete set of parametrised functions {fk(k),gk(k)}Kk=1 we train the unitary
U() to minimise the cost while keeping the encoding (hyper)parameters j fixed.
Next, we add a noise channel E which causes some points to be misclassified.
Now, we optimise the encoding parameters j in the noisy environment. For this
optimisation, the same cost function is used, and the goal is to further decrease
the cost (and hence increase the set of robust points) by varying the encoding
hyperparameters. Algorithm 1 contains pseudocode for the procedure used.
We test the algorithm on linearly separable and non-linearly separable datasets
in Fig. 5.12. In particular, we use three different encodings on three datasets. The
encodings used are the dense angle encoding, superdense angle encoding, and a
specific instance of the generalised amplitude encoding from Definition 29 given
|x :=
1+ x22
||x ||2
x1|0+
1 x21
||x ||2
x2|1
Here,  is a free parameter which is trained over.
Using these encodings and the datasets given at the beginning of this section,
we study performance for the noiseless case, noisy case, and the effect of the
encoding learning algorithm. We observe that the algorithm is not only capable
of recovering the noiseless classification accuracy achieved, but is actually able to
outperform it in some cases, as can be seen in Fig. 5.12.
good quantum cloning circuits.
116 5. Robust data encodings for quantum classifiers
Algorithm 1: Quantum encoding learning algorithm (QELA)
Input : Noise parameters, p, parametrised quantum circuit, U(), M
Labelled data examples (x i ,yi)
i=1, encoding set {fk ,gk}
k=1, cost
function C.
Result: Optimised encoding for noise and dataset.
1 Initialise encoding, (f ,g){fk ,gk}Kk=1 and parameters,
{j} (0,2]j j heuristically or at random. Initialise C =M;
2 for j=1. . . K do
3 Select subset of data: (x i ,yi)
i=1 (x i ,yi)
4 Encode each sample using encoding choice, (fj ,gj): prepare
{,jx i }
5  argminCD(,j);
6 Add noise with parameters p: {
x }Di=1{Ep(
,j
x )}Di=1;
7 {j } argminj CD(
,j);
8 Cj  CD(
,j );
9 if Cj  C
 then
10 C Cj ;
11 (f ,g) fj(j ),gj(
j ) ;
12 end
13 end
output: C,, f ,g
Finally, we consider the discussion in Sec. 5.3.5 the tradeoff between learn-
ability (expressive power) and robustness. We make this quantitative and explicit
in Fig. 5.13 by plotting accuracy (percent learned correctly) and robustness against
hyperparameters  and  in a generalised dense angle encoding:
|x= cos(x1)|0+e ix2 sin(x1)|1 (5.69)
More specifically, in Fig. 5.13, we illustrate how the noise affects the hyperparam-
eters,  and , which give maximal classification accuracy in both the noiseless
and noisy environments, and also those which give maximal robustness (in the
sense of Definition 33). Fig. 5.13(a), shows the percentage misclassified in the
noiseless environment, where red indicates the lowest accuracy on the test set,
and blue indicates the highest accuracy. We then repeat this in Fig. 5.13(b)
and Fig. 5.13(c) to find the parameters which maximise accuracy in the presence
of noise, and the maximise robustness. As expected, for the amplitude damping
channel, the best parameters (with noise) are closer to the fixed point of the
channel (i.e.  0 implies encoding in the |0 state), thereby demonstrating the
tradeoff between learnability and robustness.
Table 5.2 provides the best parameters found in the procedure, corresponding
to Fig. 5.13. Each set of parameters (each row, measured in radians) performs
optimally in one of three areas. The first is the noiseless environment, in which
a   2.9 parameter performs optimally. The second is the amplitude-damped
5.4. Numerical results 117
Ideal
Untrained Encoding + Noise
Trained Encoding + Noise
Figure 5.12: Minimum cost achieved (vertical axis) from applying the encoding learning
algorithm to three example datasets (each plot) using three different encodings (horizontal
axis). The blue crosses [ ] show the minimum cost achieved when training over only unitary
parameters without any noise present (ideal). The green triangles [ ] show the same case with
the addition of amplitude damping noise of strength p=0.3. The orange circles [ ] show minimum
cost after applying the encoding learning algorithm. The dense angle encoding (DAE) is seen to
perform well on all datasets and is capable of adapting well to noise, even outperforming the ideal
case without noise and fixed encoding. The superdense angle encoding (SDAE) does not perform
well on any shown dataset since the generated decision boundary is highly nonlinear and cannot
correctly classify more than half the dataset. The generalised amplitude encoding (GAE) performs
well on the diagonal boundary, since it generates a suitable decision boundary. Each datapoint
shows the mean and standard deviation of the costs after ten independent runs of the learning
procedure.
environment, in which   1.6 achieves the best accuracy, and finally,  = 0 is
the most robust point to encode in, for the whole dataset. For each of these
parameter sets, we also test them in the other scenarios, for example, the best
parameters found in the noisy environment ([,] = [1.6,3.9]) have a higher -
robustness (81%) than those in the noiseless environment (70%), since these
parameters force points to be encoded closer to the |0 state, i.e., the fixed point
of the channel in question.
5.4.4 Fidelity bounds on partial robustness
As a final numerical implementation, we compute the bounds on partial robust-
ness proved in Sec. 5.3.6 for several different encodings and error channels. The
implementation we consider is the previously-discussed Iris dataset classification
problem using two qubits. The results are shown in Fig. 5.14. In this Figure,
each plot corresponds to a different error channel with strength varied across the
horizontal axis. Each curve in the top row corresponds to the fidelity of noisy and
noiseless states using different encodings. Each curve in the bottom row shows
the lower bounds on partial robustness proved in Sec. 5.3.6.
118 5. Robust data encodings for quantum classifiers
0 1 2 3 4 5 6
[ , ] [ , ]
Figure 5.13: Learnability vs. robustness on the vertical dataset using the parametrised
dense angle encoding. The horizontal and vertical axes show the encoding hyperparameters 
and , respectively. Panels (a) and (b) show the classifier accuracy while Panel (c) shows the
proportion of robust points. Panel (a) shows accuracy without noise as a function of encoding
parameters. Panel (b) shows accuracy with the addition of an amplitude damping channel of
strength p = 0.3. Panel (c) shows -robustness for different parameter values. As expected, the
robust set is largest when all points are encoded into the zero state, i.e.  = 0. This leads to all
points labelled 1 being misclassified, with a resulting accuracy of approximately 50%. The orange
[ ] filled line indicates optimal  parameters in each panel. From (a) to (b), the  parameters
corresponding to highest accuracy are shifted towards the robust points (i.e., towards  = 0) in
(c). The optimal parameters in each case are given in Table 5.2
Parameters Accuracy Accuracy -Robustness
w/o noise w/ noise
[,] = [2.9,2.9] 100% 84% 70%
[,] = [1.6,3.9] 49% 100% 81%
[,] = [0,0] 43% 43% 100%
Table 5.2: Optimal parameters [,] for dense angle encoding (with param-
eters in U() trained in noiseless environment) in (a) noiseless environment,
(b) noisy environment (i.e. amplitude damping channel is added) and (c) for
maximal robustness. Optimal parameters in noisy environment are closer to fixed
point of amplitude damping channel (|0, i.e.   0) and give a higher value of
-robustness.
As can be seen in the bottom row of Fig. 5.14, upper bounds on partial ro-
bustness are different for different encodings, particularly at small noise values.
(Recall that a trivial upper bound on the size of partial robustness is one so that
curves at large channel strengths above one are mostly uninformative.)
For such low values of noise, they give us some information about the maxi-
mum cost function deviation we can expect. Based on the average fidelity over
the datasets, in Figs. (5.14(a) - 5.14(d)) all three encodings behave qualitatively
the same. However, the cost function error for the three encodings is significantly
different, especially for bit flip and dephasing errors, Figures (5.14(f) - 5.14(g)).
As expected, a depolarising channel causes no misclassification, as seen in 5.14(h),
despite the decrease in fidelity of the states. Recall that the superdense angle en-
coding was not able to achieve perfect classification accuracy on the Iris dataset,
so under amplitude damping noise, e.g. the cost function error can only decrease
5.5. Discussion and conclusions 119
0.0 0.2 0.4
0.0 0.2 0.4
0.0 0.1 0.2 0.3
0.0 0.2 0.4
0.0 0.2 0.4
0.0 0.2 0.4
0.0 0.1 0.2 0.3
0.0 0.1 0.2 0.3
Figure 5.14: Fidelity bounds on robustness. Top row: Fidelity between noisy and noiseless states
for different encodings on the Iris dataset. Comparing average fidelity over every encoded state in
dataset, versus fidelity of noisy and noiseless encoded mixed states. From left to right, the noise
models are bit-flip noise, amplitude damping noise, dephasing noise, and global depolarising noise,
with strengths varied across the horizontal axis. Bottom row: Upper bounds on partial robustness
in terms of fidelity using the bounds Eq. (A.6) and Eq. (5.66). For each plot (in both rows), three
different data encodings are considered  curves corresponds to three product qubit encodings:
dense angle encoding, amplitude encoding, and superdense angle.
by about 25% ( 77% 50%). We can also observe that the dense angle en-
coding is less susceptible to bit flip and phase errors than the amplitude encoding
in Fig. 5.14(f) and Fig. 5.14(g).
5.5 Discussion and conclusions
As our first contribution for a NISQ application in machine learning, in this chapter
we focused on the binary quantum classifier, a special case of a variational quantum
algorithm used in supervised learning. The basic ingredients of the model are
common to those in recent literature, and we began by examining different data
encoding strategies for classical data in the model. Our novel contributions in this
area was an investigation of the model in the presence of quantum noise models,
in particular in the search for robust data encoding strategies. To reiterate the
statement from the Introduction of this chapter, we found that it is indeed possible
to design quantum classifiers which are somewhat robust to noise, but perhaps
at the expense of the learnability of the model (pathological design). Specifically,
we provided some intuition for how to best design such models and backed these
intuitions up by rigorous proofs and supplementary numerical results, at least for
special cases and simple noise models.
One of the major open questions raised by this work is to what extent will
such methods scale to larger problem sizes. Certainly in theory, something must
be possible (due to the existence result we proved), but a necessary condition
for robustness is elusive. Perhaps ingredients and theory from quantum error
correction will prove useful - we know at least by bringing the entire theory of
120 5. Robust data encodings for quantum classifiers
QEC we will achieve full robustness, but what are the minimum elements we
require to at least gain some (but still remain NISQy). For example, one could
study encodings into decoherence-free subspaces [LCW98, Lid14]. An alternative
question which may be of independent is to extend the notion of a robust point to
a generalised fixed point of a channel, i.e. points which satisfy f (E()) = f ()
for some function f . (When f = y , the generalised fixed point is a robust
point, but other arbitrary functions f could be considered.) In a more empirical
research direction, perhaps using machine learning itself to find robust encodings
will be fruitful (as initiated with our QELA), or even just extending this work
to larger datasets (e.g., the MNIST dataset [LBBH98]) and more sophisticated
model design (for example by incorporating data re-uploading [PSCLGFL20] -
more about this in Sec. 5.5.1).
In conclusion, this chapter evidences that application-specific robustness (for
(variational) quantum algorithms in general) is a promising and interesting direc-
tion of study.
5.5.1 Subsequent work
In order to properly insert our work from this chapter in its proper context in the
literature, we believe it is important to not only discuss the work which came
before, but also subsequent work which came after. To this end, we briefly sum-
marise some we find interesting, which may be merged with our work, and the
other future research directions given the previous section. First, and perhaps
most importantly, [SSM21, GVT20] highlighted explicitly the effect of the data
encoding on the function families representable by quantum models such as ours.
Interestingly, the authors found that by using data-reuploading [PSCLGFL20] the
model can be expressed as a Fourier series, whose frequencies and coefficients are
given by the data uploading and parameterised unitaries respectively. This rein-
forces our findings regards the importance of clever data encodings for the purpose
of robustness. Secondly, on the theoretical side, there has been a flurry of work
studying such quantum models regarding their generalisation ability. For exam-
ple, metrics such as the effective dimension [ASZ+21], the capacity [WM20],
the pseudo-dimension [CD20] among others [BKL+21b, BKL+21c, BKL+21d,
GvVD21, HBM+21] have proven useful in deriving bounds on the generalisation
of such models, which are key in further understanding them. It would be partic-
ularly interesting to study the relationship between these concepts, and the ideas
in this chapter. For example, one may examine the effect on these generalisation
bounds when one is restricted to a specific set of robust points for a given noise
channel.
Secondly, there have also been several works following ours considering the link
between noise and defences against adversarial attacks in a quantum setting. For
example, [LW20] demonstrated how, in high dimensions, it is easy for an adversary
to perturb a quantum state such that it is misclassified. The work of [DHL+21]
showed how the simple quantum noise models studied in this chapter could lead to
natural versions of differential privacy. Finally, [GFY21] and [WLL+21] derive ro-
5.5. Discussion and conclusions 121
bustness bounds to complement our own based on the distance measures discussed
in Sec. 2.1.7.2. The latter also demonstrated a link between robust classification
and quantum hypothesis testing, revealing an interesting avenue with which to
explore in future work.
Finally, one may consider the more general influence of noise on NISQ al-
gorithms. As we touched on in Sec. 4.1.3, noise may exacerbate the effect of
barren plateaus ([WFC+21]), or enable the efficient classical simulation of such
quantum models [FGP20]. Alternatively, quantum error mitigation [ECBY21] may
also be considered as a complementary technique to robustness, which is a rapidly
expanding area of study.
Generative modelling with quantum
circuit Born machines
6.1 Introduction
What an AI cannot create, it cannot understand
 Ian Goodfellow, paraphrased from Richard Feynman
In the previous chapter, we discussed an application of VQAs in machine learn-
ing using a quantum classifier. These models are relatively simple, in the sense
that supervised learning (classically) is quite well explored both from a theoretical
and from an implementation point of view. In contrast, unsupervised learning
is significantly more challenging and this chapter is dedicated to the generative
modelling unsupervised learning problem (discussed in Sec. 3.3.2), and the model
for this task which we introduced in Sec. 4.1.4, called the quantum circuit Born
machine (QCBM).
We begin by recalling most vanilla form of the quantum circuit Born ma-
chine (QCBM) from Sec. 4.1.4. Specifically, this involved the preparation of a
parametrised state, |(), via a quantum circuit, U(), acting on a fixed initial
state |0n. The generative model which this produces is the distribution given
by measuring all qubits in the computational basis:
p(x) := |x |U()|0n|2 (6.1)
As a reminder, the task of generative modelling is to learn a representation of some
data distribution, q(y), given a set of samples, {y j}Mj=1 from q. In practice, this
means fitting the parameters of the generative model (the QCBM), , such that
p(x) q(x) x . This is usually achieved via the minimisation of some distance,
d, between p and q (so the problem is solved when d(p,q) 0, recall the
requirement of faithfulness from Sec. 4.1.1).
Given this problem statement, the goal of this chapter is threefold. We in-
vestigate the QCBM in some detail and in doing so we provide the following
contributions:
124 6. Generative modelling with quantum circuit Born machines
 A discussion of quantum advantage using the QCBM. In simple terms, the
ability to solve a generative modelling problem better than any classical
model could. Slightly more precisely, given a distribution, q, output p
which is closer - with respect to some distance measure - than any classical
generative model could achieve.
 Improved differentiable training methods for the QCBM. Informally, better
methods to fit the model parameters  so that p is closer to q.
 A numerical comparison study of the QCBM versus a comparable classical
model (specifically the restricted Boltzmann machine (RBM) discussed in
Sec. 3.3.2) in a close-to-real-world use case in finance.
Let us begin our adventure with a discussion of quantum advantage in gener-
ative modelling, using a QCBM.
6.2 Quantum advantage in generative modelling
Before diving into quantum advantage may arise in generative modelling, let us
first motivate why such a thing might be feasible in the first place.
The reason is because the a core ingredient in generative models is sampling.
It turns out, due to the probabilistic nature of quantum mechanics, sampling from
a quantum distribution is a very straightforward thing to do (as we discussed in
Sec. 4.1.4). This observation prompted the development of quantum compu-
tational supremacy (QCS)1, which refers to a specific task: the generation of
samples from a quantum state, which could not be generated by any classical al-
gorithm, in a reasonable time. The reason for defining such an abstract problem,
is primarily to illustrate the power of NISQ computers. NISQ computers cannot
perform complex algorithms due to noise limitations (see Sec. 2.1.5), but what
they can do is prepare a quantum state via a short-depth quantum circuit, U,
and measure all qubits in the computational basis. This then defines a natural
sampling problem. The question to demonstrate QCS is then twofold:
 Ensure U is simple enough to implement physically on NISQ devices.
 Prove rigorously that the sampling problem cannot be solved classically.
To achieve the former, a number of quantum circuit families have been proposed,
including instantaneous quantum polynomial time (IQP) circuits, BosonSampling,
and random circuit sampling (RCS). All of these proposals define sub-universal
complexity classes in that they cannot implement an arbitrary quantum computa-
tion in BQP (see Sec. 2.1.2), but yet they are still complex enough to be difficult
to simulate classically (or so we believe). Excluding a simple preparation and
1Also called quantum superiority, or more commonly quantum advantage to avoid any contro-
versies.
6.2. Quantum advantage in generative modelling 125
measurement operation, IQP contains only gates which are diagonal in the com-
putational (Pauli-Z) basis, BosonSampling involves interacting photonic modes in
an interferometer without feedforward, and RCS involves implementing random
layers of hardware native locally connected gates.
The next ingredient is to prove such sampling tasks cannot be achieved clas-
sically2. The most common route to do so, is to assume it can be simulated
classically, and then derive an unlikely consequence in complexity theory, i.e. the
collapse of the polynomial hierarchy ([Sto76])3. Such a collapse occurring is a
generalisation of P being equal to NP, which is widely believed to be not the case.
6.2.1 The Ising Born machine
In the work of a previous dissertation [Coy18], we introduced a specification of a
QCBM based on a generalisation of IQP circuits which we dubbed the quantum
circuit Ising Born machine (QCIBM). This model was a generalisation because we
added the possibility of measurement in alternative bases (standard IQP circuits
only measures in the Hadamard basis). Specifically, the circuits considered for the
QCIBM had the following structure (we reintroduce it here from [Coy18] since we
use this same model in our numerical results later in this thesis):
|0 H
Uz()
U1f (1,1,1) x1
|0 H U2f (2,2,2) x2
        
|0 H Unf (n,n,n) xn
(6.2)
where: xi  {0,1}; the unitaries are defined by Eq. (6.3) and Eq. (6.4); Sj
indicates the subset of qubits on which each operator, j , is applied; and  := {j}
are the trainable parameters.
Uz() :=
j ,Sj
ij 
 (6.3)
Uf (,,) := exp
kXk +kYk +kZk
(6.4)
The model was dubbed Ising since restricting to the case |Sj | 2 (since only sin-
gle and two-qubit gates are required for universal quantum computation) the term
2By achieved here, we mean cannot by simulated in polynomial time - if classical simulators
are allowed to take exponential time then we know then would be able to simulate quantum
computers by just keeping track of the exponentially large wavefunction.
3An introduction to (quantum) complexity theory is not necessary for reading this theses, and
so for a comprehensive overview of this topic see [NC10] as well as the complexity zoo for a list
of complexity classes.
https://complexityzoo.net/Complexity_Zoo
126 6. Generative modelling with quantum circuit Born machines
in the exponential of Eq. (6.3) becomes exactly the Ising Hamiltonian discussed
in Eq. (4.5).
Furthermore, we also showed in [Coy18] how the shallowest depth (p = 1)
version of the QAOA algorithm (see Eq. (4.10) in Sec. 4.1.2) could also be
recovered by a specific setting of the final measurement angles in Eq. (6.4).
By inheriting the hardness results of standalone IQP and p = 1 QAOA cir-
cuits [BJS11, FM17, FH16], we could show that the QCIBM remained in the same
complexity class as the the standalone versions during training of the generative
model, and then also sampling from a QCIBM could not be simulated classically
(up to multiplicative error).
However, while hardness-of-simulating the generative model is at least an im-
portant step on the road to true quantum advantage4, it is not the end of the
story. One would also like to make a case for the hardness of learning, i.e. that
the QCIBM (or indeed any quantum generative model) could learn a distribution
which could not be learned classically. Such a quantum learning advantage (es-
pecially in a near-term NISQy regime) would be concrete proof of the usefulness
of quantum machine learning.
6.2.2 The supremacy of quantum learning
The contribution of this section is to formalise this question, using concepts from
learning theory, and provide an initial attempt at showing such an advantage, by
focusing on the hard-to-simulate quantum supremacy circuits discussed above.
We call this quantum learning supremacy (QLS) to parallel the corresponding
QCS which deals exclusively with the underlying sampling problem. Informally
a generative quantum machine learning algorithm is said to have demonstrated
QLS, if it is possible for it to efficiently learn a representation of a distribution for
which there does not exist a classical learning algorithm achieving the same end.
More specifically, the quantum device has the ability to produce samples according
to a distribution that is close in total variation distance (Eq. (2.48))5 to some
target distribution, using a polynomial number of samples from the target.
To begin, we must understand the inputs and outputs to learning algorithm,
for which we have a certain amount of freedom. The inputs are samples, either
classical vectors, or quantum states encoding a superposition of such bitstring
states, i.e. qsamples [SP18] (a special case of the amplitude encoding from
Definition 22):
4It surely is at least a necessary condition; if it was possible to classically simulate the generative
quantum model at each point during training - one could just use the classical simulation instead
of having to actually build a quantum computer (which would be significantly more expensive,
from a practical sense).
5The reason for this choice will become apparent shortly.
6.2. Quantum advantage in generative modelling 127
Definition 38 (Qsample).
Given a discrete probability distribution, [p(x1),p(x2), . . . ,p(x2
)]T , x i 
{0,1}n, a qsample encoding of this distribution maps p(x) Eqsamp given
Eqsamp : {p(x i)}i  |p(x) :=
p(x i)|x i (6.5)
The state in Eq. (6.5) is clearly a generalisation of the classical distribution,
since we can measure all qubits in the computational basis, and we will recover
the sample x i with the correct probability,
p(x i)
= p(x i). Giving the learning
algorithm to classical samples, x i or a quantum superposition over all possible
samples (i.e. a qsample, Eq. (6.5)) can lead to advantages in supervised learning
theory [SG04], so it is not unreasonable to expect an analogue is possible for
generative modelling.
While the sample complexity of quantum versus classical learning has been
studied in the supervised learning sense (see the discussions in Sec. 3.6 and
Sec. 4.3), the same cannot be said about unsupervised learning. Fortunately,
we do have a framework in the classical setting for studying distribution learning
problems, so-called distribution learning theory, introduced by [KMR+94]. Here
we bring this forward to the quantum setting.
Unlike the supervised setting, there are more than one type of learner (or more
than two if one also counts the quantum generalisation). We can define distribu-
tion learners which much learn solely from samples from the distribution (learning
with Generators), and those which can learn from the probabilities themselves
(learning with Evaluators). For simplicity, we assume the distributions classes in
question (as in [KMR+94]), are discrete over binary vectors of length n, denoted
Dn. Let us now define Generators and Evaluators:
Definition 39 (Generator [KMR+94]).
A class of distributions, Dn, has efficient Generators, GEND, if for every
distribution D  Dn, GEND produces samples in {0,1}n according to the
exact distribution D, using polynomial resources. The generator may take
a string of uniformly random bits, of size polynomial in n, r(n), as input.
Notice this definition allows, for example, for the Generator to be either a
classical circuit, or a quantum circuit, with polynomially many gates. Further, in
the definition of a classical Generator [KMR+94] a string of uniformly random bits
is taken as input, and then transformed into the randomness of D. However, a
quantum Generator would be able to produce its own randomness and so no such
input is necessary. In this case the algorithm could ignore the input string r(n).
128 6. Generative modelling with quantum circuit Born machines
Definition 40 (Evaluator [KMR+94]).
A class of distributions, Dn has efficient Evaluators, EVALD, if for every
distribution D  Dn, EVALD produces the weight of an input x in {0,1}n
under the exact distribution D, i.e. the probability of x according to D.
The Evaluator is efficient if it uses polynomial resources.
The distinction between EVAL and GEN is important and interesting since the
output probabilities of IQP circuits are #P-Hard to compute and also hard to
sample from by classical means, as discussed above [BJS11], yet the distributions
they produce can be sampled from efficiently by a quantum computer. This draws
parallels to examples in [KMR+94] where certain classes of distributions are shown
not to be learnable efficiently with an Evaluator, but they are learnable with a
Generator.
For our purposes, the following definitions of learnable will be used. In contrast
to [KMR+94], which was concerned with defining a good generator to be one
which achieves closeness relative to the KL divergence (Eq. (2.49)), we wish to
expand this to general cost functions, d . The reason for this will become apparent
later on, when dive into quantum sampling hardness results (high typically strive
for closeness in TV) in more detail, and when we introduce our new training
methods for the QCIBM .
Definition 41 ((d,)-Generator).
For a cost function, d, let D  Dn. Let GEND be a Generator for a distri-
bution D. We say GEN is a (d,)-Generator for D if d(D,D) .
A similar notion of an -good Evaluator could be defined.
Definition 42 ((d,,C)-Learnable).
For a metric d,  > 0, and complexity class Ca, a class of distributions Dn
is called (d,,C)-learnable (with a Generator) if there exists an algorithm
A  C, called a learning algorithm for Dn, which given 0 <  < 1 as input,
and given access to GEND for any distribution D  Dn, outputs GEND, a
(d,)-Generator for D, with high probability:
 1  (6.6)
A should run in time poly(1/,1/,n).
aThe introduction of a complexity class here should not be considered rigorous, but is
used to allow the algorithm to be either quantum or classical in nature.
Finally, we define what it would mean for a quantum algorithm to be superior
to any classical algorithm for the problem of distribution learning, before moving
onto our initial attempt to achieve such a thing:
6.2. Quantum advantage in generative modelling 129
Definition 43 (Quantum learning supremacy (QLS)).
An algorithm A  BQP is said to have demonstrated the supremacy of
quantum learning over classical learning if there exists a class of distributions
Dn for which there exists d, such that Dn is (d,,BQP)-Learnable, but Dn
is not (d,,BPP)-Learnable.
6.2.3 Quantum learning supremacy via quantum computational
supremacy
Let us now move onto an initial attempt to achieve QLS. Since this question of
distribution learning advantage is motivated by the classical hardness of sampling
from the distributions generated by a quantum circuit, a natural distribution class,
Dn to learn would be exactly these quantum supremacy distributions. We focus
on IQP circuits as a concrete example, although a similar logic could in principle
be applied to alternative QCS candidates such as BosonSampling or RCS. For
QCS, there are a number of further subtleties which must be address including:
worst versus average case hardness of classical simulation, and the notions of the
error of such simulations, which relate to the probability metrics introduced in
Sec. 2.1.7.1. For our present discussion, the latter is more relevant, although for
discussions of the former and a comparison between the different QCS proposals,
see [HM17, BFNV19].
The most common error models used in proofs of QCS are multiplicative and
additive (or total variation) error:
Definition 44 (Multiplicative error).
A circuit family is weakly simulatable within multiplicative (relative) error, if
there exists a classical probabilistic algorithm, Q, which produces samples,
x , according to the distribution, q(x), in time which is polynomial in the
input size, which differs from the ideal quantum distribution, p(x), by a
multiplicative constant, c > 1:
p(x) q(x) cp(x) x (6.7)
It would be desirable to have a quantum sampler which could achieve the bound
of Eq. (6.7), but this is not believed to be experimentally achievable, i.e. it is not
believed that a physical quantum device could achieve such a multiplicative error
bound on its probabilities, relative to its ideal functionality (i.e. replacing q in Eq.
(6.7) by the output distribution of a noisy quantum device). That is why much
effort has been put into trying to find systems for which QCS could be provably
demonstrated according to the total variation distance error condition, Eq. (6.8),
which is easier to achieve on near term quantum devices.
130 6. Generative modelling with quantum circuit Born machines
Definition 45 (Total variation (TV) error).
A circuit family is weakly simulable within variation distance error, , if
there exists a classical probabilistic algorithm, Q, which produces samples,
x , according to the distribution, q(x), in polynomial time, such that it differs
from the ideal quantum distribution, p(x) in total variation distance, :
TV(p,q) :=
|p(x)q(x)|  (6.8)
It has been proven that weakly simulating IQP circuits to a fixed TV error is
classically hard. Specifically, if a classical algorithm could sample from a distribu-
tion, q, for which TV(p,q)  1/384, where p is the output distribution from an
IQP circuit, then the polynomial hierarchy would collapse to the third level (see
[BMS16] for a formal statement of the relevant theorem), assuming the hardness
of computing the Ising partition function.
Now, where does QLS fit into this? Well, let us revisit the relationship between
these definitions, and those for distribution learning in Sec. 6.2.2. Firstly, note that
there is a direct and obvious connection to the strength of classical simulators
of quantum circuits.
The two primary types of classical simulation are strong and weak6 simulators:
Definition 46 (Strong and weak classical simulation [BJS11, FM17]).
A uniformly generated quantum circuit, C, from a family of circuits, with in-
put size n, is weakly simulatable if, given a classical description of the circuit,
a classical algorithm can produce samples, x , from the output distribution,
p(x), in poly(n) time.
On the other hand, a strong simulator of the family would be able to com-
pute the output probabilities, p(x), and also all the marginal distributions
over any arbitrary subset of the outputs. Both of these apply with some
notion of error, .
As mentioned in [BJS11], strong simulation is a harder task than weak simula-
tion, and it is this weak simulatability which we want to rule out as being classically
hard, since it better captures the process of sampling. The suitable notion of er-
ror, , for strong simulation would be the precision to which the probabilities can
be computed. Recalling the definitions in Sec. 6.2.2, it is clear that an Evaluator
for a quantum circuit would be a strong simulator of it, and a Generator would be
a weak simulator.
Next, let us take the learning metric in Definition 43, to be the total variation
distance, d = TV, and let Dn be the family of IQP distributions. Imagine, we
were able to concoct a learning algorithm, A (for example the QCIBM), to output
a generator for a distribution, DQCIBM, (GENDQCIBM) given poly many samples
(classical or quantum) from an IQP distribution, DIQP. Furthermore, assume we
6We already used this terminology in Definition 44 and Definition 45.
6.2. Quantum advantage in generative modelling 131
have TV(DQCIBM,DIQP) .
Now, assuming DQCIBM distributions are classically easy to emulate
7 within a
fixed TV error, , meaning there exists a classical probabilistic algorithm, C, could
output a distribution, DC such that:
TV(DC,DQCIBM)  (6.9)
Then, by a simple triangle inequality argument, we have:
TV(DIQP,DC) =
|DIQP(x)DC(x)| (6.10)
|DIQP(x)DQCIBM(x)+DQCIBM(x)DC(x)| (6.11)
|DIQP(x)DQCIBM(x)|+
|DQCIBM(x)DC(x)| (6.12)
 + =: =
(6.13)
Therefore, if we are able to make + smaller than 1/384, we arrive at a con-
tradiction to the result of [BMS16], and as such, DQCIBM must also be difficult
to simulate within a small fixed TV distance. As a result, something classically
infeasible has been achieved8.
This above discussion covers the classical hardness of the learning task, but
we have yet to discuss the feasibility of the learning problem itself, i.e. outputting
a generator distribution achieving  closeness in TV to an IQP distribution.
Unfortunately, it is here the argument appears to fail. A key ingredient in
proofs of the hardness of classically simulating quantum supremacy distributions
is the property of anti-concentration, which formally means that given a random
instance from a hard circuit family, for example an instance of an IQP circuit,
then it does not become too unlikely that the probability of any fixed outcome, x ,
by measuring the state U|0n, is much smaller than it would be if x was drawn
from the uniform distribution. In other words, anti-concentrated distributions are
exponentially flat:
, > 0,s.t.x  {0,1}n, Pr
pU(x) := |x |U|0n|2 
 , (6.14)
where the unitary is drawn according to a measure  on the unitary group.
Now comes the rub; the work of [HKEG19] proved that verifying such anti-
concentrating distributions, for example IQP9 would require exponentially many
samples from the distribution to be verified. Verification in this sense means
7In other words, proof by contradiction.
8A very related idea was proposed by [RGS+18], where the authors studied the learnability of
classical hard distributions using (classical) variational autoencoders.
9It was also proven for BosonSampling and RCS. For IQP this anti-concentration behaviour has
been explicitly proven, for BosonSampling on the other hand it is only a (believeable) conjecture.
132 6. Generative modelling with quantum circuit Born machines
building a testing algorithm which can distinguish whether the distribution to be
verified is far or close to the ideal distribution in TV.
As a consequence of this no-go result, one can conjecture that also building
an efficient Generator for an anti-concentrating distribution is also not feasible.
This is because one could imagine a verification algorithm to be a subroutine in
any learning algorithm attempting to demonstrate QLS. The key property of QCS
circuit families being hard to simulate, is also what makes them hard to learn.
However, the above discussion assumed efficient classical sample access to the
QCS distribution - it may be possible to bypass the classical no-go theorem given
qsample access.
To conclude, we note that at the time of writing the question of a near-term
quantum advantage in generative modelling is still an open question. However,
since the completion of the works in this chapter, some interesting progress was
made towards this goal. We return to this discussion in the conclusion.
6.3 Training a quantum circuit Born machine
Let us now introduce our second contribution in the subject of quantum generative
modelling. As mentioned in Sec. 6.1 this will be the introduction of new gradi-
ent based training methods for the quantum circuit Born machine. We will use
primarily the QCIBM as a specific instance, but remark that everything presented
in this section applies generally to QCBMs. We already introduced much of the
relevant details for this section in Sec. 2.1.7.1 which described various measures
on the space of probability distributions.
Recall in Sec. 2.1.7.1 we introduced two families of distribution measures,
specifically f -divergences (Eq. (10)), and integral probability metrics (IPMs) (Eq.
(11)). In order to train generative models, these probability measures will serve
as our cost functions, C, as in Sec. 4.1.1. Since quantum circuit Born machines
are just a special case of variational algorithms applied to generative modelling,
we similarly wish for the cost functions we use to have the same well defined
properties as with any VQA (discussed in Sec. 4.1.1), namely faithfulness, effi-
cient computability and operational meaning. We go into significant detail with
these properties in this sections since they turn out to be extremely important for
training generative models.
Let us begin by discussing f -divergences, any why they may not satisfy the
efficient computability criterion. Let us take the KL divergence as a special
case. Rewriting Eq. (2.49) in terms of the model distribution (specifically the
QCIBM output distribution), p, and let us use the notation,  to denote the
data distribution we are trying to fit:
CKL() := KL(,p) :=
(x) log(x)
(x) logp(x) (6.15)
Since the first term (the entropy of ) is constant with respect to p, when
solving argminCKL() it can be ignored so we now solve argmaxXE(,p).
6.3. Training a quantum circuit Born machine 133
However, recall in Sec. 6.2, we discussed how computing the outcome probabilities
of quantum circuits was #P-hard, hence we may require exponential resources to
get a good estimate of p(x) for all x  {0,1}n. Since the expression in Eq.
(6.15) cannot be written as a sampling expression over p
10, we do not expect to
be able to efficiently compute the term XE(,p), and hence we cannot compute
KL(,p). This was realised specifically for the case of QCBMs by [LW18a], where
the corresponding gradients of the L parameters in a QCBM were computed and
found to be11:
(6.16)
where we use the notation k := (1,2, . . . ,k  /2, . . . ,L) to denote the pa-
rameter shift rule in Eq. (4.13). Hence, if we cannot compute the probabilities
p(x) efficiently, we will not be able to compute neither f -divergences, nor their
gradients. A solution to this was realised by [LW18a] (which we alluded to in
Sec. 2.1.7.1), which was to instead use the MMD Eq. (2.54) to efficiently train
QCBMs in a differentiable manner. Since the MMD can be written exclusively
as expectation values over the data distribution and the QCBM distribution it
can be evaluated efficiently (specifically with the quadratic sample complexity of
Eq. (2.56)). The same can be said for its gradients, which were computed by
[LW18a] to be:
CMMD(p,)
((a,x)) 2E
((b,x)) 2E
((a,y))+ 2E
((b,y))
(6.17)
where we define CMMD :=MMD
 from Eq. (2.54).
Both of the cost functions we have discussed so far are faithful, meaning in
this case that C(p,) = 0  p =  (by definition, since they are a divergence,
and a metric between probability distributions respectively12). We have established
how the MMD is an efficient method to train QCBMs (and generative models
generally), whereas the KL divergence may not be. The final quality we have not
10By this we mean that the KL divergence cannot be written in terms of quantities like E
which can be estimated by simply drawing samples from p. Terms of this form can usually be
estimated more efficiently since we only need to sample the high-probability values, x , to get a
good estimate of the expectation value - we do not necessarily need to know the probability at
every x (of which there are exponentially many)
11An important point to note here, we have a slightly different functional form for the gradients
than those presented in [LW18a]. This is because we will be using these gradients to train the
QCIBM in Eq. (6.2), and the gates in this circuit are of the form ei, rather than ei(/2).
These are essentially equivalent at any rate, and only introduce an alternative factor in front of
the gradient term to change its magnitude. We will use this formulation of the gradient for all the
cost functions described here.
12A subtle point for the diligent reader: it may be expected that the faithfulness of the MMD
must have something to do with the kernel function and feature map - indeed this is true, the
MMD is only faithful if the corresponding kernel is universal. We return to a discussion of kernel
universality later in this Thesis.
134 6. Generative modelling with quantum circuit Born machines
touched upon is the operational meaning of these cost functions, which we take
in this sense to be how powerful they are as training mechanisms.
By this, we mean their performance and properties relative to TV13. Firstly,
it is clear that the KL divergence is stronger than TV since the upper bound it
provides via Pinskers inequality Eq. (2.50). Hence, if we were able to efficiently
train a QCBM with the KL divergence to a particular value, we can directly upper
bound the TV from this value - closeness in KL implies closeness in TV.
The same cannot be said unfortunately about the MMD. Specifically, the
MMD provides only a lower bound on TV from [SFG+09]:
TV(p,)
MMD(p,)
(6.18)
if we have that k := supxX n (x ,x) <. For example, taking the Gaussian
kernel Eq. (3.23) (or even the quantum kernel Eq. (4.30)), we have:
G(x ,x) = e
 12 |xx |
= 1 (6.19)
hence k = 1 and the lower bound is immediate. With this in mind (we will circle
back to this operational meaning afterwards), let us move onto the new cost
functions we propose for training a QCBM. These are the Stein discrepancy, and
the Sinkhorn divergence.
6.3.1 Training with the Stein discrepancy
Firstly, lets look at the Stein discrepancy (SD). The SD has become popular for
goodness-of-fit tests [LLJ16], i.e. testing whether samples come from a particular
distribution or not14. This discrepancy is based on Steins method [Ste72], which
is a way to bound distance metrics between probabilities including, for example,
IPMs.
The original formulation of the discrepancy was done in the the continuous
case. However, in the case of a QCBM, we require a discrete version of the
discrepancy (since the output are samples, x  {0,1}n).
To set the scene, let us begin with the continuous version. A key ingredient is
Steins identity (in the case where the sample space is one-dimensional, x X R)
given by:
E [A(x)] = E [s(x)(x)+x(x)] = 0 (6.20)
where s(x) = x log((x)) is the Stein score function of the distribution ,
x f (x) is the gradient of a function, f and A is a so-called Stein operator of
. The functions, , which obey Eq. (6.20), are said to be in the Stein class15
13We take this viewpoint specifically in light of our discussion of QLS in Sec. 6.2, where total
variation distance is the relevant measure.
14In contrast, the MMD is typically used for kernel two-sample tests [GBR+07].
15These functions,  are essentially those which allow the identity to be proven via integration
by parts, i.e. those which are smooth and have appropriate boundary conditions.
6.3. Training a quantum circuit Born machine 135
of the distribution . From Steins identity, one can define a discrepancy between
the two distributions, p,, by the following optimisation problem, [YLRN18]:
dSD(p||) := sup
Ep [A]Ep [Ap]
(6.21)
If p , then dSD(p||)= 0 by Eq. (6.20). Exactly as with the MMD, the power
of the discrepancy in Eq. (6.21), will depend on the choice of the function space,
F . By choosing it to be a RKHS, a kernelised form which is computable in closed
form can be obtained. Also, this form of Eq. (6.21) is very reminiscent of that
of the integral probability metrics, Eq. (11) (and indeed the SD can be written in
such a form where the function family in the IPM is distribution-dependent).
From Eq. (6.21), the problem arises. Due to the gradient term, x , in Eq.
(6.20), the above expressions are only defined for smooth probability densities,
p,, which are supported on continuous domains (e.g. R). Therefore, in order to
make the SD compatible with training a QCBM, we must perform a discretisation
procedure.
Fortunately, this has been addressed by [YLRN18], which adapted the ker-
nelised SD to the discrete domain. This was achieved by introducing a discrete
gradient shift operator. Just as above, let us assume we have n-dimensional
sample vectors, x  X n  Rn (where X is a discrete set). First of all, we shall
need some definitions [YLRN18]:
Definition 47 (Cyclic permutation).
For a set X of finite cardinality, a cyclic permutation  :X X is a bijective
function such that for some ordering x [1],x [2], . . . ,x [|X |] of the elements in
X , = i : x [i ] 7 x [(i+1) mod |X |],i = 1,2, . . . , |X |
Definition 48 (Partial difference operator and difference score function).
Given a cyclic permutation  on X , for any vector, x = [x1, . . . ,xn]T  X n.
For any function f : X n R, denote the (partial) difference operator as:
xi f (x) := f (x) f (ix) i = 1, . . . ,d (6.22)
with f (x) = (x1f (x), . . .xn f (x))
T . Define the (difference) score func-
tion for a positive probability mass function, p(x)> 0 x as:
sp(x) :=
p(x)
, (sp(x))i =
xip(x)
p(ix)
(6.23)
Furthermore, [YLRN18] defines the inverse permutation by
=i : x [i ] 7 x [(i1) mod |X |], and the inverse shift operator by:
xi f (x) := f (x) f (
i x) i = 1, . . . ,n (6.24)
For our purposes, this generalisation is actually not necessary. Since the sample
space for a single qubit is binary, X = {0,1}, the forward and reverse permutations
are identical, so  = .
136 6. Generative modelling with quantum circuit Born machines
With this change to the gradient operator, [YLRN18] defines discrete versions
of the Stein identity, and the kernelised Stein discrepancy which we can now state.
Theorem 15:
For any functiona  : X n Cm, and a probability mass function p on X n,
the discrete Stein identity is given by:
[Ap(x)] = E
sp(x)(x)
T (x)
= 0 (6.25)
where (x) is an nm matrix: ()i j = xij(x), i.e. shifting the i
element of the j th function value.
aWe have implicitly generalised the input space to be a vector space as in [YLRN18], but
we have also taken the possibility that the feature maps may be complex valued. This allows
us the freedom to use quantum kernels for  in place of purely classical ones. This is also
possible due to the nature of the discretisation which results in all functions  : X nCm
to be in the Stein class of the operator Ap. The proof of this is a simple adaptation of the
corresponding proof in [YLRN18] for real valued functions, and we omit it here.
Now we can reproduce the following Theorem:
Theorem 16:
The discrete kernelised SD between two distributions, p,q is given by:
CSD(p,q) := Ex ,yp [q(x ,y)] (6.26)
where q is the Stein kernel:
q(x ,y) := sq(x)
T(x ,y)sq(y) sq(x)Ty(x ,y)
x(x ,y)
T sq(y)+Tr(
x ,y(x ,y)) (6.27)
Let us pause briefly here to return to the discussion above regarding valid
kernels. As mentioned above, for the MMD to be faithful, its kernel must be
characteristic or universal [FGSS07, SGF+08]. The Gaussian kernel Eq. (3.23)
is indeed one which is characteristic [FGSS07], and some effort has been made
to find conditions under which a kernel is characteristic [SFL11]. If the sample
space is discrete, it turns out the condition for determining universality is simpler.
In this case we only require the Gram matrix for the kernel, Ki j = (x
i ,y j) to be
positive definite16 [SFG+09]. If the Gram matrix for a kernel is positive definite
we refer to the kernel itself as being strictly positive definite (confusingly). Just
as with the MMD, it turns out this requirement of kernel-positive-definiteness is
again exactly what makes the SD a valid discrepancy measure [YLRN18].
Next, we can compute the gradients of Eq. (6.26) as with those of the MMD
16Recall, a matrix, K, is positive definite  x  Rd/0, xTKx > 0.
6.3. Training a quantum circuit Born machine 137
(replacing p p,q  for the QCBM and data distributions):
p(x)
(x ,y)p(y)+
p(x)(x ,y)
p(y)
(x)(x ,y)p(y)
(x)(x ,y)p(y)
p(x)(x ,y)p
(y)
p(x)(x ,y)p
(y) (6.28)
[(x ,y)] E
[(x ,y)]+ E
[(x ,y)] E
[(x ,y)]
(6.29)
Using again the parameter-shift rule Eq. (4.13) and that the Stein kernel, ,
of Eq. (6.27) does not depend on the parameter, k .
Clearly, the primary difference between the MMD and the SD is the form of
the kernel which must be computed in each case. To compute the Stein kernel,
we must evaluate the base kernel  for each pair of samples. Let us assume this
costs time T (n) (a polynomial function of the number of qubits, n).
However, adding to the computational burden, we also must compute the
following shifted versions of the kernel for each pair of samples, (x ,y), in both
parameters:
x(x ,y) y(x ,y) Tr [x ,y(x ,y)] (6.30)
which are required in Eq. (6.27). To compute one of these terms (for example,
x(x ,y)), we have:
x(x ,y) = [(x ,y), . . . ,(x ,y)]
T  [(1x ,y), . . . ,(nx ,y)]T (6.31)
so we must evaluate (ix ,y) for i = {1, . . . ,n}. Therefore, computing the shifted
kernel operator in a single parameter takes O(T (n) (n+1)). The same holds
for the kernel gradient with respect to the second argument, y(x ,y). For
x ,y(x ,y), the process is slightly more involved because:
Tr [x ,y(x ,y)] = Tr (x [y(x ,y)]) = Tr (x [(x ,y)(x ,y)]) (6.32)
= n(x ,y)
(x ,iy)
(ix ,y)+
(ix ,iy)
(6.33)
Each individual term in the respective sums requires the same complexity, i.e.
O(T (n)) so the term Tr [x ,y(x ,y)] overall requires O(T (n) (3n+1)).
However, we have not yet discussed the computation of the score function, s.
If we are given oracle access to the probabilities, (y), then there is no issue and
SD will be computable. Unfortunately, in many practical applications this will not
be the case. To deal with this scenario, in Sec. 6.3.2.1 and Sec. 6.3.2.2, we give
two methods to approximate the score function, given samples from the dataset,
138 6. Generative modelling with quantum circuit Born machines
. Notice that even with this hurdle (difficulty in compute the score), the SD is
still more suitable than the KL divergence to train these models, since the latter
requires computing the circuit probabilities, p(x), which is in general intractable,
and so could not be done for any dataset. In contrast, the Stein discrepancy
only requires the data probabilities, which may make it amenable for generative
modelling using some datasets.
6.3.2 Computing the Stein score function
Let us now discuss the final ingredient for the Stein discrepancy: the computability
of the score function Eq. (6.23). For every sample, x  p, that we receive from
the Born machine we require the score function of that same outcome being
outputted from the data distribution, x  . This involves computing (x), and
also x(x), i.e. (ix),i  {1, . . . ,n}.
This section will discuss our approach to approximate the score function of the
data, s, from samples, {y i : y  (y)}, based on the methods of [LT18, SSZ18].
We call these methods the Identity, and Spectral methods for convenience. Of
course, the most obvious approach to computing the score, using M samples
alone, would be to simply accumulate the empirical distribution which is observed
by the samples, p(y) :=
m=1 (y  y (m)) and compute the score from this
distribution. However, this immediately has a severe drawback. Since the score for
a given outcome, s(xm), requires also computing the probabilities of all shifted
samples, (ixm) i , if we have not seen any of the outcomes ixm in the
observed data, we will not have values for these outcomes in the empirical dis-
tribution, and hence we cannot compute the score. This would be a major issue
as the number of qubits grows, since we will have exponentially many outcomes,
many of which we will not see with poly(n) samples.
6.3.2.1 Identity approximation of Stein score
As a first attempt, we adopt the method of [LT18]. This involves noticing that
the score function appears explicitly in Stein identity, and inverting Steins identity
gives a procedure to approximate the score (hence we dub this approach the
Identity method).
Of course, we shall need to use the discrete version of Steins identity in our
case, and re-derive the result of [LT18] but there are no major difficulties in doing
Let us define the score matrix, G, using M samples drawn from :
G :=
s1(x
1) s1(x
2) . . . s1(x
s2(x
1) s2(x
2) . . . s2(x
. . .
sn(x
1) sn(x
2) . . . sn(x
 Gi,j := si(x j) =
(x j)
(x j)
(6.34)
Each column is the term which corresponds to the score function for the distribu-
tion, , and that given sample.
6.3. Training a quantum circuit Born machine 139
Now, to compute an approximation of G: G G we can invert the discrete
version of Steins Identity, Eq. (6.35) (as in the continuous case of [LT18]):
[s(x)(x)
T f(x)] = 0 (6.35)
where f is a complex vector valued test function.
Rearranging Eq. (6.35) in terms of the score, and following [LT18]:
[s(x)
T ] = E
[f(x)] = 
(x)s(x)f(x)
(x)f(x) (6.36)
Taking a Monte Carlo estimate on both sides with M samples, we have:
i)f(x i)T 
x i f(x
i) (6.37)
Next, defining:
F := [f(x1), f(x2), . . . , f(xM)]T , G := [s(x1),s(x2), . . . ,s(xM)]T , (6.38)
x f =
x i f(x
i), x i f(x
i) := [x i f1(x
i), . . . ,x i fl(x
i)]T (6.39)
Now the optimal value for the approximate Stein matrix, G will be the solution
to the following ridge regression problem, and adding a regularisation term, with
parameter, , to avoid the matrix being non-singular:
G = argmin
GRMn
x f 1MFG
2
G2
(6.40)
Where ||  ||F is the Frobenius norm: ||A||F =
. The analytic solution
of this ridge regression problem is well known and can be found by differentiating
the above Eq. (6.40) with respect to G and setting to zero:
G =M(K+1)1F Tx f (6.41)
G =M(K+1)1,K (6.42)
The approach of [LT18] involves implicitly setting the test function to be a feature
map in a RKHS, f = . In this case, we get K = F TF , and also ,Kab =
i=1x i
(xa,x i). Unfortunately, there is no motivation given in [LT18] for
which choice of feature map should be used to compute Eq. (6.42). A sensible
choice might be the exponentiated Hamming kernel suggested as a suitable kernel
to use in (binary) discrete spaces by [YLRN18], where dHN(x ,y) is the normalised
Hamming distance (or 1 distance) between binary vectors, x ,y :
H(x ,y) := exp(dHN(x ,y)) , dHN(x ,y) :=
|xi  yi | (6.43)
Any of these kernels could be used, since the only requirement on the above
method is that the feature map obeys the discrete Stein identity, which we have
seen is the case for any complex vector valued function.
140 6. Generative modelling with quantum circuit Born machines
6.3.2.2 Spectral approximation of Stein score
While the method used to approximate the score function method which was
shown in Sec. 6.3.2.1 is straightforward, it does not give a method of computing
the score accurately at sample points from the QCIBM which have not been seen
in the data distribution,  (which becomes exponentially more likely as n grows).
If this were to occur during training, a possible solution [LT18] is simply to add
that sample to the sample set, and recompute the score function by the Identity
method. However, this is expensive, so more streamlined approaches would be
desirable. Worse still, this tactic would potentially introduce bias to the data,
since there is no guarantee that the given sample from the Born machine, does
not have zero probability in the true data, and hence would never occur.
The approach to resolve this is that of [SSZ18] (which we dub the Spectral
method). This uses the Nystrm method as a subroutine to approximate the
score, which is a technique to approximately solve integral equations [Nys30]. The
Nystrm method works by finding eigenfunctions of a given kernel with respect to
the target probability mass function, . As in the case of thew Identity method,
the Spectral method was defined when  is a continuous probability measure, so
we must again perform a discretisation procedure.
We summarise the parts of [SSZ18] which are necessary in the discretisa-
tion. For the most part the derivation follows cleanly from [SSZ18], and from
Sec. 6.3.2.1. Firstly, the eigenfunctions in question are given by the following
summation equation:
(x ,y)j(y)(y) = j(x) (6.44)
where {j}Nj=1  
2(X ,), and 2(X ,) is the space of all square-summable17
sequences with respect to , over the discrete sample space, X . If the kernel is a
quantum one, as in Eq. (4.30), the feature space has a basis, {j = s j |}Nj=1 
2(X ,), where |sj are for example computational basis states. We also have the
constraint that these functions are orthonormal under the discrete :
i(x)j(x)(x) = i j (6.45)
Approximating Eq. (6.44) by a Monte Carlo estimate drawn with M samples, and
finding the eigenvalues and eigenvectors of the covariance kernel matrix, Ki j =
(x i ,y j), in terms of the approximate ones given by the Monte Carlo estimate,
exactly as in [SSZ18], we get:
j(x) j(x) =
m)(x ,xm) (6.46)
{uj}j=1,...J are the Jth largest eigenvalues of the kernel matrix, K, with eigen-
values, j . The true eigenfunctions are related to these sampled versions by:
Mujmm  {1, . . . ,M},j  j/M.
17Sequences, {Xn|Xn  X}n, in 2(X ,) satisfy
|Xn|2(Xn)<
6.3. Training a quantum circuit Born machine 141
Assuming that the discrete score functions are square-summable with respect
to , i.e. s i(x) 2(X ,), we can expand the score in terms of the eigenfunctions
of the 2(X ,):
s i(x) =
i jj(x) (6.47)
Since the eigenfunctions, j are complex valued, they automatically obey the
discrete Steins identity Theorem 15 and we get the same result as [SSZ18]:
i j =Exij(x) (6.48)
Proceeding, we apply the discrete shift operator, xi , to both sides of Eq. (6.44)
to give an approximation for the term, xi(x) xi(x):
xi(x) =
xi(x ,x
m) (6.49)
It can also be shown in this case that xi(x)xi (x), by comparing Eq. (6.49)
with Eq. (6.46), and hence we arrive at the estimator for the score function:
s i(x) =
i j j(x)i j =
xi j(x
m) (6.50)
If the sample space is the space of binary strings of length n, the number of
eigenfunctions, N will be exponentially large, N = 2n, and so the sum in Eq.
(6.47) is truncated to only include the Jth largest eigenvalues and corresponding
eigenvectors.
6.3.3 Training with the Sinkhorn divergence
We have left the best until last. While the above Stein discrepancy was an im-
provement in terms of computability on the KL divergence, it still left a lot to
be desired - particularly in the cumbersome and expensive methods required to
estimate the Stein kernel and score function. It also lacked a theoretical under-
standing of its operational meaning relative to TV18. The next cost function (the
Sinkhorn divergence (SHD)) we introduce circumvents these issues:
 It is provably efficient to compute (depending on a hyperparameter),
 It provides an upper bound on TV (again, depending on the same hyperpa-
rameter).
Before introducing the Sinkhorn divergence, we must revisit optimal transport
(OT) from Sec. 2.1.7.1. Recall, the definition of OT (for comparing the QCBM
18Although based on numerical experiments, it seems to provide a desirable upper bound on TV
142 6. Generative modelling with quantum circuit Born machines
and data distributions):
OTc(p,q) := min
UU(p,)
(x ,y)XY
c(x ,y)U(x ,y) (6.51)
U(x ,y) = (y), 
U(x ,y) = p(x) (6.52)
If we used this directly to train a QCBM, we would incur an exponential sampling
cost in the number of qubits (recall the sample complexity of computing Eq. (6.51)
scales as O(N1/n), given N samples from p/). However, it does provide the
operational meaning we are seeking, via the following inequality [GS02]:
dmin TV(p,)OT
(p,) diam(X )TV(p,) (6.53)
where diam(X n) = max{(x ,y),x ,y  X n}, dmin =minx =y (x ,y), and (x ,y) is
metric on the space, X n. We can achieve this bound by choosing c =  in Eq.
(6.51) since in this case optimal transport reduces to the Wasserstein metric Eq.
(2.59), OT = dW.
If, for instance, we were to choose (x ,y) to be the 1 metric between the
binary vectors of length n (a.k.a. the Hamming distance), then we get that
dmin = 1,diam(X ) = n, and so:
TV(p,)OT
1(p,) nTV(p,) (6.54)
To overcome the exponential sampling cost, we can introduce forms of regulari-
sation. Regularisation is typically introduced in ML to prevent overfitting. In this
case, regularisation will effectively smooth the problem, which enables the solution
to be found more quickly.
A particular choice for this regularisation was proposed in [Cut13], where an
entropy term, in the form of the KL divergence, is introduced to the optimal
transport optimisation problem Eq. (6.51) as follows:
OTc(p,) := min
UU(p,)
(x ,y)XY
c(x ,y)U(x ,y)+KL(U|pq)
(6.55)
KL(U|p) = 
(x ,y)XY
U(x ,y) log
U(x ,y)
(p)(x ,y)
(6.56)
The effect of the entropy term is controlled by the hyperparameter  > 0. As
 0, we obviously recover the original, unregularised, problem, and if  is large,
the coupling will be penalised the further it is from a product distribution on X Y
(i.e. U in the form p). See [PC19] for an overview of methods to compute
optimal transport metrics.
Using Eq. (6.55) as a cost function by itself however is perhaps not desirable
since, by introducing , we lose the faithfulness property: OTc(p,p) =0 in general.
However, this can be remedied by introducing two symmetric terms and defining:
CSHD(p,) :=OT
(p,)
OTc(p,p)
OTc(,) (6.57)
6.3. Training a quantum circuit Born machine 143
This quantity is the Sinkhorn divergence (SHD), and we can simply check that it
is indeed faithful.
Now, similarly to the Stein discrepancy and MMD, we can derive gradients of
the Sinkhorn divergence, with respect to the given parameter, k . According to
[FSV+19], each term in Eq. (6.57) can be written as:
OTc(p,) = p, f + ,g=
p(x)f (x)+(x)g(x) (6.58)
f and g are the so-called optimal Sinkhorn potentials, arising from a primal-dual19
formulation of optimal transport. These are computed using the Sinkhorn al-
gorithm20. These vectors are initialised at f 0(x) = 0 = g0(x), and iterated in
tandem according to Eq. (6.60) and Eq. (6.61) for a fixed number of Sinkhorn
iterations until convergence. The number of iterations required depends on ,
and the specifics of the problem. Typically, smaller values of epsilon will require
more iterations, since this is bringing the problem closer to unregularised optimal
transport, which is more challenging to compute. For further discussions on reg-
ularised optimal transport and its dual formulation, see [FSV+19, PC19]. Now,
following [FSV+19], the SHD can be written on a discrete space as:
CSHD(p,) =
[p(x)(f (x) s(x))+(x)(g(x) t(x))] (6.59)
s, t are the autocorrelation dual potentials, arising from the terms OTc(p,p),
OTc(,) in Eq. (6.57).
Again, following [FSV+19], we can see how by discretising the situation based
on N,M samples from p, respectively: x := {x1, . . . ,xN} p(x), y := {y1, . . . ,yM}
(y). With this, the optimal dual vectors, f ,g are given by:
f l+1(x i) =LSEMk=1
(y k)+
gl(y k)
Cik(x
i ,y k)
(6.60)
gl+1(y j) =LSENk=1
f l(xk)
Ckj(x
k ,y j)
(6.61)
C(x ,y) is the so-called optimal transport cost matrix derived from the cost func-
tion applied to all samples, Ci j(x
i ,y j) = c(x i ,y j) and
LSENk=1(v k) := log
exp(v k) (6.62)
is a log-sum-exp21 reduction for a vector v , used to give a smooth approxima-
tion to the true dual potentials.
19The original OT problem was a linear programming (LP) problem and is called the primal
problem. The current formulation is the dual version of the primal which (by the weak duality
theorem [BGW09]) at least provides an upper bound to the true solution of the LP.
20This gives the divergence its name [Sin64].
21The log-sum-exp function has a gradient which is given by the softmax function, which is also
used as a neural network activation function Eq. (3.2).
144 6. Generative modelling with quantum circuit Born machines
The autocorrelation potential, s, is given by:
s(x i) =LSENk=1
s(xk)
C(x i ,xk)
(6.63)
t(y i) can be derived similarly by replacing p  in Eq. (6.63) above. However,
the autocorrelation dual can be found using a well-conditioned fixed point update
[FSV+19], and convergence to the optimal potentials can be observed with much
fewer Sinkhorn iterations:
s(x i)
s(x i)LSENk=1
s(xk)
C(x i ,xk)
(6.64)
Finally, let us derive the gradient of CSHD. The derivative with respect to a
single probability of the observed samples, p(x
i), is given by [FSV+19]:
CSHD(p,)
p(x
= f (x i) s(x i) (6.65)
However, this only applies to the samples which have been used to compute f , s
in the first place. If one encounters a sample from p
(which we shall in the
gradient), x s  p
, which one has not seen in the vectors sampled from p (i.e.
x s / x , one has no value for the corresponding vectors at this point: f (x s), s(x s).
Fortunately, as shown in [FSV+19], the gradient does extend smoothly to this
point (and all points in the sample space) and in general is given by:
CSHD(p,)
p(x)
= (x) (6.66)
(x) =LSEMk=1
(y k)+
g0(y k)
C(x ,y k)
+LSENk=1
s0(xk)
C(x ,xk)
(6.67)
where g(0), s(0), are the optimal vectors which solve the original optimal transport
problem, Eq. (6.61) and Eq. (6.63) at convergence, given the samples, x , y from
p, respectively. Given this, the gradient of the Sinkhorn divergence with respect
to the parameters, k is given by:
CSHD
CSHD(p,)
p(x)
p(x)
(6.68)
(x)p+
[(x)] E
[(x)] (6.69)
Where again we have used the parameter shift rule Eq. (4.13). Therefore, one
can compute the gradient by drawing samples from the distributions, x  p, and
computing the vector (x) , for each sample, x  x , using the vectors, g(0), s(0)
already computed during the evaluation of SHD at each epoch.
6.3. Training a quantum circuit Born machine 145
6.3.3.1 Sinkhorn divergence sample complexity
The sample complexity of the Sinkhorn divergence is of great interest to us as
we claim that the TV and the KL are not suitable to be directly used as cost
functions. This is due to the difficulty of computing the outcome probabilities
of quantum circuits efficiently. We now motivate why the MMD is a weak cost
function, and why the Sinkhorn divergence should be used as an alternative. This
will depend critically on the regularisation parameter , which allows a smooth
interpolation between the OT metric and the MMD.
Firstly, we will require the Sinkhorn divergence to be faithful. Fortunately, by
the results of [FSV+19], we can be assured that no matter which value of  is
chosen, the SHD has this property:
CSHD(p,q) = 0  p  q (6.70)
CSHD(p,q) C
SHD(p,p) = 0 (6.71)
It also metrizes convergence in law, effectively meaning it can be estimated
using M samples, and will converge to its true value in the limit of large samples:
CSHD(pM ,p) 0  pM p (6.72)
Secondly, we also know that the Sinkhorn divergence inherits the sample com-
plexities of both the MMD and unregularised OT, since it becomes both of these
metrics in the extreme limits of epsilon [RTC17]:
 0 : C0SHD(p,q)OT
0(p,q) (6.73)
 : CSHD(p,q)MMD
(p,q), (x ,y) =(x ,y) (6.74)
Therefore, we expect that the sample complexity becomes large (O(N1/n)) as
 0 and small as  (O(N1/2)), with some complicated function of  in
between. Fortunately again, this has also been addressed, this time by [GCB+19].
First, however, we will need to introduce the concept of Lipschitz continuity :
Definition 49 (Lipschitz continuity).
Given two metric spaces, (A,dA),(B,dB) where dC denotes a metric on
the space C  {A,B}, a function, f :AB, is called Lipschitz continuous
if there exists an L R (called the Lipschitz constant) such that x ,y A:
dB(f (x), f (y)) LdA(x ,y) (6.75)
Now, we can address the sample complexity results from [GCB+19]: Specifi-
cally, we have the following two results22 The first is the mean difference between
the true Sinkhorn divergence, CSHD(p,q) and its estimator derived from the em-
pirical distributions, CSHD(pM , qM) is given by:
22Note the original theorems technically apply to the regularised OT cost, rather than CSHD,
but the addition of the symmetric terms in Eq. (6.57) will not affect the asymptotic sample
complexities since they add only constant overheads.
146 6. Generative modelling with quantum circuit Born machines
Theorem 17: (Theorem 3 from [GCB+19])
Consider the Sinkhorn divergence between two distributions, p, and q on
two bounded subsets, X ,Y of Rn, with a C, L-Lipshitz cost c . One has:
E|CSHD(p,q)C
SHD(pM , qM)|=O
n/2
(6.76)
where  := 2L|X |+ ||c || and constants only depend on |X |, |Y|,c and
||c l || for l = 0, . . . ,n/2.
The second is the following concentration result:
Corollary 4: (Corollary 1 from [GCB+19])
With probability at least 1, we have that:
|CSHD(p,q)C
SHD(pM , qM)| 12B
2log 1
(6.77)
where  := 2L|X |+ ||c ||, C := +e
 , B  1+e
L|X |+||c||
,=O(1+
n/2
)) and K := maxxX S(x ,x).
S is the Matern or the Sobolev kernel, associated to the Sobolev space,
Hs(Rn), which is a RKHS for s > n/2, but we will not go into further detail here.
The more exact expression for Eq. (6.76) is given by:
E|CSHDC
SHD| 12
L|X |+||||
n/2
(6.78)
where we use CSHD, rather than OT
 as in [GPC18], through the use of the
triangle inequality.
Now, for our particular case, we would like to choose the Hamming distance
as a metric on the Hamming hypercube (for a fixed dimension, n). However, due
to the smoothness requirement of the above theorems, c  C, this would not
hold in the discrete case we are dealing with. However, we can take a broader
view to simply use the 1 distance, and embed the Hamming hypercube in a larger
space. This is possible since, as we mentioned above, the Hamming distance, dH
is exactly the 1 metric restricted to binary vectors.
In this scenario, formally, we are dealing with the general hypercube in Rn, but
where the probability masses are strictly concentrated on the vertices of the hy-
percube. Now, we can compute directly some of the constants in the above, The-
orem 17 and Corollary 4. Taking X to be the unit hypercube in Rn, and c = = 1,
6.3. Training a quantum circuit Born machine 147
which is Lipschitz continuous, we can compute the following:
|X |= sup
x ,yX
||x y ||1 = n, ||(x ,y)|| = sup{|(x ,y)| : (x ,y)  X Y}= n
(6.79)
A rough upper bound for the Lipschitz constant, L, can be obtained as follows. If
we take dA to be the sum metric on the product space, X Y, and f to be the
1 distance, we get:
|(x1,y1) (x2,y2)| L
dX (x
1,x2)+dY(y
1,y2)
(6.80)
For two points, (x1,y1),(x2,y2)  X Y, and the cost c : X Y R,c = ||  ||1.
Now, i |x1i y1i |i |x2i y2i |[
i |x1i x
|+i |y1i y
]  L (6.81)
We want to find an upper bound for the left hand side of Eq. (6.81), assuming that
x1 = x2 and y1 = y2. In this case, both numerator and denominator are zero, so
any non-zero L will satisfy Eq. (6.80). Now, applying the trick of embedding the
Hamming hypercube in the general hypercube, we can assume x1,2
,y1,2
{0,1} i .
To derive such a bound, we can bound both the numerator and the denominator
of the LHS of Eq. (6.81) independently. We find the denominator is as small as
possible, when only one element of x1,2 or y1,2 is equal to one, and all the are
equal to zero. The numerator is as large as possible when one of x1,2 or y1,2 is
the all-one vector. In this case, the LHS is upper bounded by n (for a fixed n), so
we can choose L= n, which is a constant for a fixed n.
Plugging these quantities into Eq. (6.78) we arrive at:
E|CSHDC
SHD|=O
n/2
(6.82)
The constants in O
n/2
, depend on |X |, |Y|,n, and ||(k)|| which are
at most linear in n. Clearly, due to the asymptotic behaviour of the Sinkhorn
divergence, we would like to choose  sufficiently large in order to remove as
much dependence on the dimension, n, as possible. This is because, in our case,
the dimension of the space is equivalent to the number of qubits, and hence to
derive a favourable sample complexity, we would hope for the dependence on n to
be polynomial in the number of qubits. Ignoring the constant terms, we can see
that by choosing =O(n2), we get:
E|CO(n
SHD C
O(n2)
SHD |= O
(6.83)
148 6. Generative modelling with quantum circuit Born machines
Similarly the concentration bound from Corollary 4 is:
|CSHDC
SHD| 12B
2log 1
L|X |+||||
n/2
2log 1
2log 1
O(n2)
n/2
+O(n2)
O(n2)
n/2
(6.84)
since = 2n2+n=O(n2). Now, choosing the same scaling for  as in Eq. (6.83)
we get that with probability 1:
|CO(n
SHD C
O(n2)
SHD |=O
nn/2
(6.85)
log(1/)1/2
(6.86)
It is likely in practice however, that a much smaller value of  could be chosen,
without blowing up the sample complexity. This is evidenced by numerical results
in [GPC18, FSV+19, GCB+19].
Up to this point, we have discussed the favourable parts of the sample com-
plexity of the Sinkhorn divergence relating to computability. Let us now revisit
it from the point of view of quantum advantage. Recall that from Eq. (6.54),
the unregularised optimal transport provides an upper bound on TV (meaning it is
strong, as evidenced by its sample complexity), while on the other hand we have
from Eq. (6.18), the MMD provides only a lower bound (and hence is weak).
Given the interpolation between these two metrics, there must be a crossover at
some value of  for which the Sinkhorn divergence also provides an upper bound.
As with to our judicious choice of  to enable a favourable sample complexity, we
can perform a similar exercise to search for an upper bound to TV.
Again, we can adapt results from [GCB+19] for this purpose. Specifically,
we have the following bound on regularised and unregularised optimal transport
(Theorem 1 in [GCB+19]):
0OTc(p,q)OT
0(p,q) 2 log
0 2 log(1/) (6.87)
where the size of the sample space is bounded by D, |X |<D, as measured by the
metric, and L is the Lipschitz constant. As above, we can choose D = n, L = n
(for a Born machine) to arrive at:
0OT1 (p,)OT
0 (p,) 2 log
(6.88)
6.4. Numerical Results 149
The log term will be positive as long as  ne2, in which case regularised OT will
give an upper bound for the Wasserstein metric, and hence the TV through Eq.
(6.53) so finally have:
TV(p,)OT
0 (p,)OT
ne2
(6.89)
Unfortunately, comparing this with Eq. (6.83) and Eq. (6.86), we can see that
with this scaling of , the sample complexity would pick up an exponential depen-
dence on the dimension, n, so it would not be efficiently computable. As such,
we cannot hope to use the Sinkhorn divergence as an efficient means to both
bound TV (and therefore provide a verification that the Born machine has learned
a classically hard distribution, as in Sec. 6.2), and to also be provable efficiently
computable.
6.4 Numerical results
We conclude this discussion with a series of numerical results in training QCBMs.
This section will proceed in two parts.
Firstly, we provide a comparison between the training methods described in
Sec. 6.3 using a small scale example and a toy dataset. To summarise the results
of this comparison, we surprisingly find that both our new cost functions, the Stein
discrepancy and the Sinkhorn divergence are able to outperform the MMD when
training a QCBM, even at very small scales. This provides an indication that as
we scale the size of the quantum computer on which the QCBM is implemented,
we should expect better and better results using our methods (which is reinforced
by the theory of the previous section).
In the second part of this section, we change gears slightly, and compare a
QCBM to a completely classical model, the restricted Boltzmann machine, this
time on a real-world dataset. For this latter comparison, we find that a QCBM is
able to outperform the RBM at a certain scale (when both models are compared
in a fair way) and as such this provides some numerical evidence for the theoretical
arguments given in Sec. 6.2 which argues for the supremacy of quantum generative
modelling. Again, this is surprising, as there is nothing a-priori to suggest why a
quantum model should perform better on this dataset, when it has no apparent
quantum features. Of course, these results should only be taken as a preliminary
study - to be validated as part of a much larger numerical benchmark between
quantum and classical models. Finally, we implement an example of quantum
data in generative modelling, and use a QCIBM to learn a distribution outputted
by a quantum circuit.
6.4.1 The data
Next, we introduce the datasets we use for generative modelling. As mentioned
above, the first is a toy dataset, and the second is one which originates in a
real-world use case, specifically in finance. The former has the advantage in that
150 6. Generative modelling with quantum circuit Born machines
we have complete knowledge of the underlying probabilities, which is especially
relevant since we use them to compute the TV distance to comparing training
methods. This is also true for the quantum dataset, for which we can exactly
simulate the data target circuit. For the finance dataset, we unfortunately do
not have access to the underlying probabilities, but only a fixed number of samples.
6.4.1.1 A toy dataset
The toy distribution is the one given by Eq. (6.90), which is used in both [AAR+18,
VBB17] to train versions of the quantum Boltzmann machine:
(y) :=
pndH(s
k ,y)(1p)dH(s
k ,y) (6.90)
To generate this data, T binary strings of length n, written sk and called modes,
are chosen randomly. A sample y  (y) is then produced with a probability which
depends on its Hamming distance dH(s
k ,y) = i |ski y i | to each mode.
6.4.1.2 A financial dataset
Our second dataset is one considered in [KS19] and has a financial origin. This
contains 5070 samples of daily log-returns of 4 currency pairs between 19992019
(see Fig. 6.1). We use this dataset as a means to compare a quantum model
(our Born machine) with a comparable classical model (the restricted Boltzmann
machine (RBM), see Sec. 3.2).
However, in order to fit on the binary architecture of the Born and Boltzmann
machines, the spot prices of each currency pair are converted to 16 bit binary val-
ues, resulting in samples of 64 bits long. This discretisation provides a convenient
method for fitting various problem sizes onto models with different numbers of
qubits or visible nodes for the Born machine or RBM respectively. In particular,
we can tune both the number of currency pairs (i), and the precision of each pair
(j) so the problem size is described by a tuple (i , j). For example, as we revisit
in Sec. 6.4.3.3, a 12 qubit Born machine can be tasked to learn the distribution
of 4 currency pairs at 3 bits of precision, 3 pairs with 4 bits or 2 pairs at 6 bits of
precision.
6.4.1.3 A quantum dataset
As a final example, we examine a dataset generated by a quantum process, in
particular, we generate data using one instance of a Born machine (an IQP-
QCIBM specifically, and use another Born machine (a QAOA-QCIBM) to learn
this distribution. If one could make such a process theoretically rigorous at scale
(accounting for technical details), it would satisfy our definition of QLS Defini-
tion 43, since an IQP-QCIBM is an example of a distribution family, Dn, which is
not (TV, 1
,BPP)-Learnable.
6.4. Numerical Results 151
0.02 0.00 0.02 0.04
0.05 0.00 0.05
0.050 0.025 0.000 0.025
0.050 0.025 0.000 0.025
Figure 6.1: Data generated from FX spot prices of the above currency pairs. The generative
model aims to learn correlations between each pair based on a 16 bit binary representation. (a)
The selection of currency pairs we use, and (b) the marginal distributions of the log-returns of
each pair over a 20 year period. We aim to learn the joint distribution of subsets of the pairs.
Furthermore, one may also view this task as a form of weak circuit compila-
tion. The major objective in the field of quantum compilation (or gate synthesis
as it is sometimes referred to) is to compile a given target unitary, U, into a gate
sequence, V := V1V2 . . .Vk . For example, while we know from Sec. 2.1.2, that
arbitrary unitaries can be built using a universal set of gates, a question remains23
in how do we actually do this efficiently, relative to one parameter or another we
care about. For example, one may wish to minimise the number of non-Clifford
(e.g. T), or two qubit gates in a given quantum circuit. This task particularly
important for building and running algorithms on quantum computers, especially
near term ones. As we have seen above, particular hardware platforms (for exam-
ple the Rigetti Aspen QPU) may only be able to implement certain native gates
(see Sec. 2.1.6) and usually24 have a limited qubit connectivity, how can we im-
plement unitaries in an efficient and noise tolerant manner. For further reading
on the topic of compilation, see [Fow11, BJ12, CH17, HC18, VDRF18, NRS+18,
HSST18, AM19] for a non-exhaustive list of relevant resources.
We have already alluded to an interesting approach to the compilation problem
using using variational quantum algorithm techniques [JB18, KLP+19, HSNF18].
These propose approximating the target unitary by assuming that V is a PQC built
from the native gates of a particular quantum device, which can be trained. Using
a Born machine for this task is a similar methodology, but without introducing
any extra quantum resources to perform the compilation25. Specifically, we train
a Born machine to reproduce the output distribution given when a problem unitary
is applied on an input state. Of course, this is not true compilation in the sense
that the underlying unitaries (the problem unitary and that of the Born machine)
may not be equivalent (since for this to be the case the output distribution of the
two cases would have to be the same for all possible input states), which is why
23The Solovay-Kitaev is more of an existence result; it only tells us that some decomposition
exists. It does not tell us how to find it.
24This is certainly true for superconducting platforms, others may be more flexible. For example,
ion-trap based processors boast all-to-all connectivity.
25For example, the approach of [KLP+19] requires that the unitary to be compiled, U, is
actually applied on the quantum device.
152 6. Generative modelling with quantum circuit Born machines
we refer to it as weak.
As a particular example for this problem, we revert back to the QCIBM ansatz
and compile a QAOA-QCIBM to the output distribution of an IQP-QCIBM:
QCIBM
Compile
 QCIBM
(6.91)
In other words trying to find the optimal parameters, QAOA which fit the
distribution of the IQP-QCIBM with parameters, IQP. The quantum dataset,
{y j}Mj=1, is generated by running the following circuit M times:
|0 H
,0, 
|0 H U2f
,0, 
        
|0 H Unf
,0, 
(6.92)
Of course, one may take the view that this problem is not strictly one with
quantum data, but classical data which originated in a quantum process. As an
extension, one may consider the training of a QCBM, where the learning signal
is derived directly from the quantum state produced by Eq. (6.92) (before mea-
surement). In this case, the dataset would consist of qsamples (Definition 38)26.
Indeed, this is the driving principle underlying models such as quantum generative
adversarial networks (QGANs) [LW18b]. As an example for how one may use
qsamples for training a Born machine27, the learning signal may be derived by
estimating the overlaps between the quantum data and the Born machine states
using, for example, the SWAP test (see Sec. 2.1.7.3). Alternatively, one might
choose to use the quantum generalisations of the distance measures we introduced
in Sec. 2.1.7.2.
6.4.2 Comparison between training methods
Let us begin by implementing the training of the QCIBM using the new cost
functions introduced in Sec. 6.3, and comparing to the best previous known dif-
ferentiable training method, using the MMD. We aim for two properties that our
cost functions should exhibit in order to claim they have outperformed the MMD:
 Speed of Convergence: Both of the cost functions, SD and SHD, should
achieve equal or lower total variation distance (TV) than the MMD in a
shorter time period (even accounting for various learning rates).
26Obviously, with the modification that this is not a quantum encoding of a classical distribution,
but a purely quantum sample.
27Since clearly the cost functions we propose in Sec. 6.3 will no longer be directly suitable.
6.4. Numerical Results 153
 Accuracy: Since the cost functions we employ are in some sense stronger
than MMD, we would like for them to achieve a smaller TV than is possible
with the MMD in an equal or quicker time.
TV was chosen as an objective benchmark for several reasons. Firstly, it is typ-
ically the notion of distance which is required by quantum supremacy experiments
where one wants to prove hardness of classical simulation (as we mentioned in
Sec. 6.2). Secondly, we use it in the definitions of quantum learning supremacy.
Finally, it is one of the strongest notions of convergence in probability (as dis-
cussed in Sec. 6.3) one can ask for, so it follows that a training procedure which
can more effectively minimise TV, in an efficient way, should be better for gener-
ative modelling.
Since we are using an artificial dataset to compare our methods, and since
we are directly simulating the QCIBM (so we have access to the outcome prob-
abilities), we can directly compute the TV during training of the model. Note
that this would not be possible in general as the number of qubits scales. When
we examine the financial dataset Sec. 6.4.3, we cannot use TV as an objective
measure, since we do not have the exact data probabilities. Therefore we shall
need to devise an alternative third party metric to gauge the relative performance
of the model.
For all the numerical results shown in this section, we use the QCIBM structure
shown in Eq. (6.2) with trainable Ising parameters (the parameters, ), and
fixed QAOA-type measurement angles in Eq. (6.4). We choose this ansatz28
to connect to the discussions of quantum computational/learning supremacy in
Sec. 6.2.
In Fig. 6.2, Fig. 6.3, we illustrate the superior performance of our alternative
training methods, as measured by the total variation distance29. For these com-
parisons, we use a basic three and four qubit simulator provided by the Rigetti
forest platform [SCZ17], the 3q-qvm and 4q-qvm respectively. In all cases, we use
the analytic gradients derived in Sec. 6.3, and the Adam optimiser (Definition 17)
with varying initial learning rates, init. For the MMD, we use the mixture of Gaus-
sians kernel (Eq. (3.23)) with the same bandwidth parameters as in [LW18a]. In
all the plots (where error bars are present), we show the mean, maximum and min-
imum values over 5 independent training runs on the same dataset. We choose
these error bars rather than the variance to highlight the worst and best case
performance of all models.
In particular, we wish to highlight the noticeable out-performance observed for
28This can be considered a hardware efficient fixed-structure ansatz as introduced in Sec. 4.1.2.
Alternatively, one may consider it as a quantum-advantage-inspired ansatz, since at the minimum
to gain a quantum advantage in machine learning, one would desire to use a PQC structure which
is not classically simulatable.
29In both figures, for all methods we use 500 samples, with batches of 250, except for the Stein
discrepancy with the spectral method. For this method, for 3 qubits we use 40 samples and a
batch size of 20, while for 4 qubits in Fig. 6.3 we use 50 samples and batches of 25. In all cases
we have a 80%/20% train/test split.
154 6. Generative modelling with quantum circuit Born machines
the Sinkhorn divergence30 and the Stein discrepancy relative to training with the
MMD (using a Gaussian kernel), as measured by TV in Fig. 6.2. Furthermore we
observed that the gap (highlighted in the inset in Fig. 6.2(a)) which separates the
Sinkhorn divergence and Stein discrepancy (red and blue lines) from the MMD
(green, yellow and cyan lines) grows as the number of qubits grows. Unfortu-
nately, the Spectral method to approximate the Stein score does not outperform
the MMD, despite training successfully. The discrepancy between the true and
approximate versions of the Stein score is likely due to the low number of samples
used to approximate the score, with the number of samples limited by the compu-
tational inefficiency. We leave tuning the hyperparameters of the model in order
to get better performance to future work.
This behaviour is shown to persist on the QPU, as demonstrated in Fig. 6.4
and Fig. 6.531 for three and four qubits. In both cases, we show: training of
the model with both the MMD and SHD relative to TV, the learned probabilities
of both methods on, and off, the QPU and the behaviour of the cost functions
associated to both methods. This reinforces our theoretical argument that the
Sinkhorn divergence is able to better minimise TV to achieve superior results. For
these experiments, we used two sublattices of the Aspen-4 devices, specifically
the Aspen-4-3Q-A and Aspen-4-4Q-A, and their respective qvm versions. The
available qubits and connectivity for the former sublattice is shown in Fig. 6.4(e).
For the QCIBM ansatz used on the hardware, we only use a CZ connectivity which
matches this restricted topology. This is one reason why we observe high quality
results even on the quantum chip. In Sec. 6.4.3 and Chapter 7, we take this
hardware-restricted methodology much further.
Interestingly, in Fig. 6.4(d), we note that the hardware actually trains to lower
values of MMD than the simulator, despite the reverse situation relative to total
variation seen in Fig. 6.4(a) (i.e. the simulator outperforms the hardware). This
potentially indicates that the training with the MMD on noisy hardware could
lead to overconfident, and incorrect results. However, we do note that the Born
machine model does appear to demonstrate a form of optimal parameter resilience
(OPR)32, since training directly on the QPU also provides circuit parameters
which also perform well when simulated (comparing the plots of TV in Fig. 6.4(a),
Fig. 6.5(a)).
Given the performance noted above, we would recommend the Sinkhorn di-
vergence as the primary candidate for future training of these models, due to its
simplicity and competitive performance. One should also note that our goal here is
not to exactly fit the data, only to compare the training methods. Specifically, we
only use a shallow fixed circuit structure for training (i.e. a QAOA-based QCIBM
circuit) which we do not alter. For the financial dataset, we experiment further
with a variable number of parameters.
30To compute the Sinkhorn divergence and its gradients, we adopted the library of [Jea18].
31In both of these figures, we use init = 0.01 for Adam, a train size of 500 and a test size of
250 datapoints.
32This concept of OPR was introduced by [SKCC20] which evokes a particular type of noise
resilience of VQAs.
6.4. Numerical Results 155
0 50 100 150 200
0 50 100 150 200
0 50 100 150 200
0 25 50 75 100 125 150 175 200
Figure 6.2: MMD [ , , ] vs. Sinkhorn [ ] and Stein training with exact score function [ ]
and spectral score method [ ] for 3 qubits. We use a with fully connected topology, Rigetti
3q-qvm, , trained on the data, Eq. (6.90). 500 data points are used for training, with 400 used
as a training set, and 100 used as a test set. (a) TV difference between training methods, with
regularisation parameter  = 0.1 for SHD, and 3 eigenvectors for Spectral Stein method. Both
Sinkhorn divergence and Stein discrepancy are able to achieve a lower TV than the MMD. Inset
shows region of outperformance on the order of  0.01 in TV. We observe that the Spectral score
method was not able to minimise TV as well as the exact Stein discrepancy, potentially indicating
the need for better approximation methods. (b) Final learned probabilities of each training method.
(c) C0.08SHD on train and test set. (d) CMMD with three different initial learning rates. (e) CSD using
the exact and spectral score methods. Training on test set observed as thin lines without markers
for all methods, excluding the exact score method, since this uses exact probabilities.
156 6. Generative modelling with quantum circuit Born machines
0 50 100 150 200
0 50 100 150 200
0 50 100 150 200
0 25 50 75 100 125 150 175 200
Figure 6.3: MMD [ , , ] vs. Sinkhorn [ ] and Stein training with spectral score method
[ ] using 6 eigenvectors for 4 qubits. We use qubit topology in fully connected graph for four
qubits, i.e. Rigetti 4q-qvm, . (a) TV difference between training methods. Both Sinkhorn
divergence and Stein discrepancy can achieve lower TV values than the MMD. (b) Final learned
probabilities of target data, Eq. (6.90) [ ]. (c) C1SHD with regularisation parameter =1. Trained
using Hamming optimal transport cost function. (d) CMMD with three different initial learning
rates. (e) CSD with the exact and spectral scores using init = 0.05 for Adam.
0 20 40 60 80 100
0 20 40 60 80 100
0 20 40 60 80 100
Figure 6.4: MMD [ , ] vs. Sinkhorn [ , ] for 3 qubits comparing performance on the
QPU (Aspen-4-3Q-A-qvm) vs. simulated behaviour on QVM (Aspen-4-3Q-A-qvm). Target
data given in [ ]. (a) TV Difference between MMD [ , ], and Sinkhorn [ , ] with regularisation
parameter = 0.1 on QVM vs QPU. (b) Final learned probabilities of target data [ ] using MMD
[ , ] LR init = 0.2 and Sinkhorn [ , ] with = 0.1,init = 0.08. (c) C
SHD on the QVM [ ] vs.
QPU [ ]. (e) CMMD on QVM [ ] vs. QPU [ ]. (f) Qubit line topology in Rigetti Aspen-4-3Q-A
chip, using qubits, (10,11,17).
6.4. Numerical Results 157
0 20 40 60 80 100
0.000
0.025
0.050
0.075
0.100
0.125
0.150
0.175
0 20 40 60 80 100
0 20 40 60 80 100
Figure 6.5: MMD [ , ] vs. Sinkhorn [ , ] for 4 qubits comparing performance on the
QPU (Aspen-4-4Q-A) vs. simulated behaviour on QVM (Aspen-4-4Q-A-qvm). Target data
given in [ ]. (a) TV Difference between training methods with regularisation parameter = 0.08,
(b) Final learned probabilities. The probabilities given by the coloured bars are those achieved
after training the model with either the MMD or SHD on the simulator or the physical Rigetti
chip, on an average run. The probabilities of the model are generated by simulating the entire
wavefunction after training. (c) C0.08SHD on QVM [ ] vs. QPU [ ]. (d) CMMD on QVM [ ] vs. QPU
[ ]. In both latter cases, trained model performance on 100 test samples is seen as the thin lines
without markers. Again it can be seen that the Sinkhorn divergence outperforms the MMD both
simulated and on chip, with the deviation apparent towards the end of training. Similar behaviour
observed after 100 epochs, but not shown due to limited QPU time.
Now that we have warmed up with small examples using three and four qubits,
let us ramp up the scale in the next section, where we run experiments with up
to 28 qubits.
6.4.3 Quantum versus classical generative modelling in finance
The goal of this section is twofold. Firstly, as mentioned previously we experi-
ment with larger quantum Born machines for the task of generative modelling.
Secondly, we target a much more real-world use case, focusing on the financial
data distribution given in Sec. 6.4.1.2. Finally, we provide another dimension to
our quantum learning supremacy arguments presented in Sec. 6.2. We do this
by producing a rigorous numerical comparison between the QCBM as a quantum
model, versus the restricted Boltzmann machine (RBM) as a classical model in
learning the same dataset.
6.4.3.1 The ansatz
We move away from the QCIBM here, and instead focus on a more flexible ansatz,
which is closer to the hardware-efficient fixed structure (HEFS) anstze discussed
in Sec. 4.1.2. Specifically, we fit the entangling structure of the QCBM to the
sublattices of the Rigetti Aspen-7 and Aspen-8 as shown in Fig. 2.2. For example,
Fig. 6.6 shows a four qubit HEFS suited to the Aspen-7-4Q-C in Fig. 2.2(a). We
also show one layer of the matching anstze for three other sublattices in Fig. 6.7.
For each circuit in Fig. 6.7, we compute a quantity, Ent which is the average
Meyer-Wallach entanglement capacity [MW02].
This is a measure of entanglement in quantum states proposed as a method
of comparing different circuit anstze by [SJAG19]. This measure has been used
158 6. Generative modelling with quantum circuit Born machines
Figure 6.6: Hardware efficient circuit for the Aspen-7-4Q-C. Here we show the model with l
layers using the native entanglement structure native to the chip.
in a similar context by [HPSB21] in order to draw connections between ansatz
structure and classification accuracy. We first define an entanglement measure
Q, for a given input state |, as:
Q(|) :=
dpara(delj(0)|,delj(1)|) (6.93)
dpara(|u, |v) =
|u iv j u jv i |2 (6.94)
|u :=
u i |i, |v :=
v j |j (6.95)
where dpara is a particular distance between two quantum states given by: |u and
|v. This distance can be understood as the square of the area of the parallelogram
created by vectors |u and |v. The notation delj(b) is a linear map (a sort of
deletion operator) which acts on computational basis states as follows:
delj(b)|b1 . . .bn := bbj |b1 . . . bj . . .bn (6.96)
where  indicates the absence of the j th qubit. For example, del2(0)|1001 =
|101 However, to evaluate Q for a quantum state, we instead use the equivalent
formulation derived by [Bre03], which involves computing the purities of each
subsystem of the state |:
Q(|) = 2
Tr[2k ]
(6.97)
where k :=Trk (| |) is the partial trace over all n subsystems of | except k .
This reformulation of Q gives more efficient computation and operational meaning
since the purity of a quantum state is efficiently computable. Given Q, we define
[SJAG19] Ent as the average value of Q over a set, S of M randomly chosen
parameter instances, S := {i}Mi=1:
Ent :=
) (6.98)
6.4. Numerical Results 159
2 4 6 8 10
2 4 6 8 10
2 4 6 8 10
Figure 6.7: Hardware efficient circuits for 6, 8, 12 qubit Born machine ansatz. (a)-(c)
show Aspen-7-6Q-C , Aspen-7-8Q-C from the Aspen-7 chip and a 12 qubit sublattice from
the Aspen-8 chip which we consider. (d) - (f) illustrate the the entanglement structure in a
single layer, which tightly matches the chip topology. (g) - (i) show the average entangling
capability, Ent in Eq. (6.98) as a function of the number of layers in the circuit for each of the
entangling structures shown in (d)-(f). Error bars show mean and standard deviation over 100
random parameter instances, {i}100i=1,
j  U(0,2), in the single qubit rotations. U is the uniform
distribution over the interval [0,2].
Quantities such as Ent are extremely valuable for measuring the performance of
heuristic algorithms like VQAs. Since we do not, in general, have theoretical
arguments about performance, or the relative quality of one ansatz over another,
it is invaluable to have useful and interpretable metrics which one can compute in
practice. With these in hand, we can begin the process of studying which features
(e.g. entangling capability, expressibility, contextuality, etc.) of PQCs and VQAs
lead to advantages over classical methods.
6.4.3.2 Training & evaluation
Now, as mentioned at the start of this section, one of our goals here is to evaluate
the performance of the QCBM relative to a comparable classical model - the
restricted Boltzmann machine. For a fair comparison, we must compare both on
a level ground. In order to do so, we fix the number of parameters in each model
and then test which one performs better in practice. This methodology has been
performed previously in two other works relating to a QCBM, in both cases the
RBM was the classical model of choice to compare to. The first, [ALOPO20],
160 6. Generative modelling with quantum circuit Born machines
compared an RBM to a QCBM using data from a portfolio optimisation problem,
while the second, [Kon21], targeted the same dataset as we do here (the currency
pairs from Sec. 6.1). Both of these works found that the QCBM had the capacity
to outperform the RBM. We supplement these findings, in particular, by extending
the methodology of [Kon21]. We include extensive numerical experiments both
simulated, and on quantum hardware. We also implement differentiable training
methods for the QCBM and an alternative method for sample generation from the
RBM (recall the discussion from Sec. 3.4) based on path-integral Monte Carlo.
To enforce the same number of parameters in the model, we first choose
QCBM HEFS anstze as in Fig. 6.6, Fig. 6.7 which has n l parameters for l
layers. Given this choice, we then build a corresponding RBM which has nv :=
|Nv| = n visible nodes and nh := |Nh| = nl n = n (l 1) hidden nodes. Since
adding trainable two qubit parameters to the ansatz would increase our compilation
overhead, we opt to fix also the RBM weights to have random (but constant)
values. Therefore, only the local biases of each node are trained. An example of
such a fixed-weight RBM with 6 visible nodes can be found in Fig. 3.5. We revisit
weight training in Fig. 6.16.
Firstly, before detailing the training methods for both models, let us ask the
question: how should we evaluate their performance? As we discussed above,
computing a TV distance would be ideal since it is a strong benchmark of relative
performance. However, we clearly cannot do so here, since we do not have access
to the exact data probabilities, (y). As an alternate strategy, we employ an
adversarial discriminator, adapted from ideas in generative adversarial networks33.
This discriminator is essentially a classifier who is given the responsibility of de-
ciding whether a sample, z , comes from the actual data distribution, z  , and
hence is a real sample (labelled, say 1), or from the model, z  p and hence is
fake (labelled 0). The principle here is the same as that discussed in Chapter 5,
with only the model being different (or indeed, the classifier we choose could be
exactly the one presented in Chapter 5). We measure performance relative to the
classification accuracy of this discriminator (or the error it makes in classifying an
example) For an untrained generative model, the classifier should be able to easily
discriminate between the real and synthetic samples, but as training progresses,
the discriminator should increasingly make mistakes. The generative model is fully
trained when the best strategy of the classifier is simply to guess randomly when
presented a sample and so will have an error of 50%.
For simplicity, we use a common classical model called a random forest [Ho95],
an implementation of which is readily available in the scikit-learn [PVG+11]
python package. A random forest is an ensemble classification method, as it takes
a majority vote for a classification output over multiple decision tree classifiers.
We chose this since it was able to provide a separation between our models34 so
33See [ZWL+19, SHLZ18, RAG21, ARDAG20] related work on adversarial training methods for
quantum generative models
34If, on the contrary, a random forest was not able to separate the performance of both models,
we would be in an inconclusive situation - either there is truly no separation, or our classification
model is not powerful enough to isolate suitable features which label the model data as fake.
6.4. Numerical Results 161
we do not need to look at stronger classifiers.
As for training methods, we primarily use the Sinkhorn cost function, CSHD,
Eq. (6.57) plus its analytic gradients, Eq. (6.69) for training the QCBM35. For
all the results presented int the following section, we use the Adam optimiser Eq.
(3.8) with an initial learning rate of init = 0.05 and a value for = 0.5, the latter
of which we determined using a basic hyperparameter search using small problem
instances. Also to note, is that in all simulated versions of the QCBM, we use the
noisy-qvm version of a quantum device, which incorporates a simple noise model
including readout errors and standard T1 and T2 times. Details can be found in
the source code of PyQuil [SCZ17].
For the RBM, we use stochastic gradient descent (Definition 16) and the
gradient update rule given in Sec. 3.4. As we discussed in the latter section, the
most common method to generate samples using from the RBM is via contrastive
divergence. Here, we opt for an alternative method to sample from the Gibbs
distribution based the path-integral formulation of quantum mechanics (path-
integral Monte Carlo (PIMC), as discussed in Sec. 3.4). The PIMC method
is the core ingredient in QxSQA (introduced in [PWH19]), which is a GPGPU-
Accelerated simulated quantum annealer. In [PWH19], QxSQA was shown to
provide sufficiently good approximations to the required Boltzmann distribution
to be considered near exact, for a large range of model sizes, even orders of
magnitude larger than those considered here. To generate RBM samples, we
use the following hyperparameters whose definitions can be found in [PWH19]:
0 = 3, = 11020,Nsweeps = 1,PT = 0.1,Nanneals = 250,Teff = 1.
6.4.3.3 Financial dataset results
Finally, let us present the results of the comparison between the RBM and the
QCBM. In summary, we find the Born machine has the capacity to outperform
the RBM as the precision of the currency pairs increases. In Fig. 6.8, we use
data from 2 currency pairs, at 2,3,4 and 6 bits of precision. We notice the Born
machine outperforms the RBM around 4 bits (measured by a higher discriminator
error), and still performs relatively well when run on the QPU. Similar behaviour
is observed for 3 currency pairs in Fig. 6.10, which uses a precision of 2 and 4
bits, and with 4 pairs in Fig. 6.11 for a precision of 2 and 3 bits. In Fig. 6.12
we plot the entangling capability (defined by Eq. (6.97)) of the states generated
by initial and final circuits learned via training. Curiously, we notice that in the
problem instances in which the Born machine outperforms the Boltzmann machine
(those with a higher level of precision), the trained circuits produce states which
are more entangled (as measured by the Meyer-Wallach entanglement capacity,
Eq. (6.98)) than those that do not, despite the data being completely classical in
35For completeness, we also considered training the QCBM using the MMD, using a genetic
algorithm as in [Kon21] and finally also with respect to the adversarial discriminator. Of course,
training with respect to the discriminator would not provide an unbiased comparison. The MMD
is less favourable due to the arguments presented in Sec. 6.3 and we found the genetic algorithm
to be significantly slower than differentiable training.
162 6. Generative modelling with quantum circuit Born machines
0 20 40 60 80 100
0 20 40 60 80 100
0 20 40 60 80 100
0 20 40 60 80 100
Figure 6.8: 2 currency pairs (specifically EUR/USD and GBP/USD). We compare at (a) 2
bits, (b) 3 bits, (c) 4 bits and (d) 6 bits of precision. Correspondingly, we use a QCBM [ ] of 4,6,8
and 12 qubits using the anstze described above, and an RBM [ ] with the same numbers of visible
nodes. The hidden units are scaled in each case to match 2 layers of the QCBM. Results when
the QCBM is run on sublattices of the Aspen-7 QPU are shown in grey, whereas the simulated
version is given by the purple line, with simple noise model.
nature. This is especially prominent for 2 currency pairs in Fig. 6.12(a), in which
the training drives the entanglement capability at 2 and 3 bits of precision close
to zero (even for increased numbers of layers), but it is significantly higher for 4
and 6 bits of precision, when the Born machine outperforms the RBM, as seen in
Fig. 6.8. Similar behaviour is seen for 3 currency pairs, but not as evident for 4
pairs. The latter effect is possibly correlated to the similar performance of both
models for 4 currency pairs up to 3 bits of precision. The observed behaviour of
the entangling capability of the QCBM states is one direction in exploring why the
model may demonstrate an advantage relative to the RBM and provides possibly
the most important question raised by this work for future investigation. Seeking
explanations for certain advantages could be beneficial in designing future QML
algorithms which can actively exploit such features.
In all the figures presented here, where shown (we typically repeated runs only
for small problem instances due to the overheads required to simulate the QCBM),
errors bars indicate mean and standard deviations over 5 independent training runs
and deviations are due to the stochastic nature of the training procedure. In all
figures, where shown, grey lines indicate training runs on the Aspen QPU.
As a further measure of visually comparing performance of the trained models,
6.4. Numerical Results 163
Figure 6.9: QQ Plots corresponding to Fig. 6.8(d) of the marginal distributions of 2 currency
pairs (EUR/USD and GBP/USD) at 6 bits of precision. The QCBM distributions [ ] and those
generated by the RBM [ ]. (a) shows the QQ plot for the marginal distribution of each currency
pair with respect to itself as a benchmark. (b) Born machine initial (top panels) and final (bottom
panels) marginal distributions for both pairs and similarly in (c) for the RBM. While not able
to completely mimic the data due to the low number of parameters, the Born machine clearly
produces a better fit.
we show QQ (quantile-quantile) plots of the marginal output distributions from
one of the outperforming QCBM cases (2 pairs at 6 bits of precision) in Fig. 6.9.
The QCBM clearly produces a better data fit than the RBM when trained (bottom
panels), where a perfect fit would be a straight line, shown in Fig. 6.9(a) where
the data is plotted against itself. In both cases, we use 5070 samples from the
QCBM and RBM, to match the size of the dataset.
In Fig. 6.14 and Fig. 6.15 we investigate changing the parameter count in
each model for fixed problem sizes. For both models, we find that increasing the
model size past those shown in the main text did not have a significant impact
on the expressiveness, indicating that saturation has occurred for these parameter
numbers. We notice that increasing the number of hidden nodes (layers) for the
RBM (QCBM) slightly decreases (increases) convergence speed, but it does not
affect the final converged value appreciably.
We are also able to somewhat successfully train the largest instance of a
Born Machine to date in the literature, namely one consisting of 28 qubits on
the Rigetti Aspen-7 chip (whose topology is shown in Fig. 2.2(e), and we find it
performs surprisingly well. Fig. 6.13 shows the result of this, comparing a 28 qubit
QCBM with a 28 visible node RBM. While the performance of the Born machine is
significantly less than that of its counterpart, it is clear that the model is learning
(despite hardware errors), up to a discriminator error of 20%. We emphasise
that this result does not contradict those shown above since we are not able to
simulate the QCBM at this scale in a reasonable amount of time. We would not
necessarily expect the Born machine to match the performance of the RBM on
hardware at this scale for a number of reasons, the most likely cause for diminishing
performance is quantum errors in the hardware. However we cannot rule out other
factors, such as the ansatz choice. We leave thorough investigation of improving
164 6. Generative modelling with quantum circuit Born machines
0 20 40 60 80 100
0 20 40 60 80 100
Figure 6.10: 3 currency pairs. We compare at (a) 2 bits and (b) 4 bits of precision, using a
QCBM [ ] of 6 and 12 qubits and an RBM [ ] with the same numbers of visible nodes.
0 20 40 60 80 100
0 20 40 60 80 100
Figure 6.11: All 4 currency pairs. We compare at (a) 2 bits, (b) 3 bits of precision, using a
QCBM [ ] of 8 and 12 qubits and RBM [ ] with the same numbers of visible nodes. We notice the
RBM performs similarly here to the QCBM, with a possible advantage for the QCBM observed.
The lack of a more pronounced gain in this case is likely due to the smaller bit precision used here.
hardware performance to future work, perhaps by including error mitigation [HP20]
to reduce errors, thorough error modelling and parametric compilation and active
qubit reset [SCZ17, KTP+20] to improve running time and other performance
metrics.
6.4.3.4 Alternative model structures
For completeness, we showcase here the effect of using alternative model struc-
tures for the QCBM and the RBM.
Differing numbers of Born machine layers
Firstly, in Fig. 6.14, we show the effect of alternating the number of layers of
the hardware efficient anstze, shown in Fig. 6.7 for 4 and 8 qubits. In particular,
6.4. Numerical Results 165
Figure 6.12: Meyer-Wallach entangling capability Eq. (6.97) for a random choice of pa-
rameters (Initial) and the trained parameters (Final) in the same circuit. The above results
are generated by simulating the corresponding circuits. Error bars represent mean and standard
deviation over 5 independent training runs, where they are shown. The circuit anstze used are
those above in Fig. 6.7 closely matching the corresponding chip topology. In each panel we see the
entanglement in the final states trained on (a) 2 currency pairs at 2,3,4,6 bits of precision, (b) 3
currency pairs at 2 and 4 bits of precision and (c) 4 currency pairs at 2 and 3 bits of precision.
we notice that increasing the number of layers does not have a significant impact,
at least at these scales, except perhaps in convergence speed of the training. It
is likely however, that at larger scales, increased parameter numbers would be
required to improve performance.
Differing numbers of Boltzmann hidden nodes
We also demonstrate the effect of changing the number of hidden nodes in the
Boltzmann machine in Fig. 6.15, where we have 4,8 and 28 visible nodes. Again,
we observe that an increasing number of hidden nodes (and by extension, number
of parameters) does not substantially affect the performance of the model, in fact
it can hinder it, at least when training only biases of the Boltzmann machine. In
particular, it does not substantially alter the final accuracy achieved by the model.
We noticed similar behaviour when also training the weights of the Boltzmann
machine.
Weight training of Boltzmann machine
Finally, we compare the effect of weight training of the Boltzmann machine
to training the bias terms alone or in other words, the training of the edge con-
nections in the network, as opposed to just the parameters of the nodes. For
the problem instances where the Boltzmann machine was able to converge to the
best discriminator accuracy (i.e. in the small problem instances), we find training
the weights has the effect of increasing convergence speed, and also increased
accuracy where training the biases only was insufficient to achieve high discrimi-
nator error. Interestingly, we note that the Born machine still outperforms the 8
and 12 visible node RBMs, even when the weights are also trained, and this does
166 6. Generative modelling with quantum circuit Born machines
0 20 40 60 80 100
Figure 6.13: Random forest discriminator error during training for a problem size of 4 cur-
rency pairs at 7 bits of precision. We use 28 visible nodes in the Boltzmann machine, [ , ,
] and 28 qubits in the Born machine [ ]. The 28 qubit Born machine is run exclusively on the
Aspen-7-28Q-A chip using 2 layers of the hardware efficient ansatz similar to those shown in
Fig. 6.7.
0 20 40 60 80 100
0 20 40 60 80 100
Figure 6.14: 2 [ ], 3 [ ] and 4 [ ] layers of the hardware-efficient ansatz for (a) 4 and (b)
8 qubits. Models are trained on 2 currency pairs at 2 and 4 bits of precision respectively. No
major advantage observed for using an increasing number of layers, except perhaps in convergence
speed, suggesting that 2 layers is sufficient for these problem instances.
not seem to majorly affect the performance. However, training the weights does
make a large difference for 28 nodes, as seen in Fig. 6.16(c), so again further
investigation is needed in future work of this phenomenon.
6.5. Discussion and conclusion 167
0 20 40 60 80 100
0 20 40 60 80 100
0 20 40 60 80 100
Figure 6.15: Differing number of hidden nodes for RBMs. We compare (a) 4 (b) 8 and (c) 28
visible nodes. Enlarging the hidden space for the RBM again did not impact significantly for these
problem sizes and in particular would not give a performance boost to outperform the QCBM.
0 20 40 60 80 100
0 20 40 60 80 100
0 20 40 60 80 100
Figure 6.16: Weight training (WT) on the Boltzmann machine along with the node biases.
We compare (a) 8, (b) 12 and (c) 28 visible node RBMs along with the corresponding Born
machine. The latter uses 4 currency pairs, while the others use 2, as in the text above.
6.4.4 Weak quantum compilation with a QCBM
Finally, let us return to the quantum dataset from Sec. 6.4.1.3, which originated
from our definition of weak quantum circuit compilation. In Fig. 6.17 we demon-
strate this for two and three qubits. Here the error bars represent mean, maximum
and minimum values achieved over 5 independent training runs on the same data
set. In both cases, the QCIBM circuit is able to mimic the target distribution well,
even though actual parameter values, and circuit families are different. We also
use 500 data samples from the target circuit with a 400/100 train/test split.
6.5 Discussion and conclusion
At first glance, the contents of this chapter may all seem to be incremental
advances in almost disconnected topics. We discussed quantum advantage, new
training methods for a quantum circuit Born machine, and extensive numerical
experiments on both simulated and on NISQ hardware. Let us take a moment
now and tie all of these together with a grander goal in mind: the search for a
useful quantum machine learning algorithm on near-term quantum hardware with
provable advantage over classical methods.
For provable, we laid the foundations for the study of quantum learning
168 6. Generative modelling with quantum circuit Born machines
0 50 100 150 200
0 25 50 75 100 125 150 175 200
00 01 10 11
0 50 100 150 200
0 25 50 75 100 125 150 175 200
Figure 6.17: Compilation of a p= 1 QAOA-QCIBM circuit to a IQP circuit with two and three
qubits using CSHD with  = 0.1. (a, e) Initial [ ] and trained [ ] QAOA circuit parameters for
two and three qubits. Target IQP circuit parameters [ ]. Parameter values scaled by a factor
of 10 for readability. (b, f) Final learned probabilities of QAOA-QCIBM [ ] circuit versus data
probabilities (IQP-QCIBM) [ ]. (c, g) Total variation distance and (d, h) Sinkhorn divergence on
train and test set, using a Hamming optimal transport cost.
supremacy. Rooted in learning theory, we gave a formalism in which one may
claim a true quantum advantage in the problem of generative modelling. We then
discussed how one may (or may not) achieve such a proof with quantum com-
putational supremacy distributions, and gave a numerical implementation, on a
small scale, in learning such an example distribution (weak compilation).
We then provided new training methods for a particular instance of a generative
model (the Born machine), which is also our second example of a variational
quantum algorithm. This is relevant for near-term, since NISQ computers are
notoriously temperamental, and so we must use all available resources to extract
optimal performance from them. This includes, but is not limited to, any methods
to better benchmark or improve convergence in training of quantum machine
learning models.
Finally, regards useful, we studied a dataset of practical relevance, and which
is suitable for the generative modelling problem. Since finance is predicted to be
a major application area for quantum computers in the long term, any method to
solve problems (even incrementally) in this area is valuable. Returning again to
(numerically) provable quantum advantage, we benchmarked our generative model
relative to a comparable classical counterpart, the restricted Boltzmann machine,
and found promising results.
In conclusion, while we do not claim to have found such a killer application,
we hope the contents of this chapter may help along the path in the search for
such a thing.
6.5. Discussion and conclusion 169
6.5.1 Subsequent work
To round off this chapter, let us briefly highlight some (not all) related work
which appeared after the completion of the work in this chapter, in particular
those relevant to the discussion in the previous section. Firstly, relating to quan-
tum learning supremacy, the work of [SSHE21] demonstrated an instance of this,
working from, and extending, the definitions provided in Sec. 6.2. In particular,
(similar to [LAT21]), the authors showed how to embed the discrete log prob-
lem into a generative modelling scenario. Doing so, allowed them to describe a
distribution family, Dn, which could be learned efficiently by quantum means, but
not by any classical algorithm. This result does indeed satisfy our definition of
QLS (Definition 43), and does so even on a classical data input, but importantly
this advantage is not near-term in nature, and the problem it solves is somewhat
contrived. Nevertheless, it is an exciting result.
On a related note, [GAW+21] used foundational ideas from quantum mechan-
ics to demonstrate a provable separation between certain families of Bayesian
networks, and their quantum generalisations. These networks are used in gener-
ative modelling and probabilistic inference (a very related problem to generative
modelling).
Relevant to our heuristics for the QCBM, the work of [BCF+21] used the
method of the Stein discrepancy for probabilistic inference with a QCBM. Sec-
ondly, as we mentioned above, [RTK+20] defined the basis-enhanced QCBM,
which was able to effectively seed a convolutional generative adversial network
(when used as a prior distribution) and generate high-resolution MNIST digits
(see Fig. 3.4). Finally, [KCW21] proposed alternative training methods for (pre-
measurement) Born machines based on different metrics, similarly to the method-
ology we propose in Sec. 6.3 (albeit with the goal of alleviating barren plateaus
in QCBM training landscapes). Clearly, probabilistic quantum machine learning,
and in particular quantum generative modelling continues to be an exciting and
evolving area of study.
Practical quantum cryptanalysis by
variational quantum cloning
7.1 Introduction
Obi-Wan: Your clones are very impressive. You must be very proud.
 Star Wars: Episode II  Attack of the Clones
In the previous chapters, we introduced two quantum machine learning models,
and discussed their implementations on NISQ computers. Both of these applica-
tions were machine learning problems, i.e. data driven. In this chapter, we present
our third, and final, application for NISQ computers, but the problem we address
is not strictly a machine learning one, rather it is a problem in quantum informa-
tion and cryptography. While, at first, this may seem quite far removed from the
previous applications, in fact, we stress that it is not. We shall see in this chapter
how many of the concepts are transferable (with suitable adaptation), and the
method we propose will simply boil down to the same variational quantum algo-
rithm framework introduced in Sec. 4.1. We also mention how this application
may fall into the scope of either supervised or unsupervised learning1, depending
on what information the algorithm has access to.
Let us begin. In Sec. 2.2, we went to great pains to introduce the background
behind the no-cloning theorem (Theorem 1) and the subtleties surrounding it.
We introduced approximate cloning unitaries, U, which could produce two (or
more) copies of an input quantum state (from some family of states), which
were approximately the same as the input. We also introduced a circuit which
implements U in the case of phase-covariant and universal cloning in Fig. 2.4.
However, in these discussions we neglected to mention how one could actually
find U (or a circuit decomposition for U), given a particular family of quantum
states, a very related problem to quantum compilation. The approach of the
literature to date has been a theoretical one - specify the problem, assume the
most general transformation, and solve for the parameters of the unitary given
1Recalling our categorisation from Sec. 3.1 of (un)supervised learning as (not) having the
answer for the given problem beforehand.
172 7. Practical quantum cryptanalysis by variational quantum cloning
various symmetries of the problem to maximise the cloning fidelity. This needs
to be done individually for each generalisation and every set of states one might
want to consider. As we mentioned in Sec. 2.2.3, the theoretical analyses may be
tricky, as evidenced by the analytical forms for the fidelities in cloning fixed-overlap
states. Worse still, if we are able to find such a unitary, there is no guarantee
that it will perform well on near term quantum hardware to actually implement
the cloning transformation.
At this point, it is appropriate to take a slight detour and digress into the field
of quantum cryptography, and illustrate intuitively how one may connect it to
quantum cloning (we will be more concrete in Sec. 7.3). Early proposals for quan-
tum cryptographic protocols appeared as early as the 1980s with conceptual ideas
including quantum money [Wie83] and quantum key distribution [BB14]. Since
then, new protocols are being developed at a staggering rate, exploiting quantum
phenomena such as entanglement and non-locality of quantum correlations for
security proofs [GC01, BFK09, Aar09, VSC07]. On the experimental side, rapid
progress is being made as well, with the first violation of a loophole free Bell
inequalities ([Bel64]) demonstrated in 2015 and the satellite Micius implement-
ing quantum protocols over long distances, including quantum key distribution
(QKD) [YCL+17, YWC+19, RXY+17]. For an overview of recent advances in
quantum cryptography, see the topical review [PPA+20].
For each of these new protocols, they must be proven secure against malicious
behaviour in order to be useful. A standard method to prove the security of a given
quantum communication protocol (e.g. QKD2) implemented between two parties
(say Alice (A) and Bob (B)) is to assume the existence of an adversary (usually an
eavesdropper, Eve3 (E)) with unlimited power (or some computationally bounded
amount of resources) and to prove that the adversary cannot gain any (or a
limited amount of) secret information about the protocol. In the case of quantum
protocols, these security analyses, in many cases, reduce to the ability (or lack
thereof) of Eve to clone quantum states used in the protocol. Therefore, bounds
on approximate quantum cloning have a direct implication for information leakage
in a protocol. In some cases, the optimal attack Eve may perform is actually a
cloning-based one - she makes two approximate clones of a state sent by Alice,
forwards one onto Bob and keeps the other for herself to cheat. There is nothing
preventing Eve doing this - it is the job of the protocol designer to account for it,
and set security parameters in such a way that if, for example, Eve disturbs the
state beyond allowable regimes, the protocol is aborted.
2We use QKD as one of our examples in this chapter, but we note it is not the optimal use
of the algorithm presented here. Instead, we use it as a baseline application and as a benchmark
since it is well studied.
3We will use the notation E later in the chapter to denote any states Eve keeps for herself for
whatever purpose, and E to denote an ancillary system she uses to aid her malevolent behaviour,
but later discards.
7.2. Variational quantum cloning: cost functions and gradients 173
In the following sections, we describe our contribution to this area. Specifi-
cally, we introduce an algorithm for finding suitable cloning unitaries, U. We dub
it variational quantum cloning (VarQlone), and we build the various ingredients
for it. These include: differentiable cost functions for gradient-descent based
optimisation, theoretical guarantees on these cost functions including notions of
faithfulness and barren plateaus (Sec. 7.2), and variable-structure anstze with
quantum architecture search (Sec. 7.5). Importantly, using these techniques the
unitaries found by VarQlone are short-depth and hardware efficient, meaning they
are suitable for implementation on NISQ devices. We explicitly, and more rigor-
ously, make the connection between quantum cloning and cryptography in Sec. 7.3
by focusing on two distinct families of quantum protocols, quantum key distribu-
tion and quantum coin flipping. For these protocols, we show how VarQlone can
improve the performance of attacks on them, and discover novel attacks, facil-
itated by our QML algorithm. However, we stress that while our focus in this
chapter is towards cryptographic applications, the underlying primitive is that of
quantum cloning. Therefore, the approach may have broader applications wher-
ever the primitive of cloning arises, for example in quantum foundations, or perhaps
in the demonstration or benchmarking of quantum hardware. Finally, in Sec. 7.7
we conclude and discuss future work.
7.2 Variational quantum cloning: cost functions and
gradients
Let us now dive into some details and specifics of VarQlone. It is here where
we will most closely align with the terminology used in Sec. 4.1. To reiterate,
our motivation is to find short-depth circuits to clone a given family of states,
and also use this toolkit to investigate state families where the optimal figure
of merit is unknown. We note that a basic version of what the algorithm we
propose in this chapter was put forward by [JJB+19], but we significantly extend
their methods with state of the art approaches, provide theoretical arguments and
make the explicit connection to cryptography. We comment on the differences
where relevant through the chapter.
7.2.1 Cost functions
By this point in the thesis, we have hopefully sufficiently impressed the importance
of cost functions in QML applications upon the reader. VarQlone is no different.
Here, we define and study three cost functions which are suitable for approximate
cloning tasks. We begin by stating the functions, and then discussing the various
ingredients and their relative advantages.
174 7. Practical quantum cryptanalysis by variational quantum cloning
Figure 7.1: Illustration of VarQlone for M N cloning. A dataset of K states is chosen from
S, with M copies of each. These are fed with NM blank states, and possibly another ancilla,
|A into the variable structure ansatz, Ug() (more details about this in Sec. 7.5). Depending on
the problem, either the global, or local fidelities of the output state, , is compared to the input
states, |k, and the corresponding local or global cost function, C() is computed, along with
its gradient. We have two optimisation loops, one over the continuous parameters, , by gradient
descent, and the second over the circuit structure, g. Gradient descent over  in each structure
update step outputs, upon convergence, the minimum cost function value, Cbestt , for the chosen
cost function, t  {L,sq,G}.
The first, we call the local cost for M N cloning4, given by:
CMNL () := E
|S
CL ()
, CL () := Tr(O
L ) (7.1)
L := 1
| |j 1j (7.2)
where |  S is the family of states to be cloned. The subscripts j (j) indicate
operators acting on subsystem j (everything except subsystem j) respectively.
The second, we call the squared local cost or just squared cost for brevity:
CMNsq () := E
|S
(1F iL())
(F iL()F
L())
(7.3)
The notation F jL() := FL(| |,
) indicates the fidelity of Alices input
state, relative to the reduced state of output qubit j , given by j
= Trj (). As
in Eq. (4.9),  is the state outputted by the PQC acting on some fixed input
state (see Fig. 7.1). These first two cost functions are related only in that they
are both functions of local observables, i.e. the local fidelities.
The third and final cost is fundamentally different to the other two, in that it
instead uses global observables, and as such, we refer to it as the global cost:
CMNG () := E
|S
G := 1| |
N (7.4)
For compactness, we will drop the superscript M N when the meaning is clear
from context.
4Recall our footnote in Sec. 2.2 about the change of notation for qubit number in this chapter.
7.2. Variational quantum cloning: cost functions and gradients 175
Now, let us motivate our choices for the above cost functions. For Eq. (7.3),
if we restrict to the special case of 1 2 cloning (i.e. we have only two output
parties, j  {B,E}), and remove the expectation value over states, we recover the
cost function used in [JJB+19]. A useful feature of this cost is that symmetry
is explicitly enforced by the difference term (Fi()Fj())2. We highlight this
importance later in Sec. 7.5.
In contrast, the local and global cost functions are inspired by other vari-
ational algorithm literature [LTOJ+19, CSAC20, CSV+21, KLP+19, SKCC20]
where their properties have been extensively studied, particularly in relation to bar-
ren plateaus (see Definition 20). Since our primary cost functions (Eq. (7.1), Eq.
(7.3)) are local in nature, they are less susceptible to barren plateaus than those
which are global. We confirm this in our case by explicitly proving that Eq. (7.1)
is efficiently trainable with a O(logN) depth hardware efficient anstze [CSV+21]
in Sec. 7.5.1.2.
In contrast to many previous applications, by the nature of quantum cloning,
VarQlone allows the local cost functions to have immediate operational meaning
(see Sec. 4.1.1), illustrated through the following example (using the local cost,
Eq. (7.1)) for 1 2 cloning:
CL () = Tr
| |j 1j
= CL() = 1
| |,1
| |,2
where E[FL] is the average fidelity ([SIGA05]) over the possible input states. The
final expression of CL() in the above equation follows from the expression of
fidelity when one of the states is pure. Similarly, the global cost function relates
to the global fidelity of the output state with respect to the input state(s).
To estimate the cost functions in practice, we can prepare a dataset of K
samples (drawn uniformly at random from S), {|k}Kk=1, and compute a Monte-
Carlo estimate of the expectation values:
CL() 1
|kk |,1
|kk |,2
However, there is one caveat which we have not explicitly addressed as yet. Due
to the hard limits on approximate quantum cloning, the above costs cannot have
a minimum at zero5, but instead at some finite value (say CoptL for the local cost).
It is for this reason why we view VarQlone as being either both a supervised,
and unsupervised algorithm, based on our definitions of these in Sec. 3.1. If
we have prior knowledge of the optimal cloning fidelity which can be achieved
(the correct answer), this can be incorporated to achieve zero-minimum cost
5This is in contrast to the cost functions we used for the Born machine, for which we could
achieve a minimum at zero by definition, or for other VQA applications such as variational quan-
tum compilation [KLP+19] or variational quantum state diagonalisation [LTOJ+19] where again
defining a zero-minimum cost function makes sense.
176 7. Practical quantum cryptanalysis by variational quantum cloning
functions and we can train with supervision. If not, we can still use VarQlone in
an unsupervised manner take the lowest value found to be the approximation of
the cost minimum. The former is likely to be more valuable to extract the most
hardware-friendly cloning circuit possible, whereas the latter viewpoint can be used
to traverse uncharted water and discover cloning circuits for unknown families.
7.2.2 Cost function gradients
As with the applications in the previous sections, we will again opt for a gradient
descent based approach to train the VarQlone PQC6. We will again assume the
same form for the ansatz unitary, U(); that it is composed of unitaries of the
form: exp(il), where  is a generator with two distinct eigenvalues. Given this,
we again use the parameter-shift rule (Eq. (4.13)) to derive analytic gradients. We
do this explicitly for the squared local cost, Eq. (7.3), and for brevity neglect the
gradient derivations for the others, since they are simpler and follow analogously.
For this cost function we have the gradient with respect to a parameter, l as:
Csq()
(F iLF
i ,l+2
i ,l2
j,l+2
j,l2
(1F iL)(F
i ,l+2
i ,l2
(7.5)
where F j,l/2L () denotes the fidelity of a particular state, |, with j
th re-
duced state of the VarQlone circuit which has the l th parameter shifted by /2.
We suppress the  dependence in the above. We give the proof of this in Ap-
pendix A.2.1.
Using the same method, we can derive the gradient of the local cost, Eq. (7.1)
with N output clones as:
CL()
i ,l/2
i ,l+/2
(7.6)
Finally, similar techniques result in the analytical expression of the gradient of the
global cost function:
CG()
l/2
l+/2
(7.7)
where FG := F (| |N ,) is the global fidelity between the parametrised out-
put state and an n-fold tensor product of input states to be cloned.
6This is one of the primary differences between our method and that of [JJB+19], which opts
for gradient-free optimisation using Nelder-Mead ([NM65])
7.2. Variational quantum cloning: cost functions and gradients 177
7.2.3 Asymmetric cloning
The local cost functions above are defined in a way to enforce symmetry in the
output clones. However, from the purposes of eavesdropping, this may not be
the optimal attack for Eve to implement (or just generally one may not wish to
produce symmetric clones). In particular, she may wish to impose less disturbance
on Bobs state so she reduces the chance of getting detected. This subtlety was
first addressed in [Fuc96, FGG+97].
For example, if she wishes to leave Bob with a fidelity parametrised by a specific
value, p (given by F p,BL = 1p
2/2), one may define an asymmetric version of the
1 2 squared local cost function in Eq. (7.3):
CL,asym() := E
L ()
L ()
(7.8)
Given a particular value for Bobs fidelity, Eves fidelity must obviously be similarly
constrained. The corresponding value for Eves fidelity (F p,EL ) in this case can be
derived from the no-cloning inequality  [SIGA05]:
(1F p,BL )(1F
 (1F p,BL ) (1F
L ) (7.9)
where the output clones of Bob and Eve are denoted by F p,BL and F
L for the
desired parameterisations p and q.
It can be easily verified that the following fidelities saturate the above inequal-
L = 1
L = 1
, p,q  [0,1], (7.10)
where p,q satisfy p2+q2+pq=1. This implies that Eve is free to choose a desired
fidelity for either clone, by varying the parameter, p. For example, suppose Eve
wish to send a clone to Bob with a particular fidelity F p,BL = 1p
2/2, then from
Eq. (7.9) her clone would have a corresponding fidelity:
L = 1
(2p2p
43p2) (7.11)
which gives the term in Eq. (7.8) as a function of p.
The asymmetric cost function can also be naturally generalised to the case of
M N cloning. We note that CL,asym() can also be used for symmetric cloning
by enforcing p = 1
in Eq. (7.9). However, it comes with an obvious disadvan-
tage in the requirement to have knowledge of the optimal clone fidelity values
a-priori (hence, learning with this cost function would necessarily be supervised).
In contrast, our local cost functions (Eq. (7.1), Eq. (7.3)) do not have this
requirement, and thus are more suitable to be applied in general cloning scenarios,
since they can work in an unsupervised manner also.
178 7. Practical quantum cryptanalysis by variational quantum cloning
7.2.4 Cost function guarantees
As discussed above in Sec. 4.1.1 and in the previous chapters, we would also like
our variational algorithm to have certain theoretical guarantees. The primary one
we focus on here is faithfulness.
Since our cost functions, when defined in an unsupervised manner do not have
a minimum at zero, it is slightly more tricky to prove faithfulness guarantees about
them, than in other variational algorithms.We can still do so, but we must provide
slightly relaxed definitions of faithfulness. Specifically, we consider notions of
strong and weak faithfulness, relative to the error in our solution. In the following,
we denote ,jopt (
) to be the optimal (VarQlone learned) reduced state for qubit
j , for a particular input state, |  S. If the superscript j is not present, we mean
the global state of all clones.
Definition 50 (Strong faithfulness).
A cloning cost function, C, is strongly faithful if:
C() = Copt = 
opt, |  S (7.12)
where Copt is the minimum value achievable for the cost, C, according to
quantum mechanics.
Definition 51 (-weak local faithfulness).
A local cloning cost function, CL, is -weakly faithful if:
|CL()C
L |  = d(
opt) f (), |  S,j (7.13)
where d(, ) is a chosen metric in the Hilbert space between the two states
and f is some function of  (and perhaps other problem dimensions).
Definition 52 (-weak global faithfulness).
A global cloning cost function, CG, is -weakly faithful if:
|CG()C
G |  = d(
opt) f (), |  S (7.14)
One could also define local and global versions of strong faithfulness, but this
is less interesting so we do not focus on it here.
In the next few sections, we prove that each of our cost functions satisfy
these definitions of faithfulness, taking the metric to be the Bures angle, dBA, as
defined in Eq. (2.70) [Fub04, Stu05, NC10], and also the trace distance, dTr as
in Eq. (2.66). Let us begin with the squared cost function, where we go into
some details for the proofs, but we neglect this detail for the proofs of the other
cost functions, as they more or less follow straightforwardly.
7.2. Variational quantum cloning: cost functions and gradients 179
7.2.4.1 Squared cost function
We begin by writing the squared cost function as:
CMNsq () =
(1Fi())2+
(Fi()Fj())2
d (7.15)
where the expectation of a fidelity Fi over the states in distribution S is defined
as E[Fi ] = 1N
S Fi  d, with the normalisation condition being N =
S d. For
qubit states, if the normalisation is over the entire Bloch sphere in SU(2), then
N = 4. For notation simplicity, we herein denote the CMNsq () as Csq().
1. Strong faithfulness:
Theorem 18: The squared local cost function is locally strongly faithful:
Csq() = C
sq = 
opt |  S,j  [N] (7.16)
Proof. The cost function Csq() achieves a minimum at the joint maximum of
E[Fi()] for all i  [N]. In symmetric MN cloning, the expectation value of all
the N output fidelities peak at Fi = Fopt for all input states |. This corresponds
to a unique optimal joint state ,jopt = Uopt|
M0|NM|M0|NMUopt for
each |  S, where Uopt is the optimal unitary. Since the joint optimal state and
the corresponding fidelities are unique for all |  S, we conclude that the cost
function achieves a minimum under precisely the unique condition i.e. E[Fj()] =
Fopt for all j  [N]. This condition implies that,
opt, |  S,j  [N] (7.17)
We note that since Fopt is the same for all the reduced states j  [N], this implies
that the optimal reduced states are all the same for a given |  S. Therefore,
perfectly optimising the cost function results in the ideal clones being outputted.
2. Weak faithfulness:
Before proving weak faithfulness for the cost function, we first need the fol-
lowing lemma:
Lemma 2: Suppose the cost function is -close to the optimal cost in sym-
metric cloning
Csq()Coptsq   (7.18)
then we have,
opt
)||
2(1Fopt)
, |  S,j  [N] (7.19)
180 7. Practical quantum cryptanalysis by variational quantum cloning
Proof. In M N symmetric cloning, the optimal cost function value is achieved
when each output clone achieves the fidelity Fopt. Thus, using Eq. (7.15), the
optimal cost function value is Coptsq = N(1Fopt)2
The optimal cost function corresponds to all output clones having the same
fidelity. This is explicitly enforced by taking the limit  0, in which case the
difference terms of Eq. (7.15) vanish. Thus, the cost function explicitly enforces
the symmetry property. Let us assume  0, and consider the quantity Csq()
Coptsq :
Csq()Coptsq =
(1Fi())2+
(Fi()Fj())2
dN  (1Fopt)2
(1Fj())2N  (1Fopt)2
(FoptFj())(2FoptFj())
2(1Fopt)
(FoptFj())
2(1Fopt)
Tr[(,jopt
)||]d
(7.20)
The second line follows since Fopt is the same for each input state, |. Utilizing
the inequality in Eq. (7.18) and Eq. (7.20), we obtain,
opt
)||
2(1Fopt)
= Tr
opt
)||
2(1Fopt)
, |  S,j  [N]
(7.21)
Using the above lemma, we can prove the following two theorems for the
squared local cost function:
7.2. Variational quantum cloning: cost functions and gradients 181
Theorem 19: The squared cost function as defined Eq. (7.15), is -weakly
faithful with respect to the Bures angle distance measure dBA. In other
words, if we have:
Csq()Coptsq   (7.22)
where Coptsq := (Fi()Fj())2 =N(1Fopt)2 is the optimal cost, then the
following fact holds:
dBA(
opt)
2(1Fopt)sin(Fopt)
:= f1(), |  S,j  [N]
(7.23)
The proof of this is lengthy so we defer it to Appendix A.2.2.
To conclude this discussion, let us compute the constants in the special case
of phase-covariant states in Eq. (2.83). For this example, we have N = 4 and
dBA(
opt) 56  (7.24)
Now, we can also prove a similar result relative to the trace distance Eq. (2.66).
Contrary to the previous result, the following only holds true when the input states
are qubits7.
Theorem 20: The squared cost function, Eq. (7.15), is -weakly faithful
with respect to the trace distance dTr.
dTr(
opt,
) g1(), j  [N] (7.25)
where:
g1()
4Fopt(1Fopt)+
N (12Fopt)
2(1Fopt)
(7.26)
Again, the proof of this is lengthy so we defer it to Appendix A.2.3. An
immediate consequence of Eq. (7.26) is that there is a non-vanishing gap of
Fopt(1Fopt) between the states. This is due to the fact that the two output
states are only -close in distance when projected on a specific state |. However,
the trace distance is a projection independent measure and captures the maximum
over all the projectors.
7This is because we explicitly use the properties of a single qubit unitary. In contrast, for the
Bures angle proof of Theorem 19, we did not need to make any such assumption, and so would
also hold for higher dimensional quantum states, i.e. qudits.
182 7. Practical quantum cryptanalysis by variational quantum cloning
7.2.4.2 Local cost function
Next, we prove analogous results for the local cost function, defined for M  N
cloning to include the distribution S over the input states is,
CL() := E
Fj()
Fj()d (7.27)
where N =
S d is the normalisation condition. As above, we can show this cost
function also exhibits strong faithfulness:
1. Strong faithfulness:
Theorem 21: The squared local cost function is locally strongly faithful:
CL() = C
L = 
opt |  S,j  [N] (7.28)
Proof. Similar the faithfulness arguments of the squared cost function, one can
immediately see that the cost function CL() achieves a unique minimum at the
joint maximum of E[Fj()] for all j  [N]. Thus, the minimum of CL() corre-
sponds to the unique optimal joint state with its unique local reduced states ,jopt
for each j  [N] for each input state |  S. Thus the cost function achieves a
minimum under precisely the unique condition i.e. the output state is equal to the
optimal clone state.
2. Weak faithfulness:
Now, we can also prove analogous versions of weak faithfulness. Many of the
steps in the proof follow similarly to the squared cost derivations above, so we
omit them for brevity where possible. As above, we first have the following lemma:
Lemma 3: Suppose the cost function is -close to the optimal cost in sym-
metric cloning
CL()C
L   (7.29)
where we assume lim0 |E[Fi()]  E[Fj()]|  0,i , j , and therefore
Copt := 1Fopt. Then,
Tr[(
opt
)||]N , |  S,j  [N] (7.30)
The proof of Lemma 3 follows identically to Lemma 2, but with the exception
that we can write CL()C
L =E(FoptF ()) in the symmetric case, assuming
Fi() Fj(), i = j  [N].
Now, we can prove the following theorem:
7.2. Variational quantum cloning: cost functions and gradients 183
Theorem 22: The local cost function, Eq. (7.27), is -weakly faithful with
respect to dBA. If we have
CL()C
L   (7.31)
where CoptL := 1Fopt then the following fact holds:
dBA(
opt)
sin(Fopt)
=: f2(), |  S,j  [N] (7.32)
Proof. We rewrite the Eq. (7.30) in terms of the Bures angle:
opt, |)F (
, |)N 
= cos2(dBA(
opt, |))cos
2(dBA(
, |))N  (7.33)
Following the derivation in Sec. 7.2.4.1, we obtain:
dBA(
opt)
sin(Fopt)
, |  S,j  [N] (7.34)
Finally, we have Theorem 23 relating to the trace distance. The proof follows
identically to Theorem 20 so we simply state the result:
Theorem 23: The local cost function, Eq. (7.27), is -weakly faithful with
respect to dTr on qubits.
dTr(
opt,
4Fopt(1Fopt)+N (12Fopt) =: g2(), j  [N]
(7.35)
7.2.4.3 Global cost function
Finally, we show in the next theorems that the global cost function exhibits similar
notions of faithfulness:
Theorem 24: The global cost function is globally strongly faithful, i.e.:
CG() = C
G = 
opt |  S (7.36)
Proof. The global cost function CG() achieves the minimum value C
G at a
unique point corresponding to E[FG()] = F
G , where F
G corresponds to the
fidelity term for CoptG . This corresponds to the unique global clone state 
Thus the cost function, achieves a unique minimum under precisely the unique
condition i.e. the output global state is equal to the optimal clone state for all
inputs in the distribution.
184 7. Practical quantum cryptanalysis by variational quantum cloning
Now , as usual, moving the global faithfulness, we have
Lemma 4: Suppose the global cost is -close to optimality in symmetric
cloning
CG()C
G   (7.37)
where CoptG := 1F
G . Then,
opt
)| |N
N , |  S (7.38)
Proof. The proof of Lemma 4 follows identically to Lemma 3 but with the excep-
tion that CG()C
G = E[F
G FG()].
7.2. Variational quantum cloning: cost functions and gradients 185
Finally, we also have weak faithfulness:
Theorem 25: Suppose the cost function is -close to the optimal cost in
symmetric cloning
CG()C
G   (7.39)
where CoptG := 1F
G . Then we have, |  S:
dBA(
opt)
sin(F
=: f4() (7.40)
dTr(
opt,
G (1F
G )+N (12F
G ) =: g4() (7.41)
Proof. The proof of Theorem 25 follows along the same lines as that of Theo-
rem 22 and Theorem 23.
7.2.4.4 Global versus local faithfulness
This section explores the relationship between local and global cost function op-
timisation for different cloners (universal, phase-covariant, etc.). In particular, we
address the question of whether optimizing a cloner with a local or a global cost
function also achieves an optimal solution relative to the other cost (operational
meaning). If the answer is affirmative, we can use whichever cost exhibits the
most desirable qualities and be confident they will achieve the same results. If
not, we must be more careful as the choice may not lead to the optimal behaviour
we desire and will be application dependent.
We note that this relationship only manifests in symmetric cloning, since there
is no possibility to enforce asymmetry in the global cost function. As we shall see
in Eq. (7.55), the only way to enforce asymmetry is by constructing a cost function
which optimises with respect to the local asymmetric optimal fidelites.
The tradeoff between local and global faithfulness turns out to be subtle when
dealing with cloning problems, and is in contrast to similar studies in analogous
variational algorithm literature. Let us begin this discussion with the following
theorem (proof given in Appendix A.2.4):
Theorem 26: For the general case of MN cloning, the global cost func-
tion CG() and the local cost function CL() satisfy the inequality,
CL() CG() N CL() (7.42)
A similar inequality was proven in the work of, for example, [BPLC+19]. We
however note that the inequality proven in Theorem 26 (unlike in [BPLC+19])
does not allow us make statements about the similarity of individual clones from
186 7. Practical quantum cryptanalysis by variational quantum cloning
the closeness of the global cost function and vice versa. This can be seen as
follows:
CG()C
G   = CL()C
L   (CG()CL())+(C
= CL()C
L  +(C
 CL()C
L  
(7.43)
Here we have used the result of Theorem 26 that CG()  CL() and we note
that CoptL C
G = 0 for all the M  N cloning. In particular, for 1 2 cloning,
CoptL = 5/6, while C
G = 2/3. This is due to the non-vanishing property of these
cost functions, even at the theoretical optimum, and highlights the subtlety of the
case in hand.
While we are unable to leverage generic inequalities for our purpose based on
the cost functions, we can make statements in specific cases. In other words, by
restricting the cloning problem to a specific input set of states, we can guarantee
that optimizing globally will be sufficient to also optimise local figures of merit.
In particular, in the following we establish this strong and weak faithfulness
guarantees for the special cases of universal and phase-covariant cloning by analysing
problem-specific features.
Theorem 27: The global cost function is locally strongly faithful for a uni-
versal symmetric cloner, i.e,:
CG() = C
G  
opt |  H,j  {1, . . . ,N} (7.44)
Proof. In the symmetric universal case, CoptL has a unique minimum when, each
local fidelity saturates:
M(N+2)+NM
N(M+2)
(7.45)
achieved by local reduced states, {,jopt}
j=1. Now, it has been shown that the
optimal global fidelity FG that can be reached [BH98, SIGA05] is,
N(M+1)
M(N+1)
(7.46)
which also is the corresponding unique minimum value for CoptG , achieved by some
global state opt.
Finally, it was proven in [Wer98, KW99] that the cloner which achieves one
of these bounds is unique and also saturates the other, and therefore must also
achieve the unique minimum of both global and local cost functions, CoptG and
CoptL . Hence, the local states which optimise C
L must be the reduced density
matrices of the global state which optimises CoptG and so:
opt := Trj(
opt), j (7.47)
7.2. Variational quantum cloning: cost functions and gradients 187
Thus for a universal cloner, the cost function with respect to both local and global
fidelities will converge to the same minimum.
Now, before proving an analogous statement in the case of phase-covariant
cloning, we first need the following lemma (we return to the notation of B,E and
E for clarity):
Lemma 5: For any 1  2 phase-covariant cloning machine which takes
states |0B  |E and an ancillary qubit |AE as input, where | :=
(|0+e i|1), and outputs a 3-qubit state |BEE in the following form:
|BEE=
|0,0+e i(sin|0,1+cos|1,0))|0E
+e i|1,1+(cos|0,1+sin|1,0)|1E
(7.48)
the global and local fidelities are simultaneously maximised at  = 
where
0   
is the shrinking factor.
Proof. To prove this, we follow the formalism adopted by [CIVA02]. This uses the
fact that a symmetric phase-covariant cloner induces a mapping of the following
form [SIGA05]:
|0|0|0  |0|0|0
|1|0|0  (sin|0|1+cos|1|0)|0
|0|1|1  (cos|0|1+sin|1|0)|1
|1|1|1  |1|1|1
(7.49)
Next, we calculate the global state by tracing out the ancillary state to get optG :
G = TrE(|BEEBEE|) = |11|+ |22| (7.50)
where:
|1 :=
|0,0+e i(sin|0,1+cos|1,0)
|2 :=
e i|1,1+(cos|0,1+sin|1,0)
] (7.51)
Therefore, the global fidelity can be found as:
G = Tr(||
G ) = |
2 |1|2+ |2 |2|2 =
(1+sin+cos)2
(7.52)
Now, optimising F optG with respect , we see that F
G has only one extremum
between [0, 
] specifically at = 
. We can also see that the optimal local fidelity
188 7. Practical quantum cryptanalysis by variational quantum cloning
is also achieved for the same  and is equal to:
(7.53)
which is the upper bound for local fidelity of the phase-covariant cloner.
With Lemma 5 established, we can prove:
Theorem 28: The global cost function is locally strongly faithful for phase-
covariant symmetric cloning, i.e.:
CG() = C
G  
opt |  S,j  {B,E} (7.54)
where S is the distribution corresponding to phase-covariant cloning.
Proof. Now, we have in Lemma 5 that the global and local fidelities of a phase-
covariant cloner are both achieved with a cloning transformation of the form in Eq.
(7.49). Applying this transformation unitary to ||+BE (where |+BE is a
Bell state) leads to Cerfs formalism for cloning. Furthermore, we can observe
that due to the symmetry of the problem, this transformation is unique (up to
global phases) and so any optimal cloner must achieve it.
Furthermore, one can check that the ideal circuit in Fig. 2.4 does indeed
produce an output in the form of Eq. (7.48) once the preparation angles have
been set for phase-covariant cloning. By a similar argument to the above, we
can see that a variational cloning machine which achieves an optimal cost value,
i.e. CG() = C
G must also saturate the optimal cloning fidelities. Furthermore,
by the uniqueness of the above transformation (Eq. (7.49)) we also have that
the local states of VarQlone are the same as the optimal transformation, which
completes the proof.
7.2.4.5 Asymmetric faithfulness
Finally, let us prove the analogous results for the asymmetric cost function, Eq.
(7.8), which can also be written as:
CL,asym() =
L ())
L ())
d (7.55)
1. Strong faithfulness:
Theorem 29: The asymmetric 1 2 local cost function is strongly faithful:
CL,asym() = C
L,asym() = 
opt |  S,i  {B,E} (7.56)
7.2. Variational quantum cloning: cost functions and gradients 189
Proof. The cost function CL,asym() achieves the minimum value of zero, uniquely
when FBL () = F
L and F
L () = F
L for all input states |  S. This corre-
sponds to the unique reduced states ,Bopt and 
opt for Bob and Eve. Thus the
cost function, achieves a unique minimum of zero precisely when the output re-
duced state for Bob and Eve is equal to the optimal clones for all inputs in S.
2. Weak faithfulness:
Returning again to -weak faithfulness, we get similar results as in the sym-
metric case above (explicit proof given in Appendix A.2.5):
Theorem 30: The asymmetric cost function, Eq. (7.55), is -weakly faith-
ful with respect to dBA
CL,asym()C
L,asym   (7.57)
where CoptL,asym = 0. then the following fact holds for Bob and Eves reduced
states:
dBA(
opt )
sin(1p2/2)
, dBA(
opt )
sin(1q2/2)
(7.58)
Furthermore, we also have the following trace distance bounds:
dTr(
,B,
p2(2p2)
N (1p2), (7.59)
dTr(
,E,
q2(2q2)
N (1q2) (7.60)
7.2.5 Sample complexity of the algorithm
As discussed above (Sec. 4.1.1) we require the cost functions to be efficiently
computable in general, which is why we went to such pains particularly in Chap-
ter 6. However, luckily for VarQlone, the situation is more straightforward. This
is because there is a simple routine to estimate the fidelities in the cost functions,
given by the SWAP test Eq. (2.27) and for VarQlone more specifically in Fig. 7.2.
This works since at least one of the states in the SWAP test is pure in VarQlone.
Given a particular cost function, C(), for an input state, |, to be cloned,
let L be the number of copies of | to estimate C(). Then, let K be the
number of states from S required to estimate the expectation value over |,
C() = E|S [C
()].
The number of states LK is given by the following theorem.
190 7. Practical quantum cryptanalysis by variational quantum cloning
Figure 7.2: The SWAP test circuit illustrated for 1 2 cloning. In (a) for example, we compare
the global state 
, with the state |, where | := | | is the product state of two copies
of . (b) Local SWAP test with the reduced state of Bob and Eve separately. One ancilla is
required for each fidelity to be computed. The generalisation for N input states in MN cloning
is straightforward.
Theorem 31: [Sample Complexity of VarQlone] The number of samples
LK required to estimate the cost function C() up to -additive error
with a success probability  is:
LK =O
(7.61)
where K is the number of distinct states | sampled uniformly at random
from the distribution S, and L is the number of copies of each input state.
We give the proof of this in Appendix A.2.6 for the global cost function, which
is a straightforward application of Heffdings inequality [Hoe63]. The proof and
theorem could also be adapted straightforwardly to the other cost functions.
Finally, we note that the SWAP test, in practice, is somewhat challenging to
implement on NISQ devices, predominately due to the overhead of compiling the
3-local controlled SWAP into the native gateset of a particular quantum hardware
(In our numerical results in Sec. 7.5, we will circumvent the SWAP test by directly
extracting the quantum states via tomography to compute the required fidelities.)
Furthermore, in this case, we have a strict need for the copy of the input state |
to be kept coherent while implementing the SWAP test, due to the equivalence
between fidelity and overlap if one state is pure. This is due to the fact that
for mixed quantum states, there is no known efficient method to compute the
fidelity [Wat02] and one must resort to using bounds on it8. In light of this,
one could use the shorter depth circuits to compute the overlap found using
a variational approach similar to that implemented here [CSSC18]. This is the
interesting consequence that if we were to try and perform broadcasting [BCF+96]
instead of cloning, we would again need to use an alternative strategy to estimate
the fidelities.
8Perhaps one may also use bounds discovered variationally [CFUZ02, MNM+08, PM09,
CSAC20].
7.3. Variational quantum cryptanalysis 191
7.3 Variational quantum cryptanalysis
Now that we have introduced VarQlone, we can begin to discuss one of the main
additional applications for it: quantum cryptanalysis. We do this for two spe-
cific types of quantum cryptographic protocols, each of which uses one family of
state-dependent cloning states introduced in Sec. 2.2. We begin in the following
sections by introducing the particular protocols we study here and providing the-
oretical analyses about them. We then demonstrate how VarQlone can be used
to implement the attacks we propose. Using VarQlone as a quantum machine
learning toolkit in this way to supplement quantum cryptography, we refer to as
a first step into what we dub variational quantum cryptanalysis.
We already discussed at the beginning of this chapter the intimate relationship
between quantum cloning and cryptography, and now Fig. 7.3 demonstrates at
a high level how VarQlone may be inserted into an attack (on a QKD protocol
specifically). The reader should keep this image in mind as we progress through
the following sections, as it is useful for providing intuitions.
Figure 7.3: Cartoon overview of VarQlone in a cryptographic attack. Here an adversary Eve,
E, implements a 1 2 cloning attack on states used in a quantum protocol (for example QKD)
between Alice and Bob. Eve intercepts the states sent by Alice |A and may interact with an
ancillary environment, E. This interaction is trained (an optimal parameter setting  is found)
by Eve to optimally produce clones, B ,
 . In order to attack the protocol, Eve will return 
Bob and use the rest (her clone, E plus the remaining environment state, 
 ) to cheat. The
training procedure consists of using a classical computer to optimise the quantum parameters, via
a cost function. The cost is a function of k observables, Ok , measured from the output states,
which are designed to extract fidelities of the states to compare against the ideal state.
7.3.1 Quantum key distribution and cloning attacks
Let us begin with quantum key distribution protocols, and specifically the Bennett-
Brassard 84 (BB84) [BB14] protocol. In this protocol, one party (say Alice), sends
single qubit states in two orthogonal bases (for instance, the eigenstates of the
Pauli X and Pauli Y matrices, | and | i) to a second party, Bob, via a quantum
192 7. Practical quantum cryptanalysis by variational quantum cloning
channel that is susceptible to an eavesdropping adversary, Eve. Eves goal is to
extract the secret information sent between Alice and Bob, encoded in the states.
Firstly, let us clarify the types of attacks one may consider on such protocols.
The simplest attack by Eve is a so-called incoherent or individual attack, where
Eve only interacts with the quantum states one at a time, and in the same fashion,
and does so before the reconciliation phase of the protocol. In such attacks, the
security condition states that a secret key can no longer be extracted if Eve has
as much information about the states as Bob, then a secret key can no longer be
extracted. As such, the protocol requires a critical error rate, Dcrit, above which
Alice and Bob will detect malfeasance, and abort the protocol.
For incoherent attacks, this error rate is achieved if the fidelity of the states
received by Bob and stored by Eve is the same compared to the original state
sent by Alice. Therefore, for BB84, we have the critical error rate to be Dincohcrit =
1FPC,EL,opt = 14.6%.
However, this criteria does not allow for comparison between a cloning machine
using the ancilla, and one without9. This is important since, as discussed above
and in [SIGA05], a phase-covariant cloning machine with ancilla provides the op-
timal attack on both Alice and Bob. In contrast, one without an ancilla retains
no information with which Eve can use to attack Bobs side of the protocol.
In order to have a better comparison we analyse the key rate, given by the
following expression [SIGA05]:
R = I(A : B)min{(A : EQ),(B : EQ)} (7.62)
Here, we have the term , which is the so-called Holevo quantity, written in terms
of the von Neumann entropy (see Eq. (2.74)):
(Q : E) := S(E)
S(0E)
S(1E) (7.63)
In Eq. (7.62), we also have I(A :B), which is the mutual information (MI) between
Alice and Bob and the index Q denotes that Eve may employ general quantum
strategies. In Eq. (7.63), E denotes the mixed state of Eve over all of the
combinations of Alices choice of input, and 0E and 
E denotes the states of Eve
for the random variables that encode 0 and 1 in the protocol respectively.
To compute the critical error rate from this expression, Dcrit, it is enough to
calculate the Holevo quantity for Eve, set R = 0, and I(A :B) = 1H(Dcrit) and
to solve the resulting equation for Dcrit. In Sec. 7.5, we return to this calculation
for the cloning transformations we learn using VarQlone and we shall see how the
algorithm is able to approximately saturate the optimal bound for the best possible
individual attack.
9Recall in Sec. 2.2 we mentioned how phase-covariant cloning could be optimally achieved in
both cases.
7.4. Quantum coin flipping and cloning attacks 193
7.4 Quantum coin flipping and cloning attacks
Next, we move in this section to the primitive of quantum coin flipping [MSCK99,
ATSVY00] and the use of states which have a fixed overlap. Protocols of this na-
ture are a nice case study for our purposes since they provide a testbed for cloning
fixed-overlap states, and in many cases explicit security analyses are missing. In
particular in this work, to the best of our knowledge, we provide the first purely
cloning based attack on the protocols we analyse. We go into some detail in the
following in order to properly introduce our explicit attacks and the corresponding
analyses of their success.
The two protocols we consider are that of Mayers et al. [MSCK99]) and that of
Aharonov et al. [ATSVY00]. Before introducing these protocols, we first describe
more generally the goals of coin flipping.
7.4.1 Quantum coin flipping
In (quantum) coin flipping protocols, we have two parties, Alice and Bob (no Eve
this time) who share a mutual distrust, but who wish to agree the outcome of a
random coin flip. The most relevant quantity in such protocols is the bias of the
resulting coin. A biased coin has one outcome more likely than the other, for
example with the following probabilities:
Pr(y = 0) =
+, Pr(y = 1) =
 (7.64)
where y is a bit outputted by the coin. We can associate y = 0 to heads (H) and
y = 1 to tails (T). A fair coin would correspond to = 0, while the above coin is
-biased towards H.
It has been shown that it is impossible10 in an information theoretic manner, to
achieve a secure coin flipping protocol with =0 in both the classical and quantum
setting [Blu83, LC98, MSCK99]. Furthermore, there are two notions of coin
flipping studied in the literature: weak coin flipping, (where it is a-priori known that
both parties prefer opposite outcomes), and strong coin flipping, (where neither
party knows the desired outcome of the other party). In the quantum setting,
the lowest possible bias achievable by any strong coin flipping protocol is limited
by  0.207 [Ale03]. Although several different protocols have been suggested for
-biased strong coin flipping [MSCK99, ATSVY00, BB14, BBBG09], the states
used in them share a common structure.
7.4.1.1 Quantum states for strong coin flipping
Multiple qubit coin flipping protocols utilise the following set of states:
|x,a=
|x,0= cos|0+(1)x sin|1
|x,1= sin|0+(1)x1 cos|1
(7.65)
10Meaning it is not possible to define a coin flipping protocol such that neither party can enforce
any bias.
194 7. Practical quantum cryptanalysis by variational quantum cloning
where x {0,1}. This set of states are all conveniently related through a reparametri-
sation of the angle  [BM08], which makes them easier to deal with mathemati-
cally.
On top of using the same family of states, many coin flipping protocols also
have a common structure in the protocol itself. To begin, Alice encodes some
random classical bits into a subset of the above states and Bob may do the same.
Each party will then exchange classical or quantum information (or both) as part
of the protocol. Attacks (attempts to bias the coin) by either party usually reduce
to how much one party can learn about the classical bits of the other.
We explicitly treat two cases:
1. The protocol of Mayers et al. [MSCK99] in which the states, {|0,0, |1,0}
are used (which have a fixed overlap s = cos(2)). We denote this protocol
2. The protocol of Aharonov et al. [ATSVY00], which uses the full set, i.e.
{|x,a}. We denote this protocol P2.
In all strong coin flipping protocols, the security or fairness of the final shared
bit lies on the impossibility of perfect discrimination of the underlying non-orthogonal
quantum states. In general, the protocol can be analysed with either Alice or Bob
being dishonest. Here we focus, for illustration, on a dishonest Bob who tries to
bias the bit by cloning the non-orthogonal states sent by Alice.
For all of the below, the biases are computed assuming access to the ideal
cloning machine (i.e. the one which clones the input states with the optimal,
analytic fidelities). In Sec. 7.5, we compare these ideal biases with those achievable
using the quantum cloning machines learned by VarQlone.
7.4.2 2-state coin flipping protocol (P1)
The first quantum coin flipping protocol we consider is one of the earliest proposed
by [MSCK99]. The protocol was originally described with k rounds, but for
simplicity we primarily analyse the version with a single round, k = 1. This is
sufficient to describe a cloning based attack, which we discuss in the next section.
In this protocol, Alice sends the states11 |0 := |0,0 and |1 := |1,0 such
that the angle between them is  := 
= s := cos(
). Let us first describe the
general protocol.
7.4.2.1 P1 with k rounds
To begin, Alice and Bob choose k random bits, {a1, . . . ,ak} and {b1, . . . ,bk}
respectively. The final bit is then given by the XOR of input bits over all k rounds
11Since the value of the overlap is the only relevant quantity, the slightly different parameterisa-
tion of these states to those of Eq. (7.65) does not make a difference for our purposes. However,
we note that explicit cloning unitary would be different in both cases.
7.4. Quantum coin flipping and cloning attacks 195
i.e.,
bj (7.66)
In each round j = 1, . . . ,k of the protocol, and for every step i = 1, . . . ,n within
each round, Alice uniformly picks a random bit ci ,j and sends the state
12 |i ,jc  :=
|ci ,j  |ci ,j ) to Bob. Likewise, Bob uniformly picks a random bit di ,j and sends
the state |i ,j
 := |di ,j |di ,j  to Alice. Hence, each party sends multiple copies
of either |0 |1 or |1 |013.
In the next step, for each j and i , Alice announces the value ajci ,j . If it turns
out that aj ci ,j = 0, Bob returns the second state of the pair (i , j) back to Alice,
and sends the first state otherwise. Similarly Bob announces bj  di ,j and Alice
returns one of the states back to Bob accordingly. Now, we come to why it is
sufficient to only consider a single round in the protocol from the point of view of
a cloning attack. This is because a dishonest Bob can bias the protocol if he learns
about Alices bit aj , which he can do by guessing ci ,j with a probability better than
1/2. With this knowledge, Bob only needs to announce a single false bj di ,j in
order to cheat, and so this strategy can be deferred to the final round [MSCK99].
Hence a single round of the protocol is sufficient for analysis, and we herein drop
the j index.
In the last phase of the protocol, after a and b are announced by both sides (so
y can be computed by both sides), Alice measures the remaining states with the
projectors, (Eb,E
b ) and the returned states by Bob with (Ea,E
a ) (Eq. (7.67)).
She aborts the protocol if she gets the measurement result corresponding to ,
and declares Bob as being dishonest. In this sense, the use of quantum states in
this protocol is purely for the purpose of cheat-detection.
El = |ll |n, El = 1|ll |
n, l  {0,1} (7.67)
7.4.2.2 A cloning attack on P1
Now we are in a position to discuss an explicit attack that can be implemented by
Bob on P1. WLOG, we assume that Bob wishes to bias the bit towards y = 0.
For clarity, we give the attack for when Alice only sends one copy of the state
(n = 1), but we discuss the general case in the next section:
Attack 1: Cloning attack on P1 with k = 1.
Inputs. Random bit for Alice (aR {0,1}) and Bob (bR {0,1}). Bob receives
a state |ic.
12We use the notation x to denote the conjugate of bit x , i.e. 0 = 1,1 = 0.
13Note that if ci ,j and di ,j are chosen independently of aj and bj , no information about the
primary bits has been transferred.
196 7. Practical quantum cryptanalysis by variational quantum cloning
Goal. A biased bit towards 0, i.e. p(y = 0)> 1/2.
The attack:
1. for i = 1, . . . ,n:
(a) Step 1: Alice announces a ci . If a ci = 0, Bob sends the second
qubit of |ic to Alice, otherwise he sends the first qubit.
(b) Step 2: Bob runs a 1 2 state-dependent (fixed-overlap) cloner on
the qubit he has to return to Alice, producing two approximate clones.
He sends her one clone and keeps the other.
(c) Step 3: Bob runs an optimal state discrimination on the remaining
qubit and any other output of the cloner, and finds c1 with a maxi-
mum success probability P opt
disc,P1. He then guesses a bit a
 such that
Psucc,P1(a
 = a) := P
disc,P1.
(d) Step 4: If ab= 0 he continues the protocol honestly and announces
bd1, otherwise he announces ad1. The remaining qubit on Alices
side is |ia.
We return to this attack in Fig. 7.8 where we give a cartoon illustration,
plus the corresponding VarQlone numerics. Now, we can evaluate the success
probability of Bobs attack with the following theorem:
Theorem 32: [Ideal cloning attack bias on P1]
Bob can achieve a bias of   0.27 using an ideal state-dependent cloning
attack on the protocol, P1 with a single copy of Alices state.
Proof. As mentioned in the previous section, the final measurements performed
by Alice on her remaining n states, plus the n states returned to her by Bob allow
her to detect his nefarious behaviour. If he performed a cloning attack, the 
outcomes would be occasionally detected by Alice. We must compute both the
probability that he is able to guess the value of Alices bit a (by guessing the value
of the bit c1), and the probability that he is detected by Alice. This would provide
us with Bobs final success probability in cheating, and hence the bias probability.
At the start of the attack, Bob has a product state of either |0 |1 or
|1 |0 (but he does not know which). In Step 2, depending on Alices an-
nounced bit, Bob proceeds to clone one of the qubits, sends one copy to Alice
and keeps the other to himself. We again assume the announced bit of Alice is
0. In this case, at this point in the attack, he has one of the following pairs:
|00|  1c or |11|  0c , where 1c and 0c are the approximate clones for
|1 and |0 respectively.
Bob must now discriminate between the following density matrices:
1 := |00| |11| and 2 := |11|0c (7.68)
7.4. Quantum coin flipping and cloning attacks 197
Alternatively, if Alice announced aci = 1, he would have:
1 := |11| |00| and 2 := |00|1c (7.69)
In either case, we have that the minimum discrimination error for two density
matrices is given by the Holevo-Helstrom bound [Hol73, Hel69] as follows14:
||12||Tr =
dTr(1,2) (7.70)
The ideal symmetric cloning machine for these states will have an output of the
form:
c = |00|+|11|+(|01|+ |10|) (7.71)
Where , and  are functions of the overlap s = 0 |1 = cos 9 . Now, using
Eq. (7.68), 2 can be written as follows:
2 = |11| |00|+|11| |11|
+(|11| |01|+ |11| |10|) (7.72)
Finally by plugging in the values of the coefficients in Eq. (7.71) for the optimal
local cloning machine [BDE+98] and finding the eigenvalues of  := (12), we
can calculate the corresponding value for Eq. (7.70), and recover the following
minimum error probability:
Pfail,P1 = P
disc,P1 = 1P
disc,P1  0.214 (7.73)
This means that Bob can successfully guess c1 with P 1succ,P1 = 78.5% probability.
Now let us examine the probability of a cheating Bob being detected by Alice.
We note that whenever Bob guesses the bit, a, successfully, the measurements
(Eb,E
b ) will be passed with probability 1, hence we use (Ea,E
a ) where the states
sent by Bob will be measured. Using Eq. (2.88) with the value of overlap s =
cos(/9), the optimal fidelity is FL  0.997 and so the probability of Bob getting
caught is at most 1%. Putting this together with Bobs guessing probability for
a gives his overall success probability of 77.5%.
In the end, this implies that Bob is able to successfully create a bias of  
0.7750.5 = 0.275.
We also have the following corollary, for a general number of states (n) ex-
changed, which shows the protocol can be completely broken and Bob can enforce
an arbitrary bias:
Corollary 5: The probability of Bob successfully guessing a over all n copies
has the property:
P nsucc,P1 = 1 (7.74)
14This also is because the we assume a symmetric cloning machine for both |0 and |1. If this
is not the case, the guessing probability is instead the average of the discrimination probabilities
over both states.
198 7. Practical quantum cryptanalysis by variational quantum cloning
Proof. If Bob repeats Attack 1 over all n copies, he will guess n different bits
{ai}
i=1. He can then take a majority vote and announce b such that a
b = 0,
where we denote a as the bit he guesses in at least n
+1 of the rounds.
If n is even, he may have guessed a to be 0 and 1 an equal number of times.
In this case, the attack becomes indecisive and Bob is forced to guess at random.
Hence we separate the success probability for even and odd n as follows:
P nsucc,P1 =

k= n+12
(1Pfail)kP nkfail n odd,
k= n2+1
(1Pfail)kP nkfail +
(1Pfail)
n even
(7.75)
By substituting the value of Pfail one can see that the function is uniformly in-
creasing with n so lim
P nsucc,P1 = 1.
Although as Bobs success probability in guessing correctly increases with n,
the probability of his cheating strategy getting detected by Alice will also increase.
We also note that this strategy is independent of k , the number of different bits
used during the protocol.
7.4.3 4-state coin flipping protocol (P2)
Another class of coin flipping protocols are those which require all the four states
in Eq. (7.65). One such protocol was proposed by Aharonov et al. [ATSVY00],
where  is set as 
|x,a=
8 x,0
= cos
|0+(1)x sin
8 x,1
= sin
|0+(1)x1 cos
(7.76)
In protocols of this form, Alice encodes her bit in basis information of the
family of states. For instance, we can take {|0,0, |1,0} to encode the bit a= 0
and {|0,1, |1,1} to encode a=1. The goal again is to produce a final coin flip
y = ab, while ensuring that no party has biased the bit, y . A similar protocol
has also been proposed using BB84 states [BB14] where |0,0 := |0, |0,1 :=
|1, |1,0 := |+ and |1,1 := |. In this case, the states (also some protocol
steps) are different but the angle between them is the same as those in P2. A
fault-tolerant version of P2 has also been proposed in [BBBG09], which uses a
generalised angle as in Eq. (7.65).
The protocol proceeds as follows. First Alice sends one of the states, |x,a to
Bob. Later, one of two things will happen. Either, Alice sends the bits (x,a) to
Bob, who measures the qubit in the suitable basis to check if Alice was honest, or
Bob is asked to return the qubit |x,a to Alice, who measures it and verifies if it is
correct. Now, example cheating strategies for Alice involve incorrect preparation
of |x,a and giving Bob the wrong information about (x,a), or for Bob in trying
to determine the bits (x,a) from |x,a before Alice has revealed them classically.
We again focus only on Bobs strategies here to use cloning arguments. We note
7.4. Quantum coin flipping and cloning attacks 199
that the information theoretic achievable bias of  = 0.42 proven in [ATSVY00]
applies only to Alices strategy since she has greater control of the protocol (she
prepares the original state). In general, a cloning based attack strategy by Bob
will be able to achieve a lower bias, as we show. As above, Bob randomly selects
his own bit b and sends it to Alice. He then builds a QCM to clone all 4 states in
Eq. (7.76).
We next sketch the two cloning attacks on Bobs side of P2. Again, as with
the protocol, P1, Bob can cheat using as much information as he gains about
a and again, once Bob has performed the cloning, his strategy boils down to
the problem of state discrimination. In both attacks, Bob will use a (variational)
state-dependent cloning machine.
7.4.3.1 Cloning attacks on P2
In the first attack model (which we denote I - see Fig. 7.9(a) in Sec. 7.5.3) Bob
measures all the qubits outputted from the cloner to try and guess (x,a). As
such, it is the global fidelity that will be the relevant quantity. This strategy is
useful with the first possible challenge in the protocol, where Bob is not required
to send anything back to Alice. We discuss in Appendix 7.4.3.2 how the use of
cloning in this type of attack can also reduce resources for Bob from a general
POVM to projective measurements in the state discrimination, which may be of
independent interest. The main attack here boils down to Bob measuring the
global output state from his QCM using the projectors, {|vv |, |vv |}, and
from this measurement, guessing a. These projectors are constructed explicitly
relative to the input states using the Neumark theorem [BK15].
The second attack model (which we denote II, see Fig. 7.9(a) in Sec. 7.5.3)
is instead a local attack and as such will depend on the optimal local fidelity.
It may also be more relevant in the scenario where Bob is required to return a
quantum state to Alice. We note that Bob could also apply a global attack in this
scenario but we do not consider this possibility here in order to give two contrasting
examples. In the below, we compute a bias assuming he does not return a state
for Alice for simplicity and so the bias will be equivalent to his discrimination
probability. The analysis could be tweaked to take a detection probability for Alice
into account also. In this scenario, Bob again applies the QCM, but now he only
uses one of the clones to perform state discrimination (given by the Discriminator
in Fig. 7.9(a)).
200 7. Practical quantum cryptanalysis by variational quantum cloning
7.4.3.2 Attack I on P2
For attack I, which is a 4 state global attack on P2, we have:
Theorem 33: [Ideal cloning attack (I) bias on P2]
Using a cloning attack on the protocol, P2, (in attack model I) Bob can
achieve a bias:
IP2,ideal  0.35 (7.77)
We note first that this attack model (i.e. using cloning) can be considered a
constructive way of implementing the optimal discrimination strategy of the states
Alice is to send. In order to bias the bit, Bob needs to discriminate between the
four pure states in Eq. (7.65) or equivalently between the ensembles encoding a=
{0,1}, where the optimal discrimination is done via a set of POVM measurements.
However, by implementing a cloning based attack, we can simplify the discrimi-
nation. This is because the symmetric state-dependent cloner (which is a unitary)
has the interesting feature that for either case (a = 0 or a = 1), the cloners
output is a pure state in the 2-qubit Hilbert space. As such, the states (after
going through the QCM) can be optimally discriminated via a set of projective
measurements {Pv ,Pv}, rather than general POVMs (as would be the case if
the QCM was not used). So, using VarQlone to obtain optimal cloning strategies
also is a means to potentially reduce resources for quantum state discrimination
also. Now, let us prove Theorem 33:
Proof. The attack involves the global output state of the cloning machine. For
this attack we can use the fixed overlap 1 2 cloner with the global fidelity given
by Eq. (2.89):
FO,opt
G (1,2) =
1+ s3+
1 s2
1 s4
 0.983 (7.78)
where s = sin(2) = cos(
) for P2. Alternatively we can use the 4-state cloner
which clones the two states with a fixed overlap plus their orthogonal set. For
both of these cloners we are interested in the global state of the cloner which we
denote as |12x,a  for an input state |x,a.
In order for Bob to guess a he must discriminate between |0,0 (encoding
a = 0) and |1,1 (encoding a = 1) or alternatively the pair {|0,1, |1,0}. This
is since the pairs {|0,0, |0,1} are orthogonal and {|0,0, |1,0} both encode
a = 0, so the only choice is to discriminate between |0,0 and |1,1. Due to the
symmetry and without an ancilla, the cloner preserves the overlap between each
pairs i.e. 120,0 |
1,1 = 0,0 |1,1= s (we also have 
0,1 |
1,0 = s).
Now we select the projective measurements Pv = |vv | and Pv = |v
v| such
that v |v = 0. One can show that the discrimination probability is optimal
when |v and |v are symmetric with respect to the target states (illustrated in
Fig. 7.9(a)) according to the Neumark theorem. From the figure, we have that
v |v = 0 so 2+2 = 
  = 
. Finally, writing the cloners states for
7.4. Quantum coin flipping and cloning attacks 201
{|120,0 , |
1,1 } in the basis {|v, |v
} gives:
|120,0 = cos
|v+sin
|v,
|121,1 = cos
|v sin
(7.79)
where it can be checked that 120,0 |
1,1 = cos
= sin(2) = s. Hence
|v and |v can be explicitly derived. Note that these bases are also symmetric
with respect to the other pair i.e {|120,1 , |
1,0 }. Finally, the success probability
of this measurement is then given by:
opt,I
disc,P2 =
120,0 |
1,1 =
sin2= 0.853 (7.80)
which is the maximum cheating probability for Bob. From this, we derive the bias
IP2,ideal = P
opt,I
disc,P2
= 0.353 (7.81)
which completes the proof.
7.4.3.3 Attack II on P2
Finally, we consider a second attack model (attack II) on the protocol, P2, which
is in the form of a local attack. Here, we further consider two scenarios:
1. A cloning machine which is able to clone all 4 states: |0,0, |1,1 and
|0,1, |1,0,
2. A cloning machine tailored to only the two states, |0,0 and |1,1 (which
Bob needs to discriminate between).
We focus on the former scenario, since it connects more cleanly with the
VarQlone clone fidelities, but scenario 2 facilitates a more optimal attack (in the
ideal situation).
Scenario 1:
In this case, we can compute an exact discrimination probability, but it will
result in a less optimal attack.
Theorem 34: [Ideal cloning attack (II) bias on P2 in scenario 1.]
Using a cloning attack on the protocol, P2, (in attack model II with 4 states)
Bob can achieve a bias:
IIP2,ideal = 0.25 (7.82)
As a sketch proof, we explicitly evaluate the output states from an ideal cloner,
and then compute the success probability via the trace distance. The full proof
for this theorem is given in Appendix A.2.7.
202 7. Practical quantum cryptanalysis by variational quantum cloning
Scenario 2:
Here, we give a bound on the success probabilities of Bob in terms of the
local fidelities of the QCM where the cloning machine is only tailored to clone two
fixed-overlap states. Here we rely on the fact that Bob can discriminate between
the two ensembles of states (for a = 0, a = 1) with equal probabilities.
Theorem 35: The optimal discrimination probability for a cloning attack on
the protocol, P2, (in attack model II, with 2 states) is:
0.619 P opt,II
disc,P2  0.823 (7.83)
Proof. For each of the input states, |i ,j, in Eq. (7.76), we denote ci j to be a
clone outputted from the QCM. Due to symmetry, we only need to consider one
of the two output clones. We can now write the effective states for each encoding
(a = 0,a = 1) as:
(a=0) :=
(c00+
10), (a=1) :=
(c01+
11) (7.84)
Dealing with these two states is sufficient since it can be shown that discriminating
between these two density matrices, is equivalent to discriminating between the
entire set of 4 states in Eq. (7.65).
Again, we use the discrimination probability from the Holevo-Helstrom bound:
opt,II
disc,P2 := P
((a=0),(a=1)) :=
dTr((a=0),(a=1)) (7.85)
Now, we have:
dTr((a=0),(a=1)) =
(a=0)(a=1)Tr = 12
12(c00c11)+ 12(c10c01)
||(c00
11)||Tr+ ||(
01)||Tr
[dTr(
11)+dTr(
= P opt
((a=0),(a=1))
(c00,
11)+P
(c01,
(c00,
(7.86)
The last equality follows since for both ensembles, {|0,0, |1,1} and {|0,1, |1,0},
their output clones have equal discrimination probability:
(c00,
11) = P
(c01,
10) (7.87)
This is because the QCM is symmetric, and depends only on the overlap of the
states (we have in both cases 00 |11= 01 |10= sin(2)).
7.5. VarQlone numerics 203
Furthermore, since the cloning machine can only lower the discrimination prob-
ability between two states, we have:
(c00,
11) P
(c00, |1,11,1|) =: P
Now, using the relationship between fidelity and the trace distance (Eq. (2.73)),
we have the bounds:
1,1|c00|1,1
 P opt
11,1|c00|1,1 (7.88)
By plugging in the observed density matrix for the output clone, we can find this
discrimination probability. As in the previous section, the output density matrix
from the QCM for an output clone can be written as Eq. (7.71):
c00 = |0,00,0|+|1,11,1|+(|0,01,1|+ |1,10,0|) (7.89)
This state has a local fidelity, FL = 0,0|c00|0,0= + s
2+ s. On the other
hand, we have F (c00, |1,11,1 |) = s
2++ s.
Combining these two, we then have:
F (c00, |1,11,1 |) = FL+(s
21)() (7.90)
Plugging in FL from Eq. (2.88), and   =
1s4 (for an optimal state-
dependent cloner), we get:
FL+(s
1 s2
1 s4
 P opt,II
disc,P2 
1FL (s21)
1 s2
1 s4
(7.91)
To complete the proof, we use FL 0.989 and s =1/
2 which gives the numerical
discrimination probabilities above.
7.5 VarQlone numerics
In preceding sections, we introduced the VarQlone algorithm and its relationship
to cryptography at a high level. We also introduced (theoretical) cryptographic
attacks on several protocols. Here, we combine these two aspects and present
the corresponding numerical results.
Before diving back into the cryptographic applications, let us revisit some
specifics of the VarQlone algorithm, in particular the anstze we use.
The first two options are fixed-structure anstze. We include one which is
problem-inspired (based on the theoretical derivation) and one which is problem-
agnostic (a hardware-efficient fixed-structure (HEFS)) using the nomenclature
of Sec. 4.1.2. We then generalise to the primary one we opt for, a variable-
structure ansatz. Recall, in the latter case, both the continuous parameters and
the gates in the ansatz are optimised over (see Sec. 4.1.2).
204 7. Practical quantum cryptanalysis by variational quantum cloning
7.5.1 Fixed-structure anstze
To demonstrate the fixed-structure anstze, we use 1 2 phase-covariant cloning
as the primary example (recall this is the relevant cloning family to attack the BB84
protocol), which requires cloning the following states:
|xy ()=
|0+e i|1
(7.92)
7.5.1.1 Phase-covariant cloning with a fixed ideal ansatz
Let us begin with a problem-inspired option. Recall, the ideal circuit for performing
phase-covariant cloning is given by Fig. 2.4. Here, we learn the parameters of this
fixed circuit. This gives us the opportunity to illustrate the effect of measurement
noise in using the SWAP test. The results of this can be seen in Fig. 7.4. We
compare the SWAP test in Fig. 7.4(a) to direct simulation in Fig. 7.4 (b) to
compute the fidelities. Direct simulation here refers to the explicit extraction of a
classical description of the qubit density matrices. We do this using quantum state
tomography [DPS03] with the forest-benchmarking library [GCH+19]. Note,
that this intermediate step via tomography is not necessary in general as with
sufficiently low hardware noise, the fidelity could be directly extracted via the an
overlap test (i.e., the SWAP test). The effect of measurement noise can be clearly
seen in the latter case.
We note in the main text that we do not use the SWAP test when running
the experiments on the Aspen QPU. This is because the test fails to output the
fidelity since both states to compare will be mixed due to device noise. However,
this essentially reproduces the findings of [JJB+19] in a slightly different scenario.
Furthermore, this was only possible because we had prior knowledge of an optimal
circuit to implement the cloning transformation from [BBHB97, FWJ+14]. Of
course, in generality this information is not available, which precludes the use of
(problem-inspired) fixed-structure anstze.
7.5.1.2 Phase-covariant cloning with a hardware-efficient fixed ansatz
Let us now test the problem-agnostic hardware-efficient fixed-structure (HEFS)
ansatz for the sample problem as in the previous section. Here, we introduce a
number of layers in the ansatz, K, in which each layer has a fixed-structure. For
simplicity, we choose each layer to have parametrised single qubit rotations, Ry (),
and nearest neighbour CZ gates. We deal again with 1 2 cloning, so we use 3
qubits and therefore we have 2 CZ gates per layer. We show the results for K = 1
layer to K = 6 layers in Fig. 7.5. Not surprisingly, we observe convergence to the
minimum as the number of layers increases, saturating at K = 3.
7.5. VarQlone numerics 205
Figure 7.4: Learning the parameters of the fixed circuit in Fig. 2.4. We use 30 random samples
with an 80/20% train/test split. To train, we use the analytic gradient, Eq. (7.5), and the Adam
optimiser with a batch size of 10 states and an initial learning rate of 0.05. In all cases, the error
bars show mean and standard deviation over 5 independent training runs. Figure shows the results
when the fidelity is computed using (a) the SWAP test (with 50 measurement shots) and (b) using
direct density matrix simulation. In both cases, we plot the average (squared) cost (Eq. (7.3))
on the train and test set, and also the average fidelities of the output states of Bob, FB [ ], and
Eve, FE [ ], corresponding to this cost function value. Also plotted are the theoretical optimal
fidelities ([ ], magenta solid line) for this family of states, and the corresponding cost minimum
([ ], red dashed line).
Figure 7.5: Local cost, CL minimised on a training set of 24 random phase-covariant states.
We plot layers K  [1, . . .6] of the hardware-efficient ansatz shown in the inset. Fastest conver-
gence is observed for K = 5 but K = 3 is sufficient to achieve a minimal cost value, which is the
same number of entangling gates as in Fig. 2.4. Error bars shown mean and standard deviation
over 5 independent training runs.
206 7. Practical quantum cryptanalysis by variational quantum cloning
Barren Plateaus
Furthermore, we can examine VarQlone for the existence of barren plateaus in
this scenario. Recall from Sec. 4.1.1, a local VQA cost can be written as:
CL = Tr
OLU()U()
, (7.93)
OL = c01+
cjOj (7.94)
Note that taking c0 = 1,cj = 1/N j and  = 1 recovers the specific form of
our cost, Eq. (7.1). We will prove that this cost does not exhibit barren plateaus
for a sufficiently shallow alternating layered ansatz, i.e. U() contains blocks,
W , acting on alternating pairs of qubits [CSV+21]. To do so, we first recall the
following Theorem from [CSV+21]:
Theorem 36: Consider a trainable parameter, l in a block, W of an alter-
nating layered ansatz (denoted U()). Let Var[l C] be the variance of an
m-local cost function, C with respect to l . If each block in U() forms a
local 2-design, then Var[l C] is lower bounded by:
GN(K,k) Var[l C] (7.95)
GN(K,k) =
2m(k+1)1
(22m1)2(2m+1)K+k 
(p,p)pLB
j(p,p
,Oj) (7.96)
j(p,p
,Oj) := c
j dHS
p,p,
Tr(p,p)1
d(p,p)
Tr(Oj)1
(7.97)
jL are the set of j indices in the forward light-cone
a LB of the block W
and p,p is the partial trace of the input state, , down to the subsystems
Sp,Sp+1, . . . ,Sp. dM denotes the dimension of a matrix M and dHS denotes
the Hilbert-Schmidt distance from Eq. (2.67).
aThe forward light-cone is the subset of qubits which are causally connected (in the
future) to the qubit(s) on which the block W is applied.
Sp in the above represents the qubit subsystem in which W acts. Firstly, the
7.5. VarQlone numerics 207
operators Oj are all single qubit projectors (m = 1 local), | |, so we have:
Oj ,Tr(Oj)
= dHS
| |,Tr [| |]
(7.98)
Tr
| |
| |
(7.99)
| |
| |
| |
(7.100)
So G(K,k) simplifies to:
GN(K,k) =
3K+k+2
(p,p)pLB
p,p,
d(p,p)
(7.101)
If we now define SP to be the subsystems from p to p
, the reduced state of  in
SP will be one of either | ||P |, |00 ||P | or | |q|00||P |q for some
q < |P | where we denote |P | to be the number of qubits in the reduced subsys-
tem SP . Since these are all pure states, we can compute dHS(p,p,1/d(p,p)) =
11/d(p,p). Lower bounding the sum over j by 1 and
11/d(p,p) by 1/
(d(p,p) is at least 2) gives:
3K+k+22N2
 GN(K,k) (7.102)
Finally, by choosing K  O (log(N)), we have that k,K+ k  O(log(N)) and so
Gn(K,k) (1/poly(N)). Since we have that if G(K,k) vanishes no faster than
(1/poly(N)), then so does the variance of the gradient and so will not require
exponential resources to estimate. As a result, we can formalise the following
corollary:
Corollary 6: [Absence of barren plateaus in local cost]
Given the local VarQlone cost function, CL (Eq. (7.1)) in M N cloning,
and a hardware-efficient fixed-structure ansatz, U(), made up of alternat-
ing blocks, W , with a depth O(log(N)), where each block forms a local
2-design. Then the variance of the gradient of CL with respect to a param-
eter, l can be lower bounded as:
GN := min(GN(K,k)) Var[l C], GN(K,k) 
poly(N)
(7.103)
One final thing to note, is that the ansatz we choose in Fig. 7.5, does not
form an exact local 2-design, but the same ansatz is used in [CSV+21]) and is
sufficient to exhibit a cost function dependent barren plateau.
208 7. Practical quantum cryptanalysis by variational quantum cloning
7.5.2 Variable-structure anstze
For our variable-structure ansatz, we adopt the approach of [CSSC18] which fixes
the length, l , of the circuit sequence to be used,contains parametrised single
qubit gates, and un-parametrised entangling gates, which we chose to be CZ for
simplicity. For example, with a three qubit chip, we have an example gatepool:
G = { R0z(),R
z(),R
z(),R
x(),R
x(),R
x(),
R0y (),R
y (),R
y (),CZ0,1,CZ1,2,CZ0,2 } (7.104)
We use the CZ gate as the entangler for two reasons. The first is that CZ is a
native entangling gate on the Rigetti hardware. The second is that it simplifies
our problem slightly, since it is symmetric on the control and target qubit, we do
not need to worry about the ordering of the qubits: CZi ,j = CZj,i . The unitary to
be learned is given by:
Ug() = Ug1(1)Ug2(2) . . .Ugl (l) (7.105)
where each gate is from the above set G. The sequence, g := [g1, . . . ,gl ], in Eq.
(4.12) and Eq. (7.105) above, corresponds to the indices of the gates in an
ordered version of G. So using G in Eq. (7.104) as an example, g = [0,6,3,2,10]
would give the unitary:
Ug() = R
z(1)R
y (2)R
x(3)R
z(4)CZ0,1 (7.106)
and  := [1,2,3,4,0].
At the beginning of the procedure, the gate sequence is chosen randomly (,
i.e. a random sequence for the gate indices, g), and also the parameters ()
therein15.
The optimisation procedure proceeds over a number of epochs and iterations.
In each iteration, g is perturbed by altering d gates, giter giter+1, so giter+1
has at most d gates different from giter. The probability of changing d gates is
given by 1/2d , and the probability of doing nothing (i.e. giter = giter+1) is:
Pr(d = 0) = 1
(7.107)
The epochs correspond to optimisation of the parameters  using the Adam
optimiser (Eq. (3.8)). We typically set the maximum number of epochs to be
100 and iterations to be 50 in all this work. After each iteration, the best
cost, Cbestt for a chosen cost - either the local, Eq. (7.1) (t = L), the global, Eq.
(7.4) (t =G), squared, Eq. (7.3) (t = sq) or some other choice, is updated, if that
iteration has found a circuit with a lower cost. As in [CSSC18], we repeatedly
compress the sequence by removing redundant gates (e.g. combining Ugi (i) and
15If some information is known about the problem beforehand, this could be used to initialise
the sequence and improve performance.
7.5. VarQlone numerics 209
Figure 7.6: Variable-structure ansatz details for VarQlone. (a) CbestL as a function of sequence
length, l in achieving the same task as Fig. 2.4, where the Bob and Eves clones appear in qubits
1 and 2. As l increases, the number of runs which successfully approach the theoretical minimum
increases. Error bars shown mean and standard deviations for the minimum costs achieved over
20 independent runs with each sequence length. (b) Cost achieved for 50 iterations of the
structure learning protocols, using a sequence length of l = 35. Recall an iteration is an update
to the circuit structure, whereas an epoch is an update to the circuit parameters. Each line
(iteration) corresponds to a slightly different circuit structure, g. Early iterations (darker
lines) are not able to find the minimum, but eventually, a circuit is found which has this capacity.
For each g,  is trained for 100 epochs of gradient descent, using the Adam optimiser. If an
iteration has not converged close enough to CbestL by 30 epochs, the iteration is ended.
Ugi+1(i+1) if gi = gi +1), and adding random gates to keep the sequence length
fixed at gl .
Fig. 7.6 illustrates some results from this protocol. We find that with an
increasing sequence length, the procedure is more likely to find circuits which
achieve the minimum cost, and is able to first do so with a circuit with between
25-30 gates from the above gateset in Eq. (7.108). We also plot the results
achieved in a particular run of the protocol in Fig. 7.6(b). Here, an iteration
is an update to the circuit structure, whereas an epoch is an update to the
circuit parameters (as in previous chapters). As the circuit learns, it is able to
subsequently lower Cbestt , until it eventually finds a circuit capable of achieving the
optimal cost for the problem.
7.5.2.1 Phase-covariant cloning
Now, we return to phase-covariant cloning, and implement the procedure detailed
above16. We consider this to be the proof of principle implementation of our
methods. The results of this can be seen in Fig. 7.7. Let us begin by describing
some problem parameters. Firstly, we allow 3 qubits (2 output clones plus 1 an-
cilla) in the circuit. Next, we give the VarQlone the following fully connected (FC)
16For all results shown using a variable-structure ansatz, we use the forest-benchmarking
library [GCH+19] to reconstruct the output density matrix (and from this quantity then compute
the overlaps and fidelities required) in order to mitigate the effect of quantum noise
210 7. Practical quantum cryptanalysis by variational quantum cloning
Figure 7.7: Variational quantum cloning implemented on phase-covariant states using three
qubits of the Rigetti Aspen-8 chip (QPU), plus simulated results (QVM). Violin plots in (a)
show the cloning fidelities, for Bob and Eve, found using each of the circuits shown in (b)(d)
respectively. Shown in red is the maximal possible fidelity for this problem. (b) is the ideal circuit
with clones appearing in registers 2 and 3 [ , ]. (c) shows the structure-learned circuit for the
same scenario, using one less entangling gate [ , ]. (d) demonstrates the effect of allowing clones
to appear in registers 1 and 2 [ , ]. In the latter case, only four (nearest-neighbour) entangling
gates are used, demonstrating a significant boost in performance on the QPU. In all cases the
fidelity is extracted explicitly using quantum state tomography, as discussed in the text.
gateset pool for this problem (indices represent qubits of the Aspen-8 sublattice):
GPC = { R2z(),R
z(),R
z(),R
x(),R
x(),R
x(),
R2y (),R
y (),R
y (),CZ2,3,CZ3,4,CZ2,4 } (7.108)
Let us now discuss in greater detail the observations which can be drawn from
Fig. 7.7. Here, Fig. 7.7(c, d) illustrates some candidate cloning circuits learned by
VarQlone, compared to the optimal analytic circuit (the same one from Fig. 2.4)
given in [BBHB97, FMWW01, FWJ+14] (Fig. 7.7(b)).
We being by noting that all three circuits approximately saturate the optimal
bound for phase-covariant cloning (FL = 0.85, which is plotted as a red dotted
line in Fig. 7.7) when simulated (i.e. without quantum noise).
Next, we move to performance on the actual hardware, the Aspen-8 chip. We
first notice that the ideal circuit in Fig. 7.7(b) suffers a degradation in performance
when implemented on the QPU since it requires 6 entangling gates to transfer
the information across the circuit. Furthermore, since the Aspen-8 chip does not
have any 3 qubit loops in its topology, it is necessary for the compiler to insert
SWAP gates, further reducing the quality of the clones.
The two examples for our learned circuits Fig. 7.7(c, d) are trained under two
different restrictions. First, to ensure a fair comparison to the ideal circuit, we
begin by forcing the qubit clones to appear in registers 2 and 3 of the circuit,
(demonstrated in Fig. 7.7(c)) exactly as in Fig. 7.7(b). Secondly, we allow the
clones to appear instead in registers 1 and 2 (demonstrated in Fig. 7.7(d) - The
circuit labelled Rev. (Reverse).) The ability to make such a subtle change
clearly demonstrates the advantage of our flexible approach. We notice that the
7.5. VarQlone numerics 211
restriction imposed in Fig. 7.7(c) results in only a modest performance improve-
ment over the ideal. However, by allowing the clones to appear in registers 1
and 2, VarQlone is able to find much more conservative circuits, having fewer
entangling gates, and are directly implementable on a linear topology. This gives
a significant improvement in the cloning fidelities, of about 15% when the circuit
is run on the Aspen-8 QPU.
Now, let us return to discussing phase-covariant cloning through the lens of
an adversary eavesdropping on the BB84 protocol. We specifically analyse the
performance of one of these VarQlone-learned circuits (Circuit(c)) in such an
attack. We will do so by computing the corresponding critical error rate, Dcrit, via
the expression for the key rate, R, in Eq. (7.62) (recall the discussion in Sec. 7.3.1)
for the BB84 protocol protocol run in X-Y Pauli basis.
We compute this for the circuit in Fig. 7.7(c) only, since while the circuit in
Fig. 7.7(d) achieves higher fidelities on the Aspen hardware, it does not actually
make use of the ancillary qubit (the sequence of gates acting on it approximately
resolve to the identity).
Now, we compute the resulting mixed states outputted over all input states
to the cloning machine, for each basis state: {|+, |, |+i, | i} so E in Eq.
(7.63) is given by:
E :=
(+E +
E ) (7.109)
Similarly, 0E,
E in Eq. (7.63) are the mixed states encoding the symbol 0 (which
have input |+, |+i) and the symbol 1 (which have input |, | i), so are given
0E :=
(+E +
E ) 
(E +
E ) (7.110)
Calculating the minimum Holevo quantity (denoted by min) for the above density
matrices outputted by the circuit in Fig. 7.7(c) numerically gives the following:
1H(Dcrit)min = 0
 1min+(Dcrit log2Dcrit+(1Dcrit) log2 (1Dcrit)) = 0
Dcrit = 15.8%
(7.111)
which is very close to the optimal bound for the individual attack. Nevertheless as
pointed out in [SIGA05, FL12], the same bound can be reached by a collective
attack (where Eve defers any measurements until the end of the reconciliation
phase, and applies a general strategy to all collected states) where the individual
quantum operations are still given by the optimal phase-covariant cloner. As such,
the VarQlone learned attack can almost saturate the optimal collective bound as
well.
Finally, one may observe that Circuit (d) in Fig. 7.7 achieves a higher still
fidelity on hardware, but it does so by not using the ancilla to reduce the circuit
depth. As such, it is a better circuit for purely performing cloning than either (c)
or (d), but does not provide an optimal attack for BB84 [SIGA05].
212 7. Practical quantum cryptanalysis by variational quantum cloning
Local versus Global Fidelities
As a final remark on this experiment, we can investigate the difference between
the global and local fidelities achieved by the circuits VarQlone (i.e. in Fig. 7.7(c))
finds, versus the ideal one (shown in Fig. 7.7(b)). Recall that in Sec. 7.2.4.4, we
showed that the ideal circuit achieves both the optimal local and global fidelities
for this problem:
F ig. 7.7(b) =
1+ 1
 0.853
G = F
 0.72
(7.112)
In contrast, our learned circuit (Fig. 7.7(c)) maximises the local fidelity, but in
order to gain an advantage in circuit depth, compromises with respect to the
global fidelity:
F ig. 7.7(c) =
 F (c)
 F optL = 0.85
G  0.638< F
(7.113)
7.5.3 State-dependent cloning
Next, let us move onto the results of VarQlone when learning to clone the states
used in the two coin flipping protocols described in Sec. 7.3. Firstly, we focus
on the states used in the original protocol, P1 for 1 2 cloning, and then move
to the 4 state protocol, P2. In the latter we also extend from 1 2 cloning to
1 3 and 2 4. These extensions will allow us to probe certain features of
VarQlone, in particular explicit symmetry in the cost functions. In all cases, we
use the variable-structure Ansatz, and once a suitable candidate has been found,
the solution is manually optimised further. The learned circuits used to produce
the figures in this section are given in Sec. 7.6.
7.5.3.1 Cloning P1 states
As a reminder, the two states used in this protocol are:
|0 := |0,0= cos
|0+sin
|1 (7.114)
|1 := |0,1= cos
|0 sin
|1 (7.115)
The fidelities achieved by the VarQlone learned circuit can be seen in Fig. 7.8
using the gate pool Eq. (7.116) which allows a linear entangling connectivity:
GP121 :=
Rij(),CZ2,3,CZ3,4
,i  {2,3,4},j  {x,y ,z} (7.116)
A deviation from the optimal fidelity is observed in the simulated case, partly
due to tomographic errors in reconstructing the cloned states. We note that the
corresponding circuit for Fig. 7.8 only actually used 2 qubits (see Sec. 7.6). This
7.5. VarQlone numerics 213
Figure 7.8: Overview of cloning-based attack on the protocol of Mayers et. al. [MSCK99],
plus corresponding numerical results for VarQlone. (a) Cartoon of coin flipping protocols, Alice
and Bob send quantum (q) and/or classical (c) information to agree on a final coin flip bit, y .
(b) The relevant part of the protocol of Mayers et. al., P1, plus a cloning based attack on Bobs
side. He builds a cloning machine using VarQlone to produce two clones of Alices sent states,
one of which he returns, and the other is used to guess Alices input bit, a. (c) Fidelities of each
output clone, j
achieved using VarQlone when (1 2) cloning the family of states used in, P1.
In the left (right) panel, |0 (|1) is. Figure shows both simulated (QVM - [ ] purple circles)
and on Rigetti hardware (QPU - [ ] blue crosses). For the QVM (QPU) results, 256 (5) samples
of each state are used to generate statistics. Violin plots show complete distribution of outcomes
and error bars show the means and standard deviations. Inset (i) shows the two qubits of the
Aspen-8 chip which were used, with the allowed connectivity of a CZ between them. Note an
ancilla was also allowed, but VarQlone chose not to use it in this example. The corresponding
learned circuit is given in Sec. 7.6.
214 7. Practical quantum cryptanalysis by variational quantum cloning
is because while VarQlone was allowed to use the ancilla, it chose not in this case
by applying only identity gates to it. This mimics the behavior seen in the previous
example of phase-covariant cloning. As such, we only use the two qubits shown
in the inset (i) of the figure when running on the QPU to improve performance.
Now, returning to the attack on P1 above, we can compute the success prob-
abilities using these fidelities. For illustration, let us return to the example in Eq.
(7.68), where instead the cloned state is now produced from our VarQlone circuit,
0c  0VarQlone.
Theorem 37: [VarQlone attack bias on P1]
Bob can achieve a bias of  0.29 using a state-dependent VarQlone attack
on the protocol, P1, with a single copy of Alices state.
We can prove Theorem 37 by computing the success probability in the same
manner as was done in Sec. 7.4.2:
PVarQlonesucc,P1 =
Tr|1|11|0VarQlone|  0.804 (7.117)
The state 1 = |00| |11| as in Eq. (7.68). Here, we have a higher prob-
ability for Bob to correctly guess Alices bit, a, but correspondingly the detection
probability by Alice is higher than in the ideal case, due to a marginally lower local
fidelity of FVarQloneL = 0.985.
7.5.3.2 Cloning P2 states.
Next, we turn to the family of states used in the 4 states protocol, given by Eq.
(7.76):
1 2 Cloning
Firstly, we repeat the exercise from above with the same scenario, using the
same gateset and subset of the Aspen-8 lattice (GP122 = GP121 ). We use the
local cost, Eq. (7.1), to train the model, with a sequence length of 35 gates. The
results are seen in Fig. 7.9(b) both on the QVM and the QPU. We note that the
solution exhibits some small degree of asymmetry in the output states, due to the
form of the local cost function. This asymmetry is especially pronounced as we
scale the problem size and try to produce N output clones, which we discuss in
the next section.
Now, we can relate the performance of the VarQlone cloner to the attacks
discussed in Sec. 7.4.3. We do this by explicitly analysing the output states
produced in the circuits used to achieve fidelities shown in Fig. 7.9(b) and following
the derivation in Sec. 7.4 for Theorem 38 and Theorem 39:
7.5. VarQlone numerics 215
Figure 7.9: Cloning attacks and numerical results for the protocol, P2. (a) The two cloning
based attacks we consider. In attack model I (left), Bob measures both output states with a set
of fixed projective measurements, defined relative to the cloner output states, |12x,a  and guesses
Alices bit, a. In attack model II, Bob keeps one clone for either testing Alice later or to send back
the deposit qubit requested by Alice. He uses then the other local clone to discriminate and guess
a. (b) The fidelities achieved cloning the each state, {|x,a} used in P2 with VarQlone. These
numerics relate to scenario 1 from attack model II (see Sec. 7.4.3.3). Each panel (1-4) shows both
simulated (QVM - [ ] red circles) and on Rigetti hardware (QPU - [ ] orange crosses). We indicate
the fidelities of the each clone received by Alice and Bob. For the QVM (QPU) results, 256 (3)
samples of each state are used to generate statistics. Violin plots show complete distribution of
outcomes and error bars show the means and standard deviations. Inset (i) shows the connectivity
we allow in VarQlone for this example. The corresponding learned circuit is shown in Sec. 7.6.
Theorem 38: [VarQlone cloning attack (I) bias on P2]
Using a cloning attack on the protocol, P2, (in attack model I) Bob can
achieve a bias:
IP2,VarQlone  0.345 (7.118)
Similarly, we have the bias which can be achieved with attack II:
Theorem 39: [VarQlone Cloning Attack (II) Bias on P2]
Using a cloning attack on the protocol, P2, (in attack model II) Bob can
achieve a bias:
IIP2,VarQlone = 0.241 (7.119)
The discrepancy between these results and the ideal biases are primarily due to
the small degree of asymmetry induced by the heuristics of VarQlone. However,
we emphasise that these biases can now be achieved constructively.
1 3 and 2 4 Cloning
Finally, we extend the above to the more general scenario of M N cloning,
taking M = 1,2 and N = 3,4. These examples are illustrative since they demon-
strate strengths of the squared local cost function (Eq. (7.3)) over the local
cost function (Eq. (7.1)). In particular, we find the local cost function does not
enforce symmetry strongly enough in the output clones, and using only the local
216 7. Practical quantum cryptanalysis by variational quantum cloning
cost function, suboptimal solutions are found. This was particularly observed in
the example of 2 4 cloning, where VarQlone tended to take a shortcut by al-
lowing one of the input states to fly through the circuit (resulting in nearly 100%
fidelity for that clone), and then attempt to perform 1 3 cloning with the re-
maining input state. By strongly enforcing symmetry in the output clones using
the squared cost, this can be avoided as we demonstrate explicitly in Sec. 7.12.
We also test two connectivities in these examples, a fully connected (FC) and a
Figure 7.10: Clone fidelities for optimal circuits learned by VarQlone for (a) 1 3 [ ] and
(b) 2 4 [ ] cloning of the states used in the coin flipping protocol of [ATSVY00] et al..
Mean and standard deviations of 256 samples are shown (violin plots show full distribution of
fidelities), where the fidelities are computed using tomography only on the Rigetti QVM. In both
cases, VarQlone is able to achieve average fidelities > 80%. (c-d) shows the mean and standard
deviation of the optimal fidelities found by VarQlone over 15 independent runs (15 random initial
structures, g) for a nearest neighbour (NN - [ ] purple) versus (d) fully connected (FC - [ ] pink)
entanglement connectivity allowed in the variable-structure ansatz for 1 3 and 2 4 cloning of
P2 states. Insets of (c-d) shown corresponding allowed CZ gates in each example.
nearest neighbour (NN) architecture as allowed by the following gatesets:
GNNP132
= { Riz(),R
x(),R
y (),CZ2,3,CZ3,4,CZ4,5 }i  {2,3,4,5} (7.120)
GFCP132
= { Riz(),R
x(),R
y (),CZ2,3,CZ2,4,CZ2,5,CZ3,4,CZ3,5,CZ4,5 } (7.121)
i  {2,3,4,5} (7.122)
Note, that for 1 3 (2 4) cloning, we actually use 4 (5) qubits, with one being
an ancilla. The results of these experiments are given in Fig. 7.10. We use the
following hyperparameters for this experiment: 1) a sequence length of l = 35 for
1 3, and l = 40 for 1 4 with 50 iterations over g in both cases, 2) the Adam
optimiser with an initial learning rate of init = 0.05, 3) 50 training samples. In all
cases, we use the squared cost function, Csq to train, and its gradients.
7.5. VarQlone numerics 217
7.5.4 Training sample complexity
Here we study the sample complexity of the training procedure by retraining the
continuous parameters of the learned circuit (Fig. 7.7(c)) starting from a random
initialisation of the parameters, . As expected, as the number of training samples
increases (i.e. the number of random choices of the phase parameter, , in Eq.
(2.83)), the generalisation error (difference between training and test error) ap-
proaches zero. This is not surprising, since the training set will eventually cover
all states on the equator of the Bloch sphere.
Figure 7.11: Numerical sample complexity of VarQlone using the squared cost. We begin with
a random initialisation of the structure learned circuit in Fig. 7.7(c) and optimise the parameters
using different sizes in the training/text set, and different mini-batch sizes. All of the following
using a train/test split of 20% and we denote the tuple (i , j,k) as i = number of training samples,
j = number of test samples, k = batch size. (a) (1,1,1), (b) (4,1,2), (c) (8,2,5), (d) (16,4,8)
(e) (40,10,15), (f) (80,20,20).
7.5.5 Local cost function comparison
In Fig. 7.12, we revisit the weakness of the local cost function, CL, in not en-
forcing symmetry strongly enough in the problem output, and how the squared
cost function, Csq can alleviate this, for 2 4 cloning specifically. Here we show
the optimal fidelities found by VarQlone with a variable-structure ansatz, starting
from a random structure. The local cost tends towards local minima, where one
of the initial states (1) ends up with high fidelity, while the last qubit (
) has
a low fidelity. This is alleviated with the squared cost function which is clearly
more symmetric, on average, in the output fidelities. This is observed for both
circuit connectivities we try (although a NN architecture is less able to transfer
information across the circuit for a fixed depth).
218 7. Practical quantum cryptanalysis by variational quantum cloning
Figure 7.12: Comparison between the local (Eq. (7.1)) [ , ] and squared (Eq. (7.3)) [ , ] cost
functions for 2 4 cloning. (a) shows nearest neighbour (NN) and (b) has a fully connected (FC)
entanglement connectivity allowed in the variable-structure ansatz. Again, we use the family of
states in the protocol P2. Plots show the mean and standard deviation of the optimal fidelities
found by VarQlone over 10 independent runs (10 random initial circuit structures). A sequence
length of 35 is used for 1 3 and 40 for 2 4, with 50 iterations of the variable-structure Ansatz
search in both cases. Here we use the same experiment hyperparameters as in Fig. 7.10.
7.6 VarQlone learned circuits
To round off this chapter, we give the explicit circuits we use to generate the
results above. We mention as above, that these are only representative examples,
and many alternatives were also found in each case.
7.6.1 Ancilla-free phase-covariant cloning
The circuits found in Fig. 7.7 to clone phase-covariant states are slightly more
general than we may wish to use. In particular, the circuit Fig. 2.4 also has the
ability to clone universal states, due to the addition of the ancilla, which can be
used as a resource. However, it is known that phase-covariant cloning can be
implemented economically, i.e. without the ancilla [NG99, SIGA05] as mentioned
above17. As such, we could compare against a shorter depth circuit which also
does not use the ancilla. For example, the circuit from [DDZ+05] shown in
Fig. 7.13(a) is also able to achieve the optimal cloning fidelities ( 0.85). An
example VarQlone learned circuit for this task can be seen in Fig. 7.13(b) which
has 2 CZ gates. We note that this ideal circuit can be compiled to also use 2 CZ
gates, so in this case VarQlone finds a circuit which is approximately comparable
up to single qubit rotations.
17Although as discussed above in Sec. 7.3.1, this the economical version does not provide the
optimal attack on related protocols.
7.7. Discussion and conclusions 219
Figure 7.13: Circuits to clone phase-covariant states, without ancilla for 2 qubits (a) Optimal
circuit from [DDZ+05]. (b) Circuit learned by VarQlone. It can be checked that the ideal circuit
in (a) can be compiled to also use 2 CZ plus single qubit gates, so VarQlone has achieved close
to optimality. The average fidelities for B,E for the circuit in (b) is FB,PC
L,VarQlone  0.854 and
FE,PC
L,VarQlone  0.851 respectively, over 256 input samples, |A (comparing to the ideal fidelity of
FPCL,opt = 0.853).
Figure 7.14: Circuit learned by VarQlone in to clone states, |0, |1, with an overlap s =
cos(/9) in the protocol, P1. For example, A is the clone sent back to Alice, while B is kept
by Bob.
7.6.2 State-dependent cloning circuits
Fig. 7.14 shows the circuit used to achieve the fidelities in the attack on P1. In
training, we still allow an ancilla to aid the cloning, but the example in Fig. 7.14
did not make use of it (in other words, VarQlone only applied gates which resolved
to the identity on the ancilla), so we remove it to improve hardware performance.
This repeats the behaviour seen for the circuits learned in phase-covariant cloning.
We mention again, that some of the learned circuits did make use of the ancilla
with similar performance.
Fig. 7.15 shows the circuits learned by VarQlone and approximately clone all
four states in Eq. (7.76) in the protocol, P2, for 1 2,1 3 and 2 4 cloning.
These are the specific circuits used to produce the fidelities in Fig. 7.9(b) and
Fig. 7.10.
7.7 Discussion and conclusions
We have now concluded the discussion of our third and final application for near-
term quantum computers; the use of quantum machine learning in quantum in-
formation and quantum cryptography. We saw how quantum cloning is one of the
most important ingredients not just as a tool in quantum cryptanalysis, but also
with roots in foundational questions of quantum mechanics. However, given the
amount of attention this field has received, a fundamental question remained elu-
sive: how do we construct efficient, flexible, and noise-tolerant circuits to actually
perform approximate or probabilistic cloning? We demonstrated how VarQlone
provided a novel toolkit to such an exercise. We further believe that VarQlone is
220 7. Practical quantum cryptanalysis by variational quantum cloning
Figure 7.15: Circuits learned by VarQlone to clone states from the protocol, P2 for (a) 1 2,
(b) 1 3 and (c) 2 4 cloning. These specific circuits produce the fidelities in Fig. 7.9(b) for
1 2, (using the local cost function), and in Fig. 7.10 for 1 3 and 2 4 (using the squared
cost function). We allow an ancilla for all circuits, and k indicates the qubit which will be the k
output clone.
unique among variational/quantum machine learning algorithms for a number of
reasons. The first, and most obvious reason for this is that it bridges the fields of
quantum machine learning and quantum cryptography in an interesting manner.
We demonstrated how VarQlone brings into view a whole new domain of per-
forming realistic implementation of attacks on quantum cryptographic systems.
Secondly, since it is an example using quantum data directly, there is no obvi-
ous method of dequantisation, or indeed no obvious argument why dequantisation
would even make sense in such a context. Finally, unlike many other variational
algorithms, which aim to mimic performance of fault-tolerant algorithms in asymp-
totic scaling regimes, it is useful in both the small scale, and the large scale. The
former is more obvious; we have already demonstrated example use cases ion cryp-
tography. At the larger scale, one could imagine studying connections to quantum
state estimation, or simply the question of cloning by itself.
In future work, one of the most exciting possible directions is in actually using
VarQlone to aid the attack of a quantum protocol in a physical experiment. Other
directions of study include improving the robustness and performance of VarQlone.
One could utilise noise tolerant methods for quantum architecture search, or in-
corporating error mitigation methods. The latter presents an interesting question
in itself, since many error mitigation methods aim to correct expectation values of
computations by classical post-processing. It is not obvious how such techniques
may be useful when the desired output is not classical.
Conclusion
This Thesis began with the goal of finding, and developing, applications and mod-
els for near-term quantum computers. Focusing on problems and techniques from
quantum machine learning, we aimed to attach theoretical rigour, where possible,
to an experimental plug-and-play mindset in the development of such applications.
We focused on a particular primitive from modern quantum machine learning; the
variational quantum algorithm (VQA) equipped with the parameterised quantum
circuit (PQC). Let us now conclude the Thesis by briefly summarising the con-
tributions of each chapter, before discussing a more general outlook and future
directions.
As a reminder, this Thesis was structured in accordance with the increasing
complexity of the data available to each application.
We began in Chapter 5 with labelled vectors as data and focused on the use
of a PQC in supervised learning for the problem of classification. In order to build
such models to be implementable across multiple types of near-term quantum
hardware, it is important to build them in a robust manner, to account for the
inevitable quantum noise which will plague them for at least the next several years.
We did this by focusing attention on the means of encoding data into the PQC
for subsequent classification. We provided several definitions for robust data
encodings, and we proved that such encodings must always exist, for a given noise
channel. We also studied the design of the PQC outside of the data encoding, and
found that by incorporating simple alternative design principles, the model could be
naturally robust. However, we noted that this robustness came at a price; namely
in the expressiveness or learnability of the model. Finally, we performed several
numerical experiments demonstrating that an optimal classifier for near-term
devices involves a trade-off between robustness and learnability.
Next, in Chapter 6 we turned to the more complicated unsupervised learning
problem of generative modelling. Here, the data becomes probability distributions
and sample access to them. To attack this problem, we turn to a PQC in the form
of a Born machine, an implicit generative model suitable for near-term devices.
We began by discussing the possibility of a (provable) quantum advantage in gen-
erative modelling and natural reasons why the Born machine may demonstrate
such a thing. We then developed new gradient-based training methods for the
222 8. Conclusion
Born machine by changing the cost function used. Finally, we performed an ex-
tensive numerical and experimental study for the Born machine with three different
datasets. Of particular interest, was that one of these datasets originated with a
quantum process, which one of the others was relevant to a real world use case in
finances. In this latter example, we also performed a comparison to a comparable
classical generative model, the restricted Boltzmann machine, and found that the
quantum model could outperform its classical counterpart (a numerical quantum
advantage). We also surprisingly found some evidence that entanglement may be
partially responsible for such an advantage, by adding correlations which are not
classically accessible.
Finally, in Chapter 7 we changed course slightly away from machine learning
problems, into the domain of quantum foundations. However, we still used very
similar ML tools in the study of quantum cloning. Here, we designed a variational
quantum algorithm (VarQlone) to learn to approximately clone quantum states.
For the algorithm, we provided cost functions with faithfulness guarantees, derived
suitable gradients for gradient-based training, and incorporated quantum architec-
ture search to find high quality cloning circuits. In this chapter, we also made an
interesting bridge between quantum machine learning and quantum cryptography
by demonstrating how QML methods could be used as a subroutine in tools for
the cryptanalysis of quantum protocols. Specifically for this latter use case, we
focused on the common examples of quantum key distribution, and quantum coin
flipping. We illustrated how VarQlone could be used to find novel attacks on such
protocols, which could subsequently be employed to improve the protocols.
Returning to an abridged version of the question posed in the Introduction
(How can we use the quantum computers we have now . . . ), it is clear that
there are a multitude of potential applications for such devices. Whether any
of these will prove to be the killer application, only time (and bigger quantum
computers) will tell. This breakthrough may come from a dedicated research team
in a multinational conglomerate, or even from a lone quantum hacker. Both of
these are now possible due to the abundance of resources and learning material.
Regardless, the sheer scope of possibility is one of the main reasons why there is
excitement and hype surrounding the field. While such excitement is extremely
beneficial for a number of reasons, we must also be careful to temper unfounded
claims of quantum computers revolutionising all aspects of science and technology.
Throughout this thesis, I hope I have been able to provide a balanced view.
My PhD has been fortunately situated in the era of the first commercially
available quantum computers. As such, it was perfectly positioned in the QML
transition period to take advantage of these computers. With this in mind, I would
summarise my experience of performing research in quantum machine learning as
the perfectly epitomised quote by the Doctor:  . . . Its a bit dodgy, this process.
You never know what youre going to end up with., which applies equally well to
both its parent fields.
Appendix A
Proofs and derivations
224 Appendix A. Proofs and derivations
A.1 Proofs for Chapter 5
A.1.1 Proofs of Theorem 9 and Theorem 10
Here we give the proofs of the above two Theorems, which broadly have the same
structure.
Theorem 9. If E is any noise channel which factorises into a single qubit
channel, and a multiqubit channel as follows:
E() = Ec(cx)Ec(
where WLOG Ec acts only on the classification qubit (cx = Trc(x)) after
encoding and unitary evolution, and Ec acts on all other qubits arbitrarily,
(cx = Trc(x)). Further assume the state meets the robust classification
requirements for the single qubit error channel Ec . Then the classifier will
be robust to E .
Proof. The correct classification again depends on the classification qubit mea-
surement probabilities: Tr(|00 |c  1n1x),Tr(|11 |c  1n1x). If x is
robust to the single qubit error channel Ec , this means
Tr(c0x) 1/2 = Tr(
0Ec(x)) 1/2
Tr(c1x)< 1/2 = Tr(
1Ec(x))< 1/2
Then WLOG, assume the point x classified as y(x) = 0 before the noise, then:
Tr (c0E(x)) = Tr
Ec(cx Ec(
= Trc
Ec(cx)
Trc (|00 |Ec(cx))
= Tr (|00 |Ec(cx)) 1/2
Theorem 10. Let Emeasp define measurement noise acting on the classifica-
tion qubit and consider a quantum classifier on data from the set X . Then,
for any encoding E : X  S2, we have complete robustness
R(Emeasp ,E, y) = X
if the measurement assignment probabilities satisfy p00 > p01,p11 > p10.
A.1. Proofs for Chapter 5 225
Proof. We can write the measurement noise channel acting on the POVM ele-
ments as:
Emeasp (
n1) = (p00|00 |+p01|11 |)1n1
Again, if we had correct classification before the noise, Tr(|00 |x) 1/2, then:
Tr[Emeasp (
n1)x ] = Tr[({p00|00 |+p01|11 |}1n1)x ]
= p00Tr[(|00|1n1)x ]+p01Tr[(|11 |1n1)x ]
= p00Tr[(|00|1n1)x ]+p01(1Tr[(|00|1n1)x ])
= (p00p01)Tr[(|00 |1n1)x ]+p01
 (p00p01)
+p01 =
(p00+p01) =
A.1.2 Proof of Theorem 14
Here we prove the above Theorem for a bound on the size of the robust set.
Theorem 14. Consider a quantum classifier using the mixed state encod-
ing Eq. (5.63) and indicator cost function Eq. (5.64). Assuming that a
noise channel E acts only on the encoded feature vectors x i (and not the
encoded labels |yiyi |), then
|R(E ,E, y)|M
1F (E(), )
(A.1)
where F is the fidelity between quantum states, Eq. (2.69).
This Theorem is useful since, if one has an expression for the output fidelity
of a quantum circuit after a noise channel is applied, one can directly determine
a bound on how the classification results will be affected. In order to prove it, we
relate the difference between the noisy cost and the noiseless cost to the size of
the robust set.
Proof. Let the indicator cost Eq. (5.64) of the noisy classifier be CE and the
noiseless classifier be C. Define
E C := |CEC | (A.2)
to be the magnitude of the difference between noisy and noiseless costs. Due to
our choice of cost function Eq. (5.64), we can relate E C to the -robustness of
the model in Definition 33 as follows.
First, it is easy to see that the quantity 1E C is the the fraction  of robust
points Eq. (5.20) so that:
|R(E ,E, y)|=M (1E C) (A.3)
226 Appendix A. Proofs and derivations
Now, the PQC acts only on the data subsystem of the mixed state encod-
ing Eq. (5.63) so that the evolved state before measurement is:
x i |yiyi | (A.4)
By assumption, the channel E leaves the label subsystem |yiyi | invariant so
that:
E() =
x i
|yiyi |
Now, a related cost C can be evaluated by measuring the expectation of:
D := 1n1Zc Zl
where c and l denote classification and label qubits, respectively [CWV+19]. That
is, the noiseless cost is given by
C = Tr(D)
and the noisy cost is given by
CE = Tr(DE()).
Now, it is easy to see that 0 E C
  2. When all points are robust, E C
 = 0,
and when a single point goes from robust to not robust, E C
 increases by at
most 2/M. As such, we can write down a corresponding inequality for the cost,
C, as in Eq. (A.3) so that
|R(E ,E, y)|M
. (A.5)
The difference in cost Eq. (A.2) due to noise is thus
CEC= |Tr[D(E() )]|
 ||D||||E() ||1
1F (E(), ). (A.6)
The third line in this derivation follows from Hlders inequality and the last line
from the Fuchs-van de Graaf inequality [GCP+20, FG99]. We also used the fact
that ||D|| := maxj |j(D)|= 1.
Substituting the inequality Eq. (A.5) in Eq. (A.6) completes the proof.
A.2. Proofs for Chapter 7 227
A.2 Proofs for Chapter 7
A.2.1 Proof of gradients for VarQlone
We do this explicitly for the squared local cost, Eq. (7.3), and for brevity neglect
the gradient derivations for the others, since they are simpler and follow analo-
gously. For this cost function we have the gradient with respect to a parameter,
l as:
Csq()
(F iLF
i ,l+2
i ,l2
j,l+2
j,l2
(1F iL)(F
i ,l+2
i ,l2
(A.7)
where F j,l/2L () denotes the fidelity of a particular state, |, with j
th reduced
state of the VarQlone circuit which has the l th parameter shifted by /2. We
suppress the  dependence in the above.
Proof. We begin as before, by taking the derivative of Eq. (7.3) with respect to
a single parameter, l :
Csq()
= 2 E
|S
(1F iL())
F iL()
(F iL()F
L())
F iL()
We can rewrite the expression for the fidelity of the j th clone as:
L() = |j()|= Tr
||j
= Trj
||Trj
U()initU()
(A.8)
Using the linearity of the trace, the derivative of the fidelities with respect to
the parameters, l , can be computed:
= Trj
||Trj
U()initU()
(A.9)
Now plugging in the parameter shift rule:
U()initU()
= U l+
2 ()init(U()
l+2 )U l
2 ()init(U()
l2 ) (A.10)
Where the notation, U l
2 , indicates the l th parameter has been shifted by 
228 Appendix A. Proofs and derivations
i.e. U l
2 := U(d) . . .U(l  /2) . . .U(1). Now:
= Trj
||Trj
2 ()init(U()
l+2 )
||Trj
2 ()init(U()
l2 )
||l+
||l
(A.11)
(j,l+2 )
L ()F
(j,l2 )
L () (A.12)
where we define F
(l2 )
() := |l
()| the fidelity of the j th clone, when
prepared using a unitary whose l th parameter is shifted by 
, with respect to a
target input state, |.
Plugging Eq. (A.12) into Eq. (A.10), we recover the expression in Eq. (A.7).
A.2.2 Proof of Theorem 19
Theorem 19. The squared cost function as defined Eq. (7.15), is -weakly
faithful with respect to the Bures angle distance measure dBA. In other
words, if we have:
Csq()Coptsq   (A.13)
where Coptsq := (Fi()Fj())2 =N(1Fopt)2 is the optimal cost, then the
following fact holds:
dBA(
opt)
2(1Fopt)sin(Fopt)
:= f1(), |  S,j  [N]
(A.14)
Proof. To prove Theorem 19, we revisit and rewrite the Bures angle from Eq.
(2.70) as [NC10]:
dBA(,) = arccos
F (,) = arccos| (A.15)
where | and | are the purifications1 of  and  respectively which maximise
the overlap. We note that dBA(,) lies between [0,/2], with dBA(,) = /2
corresponding to  = . Since this distance is a metric, it obeys the triangle
inequality (Definition 9), i.e., for any three states , and ,
dBA(,) dBA(,)+dBA(,) (A.16)
1A purification of a mixed state, HA, is a pure state, |, in a larger Hilbert space (HAHB)
such that tracing out B leaves the original state, , = TrB(| |).
A.2. Proofs for Chapter 7 229
Rewriting the result of Lemma 2 in terms of fidelity for each |  S and
correspondingly in terms of Bures angle using Eq. (A.15) is,
opt, |)F (
, |) 
= cos2(dBA(
opt, |))cos
2(dBA(
, |)) 
(A.17)
where  = N 
2(1Fopt) . Let us denote D
 = dBA(
opt, |) dBA(
, |) This
inequality in Eq. (A.17) can be further rewritten as,
cos(dBA(
opt, |))cos(dBA(
, |))
cos(dBA(
opt, |))+cos(dBA(
, |))
cos(dBA(
opt, |))cos(dBA(
, |))
2cos(dBA(
opt, |))
2cos(dBA(
opt, |))
= d 
sin(dBA(
opt, |))
2(1Fopt)sin(Fopt)
(A.18)
In the above, we use the approximations that as  0, dBA(
opt, |) dBA(
, |)
and the trigonometric identities cos(x  y) = 2sin
, and sin2x =
2sinx cosx .
Now, using the triangle inequality on the states {,jopt,
, |}, we have:
dBA(
, |) dBA(
opt, |)+dBA(
opt) (A.19)
Combining the above inequality and Eq. (A.18) completes the proof:
dBA(
opt)
2(1Fopt)sin(Fopt)
, |  S (A.20)
A.2.3 Proof of Theorem 20
Here we give the proof of Theorem 20. We begin by repeating the Theorem:
Theorem 20. The squared cost function, Eq. (7.15), is -weakly faithful
with respect to the trace distance dTr.
dTr(
opt,
) g1(), j  [N] (A.21)
where:
g1()
4Fopt(1Fopt)+
N (12Fopt)
2(1Fopt)
(A.22)
230 Appendix A. Proofs and derivations
Proof. Firstly, we note that Fopt= |
opt| is the same value for all input states
|  S. We apply the change of basis from |  |0 by applying the unitary
V | = |0. Then the effective change on the state ,jopt to have a fidelity Fopt
with the state |0 is, ,jopt V 
, written as:
Fopt a
a 1Fopt
(A.23)
where we use the usual properties of a density matrix and a C. The upper bound
condition in Eq. (7.19) states that:
|,jopt
|= 0|V (,jopt
)V |0=
2(1Fopt)
=: 
V has the following effect on ,j
V  =
Fopt+
b 1 (Fopt+)
(A.24)
for some b  C. The condition that V ,joptV
,V 
V   0 i.e. they are positive,
implies that,
|a|2  Fopt(1Fopt) =: r2Fopt, |b|
2  (Fopt+
)(1 [Fopt+]) =: rFopt+ (A.25)
Using the relationship between the trace distance and the positive eigenvalue of the
difference between the states, we consider the eigenvalues of V ,joptV
V ,j
The two eigenvalues of this matrix is  =
2+ |ab|2. From this, the trace
distance between the two states is,
dTr(V 
,V 
V ) =
V ,joptV V ,j V = 12 |+|= 122+ |ab|2
(A.26)
We note that the trace distance is unitary invariant. Thus,
dTr(
opt,
) = dTr(V 
,V 
2+ |ab|2
2+(rFopt+ rFopt+)
4Fopt(1Fopt)+(12Fopt)
4Fopt(1Fopt)+
N (12Fopt)
2(1Fopt)
(A.27)
where we have used the inequality |ab|2  ||a|+ |b||2 for all a,b  C.
A.2. Proofs for Chapter 7 231
A.2.4 Proof of Theorem 26
Here we give the proof of Theorem 26 which is repeated below:
Theorem 26. For the general case of M  N cloning, the global cost
function CG() and the local cost function CL() satisfy the inequality,
CL() CG() N CL() (A.28)
Proof. We first prove the first part of the inequality,
CG()CL() =
Tr((O
| |j 1j | |1  | |N
= CG() CL()
(A.29)
where OL is defined in Eq. (7.2), and the inequality in the second line holds
because
| |j 1j | |1  | |N
| |j  (1j | |j) 0, |  S (A.30)
For the second part of the inequality, we consider the operator NOL O
G = (N1)1
| |j 1j
+ | |1  | |N
1j 1j | |j 1j
| |N1N+ | |1  | |N
(1| |)j 1j
(1| |)j | |N
= (1| |)1
(1| |j)| |N
(1| |)j)1j
(A.31)
where the second last line is positive because each individual operator is positive
for all |  S.
232 Appendix A. Proofs and derivations
A.2.5 Proof of Theorem 30
Here we give the proof of faithfulness for the asymmetric cost function, Theo-
rem 30.
Theorem 30. The asymmetric cost function, Eq. (7.55), is -weakly faith-
ful with respect to dBA:
CL,asym()C
L,asym   (A.32)
where CoptL,asym = 0. then the following fact holds for Bob and Eves reduced
states::
dBA(
opt )
sin(1p2/2)
, dBA(
opt )
sin(1q2/2)
(A.33)
Furthermore, we also have the following trace distance bounds:
dTr(
,B,
p2(2p2)
N (1p2), (A.34)
dTr(
,E,
q2(2q2)
N (1q2) (A.35)
Proof. Firstly, we derive a similar result to Lemma 2 and Lemma 3. By expanding
the term |CL,asym()CL,opt | in terms of the corresponding output states, we
obtain,
|CL,asym()CL,opt |=
 1N
L ())
L ())
Tr[(,B,B
)||]
Tr[(,E,E
)||]
(A.36)
Using the inequality Eq. (A.32) and Eq. (A.36), we get,
Tr[(,jopt
)||]
d   = Tr[(,jopt
)||]
(A.37)
where j  {B,E}.
Next, to derive Eq. (A.33) we rewrite the Eq. (A.37) in terms of the Bures
angle:
opt, |)F (
, |)
= cos2(dBA(
opt, |))cos
2(dBA(
, |))
(A.38)
Following the derivations in the previous sections, we have:
dBA(
opt)
sin(F
, |  S (A.39)
A.2. Proofs for Chapter 7 233
where F r,jL is the optimal cloning fidelity corresponding to j  {B,E} with r 
{p,q}. Finally, plugging in the optimal asymmetric fidelities, F p,BL = 1 p
and similarly for F q,EL we arrive at:
dBA(
opt )
sin(1p2/2)
, dBA(
opt )
sin(1q2/2)
(A.40)
Finally, to prove Eq. (A.34), we follow the trace distance derivation bounds as in
previous sections and obtain:
dTr(
,j ,
L (1F
N (12F r,jL ) (A.41)
Again, plugging in the optimal fidelities for Bob and Eve completes the proof.
A.2.6 Proof of Theorem 31
Here we prove Theorem 31 in the main text.
Theorem 31 (Sample complexity of VarQlone). The number of samples
LK required to estimate the VarQlone cost function C() up to -additive
error with a success probability  is:
LK =O
(A.42)
where K is the number of distinct states | sampled uniformly at random
from the distribution S, and L is the number of copies of each input state.
Proof. We provide the proof for the cost function CG(). However, this proof
extends in a straightforward manner to other cost functions. As a reminder, the
global cost function is defined as,
CG() = 1|
| = CG() = 1
|
|d (A.43)
The estimation of CG() requires the computation of the overlap |
|. Let
us denote | := |N as an N-fold tensor product of the input state.
This test inputs the states | and 
with an additional ancilla qubit |0, and
measures the ancilla in the end in the computational basis. The probability of
obtaining an outcome 1 in the measurement is (recall 1Pr[|anc = |0] from
Eq. (2.76)):
Pr[|anc= |1] = p =
(1|
|) (A.44)
From this, we see that CG() = 2p
. To estimate this cost function value, we
run the SWAP test L times and estimate the averaged number of 1 outcomes.
234 Appendix A. Proofs and derivations
Let us the define the estimator for cost function with L samples to be,
G,avg() =
G,i() (A.45)
where C
G,i() is equal to 2 if the SWAP test outcome at the i-th run is 1 and is
0 otherwise. From this we can see that the expected value of C
G,avg() is,
G,avg()
= CG() = 2p
 (A.46)
Now consider K different states {|1,    |K} are chosen uniformly at random
from the distribution S. The average value of the cost over K is:
CG,avg() =
G,avg() =
G,i() (A.47)
with,
CG,avg()
G,avg()d = CG() (A.48)
Using Heffdings inequality [Hoe63], we obtain a probabilistic bound on |CG,avg()
CG()|,
|CG,avg()CG()| 
 2e2KL
(A.49)
Now, setting 2e2KL
=  and solving for LK gives:
LK =O
(A.50)
A.2.7 Proof of Theorem 34
Here we prove Theorem 34. We repeat the Theorem first:
Theorem 34 (Ideal cloning attack (II) bias on P2 in scenario 1.).
Using a cloning attack on the protocol, P2, (in attack model II with 4 states)
Bob can achieve a bias:
IIP2,ideal = 0.25 (A.51)
Proof. Considering the 4 states to be in the XZ plane of the Bloch sphere, the
density matrices of each state can be represented as:
i j =
(1+mi jx X+m
z Z) (A.52)
A.2. Proofs for Chapter 7 235
where X and Z are Pauli matrices and each mi j is a three dimensional vector given
m00 := [sin(2), 0, cos(2)]
m01 := [sin(2), 0, cos(2)]
m10 := [sin(2), 0, cos(2)]
m11 := [sin(2), 0, cos(2)]
(A.53)
After the cloning (in the ideal case), the density matrix of each clone will become:
ci j =
(1+xm
x X+zm
z Z) (A.54)
where x and z are the shrinking factors in each direction given as follows:
x = sin
2(2)
sin4(2)+cos4(2)
, z = cos
2(2)
sin4(2)+cos4(2)
(A.55)
For the states used in P2, we have = 8 and hence x = z := =
. Again, we
can return to the discrimination probability between the two ensembles encoding
a = 0 and a = 1 in Eq. (7.84). Here, we have (let us define c to be the output
clone that Bob chooses to use (c  {1,2}):
opt,II
disc,P2 =
(a=0)(a=1)Tr
12 [(c00c11)+(c10c01)]

(mx00m
01)X+(m

 cos(2)
||Z||Tr
 cos(2)
Computing the bias in the same way as done previously completes the proof.
Bibliography
[AAB+19] Frank Arute, Kunal Arya, Ryan Babbush, Dave Bacon, Joseph C.
Bardin, Rami Barends, Rupak Biswas, Sergio Boixo, Fernando G.
S. L. Brandao, David A. Buell, Brian Burkett, Yu Chen, Zijun
Chen, Ben Chiaro, Roberto Collins, William Courtney, Andrew
Dunsworth, Edward Farhi, Brooks Foxen, Austin Fowler, Craig
Gidney, Marissa Giustina, Rob Graff, Keith Guerin, Steve Habeg-
ger, Matthew P. Harrigan, Michael J. Hartmann, Alan Ho, Markus
Hoffmann, Trent Huang, Travis S. Humble, Sergei V. Isakov,
Evan Jeffrey, Zhang Jiang, Dvir Kafri, Kostyantyn Kechedzhi, Ju-
lian Kelly, Paul V. Klimov, Sergey Knysh, Alexander Korotkov,
Fedor Kostritsa, David Landhuis, Mike Lindmark, Erik Lucero,
Dmitry Lyakh, Salvatore Mandr, Jarrod R. McClean, Matthew
McEwen, Anthony Megrant, Xiao Mi, Kristel Michielsen, Ma-
soud Mohseni, Josh Mutus, Ofer Naaman, Matthew Neeley,
Charles Neill, Murphy Yuezhen Niu, Eric Ostby, Andre Petukhov,
John C. Platt, Chris Quintana, Eleanor G. Rieffel, Pedram
Roushan, Nicholas C. Rubin, Daniel Sank, Kevin J. Satzinger,
Vadim Smelyanskiy, Kevin J. Sung, Matthew D. Trevithick, Amit
Vainsencher, Benjamin Villalonga, Theodore White, Z. Jamie Yao,
Ping Yeh, Adam Zalcman, Hartmut Neven, and John M. Marti-
nis. Quantum supremacy using a programmable superconducting
processor. Nature, 574(7779):505510, October 2019. Nature,
574(7779):505510. 3, 23, 76
[Aar09] Scott Aaronson. Quantum Copy-Protection and Quantum Money.
In Proceedings of the 2009 24th Annual IEEE Conference on Com-
putational Complexity, CCC 09, pages 229242, USA, July 2009.
IEEE Computer Society. CCC 09, pages 229242. 172
[Aar15] Scott Aaronson. Read the fine print. Nature Physics, 11(4):291
293, April 2015. Nature Physics, 11(4):291293. 61
[AAR+18] Mohammad H. Amin, Evgeny Andriyash, Jason Rolfe, Bohdan
Kulchytskyy, and Roger Melko. Quantum Boltzmann Machine.
Physical Review X, 8(2):21050, May 2018. PRX, 8(2):21050. 61,
https://www.nature.com/articles/s41586-019-1666-5
https://www.nature.com/articles/s41586-019-1666-5
https://doi.org/10.1109/CCC.2009.42
https://www.nature.com/articles/nphys3272
https://link.aps.org/doi/10.1103/PhysRevX.8.021050
238 Bibliography
[ABB+21] J. M. Arrazola, V. Bergholm, K. Brdler, T. R. Bromley, M. J.
Collins, I. Dhand, A. Fumagalli, T. Gerrits, A. Goussev, L. G.
Helt, J. Hundal, T. Isacsson, R. B. Israel, J. Izaac, S. Jahangiri,
R. Janik, N. Killoran, S. P. Kumar, J. Lavoie, A. E. Lita, D. H.
Mahler, M. Menotti, B. Morrison, S. W. Nam, L. Neuhaus, H. Y.
Qi, N. Quesada, A. Repingon, K. K. Sabapathy, M. Schuld, D. Su,
J. Swinarton, A. Szva, K. Tan, P. Tan, V. D. Vaidya, Z. Vernon,
Z. Zabaneh, and Y. Zhang. Quantum circuits with many photons
on a programmable nanophotonic chip. Nature, 591(7848):5460,
March 2021. Nature, 591(7848):5460. 22
[ABC+16] Martn Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy
Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geof-
frey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg,
Rajat Monga, Sherry Moore, Derek G. Murray, Benoit Steiner,
Paul Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan
Yu, and Xiaoqiang Zheng. TensorFlow: A system for large-
scale machine learning. arXiv:1605.08695 [cs], May 2016. ArXiv:
1605.08695. 45
[ABG06] Esma Ameur, Gilles Brassard, and Sbastien Gambs. Machine
Learning in a Quantum World. In Luc Lamontagne and Mario
Marchand, editors, Advances in Artificial Intelligence, Lecture
Notes in Computer Science, pages 431442, Berlin, Heidelberg,
2006. Springer. LNCS,23 vol 4013. 60
[ABO08] Dorit Aharonov and Michael Ben-Or. Fault-Tolerant Quan-
tum Computation with Constant Error Rate. SIAM Journal
on Computing, 38(4):12071282, January 2008. SIAM JoC,
38(4):12071282. 19
[ACC+20] Andrew Arrasmith, M. Cerezo, Piotr Czarnik, Lukasz Cincio, and
Patrick J. Coles. Effect of barren plateaus on gradient-free op-
timization. arXiv:2011.12245 [quant-ph, stat], November 2020.
ArXiv: 2011.12245. 71
[ACL+20] Srinivasan Arunachalam, Sourav Chakraborty, Troy Lee, Manaswi
Paraashar, and Ronald de Wolf. Two new results about quantum
exact learning. arXiv:1810.00481 [quant-ph], April 2020. ArXiv:
1810.00481. 81
[ADBL20] Juan Miguel Arrazola, Alain Delgado, Bhaskar Roy Bardhan, and
Seth Lloyd. Quantum-inspired algorithms in practice. Quantum,
4:307, August 2020. Quantum, 4:307. 62
[ADJ+20] Deanna M. Abrams, Nicolas Didier, Blake R. Johnson, Marcus
P. da Silva, and Colm A. Ryan. Implementation of XY entangling
https://www.nature.com/articles/s41586-021-03202-1
http://arxiv.org/abs/1605.08695
http://arxiv.org/abs/1605.08695
https://link.springer.com/chapter/10.1007/11766247_37
https://epubs.siam.org/doi/10.1137/S0097539799359385
https://epubs.siam.org/doi/10.1137/S0097539799359385
http://arxiv.org/abs/2011.12245
http://arxiv.org/abs/1810.00481
http://arxiv.org/abs/1810.00481
https://quantum-journal.org/papers/q-2020-08-13-307/
Bibliography 239
gates with a single calibrated pulse. Nature Electronics, 3(12):744
750, December 2020. Nature Electronics, 3(12):744750. 24
[AdW17a] Srinivasan Arunachalam and Ronald de Wolf. Guest Column:
A Survey of Quantum Learning Theory. ACM SIGACT News,
48(2):4167, June 2017. ACM SIGACT News, 48(2):4167. 56,
57, 81
[AdW17b] Srinivasan Arunachalam and Ronald de Wolf. Optimal quantum
sample complexity of learning algorithms. In Proceedings of the
32nd Computational Complexity Conference, CCC 17, pages 1
31, Riga, Latvia, July 2017. Schloss DagstuhlLeibniz-Zentrum
fuer Informatik. CCC 17, 131. 56, 81
[AGS19] Srinivasan Arunachalam, Alex B. Grilo, and Aarthi Sun-
daram. Quantum hardness of learning shallow classical cir-
cuits. arXiv:1903.02840 [quant-ph], September 2019. ArXiv:
1903.02840. 81
[AGY20] Srinivasan Arunachalam, Alex B. Grilo, and Henry Yuen. Quantum
statistical query learning. arXiv:2002.08240 [quant-ph], February
2020. ArXiv: 2002.08240. 81
[AHCC21] Andrew Arrasmith, Zo Holmes, M. Cerezo, and Patrick J. Coles.
Equivalence of quantum barren plateaus to cost concentration and
narrow gorges. arXiv:2104.05868 [quant-ph], April 2021. ArXiv:
2104.05868. 71, 72
[AKE+20] Thomas Alexander, Naoki Kanazawa, Daniel J. Egger, Lauren
Capelluto, Christopher J. Wood, Ali Javadi-Abhari, and David C.
McKay. Qiskit pulse: programming quantum computers through
the cloud with pulses. Quantum Science and Technology,
5(4):044006, August 2020. QST, 5(4):044006. 24
[Ale03] Alexei Kitaev. Quantum coin-flipping. Unpublished result. 6th An-
nual workshop on Quantum Information Processing (QIP), 2003.
QIP 2003. 193
[ALOPO20] Javier Alcazar, Vicente Leyton-Ortega, and Alejandro Perdomo-
Ortiz. Classical versus quantum models in machine learning: in-
sights from a finance application. Machine Learning: Science and
Technology, 1(3):035003, July 2020. MLST, 1(3):035003. 159
[AM19] Matthew Amy and Michele Mosca. T-Count Optimization and
ReedMuller Codes. IEEE Transactions on Information Theory,
65(8):47714784, August 2019. IEEE TIT, 65(8):47714784.
https://www.nature.com/articles/s41928-020-00498-1
https://doi.org/10.1145/3106700.3106710
https://dl.acm.org/doi/10.5555/3135595.3135620
http://arxiv.org/abs/1903.02840
http://arxiv.org/abs/1903.02840
http://arxiv.org/abs/2002.08240
http://arxiv.org/abs/2104.05868
http://arxiv.org/abs/2104.05868
https://doi.org/10.1088/2058-9565/aba404
http://web.archive.org/web/20151016194300/https://www.msri.org/workshops/204
https://doi.org/10.1088%2F2632-2153%2Fab9009
https://ieeexplore.ieee.org/document/8672175/
240 Bibliography
[Ang92] Dana Angluin. Computational learning theory: survey and selected
bibliography. In Proceedings of the twenty-fourth annual ACM
symposium on Theory of Computing, STOC 92, pages 351369,
New York, NY, USA, July 1992. Association for Computing Ma-
chinery. STOC 92, pages 351369. 55
[ARDAG20] Abhinav Anand, Jonathan Romero, Matthias Degroote, and
Aln Aspuru-Guzik. Experimental demonstration of a quan-
tum generative adversarial network for continuous distributions.
arXiv:2006.01976 [quant-ph], June 2020. ArXiv: 2006.01976. 160
[AS66] S. M. Ali and S. D. Silvey. A General Class of Coefficients of Diver-
gence of One Distribution from Another. Journal of the Royal Sta-
tistical Society. Series B (Methodological), 28(1):131142, 1966.
JRSS, Series B (Methodological), 28(1):131142. 28
[AS05] Alp Atici and Rocco A. Servedio. Improved Bounds on Quantum
Learning Algorithms. Quantum Information Processing, 4(5):355
386, November 2005. QIP, 4(5):355-386. 81
[ASZ+21] Amira Abbas, David Sutter, Christa Zoufal, Aurelien Lucchi,
Alessio Figalli, and Stefan Woerner. The power of quantum neu-
ral networks. Nature Computational Science, 1(6):403409, June
2021. Nature Computational Science, 1(6):403409. 85, 120
[ATSVY00] Dorit Aharonov, Amnon Ta-Shma, Umesh V. Vazirani, and An-
drew C. Yao. Quantum bit escrow. In Proceedings of the thirty-
second annual ACM symposium on Theory of computing, STOC
00, pages 705714, Portland, Oregon, USA, May 2000. Associa-
tion for Computing Machinery. SIAM JoC, 38(4):12071282. 193,
194, 198, 199, 216
[BB14] Charles H. Bennett and Gilles Brassard. Quantum cryptography:
Public key distribution and coin tossing. Theoretical Computer
Science, 560:711, December 2014. TCS, 560:711. 36, 172,
191, 193, 198
[BBBG09] Guido Berln, Gilles Brassard, Flix Bussires, and Nicolas God-
bout. Fair loss-tolerant quantum coin flipping. Phys. Rev. A,
80(6):062321, December 2009. PRA, 80(6):062321. 193, 198
[BBHB97] V. Buek, S. L. Braunstein, M. Hillery, and D. Bru. Quan-
tum copying: A network. Physical Review A, 56(5):34463452,
November 1997. PRA, 56(5):34463452. 37, 204, 210
[BC21] Leonardo Banchi and Gavin E. Crooks. Measuring Analytic Gradi-
ents of General Quantum Evolution with the Stochastic Parameter
Shift Rule. Quantum, 5:386, January 2021. Quantum, 5:386. 32
https://doi.org/10.1145/129712.129746
http://arxiv.org/abs/2006.01976
http://www.jstor.org/stable/2984279
https://doi.org/10.1007/s11128-005-0001-2
https://www.nature.com/articles/s43588-021-00084-1
https://doi.org/10.1145/335305.335404
http://www.sciencedirect.com/science/article/pii/S0304397514004241
https://link.aps.org/doi/10.1103/PhysRevA.80.062321
https://link.aps.org/doi/10.1103/PhysRevA.56.3446
https://quantum-journal.org/papers/q-2021-01-25-386/
Bibliography 241
[BCF+96] Howard Barnum, Carlton M. Caves, Christopher A. Fuchs, Richard
Jozsa, and Benjamin Schumacher. Noncommuting Mixed States
Cannot Be Broadcast. Physical Review Letters, 76(15):2818
2821, April 1996. PRL, 76(15):28182821. 35, 190
[BCF+21] Marcello Benedetti, Brian Coyle, Mattia Fiorentini, Michael
Lubasch, and Matthias Rosenkranz. Variational inference with a
quantum computer. Phys. Rev. Applied, 16:044057, Oct 2021.
Phys. Rev. Applied 16, 044057. x, 76, 169
[BCLK+21] Kishor Bharti, Alba Cervera-Lierta, Thi Ha Kyaw, Tobias
Haug, Sumner Alperin-Lea, Abhinav Anand, Matthias Degroote,
Hermanni Heimonen, Jakob S. Kottmann, Tim Menke, Wai-
Keong Mok, Sukin Sim, Leong-Chuan Kwek, and Aln Aspuru-
Guzik. Noisy intermediate-scale quantum (NISQ) algorithms.
arXiv:2101.08448 [cond-mat, physics:quant-ph], January 2021.
ArXiv: 2101.08448. 62
[BCMDM00] Dagmar Bru, Mirko Cinchetti, G. Mauro DAriano, and Chiara
Macchiavello. Phase-covariant quantum cloning. Physical Review
A, 62(1):012302, June 2000. PRA, 62(1):012302. 36
[BCWdW01] Harry Buhrman, Richard Cleve, John Watrous, and Ronald
de Wolf. Quantum Fingerprinting. Physical Review Letters,
87(16):167902, September 2001. PRL, 87(16):167902. 31, 32
[BDE+98] Dagmar Bru, David P. DiVincenzo, Artur Ekert, Christopher A.
Fuchs, Chiara Macchiavello, and John A. Smolin. Optimal uni-
versal and state-dependent quantum cloning. Physical Review A,
57(4):23682378, April 1998. PRA, 57(4):23682378. 38, 197
[BEHW89] Anselm Blumer, A. Ehrenfeucht, David Haussler, and Manfred K.
Warmuth. Learnability and the Vapnik-Chervonenkis dimension.
Journal of the ACM, 36(4):929965, October 1989. JACM,
36(4):929965. 56, 57
[Bel64] J. S. Bell. On the Einstein Podolsky Rosen paradox.
Physics Physique Fizika, 1(3):195200, November 1964. PPF,
1(3):195200. 172
[BFK09] Anne Broadbent, Joseph Fitzsimons, and Elham Kashefi. Universal
blind quantum computation. In 2009 50th Annual IEEE Sympo-
sium on Foundations of Computer Science, pages 517526. IEEE,
2009. FOCS 09, 517526. 36, 172
[BFNV19] Adam Bouland, Bill Fefferman, Chinmay Nirkhe, and Umesh Vazi-
rani. On the complexity and verification of quantum random circuit
sampling. Nature Physics, 15(2):159163, February 2019. Nat.
Phys., 15(2):159163. 129
https://link.aps.org/doi/10.1103/PhysRevLett.76.2818
https://link.aps.org/doi/10.1103/PhysRevApplied.16.044057
http://arxiv.org/abs/2101.08448
https://link.aps.org/doi/10.1103/PhysRevA.62.012302
https://link.aps.org/doi/10.1103/PhysRevLett.87.167902
https://link.aps.org/doi/10.1103/PhysRevA.57.2368
https://doi.org/10.1145/76359.76371
https://doi.org/10.1145/76359.76371
https://link.aps.org/doi/10.1103/PhysicsPhysiqueFizika.1.195
https://link.aps.org/doi/10.1103/PhysicsPhysiqueFizika.1.195
https://ieeexplore.ieee.org/document/5438603
https://www.nature.com/articles/s41567-018-0318-2
https://www.nature.com/articles/s41567-018-0318-2
242 Bibliography
[BGMT17] Sergey Bravyi, Jay M. Gambetta, Antonio Mezzacapo, and Kristan
Temme. Tapering off qubits to simulate fermionic Hamiltonians.
arXiv:1701.08213 [quant-ph], January 2017. ArXiv: 1701.08213.
[BGPP+19] Marcello Benedetti, Delfina Garcia-Pintos, Oscar Perdomo, Vi-
cente Leyton-Ortega, Yunseong Nam, and Alejandro Perdomo-
Ortiz. A generative modeling approach for benchmarking and train-
ing shallow quantum circuits. npj Quantum Information, 5(1):19,
May 2019. npj QI, 5(1):19. 75
[BGR+06] Karsten M Borgwardt, Arthur Gretton, Malte J Rasch, Hans-
Peter Kriegel, Bernhard Schlkopf, and Alex J Smola. Integrat-
ing structured biological data by Kernel Maximum Mean Discrep-
ancy. Bioinformatics, 22(14):e49e57, 2006. Bioinformatics,
22(14):e49e57. 27
[BGW09] Radu Ioan Bo, Sorin-Mihai Grad, and Gert Wanka. Introduction.
In Radu Ioan Bot, Sorin-Mihai Grad, and Gert Wanka, editors,
Duality in Vector Optimization, Vector Optimization, pages 17.
Springer, Berlin, Heidelberg, 2009. Duality in Vector Optimization,
Vector Optimization, pages 17. 143
[BH96] V. Buek and M. Hillery. Quantum copying: Beyond the no-cloning
theorem. Physical Review A, 54(3):18441852, September 1996.
PRA, 54(3):18441852. 33, 36
[BH98] V. Buek and Mark Hillery. Universal Optimal Cloning of Arbitrary
Quantum States: From Qubits to Quantum Registers. Physi-
cal Review Letters, 81(22):50035006, November 1998. PRL,
81(22):50035006. 186
[Bia19] Jacob Biamonte. Universal Variational Quantum Computation.
arXiv:1903.04500 [quant-ph], March 2019. ArXiv: 1903.04500.
[Bia20] Jacob Biamonte. On the Theory of Modern Quantum Algorithms.
arXiv:2009.10088 [math-ph, physics:quant-ph], September 2020.
ArXiv: 2009.10088. 60, 62
[BIS+20] Ville Bergholm, Josh Izaac, Maria Schuld, Christian Gogolin,
M. Sohaib Alam, Shahnawaz Ahmed, Juan Miguel Arrazola,
Carsten Blank, Alain Delgado, Soran Jahangiri, Keri McKiernan,
Johannes Jakob Meyer, Zeyue Niu, Antal Szva, and Nathan Kil-
loran. PennyLane: Automatic differentiation of hybrid quantum-
classical computations. arXiv:1811.04968 [physics, physics:quant-
ph], February 2020. ArXiv: 1811.04968. 69, 70
http://arxiv.org/abs/1701.08213
https://www.nature.com/articles/s41534-019-0157-8
http://dx.doi.org/10.1093/bioinformatics/btl242
http://dx.doi.org/10.1093/bioinformatics/btl242
https://doi.org/10.1007/978-3-642-02886-1_1
https://doi.org/10.1007/978-3-642-02886-1_1
https://link.aps.org/doi/10.1103/PhysRevA.54.1844
https://link.aps.org/doi/10.1103/PhysRevLett.81.5003
https://link.aps.org/doi/10.1103/PhysRevLett.81.5003
http://arxiv.org/abs/1903.04500
http://arxiv.org/abs/2009.10088
http://arxiv.org/abs/1811.04968
Bibliography 243
[BJ95] Nader H. Bshouty and Jeffrey C. Jackson. Learning DNF over the
uniform distribution using a quantum example oracle. In Proceed-
ings of the eighth annual conference on Computational learning
theory, COLT 95, pages 118127, New York, NY, USA, July
1995. Association for Computing Machinery. COLT 95, pages
118127. 60, 80, 81
[BJ12] Jeffrey Booth Jr. Quantum Compiler Optimizations.
arXiv:1206.3348 [quant-ph], June 2012. ArXiv: 1206.3348.
[BJS11] Michael J Bremner, Richard Jozsa, and Dan J Shepherd. Classi-
cal simulation of commuting quantum computations implies col-
lapse of the polynomial hierarchy. Proc. R. Soc. London A Math.
Phys. Eng. Sci., 467(2126):459472, 2011. PRS London AMPES,
467(2126):459472. 126, 128, 130
[BK10] Robin Blume-Kohout. Optimal, reliable estimation of quantum
states. New Journal of Physics, 12(4):043034, April 2010. NJP,
12(4):043034. 78
[BK15] Joonwoo Bae and Leong-Chuan Kwek. Quantum state discrim-
ination and its applications. Journal of Physics A: Mathemati-
cal and Theoretical, 48(8):083001, January 2015. JPA: M&T,
48(8):083001. 199
[BKL+21a] Hector Bombin, Isaac H. Kim, Daniel Litinski, Naomi Nickerson,
Mihir Pant, Fernando Pastawski, Sam Roberts, and Terry Rudolph.
Interleaving: Modular architectures for fault-tolerant photonic
quantum computing. arXiv:2103.08612 [quant-ph], March 2021.
ArXiv: 2103.08612. 22
[BKL+21b] Kaifeng Bu, Dax Enshan Koh, Lu Li, Qingxian Luo, and Yaobo
Zhang. Effects of quantum resources on the statistical complexity
of quantum circuits. arXiv:2102.03282 [quant-ph, stat], February
2021. ArXiv: 2102.03282. 120
[BKL+21c] Kaifeng Bu, Dax Enshan Koh, Lu Li, Qingxian Luo, and
Yaobo Zhang. On the statistical complexity of quantum cir-
cuits. arXiv:2101.06154 [quant-ph, stat], January 2021. ArXiv:
2102.03282. 120
[BKL+21d] Kaifeng Bu, Dax Enshan Koh, Lu Li, Qingxian Luo, and Yaobo
Zhang. Rademacher complexity of noisy quantum circuits.
arXiv:2103.03139 [quant-ph], March 2021. ArXiv: 2103.03139.
https://doi.org/10.1145/225298.225312
https://doi.org/10.1145/225298.225312
http://arxiv.org/abs/1206.3348
http://rspa.royalsocietypublishing.org/content/467/2126/459
http://rspa.royalsocietypublishing.org/content/467/2126/459
https://doi.org/10.1088/1367-2630/12/4/043034
https://doi.org/10.1088/1367-2630/12/4/043034
https://doi.org/10.1088/1751-8113/48/8/083001
https://doi.org/10.1088/1751-8113/48/8/083001
http://arxiv.org/abs/2103.08612
http://arxiv.org/abs/2102.03282
http://arxiv.org/abs/2101.06154
http://arxiv.org/abs/2101.06154
http://arxiv.org/abs/2103.03139
244 Bibliography
[BL17] Daniel J. Bernstein and Tanja Lange. Post-quantum cryptog-
raphy. Nature, 549(7671):188194, September 2017. Nature,
549(7671):188194. 55
[BLSF19] Marcello Benedetti, Erika Lloyd, Stefan Sack, and Mattia Fioren-
tini. Parameterized quantum circuits as machine learning models.
Quantum Science & Technology, 4(4):043001, November 2019.
QST, 4(4):043001. 62
[Blu83] Manuel Blum. Coin flipping by telephone a protocol for solving im-
possible problems. Association for Computing Machinery, January
1983. ACM January 1983. 193
[BM08] Dagmar Bru and Chiara Macchiavello. Approximate Quantum
Cloning. In Lectures on Quantum Information, pages 5371. John
Wiley & Sons, Ltd, 2008. Link. 194
[BMS16] Michael J. Bremner, Ashley Montanaro, and Dan J. Shep-
herd. Average-Case Complexity Versus Approximate Simulation
of Commuting Quantum Computations. Physical Review Letters,
117(8):080501, August 2016. PRL, 117(8):080501. 130, 131
[BNSS96] E. C. Behrman, J. Niemel, J. E. Steck, and S. R. Skinner. A
Quantum Dot Neural Network, 1996. Link. 60
[BPLC+19] Carlos Bravo-Prieto, Ryan LaRose, M. Cerezo, Yigit Subasi,
Lukasz Cincio, and Patrick J. Coles. Variational Quantum Linear
Solver: A Hybrid Algorithm for Linear Systems. arXiv:1909.05820
[quant-ph], September 2019. ArXiv: 1909.05820. 185
[Bre03] Gavin K. Brennen. An observable measure of entanglement for
pure states of multi-qubit systems. arXiv:quant-ph/0305094,
November 2003. ArXiv: 0305094. 158
[BRGPO18] Marcello Benedetti, John Realpe-Gmez, and Alejandro Perdomo-
Ortiz. Quantum-assisted Helmholtz machines: A quantum-
classical deep learning framework for industrial datasets in near-
term devices. Quantum Science & Technology, 3(3):034007, July
2018. QST, 3(3):034007. 61
[Bru19] Todd A. Brun. Quantum Error Correction. arXiv:1910.03672
[quant-ph], October 2019. ArXiv: 1910.03672. 19
[BS02] Hans-Georg Beyer and Hans-Paul Schwefel. Evolution strategies:
A comprehensive introduction. Natural Computing, 1(1):352,
March 2002. Natural Computing, 1(1):352. 47
https://www.nature.com/articles/nature23461
https://www.nature.com/articles/nature23461
https://doi.org/10.1088%2F2058-9565%2Fab4eb5
https://doi.org/10.1145/1008908.1008911
https://onlinelibrary.wiley.com/doi/abs/10.1002/9783527618637.ch4
https://link.aps.org/doi/10.1103/PhysRevLett.117.080501
http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.56.1507
http://arxiv.org/abs/1909.05820
http://arxiv.org/abs/quant-ph/0305094
http://stacks.iop.org/2058-9565/3/i=3/a=034007?key=crossref.b76591ee4bc7a7df6153c5cfaf284353
http://arxiv.org/abs/1910.03672
https://doi.org/10.1023/A:1015059928466
Bibliography 245
[BVM+20] Michael Broughton, Guillaume Verdon, Trevor McCourt, Anto-
nio J. Martinez, Jae Hyeon Yoo, Sergei V. Isakov, Philip Massey,
Murphy Yuezhen Niu, Ramin Halavati, Evan Peters, Martin Leib,
Andrea Skolik, Michael Streif, David Von Dollen, Jarrod R. Mc-
Clean, Sergio Boixo, Dave Bacon, Alan K. Ho, Hartmut Neven,
and Masoud Mohseni. TensorFlow Quantum: A Software Frame-
work for Quantum Machine Learning. arXiv:2003.02989 [cond-
mat, physics:quant-ph], March 2020. ArXiv: 2003.02989. 70
[BWP+17] Jacob Biamonte, Peter Wittek, Nicola Pancotti, Patrick Reben-
trost, Nathan Wiebe, and Seth Lloyd. Quantum Machine Learn-
ing. Nature, 549(7671):195202, September 2017. Nature,
549(7671):195202. 62
[CAB+21] M. Cerezo, Andrew Arrasmith, Ryan Babbush, Simon C. Benjamin,
Suguru Endo, Keisuke Fujii, Jarrod R. McClean, Kosuke Mitarai,
Xiao Yuan, Lukasz Cincio, and Patrick J. Coles. Variational quan-
tum algorithms. Nature Reviews Physics, pages 120, August
2021. Nat. Rev. Phys., 120. 62, 63, 64, 66, 67
[CCC19] Patrick J. Coles, M. Cerezo, and Lukasz Cincio. Strong bound
between trace distance and Hilbert-Schmidt distance for low-rank
states. Physical Review A, 100(2):022103, August 2019. PRA,
100(2):022103. 30
[CK20] Ieva epaite, Brian Coyle, and Elham Kashefi. A Continuous Vari-
able Born Machine. arXiv:2011.00904 [quant-ph], November 2020.
ArXiv: 2011.00904. x
[CCL19] Iris Cong, Soonwon Choi, and Mikhail D. Lukin. Quantum con-
volutional neural networks. Nature Physics, 15(12):12731278,
December 2019. Nat. Phys., 15(12):12731278. 68, 72, 77, 85
[CCW18] Song Cheng, Jing Chen, and Lei Wang. Information Perspective
to Probabilistic Modeling: Boltzmann Machines versus Born Ma-
chines. Entropy, 20(8):583, August 2018. Entropy, 20(8):583.
[CD20] Matthias C. Caro and Ishaun Datta. Pseudo-dimension of quantum
circuits. Quantum Machine Intelligence, 2(2):14, November 2020.
QUMI, 2(2):14. 120
[CDKK20] Brian Coyle, Mina Doosti, Elham Kashefi, and Niraj Kumar.
Variational Quantum Cloning: Improving Practicality for Quan-
tum Cryptanalysis. arXiv:2012.11424 [quant-ph], December 2020.
ArXiv: 2012.11424. ix
http://arxiv.org/abs/2003.02989
https://www.nature.com/articles/nature23474
https://www.nature.com/articles/nature23474
https://www.nature.com/articles/s42254-021-00348-9
https://link.aps.org/doi/10.1103/PhysRevA.100.022103
https://link.aps.org/doi/10.1103/PhysRevA.100.022103
http://arxiv.org/abs/2011.00904
https://www.nature.com/articles/s41567-019-0648-8
http://www.mdpi.com/1099-4300/20/8/583
https://doi.org/10.1007/s42484-020-00027-5
http://arxiv.org/abs/2012.11424
246 Bibliography
[CFUZ02] Jing-Ling Chen, Libin Fu, Abraham A. Ungar, and Xian-Geng
Zhao. Alternative fidelity measure between two states of an N-
state quantum system. Physical Review A, 65(5):054304, May
2002. PRA, 65(5):054304. 190
[CGL+20] Nai-Hui Chia, Andrs Gilyn, Tongyang Li, Han-Hsuan Lin, Ewin
Tang, and Chunhao Wang. Sampling-based sublinear low-rank
matrix arithmetic framework for dequantizing quantum machine
learning. In Proceedings of the 52nd Annual ACM SIGACT Sym-
posium on Theory of Computing, STOC 2020, pages 387400,
New York, NY, USA, June 2020. Association for Computing Ma-
chinery. STOC 20, 387400. 62
[CH17] Earl T. Campbell and Mark Howard. Unified framework for magic
state distillation and multiqubit gate synthesis with reduced re-
source cost. Physical Review A, 95(2):022316, February 2017.
PRA, 95(2):022316. 151
[CHL+21] Brian Coyle, Maxwell Henderson, Justin Chan Jin Le, Niraj Kumar,
Marco Paini, and Elham Kashefi. Quantum versus classical gen-
erative modelling in finance. Quantum Science and Technology,
6(2):024013, April 2021. QST: 6(2):024013. ix
[Cho19] Adrian Cho. IBM casts doubt on Googles
claims of quantum supremacy, October 2019.
https://www.sciencemag.org/news/2019/10/ibm-casts-doubt-
googles-claims-quantum-supremacy. 3
[CIVA02] N.J. Cerf, S. Iblisdir, and G. Van Assche. Cloning and cryptography
with quantum continuous variables. Eur. Phys. J. D, 18(2):211
218, February 2002. EPJD, 18(2):211218. 187
[CKH19] Brian Coyle, Elham Kashefi, and Matty J. Hoban. Certified Ran-
domness From Steering Using Sequential Measurements. Cryp-
tography, 3(4):27, December 2019. Cryptography, 3(4):27. x
[CLW18] Nai-Hui Chia, Han-Hsuan Lin, and Chunhao Wang. Quantum-
inspired sublinear classical algorithms for solving low-rank linear
systems. arXiv:1811.04852 [quant-ph], November 2018. ArXiv:
1811.04852. 62
[CMDK20] Brian Coyle, Daniel Mills, Vincent Danos, and Elham Kashefi. The
Born supremacy: quantum advantage and training of an Ising Born
machine. npj Quantum Information, 6(1):111, July 2020. npj QI,
6(1):1-11. ix
https://link.aps.org/doi/10.1103/PhysRevA.65.054304
https://doi.org/10.1145/3357713.3384314
https://link.aps.org/doi/10.1103/PhysRevA.95.022316
https://doi.org/10.1088/2058-9565/abd3db
https://www.sciencemag.org/news/2019/10/ibm-casts-doubt-googles-claims-quantum-supremacy
https://www.sciencemag.org/news/2019/10/ibm-casts-doubt-googles-claims-quantum-supremacy
https://doi.org/10.1140/epjd/e20020025
https://www.mdpi.com/2410-387X/3/4/27
http://arxiv.org/abs/1811.04852
http://arxiv.org/abs/1811.04852
https://www.nature.com/articles/s41534-020-00288-9
https://www.nature.com/articles/s41534-020-00288-9
Bibliography 247
[CMMS20] Chris Cade, Lana Mineh, Ashley Montanaro, and Stasja Stanisic.
Strategies for solving the Fermi-Hubbard model on near-term quan-
tum computers. Physical Review B, 102(23):235122, December
2020. PRB, 102(23):235122. 49, 64, 70
[CMO+20] Jacques Carolan, Masoud Mohseni, Jonathan P. Olson, Mihika
Prabhu, Changchen Chen, Darius Bunandar, Murphy Yuezhen Niu,
Nicholas C. Harris, Franco N. C. Wong, Michael Hochberg, Seth
Lloyd, and Dirk Englund. Variational quantum unsampling on
a quantum photonic processor. Nature Physics, 16(3):322327,
March 2020. Nat. Phys. 16, 322327. 77, 78
[Co15] Francois Chollet and others. Keras, 2015.
https://github.com/fchollet/keras. 45
[Coy18] Brian Coyle. Project / Dissertation Submission Index. PhD Thesis,
University of Edinburgh, August 2018. Link. 125, 126
[Coy20] Brian Coyle. NoiseRobustClassifier: Noise Robust Data En-
codings for Quantum Classifiers. Zenodo, February 2020.
https://github.com/BrianCoyle/NoiseRobustClassifier/. 110, 112
[CPCC20] Marco Cerezo, Alexander Poremba, Lukasz Cincio, and Patrick J.
Coles. Variational Quantum Fidelity Estimation. Quantum, 4:248,
March 2020. Quantum, 4:248. 31
[CPH05] Miguel . Carreira-Perpin and Geoffrey Hinton. On contrastive
divergence learning. In Robert G. Cowell and Zoubin Ghahramani,
editors, Proceedings of the Tenth International Workshop on Arti-
ficial Intelligence and Statistics, volume R5 of Proceedings of Ma-
chine Learning Research, pages 3340. PMLR, 0608 Jan 2005.
AISTATS 05, 33-40. 51
[Cro19] Gavin E. Crooks. Gradients of parameterized quantum
gates using the parameter-shift rule and gate decomposition.
arXiv:1905.13311 [quant-ph], May 2019. ArXiv: 1905.13311. 70
[CRSC21] Lukasz Cincio, Kenneth Rudinger, Mohan Sarovar, and Patrick J.
Coles. Machine Learning of Noise-Resilient Quantum Circuits.
PRX Quantum, 2(1):010324, February 2021. PRX Quantum,
2(1):010324. 66
[CSAC20] M. Cerezo, Kunal Sharma, Andrew Arrasmith, and Patrick J.
Coles. Variational Quantum State Eigensolver. arXiv:2004.01372
[quant-ph], April 2020. ArXiv: 2004.01372. 175, 190
[Csi67] I. Csiszar. Information-type measures of difference of probability
distributions and indirect observation. Studia Scientiarum Mathe-
maticarum Hungarica, 2:229318, 1967. SSMH, 2:229318. 28
https://link.aps.org/doi/10.1103/PhysRevB.102.235122
https://www.nature.com/articles/s41567-019-0747-6
https://github.com/fchollet/keras
https://project-archive.inf.ed.ac.uk/msc/2018-outstanding.html
https://github.com/BrianCoyle/NoiseRobustClassifier/
https://quantum-journal.org/papers/q-2020-03-26-248/
http://proceedings.mlr.press/r5/carreira-perpinan05a.html
http://arxiv.org/abs/1905.13311
https://link.aps.org/doi/10.1103/PRXQuantum.2.010324
https://link.aps.org/doi/10.1103/PRXQuantum.2.010324
http://arxiv.org/abs/2004.01372
https://ci.nii.ac.jp/naid/10028997448/en/
248 Bibliography
[CSSC18] Lukasz Cincio, Yiit Suba, Andrew T. Sornborger, and Patrick J.
Coles. Learning the quantum algorithm for state overlap. New
Journal of Physics, 20(11):113022, November 2018. NJP,
20(11):113022. 68, 190, 208
[CSU+20] D. Chivilikhin, A. Samarin, V. Ulyantsev, I. Iorsh, A. R. Oganov,
and O. Kyriienko. MoG-VQE: Multiobjective genetic variational
quantum eigensolver. arXiv:2007.04424 [cond-mat, physics:quant-
ph], July 2020. ArXiv: 2007.04424. 68
[CSV+21] M. Cerezo, Akira Sone, Tyler Volkoff, Lukasz Cincio, and
Patrick J. Coles. Cost function dependent barren plateaus in
shallow parametrized quantum circuits. Nature Communications,
12(1):1791, March 2021. Nature Communications, 12(1):1791.
65, 71, 175, 206, 207
[CT17] Giuseppe Carleo and Matthias Troyer. Solving the Quantum
Many-Body Problem with Artificial Neural Networks. Science,
355(6325):602606, February 2017. Science, 355(6325):60260.
[Cut13] Marco Cuturi. Sinkhorn distances: Lightspeed computation of
optimal transport. In C. J. C. Burges, L. Bottou, M. Welling,
Z. Ghahramani, and K. Q. Weinberger, editors, Advances in Neural
Information Processing Systems, volume 26. Curran Associates,
Inc., 2013. NeurIPS 13. 142
[CWV+19] Shuxiang Cao, Leonard Wossnig, Brian Vlastakis, Peter Leek,
and Edward Grant. Cost function embedding and dataset en-
coding for machine learning with parameterized quantum circuits.
arXiv:1910.03902 [quant-ph], October 2019. ArXiv: 1910.03902.
108, 111, 226
[DB18] Vedran Dunjko and Hans J Briegel. Machine learning & artificial
intelligence in the quantum domain: a review of recent progress.
Reports Prog. Phys., 81(7):074001, July 2018. Reports Prog.
Phys., 81(7):074001. 62
[DCEL09] Christoph Dankert, Richard Cleve, Joseph Emerson, and Etera
Livine. Exact and approximate unitary 2-designs and their applica-
tion to fidelity estimation. Physical Review A, 80(1):012304, July
2009. PRA, 80(1):012304. 71
[DCLT08] Daoyi Dong, Chunlin Chen, Hanxiong Li, and Tzyh-Jong Tarn.
Quantum Reinforcement Learning. IEEE Transactions on Systems,
Man, and Cybernetics, Part B (Cybernetics), 38(5):12071220,
October 2008. IEEE TSMC, 38(5):1207-1220. 61
https://iopscience.iop.org/article/10.1088/1367-2630/aae94a/meta
https://iopscience.iop.org/article/10.1088/1367-2630/aae94a/meta
http://arxiv.org/abs/2007.04424
https://www.nature.com/articles/s41467-021-21728-w
https://science.sciencemag.org/content/355/6325/602
https://proceedings.neurips.cc/paper/2013/hash/af21d0c97db2e27e13572cbf59eb343d-Abstract.html
http://arxiv.org/abs/1910.03902
http://stacks.iop.org/0034-4885/81/i=7/a=074001?key=crossref.484b39e1cdde454de1cbc5aba8d6de34
http://stacks.iop.org/0034-4885/81/i=7/a=074001?key=crossref.484b39e1cdde454de1cbc5aba8d6de34
https://link.aps.org/doi/10.1103/PhysRevA.80.012304
10.1109/TSMCB.2008.925743
Bibliography 249
[DDZ+05] Jiangfeng Du, Thomas Durt, Ping Zou, Hui Li, L. C. Kwek,
C. H. Lai, C. H. Oh, and Artur Ekert. Experimental Quantum
Cloning with Prior Partial Information. Physical Review Letters,
94(4):040505, February 2005. PRL, 94(4):040505. 218, 219
[DG84] Peter J. Diggle and Richard J. Gratton. Monte Carlo Methods of
Inference for Implicit Statistical Models. Journal of the Royal Sta-
tistical Society. Series B (Methodological), 46(2):193227, 1984.
JRSS, 46(2):193227. 75
[DG97] Lu-Ming Duan and Guang-Can Guo. Two non-orthogonal states
can be cloned by a unitary-reduction process. arXiv:quant-
ph/9704020, April 1997. ArXiv: 9704020. 33
[DG98] Lu-Ming Duan and Guang-Can Guo. Probabilistic Cloning and
Identification of Linearly Independent Quantum States. Phys-
ical Review Letters, 80(22):49995002, June 1998. PRL,
80(22):49995002. 33
[DHL+21] Yuxuan Du, Min-Hsiu Hsieh, Tongliang Liu, Dacheng Tao, and
Nana Liu. Quantum noise protects quantum classifiers against
adversaries. Phys. Rev. Research, 3(2):023153, May 2021. PRR,
3(2):023153. 120
[DHM+18] Danial Dervovic, Mark Herbster, Peter Mountney, Simone Severini,
Nari Usher, and Leonard Wossnig. Quantum linear systems algo-
rithms: a primer. arXiv:1802.08227 [quant-ph], February 2018.
ArXiv: 1802.08227. 61
[DHS11] John Duchi, Elad Hazan, and Yoram Singer. Adaptive Subgradient
Methods for Online Learning and Stochastic Optimization. Journal
of Machine Learning Research, 12(61):21212159, 2011. JMLR,
12(61):21212159. 49
[DI00] David P. DiVincenzo and IBM. The Physical Implementation of
Quantum Computation. Fortschritte der Physik, 48(9):771783,
September 2000. FdP,48(9):771-783. 9
[Die82] D. Dieks. Communication by EPR devices. Physics Letters A,
92(6):271272, November 1982. PLA, 92(6):271272. 32
[DLWT18] Vedran Dunjko, Yi-Kai Liu, Xingyao Wu, and Jacob M. Tay-
lor. Exponential improvements for quantum-accessible reinforce-
ment learning. arXiv:1710.11160 [quant-ph], August 2018. ArXiv:
1710.11160. 61
[Doz16] Timothy Dozat. Incorporating Nesterov Momentum into Adam. -,
February 2016. OpenReview. 49
https://link.aps.org/doi/10.1103/PhysRevLett.94.040505
https://link.aps.org/doi/10.1103/PhysRevLett.94.040505
http://arxiv.org/abs/quant-ph/9704020
https://link.aps.org/doi/10.1103/PhysRevLett.80.4999
https://link.aps.org/doi/10.1103/PhysRevLett.80.4999
https://link.aps.org/doi/10.1103/PhysRevResearch.3.023153
https://link.aps.org/doi/10.1103/PhysRevResearch.3.023153
http://arxiv.org/abs/1802.08227
http://jmlr.org/papers/v12/duchi11a.html
http://jmlr.org/papers/v12/duchi11a.html
10.1002/1521-3978(200009)48:9/11<771::AID-PROP771>3.0.CO;2-E
https://www.sciencedirect.com/science/article/pii/0375960182900846
http://arxiv.org/abs/1710.11160
http://arxiv.org/abs/1710.11160
https://openreview.net/forum?id=OM0jvwB8jIp57ZJjtNEZ
250 Bibliography
[DPMTL20] Giacomo De Palma, Milad Marvian, Dario Trevisan, and
Seth Lloyd. The quantum Wasserstein distance of order 1.
arXiv:2009.04469 [math-ph, physics:quant-ph], September 2020.
ArXiv: 2009.04469. 31
[DPS03] G. Mauro DAriano, Matteo G. A. Paris, and Massimiliano F. Sac-
chi. Quantum Tomography. arXiv:quant-ph/0302028, February
2003. ArXiv: 0302028. 78, 204
[DTB17] Vedran Dunjko, Jacob M. Taylor, and Hans J. Briegel. Advances
in quantum reinforcement learning. In 2017 IEEE International
Conference on Systems, Man, and Cybernetics (SMC), pages 282
287, October 2017. IEEE ICSMC, 282287. 61
[Dud02] R. M. Dudley. Real Analysis and Probability. Cambridge Studies
in Advanced Mathematics. Cambridge University Press, 2 edition,
2002. Link. 28
[ECBY21] Suguru Endo, Zhenyu Cai, Simon C. Benjamin, and Xiao Yuan.
Hybrid Quantum-Classical Algorithms and Quantum Error Miti-
gation. Journal of the Physical Society of Japan, 90(3):032001,
March 2021. JPSJ, 90(3):032001. 62, 121
[FG99] C. A. Fuchs and J. van de Graaf. Cryptographic distinguishabil-
ity measures for quantum-mechanical states. IEEE Transactions
on Information Theory, 45(4):12161227, May 1999. IEEE TIT,
45(4):12161227. 226
[FGG+97] Christopher A. Fuchs, Nicolas Gisin, Robert B. Griffiths, Chi-Sheng
Niu, and Asher Peres. Optimal eavesdropping in quantum cryptog-
raphy. I. Information bound and optimal strategy. Physical Review
A, 56(2):11631172, August 1997. PRA, 56(2):11631172. 177
[FGG14] Edward Farhi, Jeffrey Goldstone, and Sam Gutmann. A Quantum
Approximate Optimization Algorithm. arXiv:1411.4028 [quant-
ph], November 2014. ArXiv: 1411.4028. 66
[FGGS00] Edward Farhi, Jeffrey Goldstone, Sam Gutmann, and Michael
Sipser. Quantum Computation by Adiabatic Evolution.
arXiv:quant-ph/0001106, January 2000. ArXiv: 0001106. 10,
14, 67
[FGP20] Daniel Stilck Franca and Raul Garcia-Patron. Limitations of opti-
mization algorithms on noisy quantum devices. arXiv:2009.05532
[quant-ph], September 2020. ArXiv: 2009.05532. 121
[FGSS07] Kenji Fukumizu, Arthur Gretton, Xiaohai Sun, and Bernhard
Schlkopf. Kernel Measures of Conditional Dependence. In NIPS,
2007. NIPS 07. 136
http://arxiv.org/abs/2009.04469
http://arxiv.org/abs/quant-ph/0302028
https://ieeexplore.ieee.org/document/8122616/
https://www.cambridge.org/core/books/real-analysis-and-probability/26DDF2D09E526185F2347AA5658B96F6
https://journals.jps.jp/doi/full/10.7566/JPSJ.90.032001
https://arxiv.org/abs/quant-ph/9712042
https://arxiv.org/abs/quant-ph/9712042
https://link.aps.org/doi/10.1103/PhysRevA.56.1163
http://arxiv.org/abs/1411.4028
http://arxiv.org/abs/quant-ph/0001106
http://arxiv.org/abs/2009.05532
https://papers.nips.cc/paper/2007/hash/3a0772443a0739141292a5429b952fe6-Abstract.html
Bibliography 251
[FH16] Edward Farhi and Aram W. Harrow. Quantum Supremacy
through the Quantum Approximate Optimization Algorithm.
arXiv:1602.07674 [quant-ph], February 2016. ArXiv: 1602.07674.
[Fis36] R. A. Fisher. The Use of Multiple Measurements in Taxo-
nomic Problems. Annals of Eugenics, 7(2):179188, 1936. AoE,
7(2):179188. 111
[FL12] Agnes Ferenczi and Norbert Ltkenhaus. Symmetries in quantum
key distribution and the connection between optimal attacks and
optimal cloning. Physical Review A, 85(5):052310, May 2012.
PRA, 85(5):052310. 211
[FM17] Keisuke Fujii and Tomoyuki Morimae. Commuting quantum cir-
cuits and complexity of Ising partition functions. New J. Phys.,
19(3):33003, March 2017. NJP, 19(3):33003. 126, 130
[FMWW01] Heng Fan, Keiji Matsumoto, Xiang-Bin Wang, and Miki Wadati.
Quantum cloning machines for equatorial qubits. Physical Review
A, 65(1):012304, December 2001. PRA, 65(1):012304. 37, 210
[FN18] Edward Farhi and Hartmut Neven. Classification with Quantum
Neural Networks on Near Term Processors. arXiv:1802.06002
[quant-ph], February 2018. ArXiv: 1802.06002. 84
[Fow11] Austin G. Fowler. Constructing arbitrary steane code single logical
qubit fault-tolerant gates. Quantum Information & Computation,
11(9-10):867873, September 2011. QIC, 11(9-10):867873. 151
[FSV+19] Jean Feydy, Thibault Sjourn, Franois-Xavier Vialard, Shun-ichi
Amari, Alain Trouve, and Gabriel Peyr. Interpolating between Op-
timal Transport and MMD using Sinkhorn Divergences. In Kama-
lika Chaudhuri and Masashi Sugiyama, editors, Proc. Mach. Learn.
Res., volume 89 of Proceedings of Machine Learning Research,
pages 26812690. PMLR, April 2019. PMLR, (89)2681-2690.
143, 144, 145, 148
[Fub04] Guido Fubini. Sulle metriche definite da una forma hermitiana:
nota. Atti del Reale Istituto Veneto di Scienze, Lettere ed Arti,
63:502513, 1904. LeA, 63:502513. 178
[Fuc96] Christopher A. Fuchs. Information Gain vs. State Disturbance
in Quantum Theory. arXiv:quant-ph/9611010, November 1996.
ArXiv: 9611010. 177
http://arxiv.org/abs/1602.07674
https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1469-1809.1936.tb02137.x
https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1469-1809.1936.tb02137.x
https://link.aps.org/doi/10.1103/PhysRevA.85.052310
http://stacks.iop.org/1367-2630/19/i=3/a=033003?key=crossref.cefbe34cf11242886552ceea447a4526
https://link.aps.org/doi/10.1103/PhysRevA.65.012304
http://arxiv.org/abs/1802.06002
https://dl.acm.org/doi/10.5555/2230936.2230946
http://proceedings.mlr.press/v89/feydy19a.html
https://link.springer.com/article/10.1007/BF02420184
http://arxiv.org/abs/quant-ph/9611010
252 Bibliography
[FWJ+14] Heng Fan, Yi-Nan Wang, Li Jing, Jie-Dong Yue, Han-Duo Shi,
Yong-Liang Zhang, and Liang-Zhu Mu. Quantum Cloning Ma-
chines and the Applications. Physics Reports, 544(3):241322,
November 2014. Phys. Rep., 544(3):241322. 37, 204, 210
[GAW+21] Xun Gao, Eric R. Anschuetz, Sheng-Tao Wang, J. Ignacio Cirac,
and Mikhail D. Lukin. Enhancing Generative Models via Quan-
tum Correlations. arXiv:2101.08354 [cond-mat, physics:quant-ph,
stat], January 2021. ArXiv: 2101.08354. 169
[GBC16] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learn-
ing. MIT Press, 2016. https://www.deeplearningbook.org/. 41
[GBC+18] Edward Grant, Marcello Benedetti, Shuxiang Cao, Andrew Hallam,
Joshua Lockhart, Vid Stojevic, Andrew G. Green, and Simone Sev-
erini. Hierarchical quantum classifiers. npj Quantum Information,
4(1):18, December 2018. npj QI, 4(1):18. 74, 84, 85, 102,
111, 112
[GBR+07] Arthur Gretton, Karsten M. Borgwardt, Malte Rasch, Bernhard
Schlkopf, and Alex J. Smola. A Kernel Method for the Two-
Sample-Problem. In B. Schlkopf, J. C. Platt, and T. Hoffman,
editors, Advances in Neural Information Processing Systems 19,
pages 513520. MIT Press, 2007. NIPS 19, 513-520. 27, 134
[GC01] Daniel Gottesman and Isaac Chuang. Quantum Digital Signatures.
arXiv:quant-ph/0105032, November 2001. ArXiv: 0105032. 172
[GCB+19] Aude Genevay, Lnac Chizat, Francis Bach, Marco Cuturi, and
Gabriel Peyr. Sample Complexity of Sinkhorn Divergences. In
Kamalika Chaudhuri and Masashi Sugiyama, editors, Proc. Mach.
Learn. Res., volume 89 of Proceedings of Machine Learning Re-
search, pages 15741583. PMLR, 2019. PMLR, 89:1574-1583.
145, 146, 148
[GCH+19] Kyle Gulshen, Joshua Combes, Matthew P. Harrigan, Peter J.
Karalekas, Marcus P. da Silva, M. Sohaib Alam, Amy Brown,
Shane Caldwell, Lauren Capelluto, Gavin Crooks, Daniel Gir-
shovich, Blake R. Johnson, Eric C. Peterson, Anthony Polloreno,
Nicholas C. Rubin, Colm A. Ryan, Alexa Staley, Nikolas A. Tezak,
and Joseph Valery. Forest Benchmarking: QCVV using PyQuil,
2019. https://github.com/rigetti/forest-benchmarking/. 78, 204,
[GCP+20] Laura Gentini, Alessandro Cuccoli, Stefano Pirandola, Paola
Verrucchi, and Leonardo Banchi. Noise-resilient variational
hybrid quantum-classical optimization. Physical Review A,
102(5):052414, November 2020. PRA, 02(5):052414. 226
https://www.sciencedirect.com/science/article/abs/pii/S0370157314002099
http://arxiv.org/abs/2101.08354
https://www.deeplearningbook.org/
https://www.nature.com/articles/s41534-018-0116-9
https://proceedings.neurips.cc/paper/2006/hash/e9fb2eda3d9c55a0d89c98d6c54b5b3e-Abstract.html
http://arxiv.org/abs/quant-ph/0105032
http://proceedings.mlr.press/v89/genevay19a.html
https://doi.org/10.5281/zenodo.3455847
https://link.aps.org/doi/10.1103/PhysRevA.102.052414
Bibliography 253
[GE21] Craig Gidney and Martin Eker. How to factor 2048 bit RSA
integers in 8 hours using 20 million noisy qubits. Quantum, 5:433,
April 2021. Quantum, 5:433. 20
[GEBM19] Harper R. Grimsley, Sophia E. Economou, Edwin Barnes, and
Nicholas J. Mayhall. An adaptive variational algorithm for exact
molecular simulations on a quantum computer. Nature Commu-
nications, 10(1):3007, July 2019. Nat. Comm., 10(1):3007. 66,
[GF19] Craig Gidney and Austin G. Fowler. Efficient magic state factories
with a catalyzed CCZ to 2T transformation. Quantum, 3:135,
April 2019. Quantum, 3:135. 19
[GFY21] Ji Guan, Wang Fang, and Mingsheng Ying. Robustness Verification
of Quantum Classifiers. arXiv:2008.07230 [quant-ph], May 2021.
ArXiv: 2008.07230. 120
[GKK19] Alexandru Gheorghiu, Theodoros Kapourniotis, and Elham
Kashefi. Verification of Quantum Computation: An Overview of
Existing Approaches. Theory of Computing Systems, 63(4):715
808, May 2019. TCS, 63(4):715808. 20
[GLF+10] David Gross, Yi-Kai Liu, Steven T. Flammia, Stephen Becker, and
Jens Eisert. Quantum State Tomography via Compressed Sensing.
Physical Review Letters, 105(15):150401, October 2010. PRL,
105(15):150401. 78
[GLM08] Vittorio Giovannetti, Seth Lloyd, and Lorenzo Maccone. Quantum
Random Access Memory. Phys. Rev. Letters, 100(16), April 2008.
PRL, 100(16). 61
[GLT18] Andrs Gilyn, Seth Lloyd, and Ewin Tang. Quantum-inspired
low-rank stochastic regression with logarithmic dependence on the
dimension. arXiv:1811.04909 [quant-ph], November 2018. ArXiv:
1811.04909. 62
[GM97] N. Gisin and S. Massar. Optimal Quantum Cloning Machines.
Physical Review Letters, 79(11):21532156, September 1997.
PRL, 79(11):21532156. 35
[GPAM+14] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David
Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Ben-
gio. Generative Adversarial Nets. In Z. Ghahramani, M. Welling,
C. Cortes, N. Lawrence, and K. Q. Weinberger, editors, Advances
in Neural Information Processing Systems, volume 27. Curran As-
sociates, Inc., 2014. NIPS 14, 27. 76
https://doi.org/10.22331/q-2021-04-15-433
https://www.nature.com/articles/s41467-019-10988-2
https://quantum-journal.org/papers/q-2019-04-30-135/
http://arxiv.org/abs/2008.07230
https://doi.org/10.1007/s00224-018-9872-3
https://link.aps.org/doi/10.1103/PhysRevLett.105.150401
https://link.aps.org/doi/10.1103/PhysRevLett.105.150401
https://link.aps.org/doi/10.1103/PhysRevLett.100.160501
http://arxiv.org/abs/1811.04909
http://arxiv.org/abs/1811.04909
https://link.aps.org/doi/10.1103/PhysRevLett.79.2153
https://proceedings.neurips.cc/paper/2014/hash/5ca3e9b122f61f8f06494c97b1afccf3-Abstract.html
254 Bibliography
[GPC18] Aude Genevay, Gabriel Peyre, and Marco Cuturi. Learning Genera-
tive Models with Sinkhorn Divergences. In Proc. Twenty-First Int.
Conf. Artif. Intell. Stat., volume 84 of Proceedings of Machine
Learning Research, pages 16081617, Playa Blanca, Lanzarote,
Canary Islands, April 2018. PMLR. PMLR, 84:1608-1617. 146,
[Gro96] Lov K. Grover. A fast quantum mechanical algorithm for database
search. In Proceedings of the twenty-eighth annual ACM sym-
posium on Theory of Computing, STOC 96, pages 212219,
Philadelphia, Pennsylvania, USA, July 1996. Association for Com-
puting Machinery. STOC 96, 212219. 19, 60
[GS02] Alison L. Gibbs and Francis Edward Su. On Choosing and Bound-
ing Probability Metrics. International Statistical Review / Re-
vue Internationale de Statistique, 70(3):419435, 2002. ISV,
70(3):419435. 142
[GVT20] Francisco Javier Gil Vidal and Dirk Oliver Theis. Input Redundancy
for Parameterized Quantum Circuits. Frontiers in Physics, 8, 2020.
FiP, 8. 93, 120
[GvVD21] Casper Gyurik, Dyon van Vreumingen, and Vedran Dun-
jko. Structural risk minimization for quantum linear classifiers.
arXiv:2105.05566 [quant-ph], May 2021. ArXiv: 2105.05566. 120
[GWOB19] Edward Grant, Leonard Wossnig, Mateusz Ostaszewski, and Mar-
cello Benedetti. An initialization strategy for addressing barren
plateaus in parametrized quantum circuits. Quantum, 3:214, De-
cember 2019. Quantum, 3:214. 72
[GZCW21] Julien Gacon, Christa Zoufal, Giuseppe Carleo, and Stefan Wo-
erner. Simultaneous Perturbation Stochastic Approximation of the
Quantum Fisher Information. arXiv:2103.09232 [quant-ph], March
2021. ArXiv: 2103.09232. 49
[GZD18] X. Gao, Z.-Y. Zhang, and L.-M. Duan. A quantum machine
learning algorithm based on generative models. Science Advances,
4(12):eaat9004, December 2018. Sci. Adv., 4(12):eaat9004. 77
[Han16] Steve Hanneke. The Optimal Sample Complexity of PAC Learning.
Journal of Machine Learning Research, 17(38):115, 2016. JMLR,
17(38):115. 56, 57
[HBM+21] Hsin-Yuan Huang, Michael Broughton, Masoud Mohseni, Ryan
Babbush, Sergio Boixo, Hartmut Neven, and Jarrod R. McClean.
Power of data in quantum machine learning. Nature Communica-
tions, 12(1):2631, May 2021. ArXiv: 2105.02276. 80, 120
http://proceedings.mlr.press/v84/genevay18a.html
https://doi.org/10.1145/237814.237866
http://www.jstor.org/stable/1403865
http://www.jstor.org/stable/1403865
https://www.frontiersin.org/articles/10.3389/fphy.2020.00297/full
http://arxiv.org/abs/2105.05566
https://quantum-journal.org/papers/q-2019-12-09-214/
http://arxiv.org/abs/2103.09232
https://advances.sciencemag.org/content/4/12/eaat9004
http://jmlr.org/papers/v17/15-389.html
http://jmlr.org/papers/v17/15-389.html
https://www.nature.com/articles/s41467-021-22539-9
Bibliography 255
[HC18] Luke Heyfron and Earl T. Campbell. An Efficient Quantum Com-
piler that reduces T count. arXiv:1712.01557 [quant-ph], June
2018. ArXiv: 1712.01557. 151
[HCT+19] Vojtch Havlek, Antonio D. Crcoles, Kristan Temme, Aram W.
Harrow, Abhinav Kandala, Jerry M. Chow, and Jay M. Gam-
betta. Supervised learning with quantum-enhanced feature
spaces. Nature, 567(7747):209212, March 2019. Nature,
567(7747):209212. 78, 79
[Hel69] Carl W. Helstrom. Quantum detection and estimation theory.
Journal of Statistical Physics, 1(2):231252, June 1969. J. Stat.
Phys., 1(2):231252. 197
[HHL09] Aram W. Harrow, Avinatan Hassidim, and Seth Lloyd. Quantum
Algorithm for Linear Systems of Equations. Physical Review Let-
ters, 103(15):150502, October 2009. PRL, 103(15):150502. 19,
[Hin02] Geoffrey E. Hinton. Training Products of Experts by Minimizing
Contrastive Divergence. Neural Computation, 14(8):17711800,
August 2002. Neural Comp., 14(8):17711800. 51, 52
[Hin12] Geoffrey E Hinton. A Practical Guide to Training Restricted
Boltzmann Machines. In Grgoire Montavon, Genevive B Orr,
and Klaus-Robert Mller, editors, Neural Networks: Tricks of the
Trade, volume 7700, pages 599619. Springer Berlin Heidelberg,
Berlin, Heidelberg, 2012. NNs: TotT, 7700: 599619. 51
[HKEG19] Dominik Hangleiter, Martin Kliesch, Jens Eisert, and Chris-
tian Gogolin. Sample Complexity of Device-Independently
Certified Quantum Supremacy". Physical Review Letters,
122(21):210502, May 2019. PRL, 122(21):210502. 131
[HM17] Aram W. Harrow and Ashley Montanaro. Quantum computational
supremacy. Nature, 549(7671):203209, September 2017. Na-
ture, 549(7671):203209. 129
[HN21] Aram W. Harrow and John C. Napp. Low-Depth Gradi-
ent Measurements Can Improve Convergence in Variational Hy-
brid Quantum-Classical Algorithms. Physical Review Letters,
126(14):140502, April 2021. PRL, 126(14):140502. 69
[Ho95] Tin Kam Ho. Random decision forests. In Proceedings of 3rd
International Conference on Document Analysis and Recognition,
volume 1, pages 278282 vol.1, August 1995. ICDAR 95, 1:278-
282. 160
http://arxiv.org/abs/1712.01557
https://www.nature.com/articles/s41586-019-0980-2
https://www.nature.com/articles/s41586-019-0980-2
https://doi.org/10.1007/BF01007479
https://doi.org/10.1007/BF01007479
https://link.aps.org/doi/10.1103/PhysRevLett.103.150502
https://doi.org/10.1162/089976602760128018
http://link.springer.com/10.1007/978-3-642-35289-8_32
https://link.aps.org/doi/10.1103/PhysRevLett.122.210502
https://www.nature.com/articles/nature23458
https://www.nature.com/articles/nature23458
https://link.aps.org/doi/10.1103/PhysRevLett.126.140502
https://ieeexplore.ieee.org/document/598994
https://ieeexplore.ieee.org/document/598994
256 Bibliography
[Hoe63] Wassily Hoeffding. Probability Inequalities for Sums of Bounded
Random Variables. Journal of the American Statistical Association,
58(301):1330, 1963. JASA, 58(301):133. 190, 234
[Hol73] A. S Holevo. Statistical decision theory for quantum systems.
Journal of Multivariate Analysis, 3(4):337394, December 1973.
JMA, 3(4):337394. 197
[Hop82] J. J. Hopfield. Neural networks and physical systems with emer-
gent collective computational abilities. Proceedings of the Na-
tional Academy of Sciences, 79(8):25542558, April 1982. PNAS,
79(8):25542558. 44
[HP20] Kathleen E. Hamilton and Raphael C. Pooser. Error-mitigated
data-driven circuit learning on noisy quantum hardware. Quantum
Machine Intelligence, 2(1):10, July 2020. QUMI, (1):10. 164
[HPSB21] Thomas Hubregtsen, Josef Pichlmeier, Patrick Stecher, and Koen
Bertels. Evaluation of parameterized quantum circuits: on the rela-
tion between classification accuracy, expressibility, and entangling
capability. Quantum Machine Intelligence, 3(1):9, March 2021.
QUMI, 3(1):9. 158
[HSCC21] Zo Holmes, Kunal Sharma, M. Cerezo, and Patrick J. Coles.
Connecting ansatz expressibility to gradient magnitudes and barren
plateaus. arXiv:2101.02138 [quant-ph, stat], January 2021. ArXiv:
2101.02138. 71
[HSNF18] Kentaro Heya, Yasunari Suzuki, Yasunobu Nakamura, and Keisuke
Fujii. Variational Quantum Gate Optimization. arXiv:1810.12745
[quant-ph], October 2018. ArXiv:1810.12745. 77, 151
[HSPC20] Maxwell Henderson, Samriddhi Shakya, Shashindra Pradhan, and
Tristan Cook. Quanvolutional neural networks: powering image
recognition with quantum circuits. Quantum Machine Intelligence,
2(1):19, June 2020. QUMI, 2(1):19. 85
[HSST18] Thomas Hner, Damian S. Steiger, Krysta Svore, and Matthias
Troyer. A software methodology for compiling quantum programs.
Quantum Science and Technology, 3(2):020501, February 2018.
QST, 3(2):020501. 151
[HWFZ20] He-Liang Huang, Dachao Wu, Daojin Fan, and Xiaobo Zhu. Su-
perconducting quantum computing: a review. Science China Infor-
mation Sciences, 63(8):180501, July 2020. SCIS, 63(8):180501.
https://www.tandfonline.com/doi/abs/10.1080/01621459.1963.10500830
http://www.sciencedirect.com/science/article/pii/0047259X73900286
https://www.pnas.org/content/79/8/2554
https://www.pnas.org/content/79/8/2554
https://doi.org/10.1007/s42484-020-00021-x
https://doi.org/10.1007/s42484-021-00038-w
http://arxiv.org/abs/2101.02138
http://arxiv.org/abs/2101.02138
http://arxiv.org/abs/1810.12745
https://doi.org/10.1007/s42484-020-00012-y
https://doi.org/10.1088/2058-9565/aaa5cc
https://doi.org/10.1007/s11432-020-2881-9
Bibliography 257
[HWGF+21] Thomas Hubregtsen, David Wierichs, Elies Gil-Fuster, Peter-Jan
H. S. Derks, Paul K. Faehrmann, and Johannes Jakob Meyer.
Training Quantum Embedding Kernels on Near-Term Quantum
Computers. arXiv:2105.02276 [quant-ph], May 2021. ArXiv:
2105.02276. 80
[HWO+19] Stuart Hadfield, Zhihui Wang, Bryan OGorman, Eleanor G. Ri-
effel, Davide Venturelli, and Rupak Biswas. From the Quantum
Approximate Optimization Algorithm to a Quantum Alternating
Operator Ansatz. Algorithms, 12(2):34, February 2019. Algo-
rithms, 12(2):34. 67
[JB18] Tyson Jones and Simon C. Benjamin. Quantum compilation
and circuit optimisation via energy dissipation. arXiv:1811.03147
[quant-ph], December 2018. ArXiv:1811.03147. 77, 151
[JCKK21] Nishant Jain, Brian Coyle, Elham Kashefi, and Niraj Kumar.
Graph neural network initialisation of quantum approximate op-
timisation. arXiv:2111.03016 [quant-ph], November 2021. ArXiv:
2111.03016. x
[JDM+21] Sonika Johri, Shantanu Debnath, Avinash Mocherla, Alexandros
Singk, Anupam Prakash, Jungsang Kim, and Iordanis Kerenidis.
Nearest centroid classification on a trapped ion quantum com-
puter. npj Quantum Information, 7(1):111, August 2021. npj
QI, 7(1):111. 74
[Jea18] Jeanfeydy. MMD, Hausdorff and Sinkhorn divergences
scaled up to 1,000,000 samples., November 2018.
https://github.com/jeanfeydy/global-divergences. 154
[JGM+21] Sofiene Jerbi, Casper Gyurik, Simon Marshall, Hans J. Briegel, and
Vedran Dunjko. Variational quantum policies for reinforcement
learning. arXiv:2103.05577 [quant-ph, stat], March 2021. ArXiv:
2103.05577. 61
[JJB+19] Jan Jaek, Kateina Jirkov, Karol Bartkiewicz, Karol
Bartkiewicz, Antonn ernoch, Tom Frst, and Karel Lemr. Ex-
perimental hybrid quantum-classical reinforcement learning by bo-
son sampling: how to train a quantum cloner. Optics Express,
27(22):3245432464, October 2019. OE, 27(22):3245432464.
173, 175, 176, 204
[Joz94] Richard Jozsa. Fidelity for Mixed Quantum States. Journal
of Modern Optics, 41(12):23152323, December 1994. JMO,
41(12):23152323. 30
http://arxiv.org/abs/2105.02276
http://arxiv.org/abs/2105.02276
https://www.mdpi.com/1999-4893/12/2/34
https://www.mdpi.com/1999-4893/12/2/34
http://arxiv.org/abs/1811.03147
http://arxiv.org/abs/2111.03016
http://arxiv.org/abs/2111.03016
https://www.nature.com/articles/s41534-021-00456-5
https://www.nature.com/articles/s41534-021-00456-5
https://github.com/jeanfeydy/global-divergences
http://arxiv.org/abs/2103.05577
http://arxiv.org/abs/2103.05577
https://www.osapublishing.org/oe/abstract.cfm?uri=oe-27-22-32454
https://doi.org/10.1080/09500349414552171
https://doi.org/10.1080/09500349414552171
258 Bibliography
[JRO+17] Peter D. Johnson, Jonathan Romero, Jonathan Olson, Yudong
Cao, and Aln Aspuru-Guzik. QVECTOR: an algorithm for device-
tailored quantum error correction. arXiv:1711.02249 [quant-ph],
November 2017. ArXiv: 1711.02249. 77
[JTPN+21] Sofiene Jerbi, Lea M. Trenkwalder, Hendrik Poulsen Nautrup,
Hans J. Briegel, and Vedran Dunjko. Quantum Enhancements
for Deep Reinforcement Learning in Large Spaces. PRX Quan-
tum, 2(1):010328, February 2021. PRX Quantum, 2(1):010328.
[JWHT13a] Gareth James, Daniela Witten, Trevor Hastie, and Robert Tib-
shirani. Introduction. In An Introduction to Statistical Learning:
with Applications in R, Springer Texts in Statistics, pages 114.
Springer, New York, NY, 2013. STS, 1-14. 39
[JWHT13b] Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshi-
rani. Statistical Learning. In An Introduction to Statistical Learn-
ing: with Applications in R, Springer Texts in Statistics, pages
1557. Springer, New York, NY, 2013. STS, 15-57. 44
[KACC20] Jonas M. Kbler, Andrew Arrasmith, Lukasz Cincio, and Patrick J.
Coles. An Adaptive Optimizer for Measurement-Frugal Variational
Algorithms. Quantum, 4:263, May 2020. Quantum, 4:263. 49
[KB15] Diederik P Kingma and Jimmy Ba. Adam: A Method for Stochas-
tic Optimization. In Yoshua Bengio and Yann LeCun, editors, 3rd
Int. Conf. Learn. Represent. ICLR 2015, San Diego, CA, USA,
May 7-9, 2015, Conf. Track Proc., 2015. ICLR 15. 49
[KB20a] Blint Koczor and Simon C. Benjamin. Quantum Analytic De-
scent. arXiv:2008.13774 [quant-ph], December 2020. ArXiv:
2008.13774. 70
[KB20b] Blint Koczor and Simon C. Benjamin. Quantum natural gradient
generalised to non-unitary circuits. arXiv:1912.08660 [quant-ph],
December 2020. ArXiv: 1912.08660. 70
[KBDW83] Karl Kraus, A. Bhm, J. D. Dollard, and W. H. Wootters, editors.
States, Effects, and Operations Fundamental Notions of Quantum
Theory. Lecture Notes in Physics. Springer, Berlin, Heidelberg,
1983. Link. 13
[KCW21] Maria Kieferova, Ortiz Marrero Carlos, and Nathan Wiebe.
Quantum Generative Training Using Rnyi Divergences.
arXiv:2106.09567 [quant-ph], June 2021. ArXiv: 2106.09567.
http://arxiv.org/abs/1711.02249
https://link.aps.org/doi/10.1103/PRXQuantum.2.010328
https://doi.org/10.1007/978-1-4614-7138-7_2
https://doi.org/10.1007/978-1-4614-7138-7_2
https://quantum-journal.org/papers/q-2020-05-11-263/
http://arxiv.org/abs/1412.6980
http://arxiv.org/abs/2008.13774
http://arxiv.org/abs/2008.13774
http://arxiv.org/abs/1912.08660
https://doi.org/10.1007/3540127321_22
http://arxiv.org/abs/2106.09567
Bibliography 259
[KGV83] S. Kirkpatrick, C. D. Gelatt, and M. P. Vecchi. Optimization by
Simulated Annealing. Science, 220(4598):671680, May 1983.
Science, 220(4598):671680. 47
[Kit97] A. Yu Kitaev. Quantum computations: algorithms and error cor-
rection. Russian Mathematical Surveys, 52(6):1191, December
1997. RMS, 2(6):1191. 17
[Kit03] A. Yu. Kitaev. Fault-tolerant quantum computation by anyons.
Annals of Physics, 303(1):230, January 2003. AoP, 303(1):230.
[KKR06] Julia Kempe, Alexei Kitaev, and Oded Regev. The Complexity
of the Local Hamiltonian Problem. SIAM Journal on Computing,
35(5):10701097, January 2006. SIAM JoC, 35(5):10701097.
[KL51] S Kullback and R A Leibler. On Information and Sufficiency.
Annals of Mathematical Statistics, 22(1):7986, 1951. AMS,
22(1):7986. 26
[KL20] Iordanis Kerenidis and Alessandro Luongo. Classification of the
MNIST data set with quantum slow feature analysis. Physical
Review A, 101(6):062327, June 2020. PRA, 101(6):062327. 59
[KLP+19] Sumeet Khatri, Ryan LaRose, Alexander Poremba, Lukasz Cincio,
Andrew T. Sornborger, and Patrick J. Coles. Quantum-assisted
quantum compiling. Quantum, 3:140, May 2019. Quantum,
3:140. 65, 77, 151, 175
[KLZ98] Emanuel Knill, Raymond Laflamme, and Wojciech H. Zurek. Re-
silient Quantum Computation. Science, 279(5349):342345, Jan-
uary 1998. Science, 279(5349):342-345. 19
[KMF+16] Mario Krenn, Mehul Malik, Robert Fickler, Radek Lapkiewicz, and
Anton Zeilinger. Automated Search for new Quantum Experi-
ments. Physical Review Letters, 116(9):090405, March 2016.
PRL, 116(9):090405. 5, 68
[KMR+94] Michael Kearns, Yishay Mansour, Dana Ron, Ronitt Rubinfeld,
Robert E Schapire, and Linda Sellie. On the Learnability of Dis-
crete Distributions. In Proc. Twenty-sixth Annual ACM Sympo-
sium Theory Computing, STOC 94, pages 273282, New York,
NY, USA, 1994. ACM. STOC 94, 273-282. 127, 128
[KMS19] Jonas M. Kbler, Krikamol Muandet, and Bernhard Schlkopf.
Quantum mean embedding of probability distributions. Phys. Rev.
Research, 1(3):033159, December 2019. PRR, 1(3):033159. 27
http://science.sciencemag.org/content/220/4598/671
https://iopscience.iop.org/article/10.1070/RM1997v052n06ABEH002155/meta
https://www.sciencedirect.com/science/article/pii/S0003491602000180
https://epubs.siam.org/doi/10.1137/S0097539704445226
https://doi.org/10.1214/aoms/1177729694
https://doi.org/10.1214/aoms/1177729694
https://link.aps.org/doi/10.1103/PhysRevA.101.062327
https://quantum-journal.org/papers/q-2019-05-13-140/
https://quantum-journal.org/papers/q-2019-05-13-140/
https://science.sciencemag.org/content/279/5349/342
https://link.aps.org/doi/10.1103/PhysRevLett.116.090405
http://doi.acm.org/10.1145/195058.195155
https://link.aps.org/doi/10.1103/PhysRevResearch.1.033159
260 Bibliography
[Kon21] Alex Kondratyev. Non-Differentiable Leaning of Quantum Circuit
Born Machine with Genetic Algorithm. Wilmott, 2021(114):50
61, 2021. Wilmott, 2021(114):506. 160, 161
[Kot14] Robin Kothari. An optimal quantum algorithm for the oracle iden-
tification problem. In 31st International Symposium on Theoretical
Aspects of Computer Science (STACS 2014), volume 25 of Leibniz
International Proceedings in Informatics (LIPIcs), pages 482493,
Dagstuhl, Germany, 2014. Schloss DagstuhlLeibniz-Zentrum fuer
Informatik. STACS 14, 482-493. 81
[KP16] Iordanis Kerenidis and Anupam Prakash. Quantum Recommen-
dation Systems. arXiv:1603.08675 [quant-ph], September 2016.
ArXiv: 1603.08675. 61, 62
[KP20] Iordanis Kerenidis and Anupam Prakash. Quantum gradient de-
scent for linear systems and least squares. Physical Review A,
101(2):022316, February 2020. PRA, 101(2):022316. 72
[KS19] Alexei Kondratyev and Christian Schwarz. The Market Generator.
Available SSRN 3384948, 2019. SSRN 3384948. 150
[KSB+20] Morten Kjaergaard, Mollie E. Schwartz, Jochen Braumller, Philip
Krantz, Joel I.-J. Wang, Simon Gustavsson, and William D. Oliver.
Superconducting Qubits: Current State of Play. Annual Review
of Condensed Matter Physics, 11(1):369395, 2020. ARCMP,
1(1):369395. 22
[KTP+20] Peter J Karalekas, Nikolas A Tezak, Eric C Peterson, Colm A Ryan,
Marcus P da Silva, and Robert S Smith. A quantum-classical cloud
platform optimized for variational hybrid algorithms. Quantum
Science & Technology, 5(2):24003, April 2020. QST, 5(2):24003.
[KV94] Michael J. Kearns and Umesh V. Vazirani. An introduction to
computational learning theory. MIT Press, Cambridge, MA, USA,
1994. Link. 55
[KW99] M. Keyl and R. F. Werner. Optimal cloning of pure states, testing
single clones. Journal of Mathematical Physics, 40(7):32833299,
June 1999. JMP, 40(7):32833299. 186
[KW17] Maria Kieferova and Nathan Wiebe. Tomography and Generative
Data Modeling via Quantum Boltzmann Training. Physical Review
A, 96(6), December 2017. PRA, 96(6):062327. 61
[LA87] P. J. van Laarhoven and E. H. Aarts. Simulated Annealing: The-
ory and Applications. Mathematics and Its Applications. Springer
Netherlands, 1987. Link. 47
https://onlinelibrary.wiley.com/doi/pdf/10.1002/wilm.10943
http://drops.dagstuhl.de/opus/volltexte/2014/4481
http://arxiv.org/abs/1603.08675
https://link.aps.org/doi/10.1103/PhysRevA.101.022316
https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3384948
https://doi.org/10.1146/annurev-conmatphys-031119-050605
https://doi.org/10.1146/annurev-conmatphys-031119-050605
https://onlinelibrary.wiley.com/doi/pdf/10.1002/wilm.10943
https://mitpress.mit.edu/books/introduction-computational-learning-theory
https://aip.scitation.org/doi/abs/10.1063/1.532887
https://link.aps.org/doi/10.1103/PhysRevA.96.062327
https://www.springer.com/gp/book/9789027725134
Bibliography 261
[LaR19] Ryan LaRose. Overview and Comparison of Gate Level Quantum
Software Platforms. Quantum, 3:130, March 2019. Quantum,
3:130. 60
[LAT21] Yunchao Liu, Srinivasan Arunachalam, and Kristan Temme. A rig-
orous and robust quantum speed-up in supervised machine learning.
Nature Physics, pages 15, July 2021. Nat. Phys., 15. 81, 169
[LBBH98] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based
learning applied to document recognition. Proceedings of the IEEE,
86(11):22782324, November 1998. PIEEE, 86(11):2278-2324.
[LC98] Hoi-Kwong Lo and H. F. Chau. Why quantum bit commitment
and ideal quantum coin tossing are impossible. Physica D: Non-
linear Phenomena, 120(1):177187, September 1998. PD:NP,
120(1):177187. 193
[LC20] Ryan LaRose and Brian Coyle. Robust data encodings for quantum
classifiers. Physical Review A, 102(3):032420, September 2020.
PRA, 102(3):032420. ix
[LCW98] D. A. Lidar, I. L. Chuang, and K. B. Whaley. Decoherence
Free Subspaces for Quantum Computation. Phys. Rev. Letters,
81(12):25942597, September 1998. ArXiv: 9807004. 120
[LFC+20] Li Li, Minjie Fan, Marc Coram, Patrick Riley, and Stefan Le-
ichenauer. Quantum optimization with a novel Gibbs objective
function and ansatz architecture search. Phys. Rev. Research,
2(2):023074, April 2020. PRR, 2(2):023074. 68
[LHF21] Yidong Liao, Min-Hsiu Hsieh, and Chris Ferrie. Quan-
tum Optimization for Training Quantum Neural Networks.
arXiv:2103.17047 [quant-ph], March 2021. ArXiv: 2103.17047.
[Lid14] Daniel A. Lidar. Review of Decoherence-Free Subspaces, Noiseless
Subsystems, and Dynamical Decoupling. In Quantum Information
and Computation for Chemistry, pages 295354. John Wiley &
Sons, Ltd, 2014. QICC, 295354. 120
[LJL+10] T. D. Ladd, F. Jelezko, R. Laflamme, Y. Nakamura, C. Monroe,
and J. L. OBrien. Quantum computers. Nature, 464(7285):45
53, March 2010. Science, 279(5349):342345. 10
[LLJ16] Qiang Liu, Jason D. Lee, and Michael Jordan. A Kernelized Stein
Discrepancy for Goodness-of-fit Tests. In Proceedings of the 33rd
International Conference on International Conference on Machine
https://quantum-journal.org/papers/q-2019-03-25-130/
https://quantum-journal.org/papers/q-2019-03-25-130/
https://www.nature.com/articles/s41567-021-01287-z
https://ieeexplore.ieee.org/document/726791
https://www.sciencedirect.com/science/article/pii/S0167278998000530
https://www.sciencedirect.com/science/article/pii/S0167278998000530
https://link.aps.org/doi/10.1103/PhysRevA.102.032420
http://arxiv.org/abs/quant-ph/9807004
https://link.aps.org/doi/10.1103/PhysRevResearch.2.023074
http://arxiv.org/abs/2103.17047
https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118742631.ch11
https://www.nature.com/articles/nature08812
262 Bibliography
Learning - Volume 48, ICML16, pages 276284, New York, NY,
USA, 2016. JMLR.org. ICML 16, 276-284. 134
[LMR13] Seth Lloyd, Masoud Mohseni, and Patrick Rebentrost. Quan-
tum algorithms for supervised and unsupervised machine learning.
arXiv:1307.0411 [quant-ph], July 2013. ArXiv: 1307.0411. 59
[LMR14] Seth Lloyd, Masoud Mohseni, and Patrick Rebentrost. Quan-
tum principal component analysis. Nature Physics, 10(9):631633,
September 2014. Nat. Phys., 10(9):631633. 59
[LS20a] Owen Lockwood and Mei Si. Reinforcement Learning with Quan-
tum Variational Circuit. Proceedings of the AAAI Conference
on Artificial Intelligence and Interactive Digital Entertainment,
16(1):245251, October 2020. CAIIDE, 16(1):245251. 61
[LS20b] Matteo Lostaglio and Gabriel Senno. Contextual advantage for
state-dependent cloning. Quantum, 4:258, April 2020. Quantum,
4:258. 38
[LSCB21] Chiara Leadbeater, Louis Sharrock, Brian Coyle, and Marcello
Benedetti. F-divergences and cost function locality in generative
modelling with quantum circuits. Entropy, 23(10), 2021. Entropy
2021, 23(10), 1281. x
[LSI+20] Seth Lloyd, Maria Schuld, Aroosa Ijaz, Josh Izaac, and
Nathan Killoran. Quantum embeddings for machine learning.
arXiv:2001.03622 [quant-ph], February 2020. ArXiv: 2001.03622.
80, 93, 94, 111, 114
[LSY19] Hanxiao Liu, Karen Simonyan, and Yiming Yang. DARTS: Dif-
ferentiable Architecture Search. In 7th International Conference
on Learning Representations, ICLR 2019, New Orleans, LA, USA,
May 6-9, 2019. OpenReview.net, 2019. OpenReview. 68
[LT18] Yingzhen Li and Richard E. Turner. Gradient estimators for implicit
models. In 6th International Conference on Learning Representa-
tions, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018,
Conference Track Proceedings. OpenReview.net, 2018. ICLR 18.
138, 139, 140
[LTOJ+19] Ryan LaRose, Arkin Tikku, tude ONeel-Judy, Lukasz Cincio, and
Patrick J. Coles. Variational quantum state diagonalization. npj
Quantum Information, 5(1):110, June 2019. npj QI, 5(1):110.
77, 175
[LW18a] Jin-Guo Liu and Lei Wang. Differentiable learning of quantum cir-
cuit Born machines. Physical Review A, 98(6):062324, December
2018. PRA, 98(6):062324. 75, 133, 153
http://dl.acm.org/citation.cfm?id=3045390.3045421
http://arxiv.org/abs/1307.0411
https://www.nature.com/articles/nphys3029
https://ojs.aaai.org/index.php/AIIDE/article/view/7437
https://quantum-journal.org/papers/q-2020-04-27-258/
https://quantum-journal.org/papers/q-2020-04-27-258/
https://www.mdpi.com/1099-4300/23/10/1281
https://www.mdpi.com/1099-4300/23/10/1281
http://arxiv.org/abs/2001.03622
https://openreview.net/forum?id=S1eYHoC5FX
https://openreview.net/forum?id=SJi9WOeRb
https://www.nature.com/articles/s41534-019-0167-6
https://link.aps.org/doi/10.1103/PhysRevA.98.062324
Bibliography 263
[LW18b] Seth Lloyd and Christian Weedbrook. Quantum Generative Ad-
versarial Learning. Physical Review Letters, 121(4):040502, July
2018. PRL, 121(4):040502. 152
[LW20] Nana Liu and Peter Wittek. Vulnerability of quantum classification
to adversarial perturbations. Physical Review A, 101:062331, Jun
2020. PRA, 101(6):062331. 120
[LWF+17] Bjoern Lekitsch, Sebastian Weidt, Austin G. Fowler, Klaus
Mlmer, Simon J. Devitt, Christof Wunderlich, and Winfried K.
Hensinger. Blueprint for a microwave trapped ion quantum com-
puter. Science Advances, 3(2):e1601540, February 2017. Sci.
Adv., 3(2):e1601540. 22
[MBK21] Andrea Mari, Thomas R. Bromley, and Nathan Killoran. Estimat-
ing the gradient and higher-order derivatives on quantum hard-
ware. Physical Review A, 103(1):012405, January 2021. PRA,
103(1):012405. 69, 70
[MBS+18] Jarrod R. McClean, Sergio Boixo, Vadim N. Smelyanskiy, Ryan
Babbush, and Hartmut Neven. Barren plateaus in quantum neural
network training landscapes. Nature Communications, 9(1):16,
November 2018. Nat. Comm., 9(1):1-6. 71
[MEAG+20] Sam McArdle, Suguru Endo, Aln Aspuru-Guzik, Simon C. Ben-
jamin, and Xiao Yuan. Quantum computational chemistry. Reviews
of Modern Physics, 92(1):015003, March 2020. Rev.Mod. Phys.,
92(1):015003. 64
[MFSS17] Krikamol Muandet, Kenji Fukumizu, Bharath Sriperumbudur, and
Bernhard Schlkopf. Kernel Mean Embedding of Distributions: A
Review and Beyond. Found. Trendsin Mach. Learn., 10(1-2):1
141, 2017. FTML, 10(1-2):1141. 55
[MJE+19] Sam McArdle, Tyson Jones, Suguru Endo, Ying Li, Simon C. Ben-
jamin, and Xiao Yuan. Variational ansatz-based quantum sim-
ulation of imaginary time evolution. npj Quantum Information,
5(1):16, September 2019. npj QI, 5(1): 1-6. 70
[MKW21] Carlos Ortiz Marrero, Mria Kieferov, and Nathan Wiebe. Entan-
glement Induced Barren Plateaus. arXiv:2010.15968 [quant-ph],
March 2021. ArXiv: 2010.15968. 71
[ML17] Shakir Mohamed and Balaji Lakshminarayanan. Learning in Implicit
Generative Models. arXiv:1610.03483 [cs, stat], February 2017.
ArXiv: 1610.03483. 75
https://link.aps.org/doi/10.1103/PhysRevLett.121.040502
https://link.aps.org/doi/10.1103/PhysRevA.101.062331
https://advances.sciencemag.org/content/3/2/e1601540
https://advances.sciencemag.org/content/3/2/e1601540
https://link.aps.org/doi/10.1103/PhysRevA.103.012405
https://link.aps.org/doi/10.1103/PhysRevA.103.012405
https://www.nature.com/articles/s41467-018-07090-4
https://link.aps.org/doi/10.1103/RevModPhys.92.015003
https://link.aps.org/doi/10.1103/RevModPhys.92.015003
https://www.nowpublishers.com/article/Details/MAL-060
https://www.nature.com/articles/s41534-019-0187-2
http://arxiv.org/abs/2010.15968
http://arxiv.org/abs/1610.03483
264 Bibliography
[MNK+18] Alexey A. Melnikov, Hendrik Poulsen Nautrup, Mario Krenn, Ve-
dran Dunjko, Markus Tiersch, Anton Zeilinger, and Hans J.
Briegel. Active learning machine learns to create new quantum
experiments. Proceedings of the National Academy of Sciences,
115(6):12211226, February 2018. PNAS, 115(6):12211226. 68
[MNKF18] K. Mitarai, M. Negoro, M. Kitagawa, and K. Fujii. Quantum cir-
cuit learning. Physical Review A, 98(3):032309, September 2018.
PRA, 98(3):032309. 69
[MNM+08] Paulo E. M. F. Mendona, Reginaldo d. J. Napolitano, Marcelo A.
Marchiolli, Christopher J. Foster, and Yeong-Cherng Liang. Alter-
native fidelity measure between quantum states. Physical Review
A, 78(5):052330, November 2008. PRA, 78(5):052330. 190
[Moc89] Jonas Mockus. Global Optimization and the Bayesian Approach. In
Jonas Mockus, editor, Bayesian Approach to Global Optimization:
Theory and Applications, Mathematics and Its Applications, pages
13. Springer Netherlands, Dordrecht, 1989. Link. 47
[Mon16] Ashley Montanaro. Quantum algorithms: an overview. npj Quan-
tum Information, 2:15023, January 2016. npj QI, 2:15023. 19
[MP43] Warren S. McCulloch and Walter Pitts. A logical calculus of the
ideas immanent in nervous activity. The bulletin of mathematical
biophysics, 5(4):115133, December 1943. BMB, 5(4):115133.
[MRBAG16] Jarrod R. McClean, Jonathan Romero, Ryan Babbush, and Aln
Aspuru-Guzik. The theory of variational hybrid quantum-classical
algorithms. New Journal of Physics, 18(2):023023, February 2016.
NJP, 18(2):023023. 62
[MSCK99] Dominic Mayers, Louis Salvail, and Yoshie Chiba-Kohno. Uncondi-
tionally secure quantum coin tossing, 1999. ArXiv: 9904078. 193,
194, 195, 213
[MSSD21] Daniel Mills, Seyon Sivarajah, Travis L. Scholten, and Ross Dun-
can. Application-Motivated, Holistic Benchmarking of a Full Quan-
tum Computing Stack. Quantum, 5:415, March 2021. Quantum,
5:415. 20
[MW02] David A. Meyer and Nolan R. Wallach. Global entanglement
in multiparticle systems. Journal of Mathematical Physics,
43(9):42734278, September 2002. JMP, 43(9):42734278. 157
[NC10] Michael A. Nielsen and Isaac L. Chuang. Quantum computation
and quantum information. Cambridge University Press, Cambridge
https://www.pnas.org/content/115/6/1221
https://link.aps.org/doi/10.1103/PhysRevA.98.032309
https://link.aps.org/doi/10.1103/PhysRevA.78.052330
https://doi.org/10.1007/978-94-009-0909-0_1
https://www.nature.com/articles/npjqi201523
https://doi.org/10.1007/BF02478259
https://doi.org/10.1088%2F1367-2630%2F18%2F2%2F023023
https://core.ac.uk/display/2669299
https://quantum-journal.org/papers/q-2021-03-22-415/
https://quantum-journal.org/papers/q-2021-03-22-415/
http://aip.scitation.org/doi/10.1063/1.1497700
Bibliography 265
; New York, 10th anniversary ed edition, 2010. Link. 12, 13, 14,
17, 18, 30, 88, 93, 112, 125, 178, 228
[NG99] Chi-Sheng Niu and Robert B. Griffiths. Two-qubit copying ma-
chine for economical quantum eavesdropping. Physical Review A,
60(4):27642776, October 1999. PRA, 60(4):27642776. 36,
[NM65] J. A. Nelder and R. Mead. A Simplex Method for Function Min-
imization. The Computer Journal, 7(4):308313, January 1965.
ArXiv: 1811.04909. 176
[NRS+18] Yunseong Nam, Neil J. Ross, Yuan Su, Andrew M. Childs, and
Dmitri Maslov. Automated optimization of large quantum circuits
with continuous parameters. npj Quantum Information, 4(1):112,
May 2018. npj QI, 4(1):112. 151
[Nys30] E. J. Nystrm. ber Die Praktische Auflsung von Integralgle-
ichungen mit Anwendungen auf Randwertaufgaben. Acta Math.,
54:185204, 1930. Am, 54:185-204. 140
[OBK+16] P. J. J. OMalley, R. Babbush, I. D. Kivlichan, J. Romero, J. R.
McClean, R. Barends, J. Kelly, P. Roushan, A. Tranter, N. Ding,
B. Campbell, Y. Chen, Z. Chen, B. Chiaro, A. Dunsworth, A. G.
Fowler, E. Jeffrey, E. Lucero, A. Megrant, J. Y. Mutus, M. Neeley,
C. Neill, C. Quintana, D. Sank, A. Vainsencher, J. Wenner, T. C.
White, P. V. Coveney, P. J. Love, H. Neven, A. Aspuru-Guzik,
and J. M. Martinis. Scalable Quantum Simulation of Molecu-
lar Energies. Physical Review X, 6(3):031007, July 2016. PRX,
6(3):031007. 114
[OGB21] Mateusz Ostaszewski, Edward Grant, and Marcello Benedetti.
Structure optimization for parameterized quantum circuits. Quan-
tum, 5:391, January 2021. Quantum, 5:391. 68, 70, 115
[Par70] James L. Park. The concept of transition in quantum mechan-
ics. Foundations of Physics, 1(1):2333, March 1970. FoP,
1(1):2333. 32
[PBO20] Kyle Poland, Kerstin Beer, and Tobias J. Osborne. No Free Lunch
for Quantum Machine Learning. arXiv:2003.14103 [quant-ph],
March 2020. ArXiv: 2003.14103. 78
[PC19] Gabriel Peyr and Marco Cuturi. Computational Optimal Trans-
port: With Applications to Data Science. Foundations and
Trends in Machine Learning, 11(5-6):355607, February 2019.
FTML, 1(5-6):355607. 28, 142, 143
https://dl.acm.org/doi/10.5555/1972505
https://link.aps.org/doi/10.1103/PhysRevA.60.2764
https://academic.oup.com/comjnl/article/7/4/308/354237
https://www.nature.com/articles/s41534-018-0072-4
https://doi.org/10.1007/BF02547521
https://link.aps.org/doi/10.1103/PhysRevX.6.031007
https://link.aps.org/doi/10.1103/PhysRevX.6.031007
https://quantum-journal.org/papers/q-2021-01-28-391/
https://doi.org/10.1007/BF00708652
https://doi.org/10.1007/BF00708652
http://arxiv.org/abs/2003.14103
https://www.nowpublishers.com/article/Details/MAL-073
266 Bibliography
[PCW+20] Arthur Pesah, M. Cerezo, Samson Wang, Tyler Volkoff, Andrew T.
Sornborger, and Patrick J. Coles. Absence of Barren Plateaus
in Quantum Convolutional Neural Networks. arXiv:2011.02966
[quant-ph, stat], November 2020. ArXiv: 2011.02966. 71
[PDF+21] J. M. Pino, J. M. Dreiling, C. Figgatt, J. P. Gaebler, S. A.
Moses, M. S. Allman, C. H. Baldwin, M. Foss-Feig, D. Hayes,
K. Mayer, C. Ryan-Anderson, and B. Neyenhuis. Demonstration
of the trapped-ion quantum CCD computer architecture. Nature,
592(7853):209213, April 2021. Nature, 592(7853):209213. 22
[PGM+19] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James
Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia
Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank
Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith
Chintala. PyTorch: An Imperative Style, High-Performance Deep
Learning Library. In Advances in Neural Information Processing
Systems 32, pages 80248035. Curran Associates, Inc., 2019.
NeurIPS, 32:80248035. 45
[PM09] Zbigniew Puchaa and Jarosaw Adam Miszczak. Bound on trace
distance based on superfidelity. Physical Review A, 79(2):024302,
February 2009. PRA, 79(2):024302. 190
[PMS+14] Alberto Peruzzo, Jarrod McClean, Peter Shadbolt, Man-Hong
Yung, Xiao-Qi Zhou, Peter J. Love, Aln Aspuru-Guzik, and
Jeremy L. OBrien. A variational eigenvalue solver on a photonic
quantum processor. Nature Communications, 5(1):17, July 2014.
Nat. Comm., 5(1):17. 64
[PNGY20] Taylor L. Patti, Khadijeh Najafi, Xun Gao, and Su-
sanne F. Yelin. Entanglement Devised Barren Plateau Mitiga-
tion. arXiv:2012.12658 [quant-ph], December 2020. ArXiv:
2012.12658. 72
[PPA+20] S. Pirandola, S. Pirandola, U. L. Andersen, L. Banchi, M. Berta,
D. Bunandar, R. Colbeck, D. Englund, T. Gehring, C. Lupo, C. Ot-
taviani, J. L. Pereira, M. Razavi, J. Shamsul Shaari, J. Shamsul
Shaari, M. Tomamichel, M. Tomamichel, V. C. Usenko, G. Val-
lone, P. Villoresi, and P. Wallden. Advances in quantum cryp-
tography. Advances in Optics and Photonics, 12(4):10121236,
December 2020. AOP, 12(4):10121236. 20, 172
[Pre18] John Preskill. Quantum Computing in the NISQ era and beyond.
Quantum, 2:79, August 2018. Quantum, 2:79. 19
http://arxiv.org/abs/2011.02966
https://www.nature.com/articles/s41586-021-03318-4
https://papers.nips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html
https://link.aps.org/doi/10.1103/PhysRevA.79.024302
https://www.nature.com/articles/ncomms5213
http://arxiv.org/abs/2012.12658
http://arxiv.org/abs/2012.12658
https://www.osapublishing.org/aop/abstract.cfm?uri=aop-12-4-1012
https://quantum-journal.org/papers/q-2018-08-06-79/
Bibliography 267
[PSCLGFL20] Adrin Prez-Salinas, Alba Cervera-Lierta, Elies Gil-Fuster, and
Jos I. Latorre. Data re-uploading for a universal quantum classi-
fier. Quantum, 4:226, February 2020. Quantum, 4:226. 84, 86,
88, 114, 120
[PT21] Mohammad Pirhooshyaran and Tamas Terlaky. Quantum Cir-
cuit Design Search. arXiv:2012.04046 [quant-ph], January 2021.
ArXiv: 2012.04046. 68
[PVG+11] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion,
O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg,
J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot,
and E. Duchesnay. Scikit-learn: Machine Learning in Python. Jour-
nal of Machine Learning Research, 12:28252830, 2011. JMLR,
12:28252830. 110, 111, 160
[PWH19] D. Padilha, S. Weinstock, and M. Hodson. QxSQA: GPGPU-
Accelerated Simulated Quantum Annealer within a Non-Linear Op-
timization and Boltzmann Sampling Framework. In 2019 IEEE
High Performance Extreme Computing Conference (HPEC), pages
18, 2019. HPEC 19, 1-8 . 161
[RAG21] Jonathan Romero and Aln Aspuru-Guzik. Variational Quantum
Generators: Generative Adversarial Quantum Machine Learning
for Continuous Distributions. Advanced Quantum Technologies,
4(1):2000003, 2021. AQT, 4(1):2000003. 76, 160
[RB01] Robert Raussendorf and Hans J. Briegel. A One-Way Quan-
tum Computer. Physical Review Letters, 86(22):51885191, May
2001. PRL, 86(22):51885191. 10, 14
[RGS+18] Andrea Rocchetto, Edward Grant, Sergii Strelchuk, Giuseppe Car-
leo, and Simone Severini. Learning hard quantum distributions with
variational autoencoders. npj Quantum Inf., 4(1):28, June 2018.
npj QI, 4(1):28. 131
[RHP+20] Arthur G. Rattew, Shaohan Hu, Marco Pistoia, Richard
Chen, and Steve Wood. A Domain-agnostic, Noise-resistant,
Hardware-efficient Evolutionary Variational Quantum Eigensolver.
arXiv:1910.09694 [quant-ph], January 2020. ArXiv: 1910.09694.
[RM87] David E. Rumelhart and James L. McClelland. Learning Internal
Representations by Error Propagation. In Parallel Distributed Pro-
cessing: Explorations in the Microstructure of Cognition: Founda-
tions, pages 318362. MITP, 1987. PDP, EMC:Found.:, 318362.
https://quantum-journal.org/papers/q-2020-02-06-226/
http://arxiv.org/abs/2012.04046
https://www.jmlr.org/papers/v12/pedregosa11a.html
https://www.jmlr.org/papers/v12/pedregosa11a.html
https://ieeexplore.ieee.org/document/8916450
https://onlinelibrary.wiley.com/doi/abs/10.1002/qute.202000003
https://link.aps.org/doi/10.1103/PhysRevLett.86.5188
https://doi.org/10.1038/s41534-018-0077-z
http://arxiv.org/abs/1910.09694
https://ieeexplore.ieee.org/document/6302929
268 Bibliography
[RML14] Patrick Rebentrost, Masoud Mohseni, and Seth Lloyd. Quantum
support vector machine for big data classification. Phys. Rev. Let-
ters, 113(13), September 2014. PRL, 113(13). 59
[ROAG17] Jonathan Romero, Jonathan P. Olson, and Alan Aspuru-Guzik.
Quantum autoencoders for efficient compression of quantum data.
Quantum Science and Technology, 2(4):045001, December 2017.
QST, 2(4):045001. 77
[RPK+17] Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, and
Jascha Sohl-Dickstein. On the Expressive Power of Deep Neural
Networks. In Proceedings of the 34th International Conference on
Machine Learning, volume 70 of Proceedings of Machine Learning
Research, pages 28472854, August 2017. ICLR 17. 89
[RS13] Luis Miguel Rios and Nikolaos V. Sahinidis. Derivative-free opti-
mization: a review of algorithms and comparison of software im-
plementations. J Glob Optim, 56(3):12471293, July 2013. JGO,
56(3):12471293. 47
[RSA78] R. L. Rivest, A. Shamir, and L. Adleman. A method for obtain-
ing digital signatures and public-key cryptosystems. Communica-
tions of the ACM, 21(2):120126, February 1978. Comm. ACM,
21(2):120126. 34
[RTC17] Aaditya Ramdas, Nicols Garca Trillos, and Marco Cuturi. On
Wasserstein Two-Sample Testing and Related Families of Non-
parametric Tests. Entropy, 19(2):47, February 2017. Entropy,
19(2):47. 145
[RTK+20] Manuel S. Rudolph, Ntwali Bashige Toussaint, Amara Katabarwa,
Sonika Johri, Borja Peropadre, and Alejandro Perdomo-Ortiz.
Generation of High-Resolution Handwritten Digits with an Ion-
Trap Quantum Computer. arXiv:2012.03924 [quant-ph], Decem-
ber 2020. ArXiv: 2012.03924. 76, 169
[RWJ+14] Troels F. Rnnow, Zhihui Wang, Joshua Job, Sergio Boixo,
Sergei V. Isakov, David Wecker, John M. Martinis, Daniel A.
Lidar, and Matthias Troyer. Defining and detecting quantum
speedup. Science, 345(6195):420424, July 2014. Science,
345(6195):420424. 3
[RXY+17] Ji-Gang Ren, Ping Xu, Hai-Lin Yong, Liang Zhang, Sheng-Kai
Liao, Juan Yin, Wei-Yue Liu, Wen-Qi Cai, Meng Yang, Li Li, Kui-
Xing Yang, Xuan Han, Yong-Qiang Yao, Ji Li, Hai-Yan Wu, Song
Wan, Lei Liu, Ding-Quan Liu, Yao-Wu Kuang, Zhi-Ping He, Peng
Shang, Cheng Guo, Ru-Hua Zheng, Kai Tian, Zhen-Cai Zhu, Nai-
Le Liu, Chao-Yang Lu, Rong Shu, Yu-Ao Chen, Cheng-Zhi Peng,
https://link.aps.org/doi/10.1103/PhysRevLett.113.130503
https://iopscience.iop.org/article/10.1088/2058-9565/aa8072
http://proceedings.mlr.press/v70/raghu17a.html
https://link.springer.com/article/10.1007/s10898-012-9951-y
https://link.springer.com/article/10.1007/s10898-012-9951-y
https://doi.org/10.1145/359340.359342
https://doi.org/10.1145/359340.359342
http://creativecommons.org/licenses/by/3.0/
http://creativecommons.org/licenses/by/3.0/
http://arxiv.org/abs/2012.03924
https://science.sciencemag.org/content/345/6195/420
https://science.sciencemag.org/content/345/6195/420
Bibliography 269
Jian-Yu Wang, and Jian-Wei Pan. Ground-to-satellite quantum
teleportation. Nature, 549(7670):7073, September 2017. Na-
ture, 549(7670):7073. 172
[SB09] Dan Shepherd and Michael J Bremner. Temporally unstructured
quantum computation. Proceedings of the Royal Society A,
465:14131439, January 2009. Proc. R. Soc. A. 465:14131439.
[SBG+19] Maria Schuld, Ville Bergholm, Christian Gogolin, Josh Izaac, and
Nathan Killoran. Evaluating analytic gradients on quantum hard-
ware. Physical Review A, 99(3):032331, March 2019. PRA,
99(3):032331. 69
[SBSW20] Maria Schuld, Alex Bocharov, Krysta M. Svore, and Nathan
Wiebe. Circuit-centric quantum classifiers. Physical Review A,
101(3):032308, March 2020. PRA, 99(3):032331. 84, 85, 86
[SCD+20] Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ra-
makrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-CAM:
Visual Explanations from Deep Networks via Gradient-Based Lo-
calization. International Journal of Computer Vision, 128(2):336
359, February 2020. IJCV, 128(2):336-359. 83
[Sch05] Maximilian Schlosshauer. Decoherence, the measurement prob-
lem, and interpretations of quantum mechanics. Rev. Mod.
Phys., 76(4):12671305, February 2005. Rev. Mod. Phys.,
76(4):12671305. 19
[Sch15] Juergen Schmidhuber. Deep Learning in Neural Networks: An
Overview. Neural Networks, 61:85117, January 2015. Neural
Networks, 61:85117. 41
[Sch16] Roman Schmied. Quantum state tomography of a single qubit:
comparison of methods. Journal of Modern Optics, 63(18):1744
1758, October 2016. JMO, 63(18):17441758. 78
[SCH+20] Kunal Sharma, M. Cerezo, Zo Holmes, Lukasz Cincio, Andrew
Sornborger, and Patrick J. Coles. Reformulation of the No-
Free-Lunch Theorem for Entangled Data Sets. arXiv:2007.04900
[quant-ph], July 2020. ArXiv: 2007.04900. 78
[Sch21] Maria Schuld. Supervised quantum machine learning models are
kernel methods. arXiv:2101.11020 [quant-ph, stat], April 2021.
ArXiv: 2101.11020. 79
[SCZ17] Robert S. Smith, Michael J. Curtis, and William J. Zeng. A Prac-
tical Quantum Instruction Set Architecture. arXiv:1608.03355
https://www.nature.com/articles/nature23675
https://www.nature.com/articles/nature23675
http://rspa.royalsocietypublishing.org/content/early/2009/02/18/rspa.2008.0443.abstract
https://link.aps.org/doi/10.1103/PhysRevA.99.032331
https://link.aps.org/doi/10.1103/PhysRevA.99.032331
https://link.aps.org/doi/10.1103/PhysRevA.101.032308
https://doi.org/10.1007/s11263-019-01228-7
https://link.aps.org/doi/10.1103/RevModPhys.76.1267
https://link.aps.org/doi/10.1103/RevModPhys.76.1267
https://www.sciencedirect.com/science/article/pii/S0893608014002135
https://www.sciencedirect.com/science/article/pii/S0893608014002135
https://doi.org/10.1080/09500340.2016.1142018
http://arxiv.org/abs/2007.04900
http://arxiv.org/abs/2101.11020
270 Bibliography
[quant-ph], February 2017. ArXiv: 2004.01372. 67, 153, 161,
[SFG+09] Bharath K. Sriperumbudur, Kenji Fukumizu, Arthur Gretton, Bern-
hard Schlkopf, and Gert R. G. Lanckriet. On integral probability
metrics, phi-divergences and binary classification. arXiv:0901.2698
[cs, math], October 2009. ArXiv: 0901.2698. 27, 28, 29, 134,
[SFL11] Bharath K Sriperumbudur, Kenji Fukumizu, and Gert R G Lanck-
riet. Universality, Characteristic Kernels and RKHS Embedding of
Measures. Journal of Machine Learning Research, 12(Jul):2389
2410, 2011. JMLR, 12(Jul):23892410. 136
[SG01] Valerio Scarani and Nicolas Gisin. Quantum key distribution be-
tween N partners: Optimal eavesdropping and Bells inequali-
ties. Physical Review A, 65(1):012311, December 2001. PRA,
65(1):012311. 36
[SG04] Rocco A. Servedio and Steven J. Gortler. Equivalences and Sep-
arations Between Quantum and Classical Learnability. SIAM J.
Comput., 33(5):10671092, May 2004. QIP, 4(5):355386. 81,
[SGF+08] BK. Sriperumbudur, A. Gretton, K. Fukumizu, G. Lanckriet, and
B. Schlkopf. Injective Hilbert Space Embeddings of Probabil-
ity Measures. In Proceedings of the 21st Annual Conference on
Learning Theory, pages 111122, Madison, WI, USA, July 2008.
Biologische Kybernetik. COLT 08, 111-122. 136
[SHLZ18] Haozhen Situ, Zhimin He, Lvzhou Li, and Shenggen Zheng. Quan-
tum generative adversarial network for generating discrete data.
arXiv:1807.01235 [quant-ph], July 2018. ArXiv: 1807.01235. 160
[Sho94] P.W. Shor. Algorithms for quantum computation: discrete log-
arithms and factoring. In Proceedings 35th Annual Symposium
on Foundations of Computer Science, pages 124134, November
1994. FOCS 94, 124-134. 19
[SIGA05] Valerio Scarani, Sofyan Iblisdir, Nicolas Gisin, and Antonio Acn.
Quantum cloning. Rev. Mod. Phys., 77(4):12251256, November
2005. Rev. Mod. Phys., 77(4):12251256. 34, 35, 36, 175, 177,
186, 187, 192, 211, 218
[SIKC20] James Stokes, Josh Izaac, Nathan Killoran, and Giuseppe Carleo.
Quantum Natural Gradient. Quantum, 4:269, May 2020. Quan-
tum, 4:269. 70
http://arxiv.org/abs/2004.01372
http://arxiv.org/abs/0901.2698
http://www.jmlr.org/papers/v12/sriperumbudur11a.html
https://link.aps.org/doi/10.1103/PhysRevA.65.012311
https://link.aps.org/doi/10.1103/PhysRevA.65.012311
https://doi.org/10.1137/S0097539704412910
https://www.is.mpg.de/publications/5122
http://arxiv.org/abs/1807.01235
https://ieeexplore.ieee.org/document/365700
https://link.aps.org/doi/10.1103/RevModPhys.77.1225
https://quantum-journal.org/papers/q-2020-05-25-269/
https://quantum-journal.org/papers/q-2020-05-25-269/
Bibliography 271
[Sin64] Richard Sinkhorn. A Relationship Between Arbitrary Positive Ma-
trices and Doubly Stochastic Matrices. Annals of Mathematical
Statistics, 35(2):876879, June 1964. AMS, 35(2):876879. 143
[SJAG19] Sukin Sim, Peter D. Johnson, and Aln Aspuru-Guzik. Ex-
pressibility and Entangling Capability of Parameterized Quantum
Circuits for Hybrid Quantum-Classical Algorithms. Advanced
Quantum Technologies, 2(12):1900070, December 2019. AQT,
2(12):1900070. 157, 158
[SJD21] Andrea Skolik, Sofiene Jerbi, and Vedran Dunjko. Quantum agents
in the Gym: a variational quantum algorithm for deep Q-learning.
arXiv:2103.15084 [quant-ph], March 2021. ArXiv: 2103.15084.
[SK19] Maria Schuld and Nathan Killoran. Quantum Machine Learning in
Feature Hilbert Spaces. Physical Review Letters, 122(4):040504,
February 2019. PRL, 122(4):040504. 53, 54, 73, 78
[SKCC20] Kunal Sharma, Sumeet Khatri, Marco Cerezo, and Patrick Coles.
Noise Resilience of Variational Quantum Compiling. New J. Phys.,
22(4):043006, 2020. NJP, 22 043006. 101, 103, 114, 154, 175
[SM21] Changpeng Shao and Ashley Montanaro. Faster quantum-inspired
algorithms for solving linear systems. arXiv:2103.10309 [quant-ph],
March 2021. ArXiv: 2103.10309. 62
[SP18] Maria Schuld and Francesco Petruccione. Supervised Learning with
Quantum Computers. Quantum Science and Technology. Springer
International Publishing, 2018. Link. 44, 61, 73, 84, 111, 126
[SPR+20] Mohan Sarovar, Timothy Proctor, Kenneth Rudinger, Kevin
Young, Erik Nielsen, and Robin Blume-Kohout. Detecting
crosstalk errors in quantum information processors. Quantum,
4:321, September 2020. Quantum, 4:321. 22
[SS16] Edwin Stoudenmire and David J Schwab. Supervised Learning
with Tensor Networks. In D. D. Lee, M. Sugiyama, U. V. Luxburg,
I. Guyon, and R. Garnett, editors, Advances in Neural Information
Processing Systems 29, pages 47994807. Curran Associates, Inc.,
2016. NIPS 16: 4799-4807. 85, 111
[SSHE21] Ryan Sweke, Jean-Pierre Seifert, Dominik Hangleiter, and Jens
Eisert. On the Quantum versus Classical Learnability of Discrete
Distributions. Quantum, 5:417, March 2021. Quantum, 5:417.
https://projecteuclid.org/euclid.aoms/1177703591
https://onlinelibrary.wiley.com/doi/abs/10.1002/qute.201900070
https://onlinelibrary.wiley.com/doi/abs/10.1002/qute.201900070
http://arxiv.org/abs/2103.15084
https://link.aps.org/doi/10.1103/PhysRevLett.122.040504
https://iopscience.iop.org/article/10.1088/1367-2630/ab784c
http://arxiv.org/abs/2103.10309
https://www.springer.com/us/book/9783319964232
https://quantum-journal.org/papers/q-2020-09-11-321/
https://papers.nips.cc/paper/2016/hash/5314b9674c86e3f9d1ba25ef9bb32895-Abstract.html
https://quantum-journal.org/papers/q-2021-03-23-417/
272 Bibliography
[SSM21] Maria Schuld, Ryan Sweke, and Johannes Jakob Meyer. Effect of
data encoding on the expressive power of variational quantum-
machine-learning models. Physical Review A, 103(3):032430,
March 2021. PRA, 103(3):032430. 93, 120
[SSP14] Maria Schuld, Ilya Sinayskiy, and Francesco Petruccione. The quest
for a Quantum Neural Network. Quantum Information Processing,
13(11):25672586, November 2014. QIP, 13(11):25672586. 40,
[SSP15] Maria Schuld, Ilya Sinayskiy, and Francesco Petruccione. An in-
troduction to quantum machine learning. Contemporary Physics,
56(2):172185, April 2015. CP, 56(2):172185. 62
[SSZ18] Jiaxin Shi, Shengyang Sun, and Jun Zhu. A Spectral Approach to
Gradient Estimation for Implicit Distributions. arXiv:1806.02925
[cs, stat], June 2018. ArXiv: 1806.02925. 138, 140, 141
[Ste72] Charles Stein. A bound for the error in the normal approximation
to the distribution of a sum of dependent random variables. In
Proceedings of the Sixth Berkeley Symposium on Mathematical
Statistics and Probability, Volume 2: Probability Theory, pages
583602, Berkeley, Calif., 1972. University of California Press.
SMSP 72, 2:583-602. 134
[Sto76] Larry J Stockmeyer. The polynomial-time hierarchy. Theor. Com-
put. Sci., 3(1):122, 1976. TCS, 3(1):122. 125
[Stu05] E. Study. Krzeste Wege im komplexen Gebiet. Mathematische
Annalen, 60(3):321378, September 1905. MA, 60(3):321378.
[SWM+20] Ryan Sweke, Frederik Wilde, Johannes Meyer, Maria Schuld,
Paul K. Faehrmann, Barthlmy Meynard-Piganeau, and Jens Eis-
ert. Stochastic gradient descent for hybrid quantum-classical op-
timization. Quantum, 4:314, August 2020. Quantum, 4:314. 69
[Tan18a] Ewin Tang. A quantum-inspired classical algorithm for recommen-
dation systems. arXiv:1807.04271 [quant-ph], July 2018. ArXiv:
1807.04271. 61, 62
[Tan18b] Ewin Tang. Quantum-inspired classical algorithms for principal
component analysis and supervised clustering. arXiv:1811.00414
[quant-ph], October 2018. ArXiv: 1811.00414. 62
[TGR15] Andrew Trask, David Gilmore, and Matthew Russell. Modeling
Order in Neural Word Embeddings at Scale. arXiv:1506.02338
[cs], June 2015. ArXiv: 1506.02338. 45
https://link.aps.org/doi/10.1103/PhysRevA.103.032430
https://doi.org/10.1007/s11128-014-0809-8
https://doi.org/10.1080/00107514.2014.964942
http://arxiv.org/abs/1806.02925
https://projecteuclid.org/euclid.bsmsp/1200514239
http://www.sciencedirect.com/science/article/pii/030439757690061X
https://doi.org/10.1007/BF01457616
https://quantum-journal.org/papers/q-2020-08-31-314/
http://arxiv.org/abs/1807.04271
http://arxiv.org/abs/1807.04271
http://arxiv.org/abs/1811.00414
http://arxiv.org/abs/1506.02338
Bibliography 273
[VBB17] Guillaume Verdon, Michael Broughton, and Jacob Biamonte. A
quantum algorithm to train neural networks using low-depth cir-
cuits. arXiv:1712.05304 [cond-mat, physics:quant-ph], December
2017. ArXiv: 1712.05304. 150
[VC15] V. N. Vapnik and A. Ya. Chervonenkis. On the Uniform Con-
vergence of Relative Frequencies of Events to Their Probabilities.
In Vladimir Vovk, Harris Papadopoulos, and Alexander Gammer-
man, editors, Measures of Complexity: Festschrift for Alexey Cher-
vonenkis, pages 1130. Springer International Publishing, Cham,
2015. TPA XVI, 2, 264-280. 56
[VC21] Tyler Volkoff and Patrick J. Coles. Large gradients via correlation
in random parameterized quantum circuits. Quantum Science and
Technology, 6(2):025008, January 2021. QST, 6(2):025008. 72
[VD04] G. Vidal and C. M. Dawson. Universal quantum circuit for two-
qubit transformations with three controlled-NOT gates. Physical
Review A, 69(1):010301, January 2004. PRA, 69(1):010301. 112
[VDRF18] Davide Venturelli, Minh Do, Eleanor Rieffel, and Jeremy Frank.
Compiling quantum circuits to realistic hardware architectures
using temporal planners. Quantum Science and Technology,
3(2):025004, February 2018. QST, 3(2):025004. 151
[Vil09] Cdric Villani. Optimal Transport: Old and New. Grundlehren
der mathematischen Wissenschaften. Springer-Verlag, Berlin Hei-
delberg, 2009. Link. 28
[VMN+19] Guillaume Verdon, Jacob Marks, Sasha Nanda, Stefan Le-
ichenauer, and Jack Hidary. Quantum Hamiltonian-Based
Models and the Variational Quantum Thermalizer Algorithm.
arXiv:1910.02071 [quant-ph], October 2019. ArXiv: 1910.02071.
68, 76, 77
[VPB18] Guillaume Verdon, Jason Pye, and Michael Broughton. A Universal
Training Algorithm for Quantum Deep Learning. arXiv:1806.09729
[quant-ph], June 2018. ArXiv: 1806.09729. 72
[VSC07] J. A. Vaccaro, Joseph Spring, and Anthony Chefles. Quantum
protocols for anonymous voting and surveying. Physical Review A,
75:012333, Jan 2007. PRA, 75(1):012333. 172
[Wat02] John Watrous. Quantum statistical zero-knowledge. arXiv:quant-
ph/0202111, February 2002. ArXiv: 0202111. 190
[WBD+19] K. Wright, K. M. Beck, S. Debnath, J. M. Amini, Y. Nam, N. Grze-
siak, J.-S. Chen, N. C. Pisenti, M. Chmielewski, C. Collins, K. M.
http://arxiv.org/abs/1712.05304
https://doi.org/10.1007/978-3-319-21852-6_3
https://doi.org/10.1088/2058-9565/abd891
https://link.aps.org/doi/10.1103/PhysRevA.69.010301
https://doi.org/10.1088/2058-9565/aaa331
https://www.springer.com/gp/book/9783540710493
http://arxiv.org/abs/1910.02071
http://arxiv.org/abs/1806.09729
https://link.aps.org/doi/10.1103/PhysRevA.75.012333
http://arxiv.org/abs/quant-ph/0202111
274 Bibliography
Hudek, J. Mizrahi, J. D. Wong-Campos, S. Allen, J. Apisdorf,
P. Solomon, M. Williams, A. M. Ducore, A. Blinov, S. M. Kreike-
meier, V. Chaplin, M. Keesan, C. Monroe, and J. Kim. Bench-
marking an 11-qubit quantum computer. Nature Communications,
10(1):5464, November 2019. Nat. Comm., 10(1):5464. 22, 23
[WBL12] Nathan Wiebe, Daniel Braun, and Seth Lloyd. Quantum Algorithm
for Data Fitting. Physical Review Letters, 109(5):050505, August
2012. PRL, 109(5):050505. 59
[Wer98] R. F. Werner. Optimal cloning of pure states. Physical Review A,
58(3):18271832, September 1998. PRA, 58(3):18271832. 34,
[WFC+21] Samson Wang, Enrico Fontana, M. Cerezo, Kunal Sharma, Akira
Sone, Lukasz Cincio, and Patrick J. Coles. Noise-Induced Barren
Plateaus in Variational Quantum Algorithms. arXiv:2007.14384
[quant-ph], February 2021. ArXiv: 2007.14384. 71, 121
[Wie83] Stephen Wiesner. Conjugate coding. ACM SIGACT News,
15(1):7888, January 1983. ACM SIGACT News, 15(1):7888.
[Wit14] Peter Wittek. Quantum Machine Learning: What Quantum Com-
puting Means to Data Mining. Academic Press, August 2014.
Link. 62
[WKGS15] Nathan Wiebe, Ashish Kapoor, Christopher Granade, and Krysta M
Svore. Quantum Inspired Training for Boltzmann Machines.
arXiv:1507.02642 [quant-ph], July 2015. ArXiv: 1507.02642. 61
[WLL+21] Maurice Weber, Nana Liu, Bo Li, Ce Zhang, and Zhikuan Zhao.
Optimal provable robustness of quantum classification via quantum
hypothesis testing. npj Quantum Information, 7(1):112, May
2021. npj QI, 7(1):112. 120
[WM97] D.H. Wolpert and W.G. Macready. No free lunch theorems for
optimization. IEEE Transactions on Evolutionary Computation,
1(1):6782, April 1997. IEEE TEC, 1(1):67-82. 78
[WM20] Logan G. Wright and Peter L. McMahon. The Capacity of Quan-
tum Neural Networks. In Conference on Lasers and Electro-Optics
(2020), paper JM4G.5, page JM4G.5. Optical Society of America,
May 2020. CLEO, JM4G.5. 120
[WMDB20] Julius Wallnfer, Alexey A. Melnikov, Wolfgang Dr, and Hans J.
Briegel. Machine learning for long-distance quantum communi-
cation. PRX Quantum, 1:010301, Sep 2020. PRX Quantum
1(1):010301. 68
https://www.nature.com/articles/s41467-019-13534-2
https://link.aps.org/doi/10.1103/PhysRevLett.109.050505
https://link.aps.org/doi/10.1103/PhysRevA.58.1827
http://arxiv.org/abs/2007.14384
https://doi.org/10.1145/1008908.1008920
https://www-sciencedirect-com.ezproxy.is.ed.ac.uk/book/9780128009536/quantum-machine-learning
http://arxiv.org/abs/1507.02642
https://www.nature.com/articles/s41534-021-00410-5
https://ieeexplore.ieee.org/document/585893
https://www.osapublishing.org/abstract.cfm?uri=CLEO_SI-2020-JM4G.5
https://link.aps.org/doi/10.1103/PRXQuantum.1.010301
https://link.aps.org/doi/10.1103/PRXQuantum.1.010301
Bibliography 275
[WVHR19] Max Wilson, Thomas Vandal, Tad Hogg, and Eleanor Rief-
fel. Quantum-assisted associative adversarial network: Apply-
ing quantum annealing in deep learning. arXiv e-prints, page
arXiv:1904.10573, April 2019. ArXiv: 1904.10573. 61
[WW19] Nathan Wiebe and Leonard Wossnig. Generative training of quan-
tum Boltzmann machines with hidden units. arXiv:1905.09902
[quant-ph], May 2019. ArXiv: 1905.09902. 61
[WZ82] W. K. Wootters and W. H. Zurek. A single quantum cannot
be cloned. Nature, 299(5886):802803, October 1982. Nature,
299(5886):802803. 32
[WZdS+20] Roeland Wiersema, Cunlu Zhou, Yvette de Sereville, Juan Felipe
Carrasquilla, Yong Baek Kim, and Henry Yuen. Exploring Entangle-
ment and Optimization within the Hamiltonian Variational Ansatz.
PRX Quantum, 1(2):020319, December 2020. PRX Quantum,
1(2):020319. 71
[YCL+17] Juan Yin, Yuan Cao, Yu-Huai Li, Sheng-Kai Liao, Liang Zhang,
Ji-Gang Ren, Wen-Qi Cai, Wei-Yue Liu, Bo Li, Hui Dai, Guang-
Bing Li, Qi-Ming Lu, Yun-Hong Gong, Yu Xu, Shuang-Lin Li,
Feng-Zhi Li, Ya-Yun Yin, Zi-Qing Jiang, Ming Li, Jian-Jun Jia,
Ge Ren, Dong He, Yi-Lin Zhou, Xiao-Xiang Zhang, Na Wang, Xi-
ang Chang, Zhen-Cai Zhu, Nai-Le Liu, Yu-Ao Chen, Chao-Yang
Lu, Rong Shu, Cheng-Zhi Peng, Jian-Yu Wang, and Jian-Wei
Pan. Satellite-based entanglement distribution over 1200 kilo-
meters. Science, 356(6343):11401144, June 2017. Science,
356(6343):11401144. 172
[YLRN18] Jiasen Yang, Qiang Liu, Vinayak Rao, and Jennifer Neville.
Goodness-of-Fit Testing for Discrete Distributions via Stein Dis-
crepancy. In Jennifer Dy and Andreas Krause, editors, Proceed-
ings of the 35th International Conference on Machine Learning,
volume 80 of Proceedings of Machine Learning Research, pages
55615570, Stockholmsmssan, Stockholm Sweden, July 2018.
PMLR. ICML, PMLR, 80, 55615570. 135, 136, 139
[YWC+19] Quanming Yao, Mengshuo Wang, Yuqiang Chen, Wenyuan Dai,
Yu-Feng Li, Wei-Wei Tu, Qiang Yang, and Yang Yu. Taking Human
out of Learning Applications: A Survey on Automated Machine
Learning. arXiv:1810.13306 [cs, stat], December 2019. ArXiv:
1810.13306. 68, 172
[Zei12] Matthew D. Zeiler. ADADELTA: An Adaptive Learning Rate
Method. arXiv:1212.5701 [cs], December 2012. ArXiv:
1212.5701. 49
http://arxiv.org/abs/1904.10573
http://arxiv.org/abs/1905.09902
https://www.nature.com/articles/299802a0
https://www.nature.com/articles/299802a0
https://link.aps.org/doi/10.1103/PRXQuantum.1.020319
https://link.aps.org/doi/10.1103/PRXQuantum.1.020319
https://science.sciencemag.org/content/356/6343/1140
https://science.sciencemag.org/content/356/6343/1140
http://proceedings.mlr.press/v80/yang18c.html
http://arxiv.org/abs/1810.13306
http://arxiv.org/abs/1810.13306
http://arxiv.org/abs/1212.5701
http://arxiv.org/abs/1212.5701
276 Bibliography
[ZFF19] Zhikuan Zhao, Jack K Fitzsimons, and Joseph F Fitzsimons.
Quantum-assisted Gaussian process regression. Physical Review
A, 99(5):52331, May 2019. PRA, 99(5):52331. 59
[ZFR+21] Zhikuan Zhao, Jack K. Fitzsimons, Patrick Rebentrost, Vedran
Dunjko, and Joseph F. Fitzsimons. Smooth input preparation for
quantum and quantum-inspired machine learning. Quantum Ma-
chine Intelligence, 3(1):14, April 2021. QUMI, 3(1):14. 61
[ZG21] Chen Zhao and Xiao-Shan Gao. Analyzing the barren plateau
phenomenon in training quantum neural networks with the ZX-
calculus. Quantum, 5:466, June 2021. Quantum, 5:466. 71
[ZHLT20] Kaining Zhang, Min-Hsiu Hsieh, Liu Liu, and Dacheng Tao. To-
ward Trainability of Quantum Neural Networks. arXiv:2011.06258
[quant-ph], December 2020. ArXiv: 2011.06258. 71
[ZHZY20] Shi-Xin Zhang, Chang-Yu Hsieh, Shengyu Zhang, and Hong Yao.
Differentiable Quantum Architecture Search. arXiv:2010.08561
[quant-ph], October 2020. ArXiv: 2010.08561. 68
[ZS05] Karol Zyczkowski, Karol and Hans-Jrgen Sommers. Average
fidelity between random quantum states. Physical Review A,
71(3):032313, March 2005. PRA, 71(3):032313. 30
[ZTB+20] Linghua Zhu, Ho Lun Tang, George S. Barron, F. A. Calderon-
Vargas, Nicholas J. Mayhall, Edwin Barnes, and Sophia E.
Economou. An adaptive quantum approximate optimization al-
gorithm for solving combinatorial problems on a quantum com-
puter. arXiv:2005.10258 [quant-ph], December 2020. ArXiv:
2005.10258. 68
[ZWD+20] Han-Sen Zhong, Hui Wang, Yu-Hao Deng, Ming-Cheng Chen, Li-
Chao Peng, Yi-Han Luo, Jian Qin, Dian Wu, Xing Ding, Yi Hu,
Peng Hu, Xiao-Yan Yang, Wei-Jun Zhang, Hao Li, Yuxuan Li, Xiao
Jiang, Lin Gan, Guangwen Yang, Lixing You, Zhen Wang, Li Li,
Nai-Le Liu, Chao-Yang Lu, and Jian-Wei Pan. Quantum computa-
tional advantage using photons. Science, 370(6523):14601463,
December 2020. Science, 370(6523):14601463. 23
[ZWL+19] Jinfeng Zeng, Yufeng Wu, Jin-Guo Liu, Lei Wang, and Jiangping
Hu. Learning and inference on generative adversarial quantum
circuits. Physical Review A, 99(5):052306, May 2019. PRA,
99(5):052306. 160
https://link.aps.org/doi/10.1103/PhysRevA.99.052331
https://doi.org/10.1007/s42484-021-00045-x
https://quantum-journal.org/papers/q-2021-06-04-466/
http://arxiv.org/abs/2011.06258
http://arxiv.org/abs/2010.08561
https://link.aps.org/doi/10.1103/PhysRevA.71.032313
http://arxiv.org/abs/2005.10258
http://arxiv.org/abs/2005.10258
https://science.sciencemag.org/content/370/6523/1460
https://link.aps.org/doi/10.1103/PhysRevA.99.052306
https://link.aps.org/doi/10.1103/PhysRevA.99.052306
	cover sheet.pdf
	PHD_THESIS_FINAL_Brian_Coyle.pdf
	Introduction & background
	 Introduction
	 Quantum computing for machine learning
	 Machine learning for quantum computing
	 Thesis overview
	Preliminaries I: Quantum information
	 Quantum computing
	 Quantum states
	 Quantum operations
	 Quantum gates
	 Quantum measurements
	 Quantum noise
	 Quantum hardware
	 Distance measures
	 Quantum cloning
	 Beyond the no-cloning theorem
	 Approximate cloning
	 Cloning of fixed overlap states
	Preliminaries II: Machine learning
	 Machine learning
	 Neural networks
	 Feedforward neural networks
	 The Boltzmann machine
	 Machine learning tasks
	 Classification as supervised learning
	 Generative modelling as unsupervised learning
	 Training a neural network
	 Computing gradients
	 Kernel methods
	 Learning theory
	Preliminaries III: Quantum machine learning
	 Variational quantum algorithms
	 Cost functions
	 Anstze
	 Cost function optimisation
	  inputs and outputs
	 Quantum kernel methods
	 Quantum learning theory
	Robust data encodings for quantum classifiers
	 Introduction
	 Quantum classifiers
	 Data encodings
	 Robust data encodings
	 Analytic results
	 Classes of learnable decision boundaries
	 Characterisation of robust points
	 Robustness results
	 Modifications for finite sampling
	 Existence of robust encodings
	 Lower bounds on partial robustness
	 Numerical results
	 Decision boundaries and implementations
	 Robust sets for partially robust encodings
	 An encoding learning algorithm
	 Fidelity bounds on partial robustness
	 Discussion and conclusions
	 Subsequent work
	Generative modelling with quantum circuit Born machines
	 Introduction
	 Quantum advantage in generative modelling
	 The Ising Born machine
	 The supremacy of quantum learning
	 A learning advantage via quantum computational supremacy
	 Training a quantum circuit Born machine
	 Training with the Stein discrepancy
	 Computing the Stein score function
	 Training with the Sinkhorn divergence
	 Numerical Results
	 The data
	 Comparison between training methods
	 Quantum versus classical generative modelling in finance
	 Weak quantum compilation with a QCBM
	 Discussion and conclusion
	 Subsequent work
	Practical quantum cryptanalysis by variational quantum cloning
	 Introduction
	 Variational quantum cloning: cost functions and gradients
	 Cost functions
	 Cost function gradients
	 Asymmetric cloning
	 Cost function guarantees
	 Sample complexity of the algorithm
	 Variational quantum cryptanalysis
	 Quantum key distribution and cloning attacks
	 Quantum coin flipping and cloning attacks
	 Quantum coin flipping
	 2-state coin flipping protocol ()
	 4-state coin flipping protocol ()
	  numerics
	 Fixed-structure 
	 Variable-structure 
	 State-dependent cloning
	 Training sample complexity
	 Local cost function comparison
	  learned circuits
	 Ancilla-free phase-covariant cloning
	 State-dependent cloning circuits
	 Discussion and conclusions
	Conclusion
	Proofs and derivations
	 Proofs for 
	 Proofs of  and 
	 Proof of 
	 Proofs for 
	 Proof of gradients for 
	 Proof of 
	 Proof of 
	 Proof of 
	 Proof of 
	 Proof of 
	 Proof of 
	Bibliography
