Statistical Modelling of
Induced Earthquakes
Zak Varty, M.Sci, M.Res
Submitted for the degree of Doctor of
Philosophy at Lancaster University.
July 2021
Abstract
Earthquakes induced by human activities present a unique set of challenges to the sta-
tistical modeller. Relative to tectonic earthquakes, the recorded number of induced
earthquakes can be very small, while interventions to better record and prevent these
earthquakes make the use of stationary models either statistically inefficient or in-
appropriate. On the other hand, the human activity causing seismicity is often well
documented and can be a valuable resource that is not available in the tectonic set-
ting.
This thesis focuses on how to model anthropogenic earthquakes while making best
use of the limited available data. This research provides three main contributions to
statistical seismology, each motivated by the induced earthquakes in the Groningen
gas field.
Firstly, we consider the link between earthquake locations and gas extraction, using
a state-of-the-art, physically-motivated model as our baseline. We investigate model
simplifications to ensure parsimony of the baseline model and explore model extensions
that assess the statistical evidence for additional physical characteristics that are not
currently represented.
Secondly, we consider how to include developments to the earthquake detection net-
work when modelling earthquake magnitudes. We develop a method for selecting a
time-varying threshold above which the earthquake catalogue may be considered com-
plete. This allows small magnitude events, unused by existing analyses, to contribute
to our understanding of the largest events.
Finally, we turn our focus to aftershock activity and the Epidemic Type Aftershock
Sequence (ETAS) model. The use of this model is widespread, but the conventional
formulation represents a narrow model class with strong parameter dependence and
assumes independent and identically distributed magnitudes. We introduce a repa-
rameterisation and two extensions of the conventional ETAS model, along with effi-
cient inference procedures, which alleviate these issues.
Acknowledgements
I would firstly like to thank my supervisors Jon Tawn, Pete Atkinson and Stijn Bier-
man for their support, guidance and encouragement over the course of my PhD. I have
learned a great deal from each of you and I am very grateful for all the opportunities
that have come my way as a result of working with you.
I would also like to thank the wider group of people that I have had the pleasure to
work alongside both within STOR-i and at Shell. Research is much more enjoyable
when surrounded by such exceptional people.
I give special thanks to the friends who saw me through an isolating final year with
coffee chats, pomodoros, book clubs and film nights. Thank you to Edwin, Rob,
Georgia, Euan, Hankui, Sean, Srshti and Anja. Thank you also to Jamie-Leigh,
Aaron, Emma, Tom, Christian, Jon and Julia for the Friday night social calls.
A huge thank you must go to my family and in particular to my parents, Tracey and
Alan; without your dedication, love and support I would not be where I am today.
This PhD would have been a lot easier, and shorter, if the answer was always six.
Finally, and especially, I would like to give thanks to my partner Paul. Thank you for
supporting me when I thought this was impossible and for always being my favourite
human.
Declaration
I declare that the work in this thesis has been done by myself and has not been
submitted elsewhere for the award of any other degree.
A version of Chapter 5 has been submitted for publication as: Varty, Z., Tawn J.A.,
Atkinson P.M. and Bierman S. (2021). Inference for extreme earthquake magnitudes
accounting for a time-varying measurement process.
- Zak Dennis Varty
"Only fools, liars and charlatans predict earthquakes."
- Charles Francis Richter.
Contents
Abstract I
Acknowledgements III
Declaration IV
Contents XII
List of figures XVIII
List of tables XIX
1 Introduction 1
1.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.1.1 A brief history of the Groningen gas field . . . . . . . . . . . . 4
1.1.2 Geology of the Groningen gas field . . . . . . . . . . . . . . . 6
1.1.3 From gas extraction to induced earthquakes . . . . . . . . . . 8
1.1.4 Shell seismic risk analysis . . . . . . . . . . . . . . . . . . . . 10
1.2 Thesis outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
2 Data 14
2.1 Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
CONTENTS VII
2.2 Earthquake catalogue . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
2.3 Static covariates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
2.3.1 Fault structure . . . . . . . . . . . . . . . . . . . . . . . . . . 16
2.3.2 Reservoir thickness and topographic gradient . . . . . . . . . . 17
2.4 Extraction covariates . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
2.5 Exploratory analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
2.5.1 Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
2.5.2 Exploratory spatial analysis . . . . . . . . . . . . . . . . . . . 21
2.5.3 Exploratory temporal analysis . . . . . . . . . . . . . . . . . . 25
2.5.4 Exploratory magnitude analysis . . . . . . . . . . . . . . . . . 28
2.5.5 Review . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
3 Literature review 36
3.1 Point process models . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
3.1.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
3.1.2 Poisson point processes . . . . . . . . . . . . . . . . . . . . . . 37
3.1.3 Generalisations of the Poisson process . . . . . . . . . . . . . . 42
3.1.4 Measures of clustering . . . . . . . . . . . . . . . . . . . . . . 45
3.2 Extreme value methods . . . . . . . . . . . . . . . . . . . . . . . . . . 46
3.2.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
3.2.2 Block maxima approach . . . . . . . . . . . . . . . . . . . . . 47
3.2.3 Peaks over threshold . . . . . . . . . . . . . . . . . . . . . . . 49
3.2.4 Point process representation . . . . . . . . . . . . . . . . . . . 52
3.2.5 Inference for extreme value models . . . . . . . . . . . . . . . 53
3.3 Earthquake modelling . . . . . . . . . . . . . . . . . . . . . . . . . . 55
3.3.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
3.3.2 Epidemic Type Aftershock Sequence models . . . . . . . . . . 55
CONTENTS VIII
3.3.3 Physics-based modelling . . . . . . . . . . . . . . . . . . . . . 58
3.4 Recent work on Groningen seismicity . . . . . . . . . . . . . . . . . . 62
4 Covariate-based models for induced earthquake locations 64
4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
4.1.1 Induced earthquakes . . . . . . . . . . . . . . . . . . . . . . . 64
4.1.2 Motivation and aims . . . . . . . . . . . . . . . . . . . . . . . 65
4.1.3 Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
4.2 Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
4.2.1 Point process models for earthquakes . . . . . . . . . . . . . . 68
4.2.2 Baseline intensity model . . . . . . . . . . . . . . . . . . . . . 70
4.3 Alternative models . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
4.3.1 Approach outline . . . . . . . . . . . . . . . . . . . . . . . . . 72
4.3.2 Model simplifications . . . . . . . . . . . . . . . . . . . . . . . 73
4.3.3 Model extensions . . . . . . . . . . . . . . . . . . . . . . . . . 74
4.4 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80
4.4.1 Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80
4.4.2 Covariate smoothing . . . . . . . . . . . . . . . . . . . . . . . 80
4.4.3 Model reductions . . . . . . . . . . . . . . . . . . . . . . . . . 82
4.4.4 Model extensions . . . . . . . . . . . . . . . . . . . . . . . . . 83
4.5 Conclusions and further work . . . . . . . . . . . . . . . . . . . . . . 85
5 Inference for extreme earthquake magnitudes accounting for a time-
varying censoring process 88
5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
5.1.1 Aims and motivation . . . . . . . . . . . . . . . . . . . . . . . 88
5.1.2 Earthquake data . . . . . . . . . . . . . . . . . . . . . . . . . 89
5.1.3 Magnitude of completion . . . . . . . . . . . . . . . . . . . . . 90
CONTENTS IX
5.1.4 Extreme value methods . . . . . . . . . . . . . . . . . . . . . . 91
5.1.5 Shortcomings of current methods . . . . . . . . . . . . . . . . 92
5.1.6 Contributions and outline . . . . . . . . . . . . . . . . . . . . 93
5.2 Motivating data and model formulation . . . . . . . . . . . . . . . . . 94
5.2.1 Data description . . . . . . . . . . . . . . . . . . . . . . . . . 94
5.2.2 Data model and inference . . . . . . . . . . . . . . . . . . . . 95
5.3 Motivating the inclusion of small magnitudes . . . . . . . . . . . . . . 97
5.3.1 Simulation study overview . . . . . . . . . . . . . . . . . . . . 97
5.3.2 Simulation study results . . . . . . . . . . . . . . . . . . . . . 99
5.4 Threshold selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
5.4.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
5.4.2 Graphical assessment . . . . . . . . . . . . . . . . . . . . . . . 102
5.4.3 Metric-based assessment . . . . . . . . . . . . . . . . . . . . . 103
5.4.4 Minimisation procedure . . . . . . . . . . . . . . . . . . . . . 107
5.5 Threshold selection on simulated catalogues . . . . . . . . . . . . . . 108
5.5.1 Simulation study overview . . . . . . . . . . . . . . . . . . . . 108
5.5.2 Constant threshold, hard censoring . . . . . . . . . . . . . . . 109
5.5.3 Constant threshold, phased censoring . . . . . . . . . . . . . . 111
5.5.4 Non-constant threshold selection . . . . . . . . . . . . . . . . 112
5.6 Application to Groningen earthquakes . . . . . . . . . . . . . . . . . 114
5.6.1 Validating data model for Groningen catalogue . . . . . . . . 114
5.6.2 Parametric threshold forms . . . . . . . . . . . . . . . . . . . 116
5.6.3 Threshold selection . . . . . . . . . . . . . . . . . . . . . . . . 117
5.7 Discussion / Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . 122
6 Improving and extending the ETAS formulation 125
6.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
CONTENTS X
6.1.1 The ETAS model . . . . . . . . . . . . . . . . . . . . . . . . . 125
6.1.2 Magnitude modelling . . . . . . . . . . . . . . . . . . . . . . . 127
6.1.3 Bayesian ETAS modelling . . . . . . . . . . . . . . . . . . . . 130
6.1.4 Contributions and chapter outline . . . . . . . . . . . . . . . . 132
6.2 Estimation of ETAS parameters . . . . . . . . . . . . . . . . . . . . . 133
6.2.1 Direct estimation: ETAS as a point process . . . . . . . . . . 133
6.2.2 Latent estimation: ETAS as a branching process . . . . . . . . 135
6.2.3 Benefits of the latent estimation approach . . . . . . . . . . . 137
6.3 Extreme value reparameterisation of empirical laws . . . . . . . . . . 139
6.3.1 Empirical laws . . . . . . . . . . . . . . . . . . . . . . . . . . 139
6.3.2 Proposed parameterisation . . . . . . . . . . . . . . . . . . . . 140
6.3.3 Comparing parameterisations . . . . . . . . . . . . . . . . . . 143
6.3.4 Demonstration on simulated catalogue . . . . . . . . . . . . . 146
6.4 Extensions of the magnitude model . . . . . . . . . . . . . . . . . . . 157
6.4.1 Dual magnitude extension . . . . . . . . . . . . . . . . . . . . 157
6.4.2 Correlated magnitude extension . . . . . . . . . . . . . . . . . 159
6.5 Application of extended ETAS models to simulated catalogues . . . . 164
6.5.1 Dual magnitude extension . . . . . . . . . . . . . . . . . . . . 164
6.5.2 Correlated magnitude extension . . . . . . . . . . . . . . . . . 175
6.6 Conclusions and further work . . . . . . . . . . . . . . . . . . . . . . 183
7 Conclusions and further work 186
A Supplementary materials to Chapter 4 194
A.1 Integrated intensity functions . . . . . . . . . . . . . . . . . . . . . . 194
A.2 Maps of annual expected earthquake counts . . . . . . . . . . . . . . 196
B Supplementary materials to Chapter 5 201
CONTENTS XI
B.1 Assessing i.i.d. assumption for Groningen earthquakes . . . . . . . . . 201
B.1.1 Connection to main text . . . . . . . . . . . . . . . . . . . . . 201
B.1.2 Exploratory analysis . . . . . . . . . . . . . . . . . . . . . . . 201
B.2 Bootstrap datasets and parameter estimates . . . . . . . . . . . . . . 203
B.2.1 Connection to main text . . . . . . . . . . . . . . . . . . . . . 203
B.2.2 Generating bootstrapped data-sets . . . . . . . . . . . . . . . 203
B.2.3 Generating bootstrap maximum likelihood estimates . . . . . 208
B.3 Sampling standardised threshold exceedances . . . . . . . . . . . . . . 208
B.3.1 Connection to main text . . . . . . . . . . . . . . . . . . . . . 208
B.3.2 Sampling unrounded threshold exceedances . . . . . . . . . . . 209
B.3.3 Transformation onto common margins . . . . . . . . . . . . . 210
B.4 Supplementary figures . . . . . . . . . . . . . . . . . . . . . . . . . . 212
C Supplementary materials to Chapter 6 215
C.1 B conditional posterior, dual magnitude model . . . . . . . . . . . . . 215
C.2  conditional posterior, correlated magnitudes . . . . . . . . . . . . . 218
C.3 B conditional posterior, correlated magnitudes . . . . . . . . . . . . . 220
D Supplementary materials to Chapter 7 224
D.1 Outline for combined covariate and aftershock model . . . . . . . . . 224
D.1.1 Connection to main text . . . . . . . . . . . . . . . . . . . . . 224
D.1.2 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225
D.1.3 Suggested model . . . . . . . . . . . . . . . . . . . . . . . . . 226
D.1.4 Likelihood function . . . . . . . . . . . . . . . . . . . . . . . . 230
D.1.5 Condition for sub-criticality . . . . . . . . . . . . . . . . . . . 232
D.1.6 Parametric forms for the temporal change in stress . . . . . . 233
D.1.7 Parametric forms for the spatial change in stress . . . . . . . . 235
D.1.8 Parametric forms for the effect of magnitude on additional stress238
CONTENTS XII
D.1.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239
Bibliography 242
List of Figures
1.1.1 Schematic cross-section through the Groningen field indicating the main
stratigraphic intervals. Source: TNO (2017) . . . . . . . . . . . . . . 7
1.1.2 Lithostratigraphic subdivision of the Rotliegend in the Groningen area.
Source: NAM (2016a) . . . . . . . . . . . . . . . . . . . . . . . . . . 7
1.1.3 Schematic diagram of seismic risk assessment procedure after modelling
of pressure depletion and reservoir compaction. . . . . . . . . . . . . 11
2.3.1 Maps of human settlements, gas production clusters, fault lines and
earthquakes in the Groningen region. . . . . . . . . . . . . . . . . . . 18
2.3.2 Topographic gradient and initial thickness of Groningen reservoir. . . 18
2.5.1 Contour plots of the spatial distribution of earthquakes in the Gronin-
gen gas field . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
2.5.2 Estimated Ripley K-function for Groningen earthquakes . . . . . . . 23
2.5.3 Boundary of high density regions HDR1 and HDR2 and temporal event
density in each region. . . . . . . . . . . . . . . . . . . . . . . . . . . 25
2.5.4 Histogram displaying the number of earthquakes each year since 1995. 26
2.5.5 Estimated earthquake intensity through time. . . . . . . . . . . . . . 27
2.5.6 Quantile-quantile plot of inter-event times. . . . . . . . . . . . . . . . 27
2.5.7 Kernel density estimated magnitude distributions using all events and
only those exceeding 1.5ML. . . . . . . . . . . . . . . . . . . . . . . . 29
LIST OF FIGURES XIV
2.5.8 Magnitudes of Groningen earthquakes through time. . . . . . . . . . . 31
2.5.9 Estimated magnitude distributions for events exceeding 1.5ML. . . . . 33
2.5.10Quantile-quantile plot of magnitude exceedances of 1.5ML. . . . . . . 34
3.2.1 Data used in block maxima and peaks over threshold analysis . . . . 50
4.3.1 Groningen earthquakes and line used to separate lower and upper
modes of earthquake activity. . . . . . . . . . . . . . . . . . . . . . . 76
4.3.2 Maps of expected event counts under the fitted baseline model for years
2005-2007. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
5.2.1 Event times and mangitudes of Groningen earthquakes. . . . . . . . . 95
5.3.1 Catalogue structure and conditional return level plots for conservative,
extended and stepped approaches to inference. . . . . . . . . . . . . . 100
5.4.1 Examples of modified PP and QQ plots for valid and invalid flat thresh-
olds. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104
5.5.1 PP- and QQ- distance metrics to select a constant threshold for a single
simulated catalogue. . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
5.5.2 Sampling distribution of threshold selection methods for quantile-based
metrics and catalogues simulated with a constant threshold and hard
censoring. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
5.5.3 Example simulated catalogues with hard censoring and phased censor-
ing applied to a stepped threshold. . . . . . . . . . . . . . . . . . . . 113
5.5.4 Marginal sampling distributions of errors in the selected values of v(1),
v(2) and   for simulated catalogues with change-point type thresholds
and either hard or phased censoring. . . . . . . . . . . . . . . . . . . 113
5.6.1 Modified PP (left) and QQ (right) plots for Groningen magnitudes
exceeding 1.45ML under the GPD model. . . . . . . . . . . . . . . . . 116
LIST OF FIGURES XV
5.6.2 Threshold selection and conditional return level plots for Groningen
earthquakes and a constant threshold. . . . . . . . . . . . . . . . . . 118
5.6.3 Selected sigmoid thresholds fro Groningen earthquakes using Bayesian
optimisation from 5 random initial parameter sets. . . . . . . . . . . . 119
5.6.4 Bootstrap GPD parameter estimates and conditional return levels based
on Groningen earthquake magnitudes exceeding the selected conserva-
tive, flat and sigmoid thresholds . . . . . . . . . . . . . . . . . . . . . 122
6.2.1 Graphical representation of a toy example from the ETAS model. . . 136
6.3.1 Simulated earthquake catalogue of 863 events used to compare conv
and prop parameterisations of the ETAS model. . . . . . . . . . . . . 147
6.3.2 Contour plots of pair-wise marginal posterior distributions of ETAS
parameters  using the conv and prop parameterisations. . . . . . . . 148
6.3.3 Estimated posterior distributions using samples from the prop and conv
parameterisations, shown on each parameter space. . . . . . . . . . . 149
6.3.4 Posterior distributions of physically meaningful quantities phys using
estimated using samples from the prop and conv parameterisations. . 151
6.3.5 Trace plots and posterior pmfs for branching vector elements B51, B81
and B199. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154
6.3.6 Proportion of sampled Bi values equal to the true value in the conv
and prop chains. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155
6.3.7 Proportion of updates at which each branching vector element was
unchanged for the conv chain and prop chain. . . . . . . . . . . . . . 156
6.5.1 Marginal posterior distributions of ETAS parameters prop given a dual
magnitude earthquake catalogue, estimated using a dual magnitude
model or single magnitude model. . . . . . . . . . . . . . . . . . . . . 165
LIST OF FIGURES XVI
6.5.2 Marginal and joint posterior distributions of magnitude parameters
based on a simulated dual magnitude earthquake catalogue. . . . . . 167
6.5.3 Return level plots for background magnitudes, triggered magnitudes,
and all magnitudes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
6.5.4 Posterior proportions of triggered events and correctly allocated ele-
ments of B, based the dual magnitude and single magnitude models. 169
6.5.5 Marginal posterior distributions of ETAS parameters prop given an
earthquake catalogue with i.i.d. magnitudes, estimated using a dual
magnitude model or single magnitude model. . . . . . . . . . . . . . . 171
6.5.6 Marginal and joint posterior distributions of magnitude parameters
shown by event type under a dual or single magnitude model fitted to
a catalogue with i.i.d. magnitudes. . . . . . . . . . . . . . . . . . . . . 172
6.5.7 Joint posterior distributions of the differences in magnitude parameters.173
6.5.8 Return level plots for background magnitudes, triggered magnitudes,
and all magnitudes fitted using a catalogue with i.i.d. magnitudes. . . 173
6.5.9 Posterior proportions of triggered events and correctly allocated ele-
ments of B, based the dual magnitude and single magnitude models
and a simulated catalogue with i.i.d. magnitudes. . . . . . . . . . . . 174
6.5.10 Marginal posterior distributions of ETAS parameters prop given an
earthquake catalogue with dual magnitudes, estimated using a dual
magnitude model and correlated magnitude model. . . . . . . . . . . 177
6.5.11 Marginal posterior distributions of magnitude parameters  given a
earthquake catalogue with dual magnitudes, estimated using a dual
magnitude model and correlated magnitude model. . . . . . . . . . . 177
LIST OF FIGURES XVII
6.5.12Posterior proportions of triggered events and correctly allocated ele-
ments ofB, based the dual magnitude and correlated magnitude models
for a simulated catalogue with dual magnitudes. . . . . . . . . . . . . 178
6.5.13 Marginal posterior distributions of ETAS parameters prop given an
earthquake catalogue with correlated magnitudes, estimated using a
dual magnitude model and correlated magnitude model. . . . . . . . 180
6.5.14Marginal posterior distributions of magnitude parameters  given an
earthquake catalogue with correlated magnitudes, estimated using a
dual magnitude model and correlated magnitude model. . . . . . . . 180
6.5.15Posterior proportions of triggered events and correctly allocated ele-
ments ofB, based the dual magnitude and correlated magnitude models
for a simulated catalogue with correlated magnitudes. . . . . . . . . . 181
A.2.1Observed and expected event counts in each year (1995-2006) under
the fitted baseline model B0. . . . . . . . . . . . . . . . . . . . . . . . 197
A.2.2Observed and expected event counts in each year (2007-2016) under
the fitted baseline model B0. . . . . . . . . . . . . . . . . . . . . . . . 198
A.2.3Observed and expected event counts in each year (1995-2006) under
the fitted model extension E2. . . . . . . . . . . . . . . . . . . . . . . 199
A.2.4Observed and expected event counts in each year (2007-2016) under
the fitted model extension E2. . . . . . . . . . . . . . . . . . . . . . . 200
B.1.1Frequency plots of the number of earthquakes exceeding 1.45ML that
separate earthquakes exceeding exceeding 1.65ML, 1.75ML, and 1.85ML
in the Groningen earthquake catalogue. . . . . . . . . . . . . . . . . . 202
B.2.1Threshold exceedances as point process. . . . . . . . . . . . . . . . . 204
B.4.1Sampling distribution of threshold selection methods for PP-based met-
rics for simulated catalogues with constant threshold and hard censoring.213
LIST OF FIGURES XVIII
B.4.2Sampling distribution of Maximum likelihood estimates of GPD param-
eters obtained using a conservative, stepped and extended approach,
along with mean squared error decomposition under each approach. . 213
B.4.3Bootstrap maximum likelihood estimates for GPD parameters and con-
ditional return levels for Groningen magnitudes above 1.45ML assum-
ing GPD and exponential models. . . . . . . . . . . . . . . . . . . . . 214
B.4.4Sampling distribution of threshold selection methods for QQ-based and
PP-based metrics for catalogues simulated with a constant threshold
and phased censoring. . . . . . . . . . . . . . . . . . . . . . . . . . . 214
D.1.1 Schematic of ETAS model inputs, component models, and outputs. . 227
D.1.2 Schematic of combined model inputs, component models, and outputs. 227
D.1.3 Proposed forms for additional stress and stress rate as functions of time.234
D.1.4 Proposed forms for additional stress as a function of distance x. . . . 237
D.1.5 Proposed forms for the relationship between magnitude and total ad-
ditional stress. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 240
List of Tables
3.3.1 Reservoir models considered by Bourne and Oates (2017a) . . . . . . 60
4.3.1 Intensity functions for sub-models, the baseline model and model ex-
tensions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
4.4.1 Summaries of fitted sub-models, baseline model and model extensions. 81
6.3.1 Element-wise relative root mean squared errors (103) of conv, prop
and phys for MCMC chains on the prop and prop parameter spaces. . 150
6.3.2 Element-wise effective sample sizes of conv, prop and phys for MCMC
chains of 10,000 sampled values on the prop and prop parameter spaces. 152
A.1.1Integrated intensity functions for sub-models, the baseline model and
model extensions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195
Chapter 1
Introduction
1.1 Motivation
Statistical seismology is the study of earthquakes as a stochastic phenomenon. This
approach to earthquake modelling is both useful and necessary because the physical
processes that cause earthquakes are highly complex. These processes cannot be ob-
served or modelled at the necessary scale and precision for seismicity to be effectively
modelled deterministically. Taking a more descriptive, statistical approach permits
modelling using the noisy, incomplete knowledge that is available.
Much of the literature in statistical seismicity was developed for earthquakes caused
by the motion of the Earths tectonic plates. The catalogues of earthquakes used
to construct these models consist of many, large magnitude earthquakes that are
observed on an expansive spatio-temporal region. We are interested in modelling
induced earthquakes, which are caused by human activity. While these are related
to tectonic earthquakes, they present their own unique set of modelling issues and
opportunities.
CHAPTER 1. INTRODUCTION 2
Catalogues of induced earthquakes are typically much smaller than those of tectonic
earthquakes. They are smaller in terms of the number of earthquakes, the magnitude
of those earthquakes and the region on which they are observed. Smaller data sets
make estimating model parameters and comparison of models more difficult, because
there is less information on which to base these decisions. Smaller magnitude events
are also more difficult to discern from background vibrations, making them harder
to measure and locate. A smaller observation window increases the importance of
boundary effects and measurement errors. When modelling induced earthquakes we
are not in a data-rich setting, and so making the most of the available data and expert
knowledge is of paramount importance.
Models for tectonic earthquakes often assume that the system being modelled has
settled to a steady state. Changes to this system caused by human behaviour are
rapid on a geological time-scale. Steady state models for induced earthquakes are
therefore rarely appropriate and models for non-stationary behaviour are required.
This presents a modelling challenge in a low-data setting, but presents an opportunity
from the application perspective; with the correct intervention there is potential to
prevent earthquakes by changing the human activity that is causing them, rather than
only defending against them.
This ability to alter the earthquake generating process can lead to greater non-
stationarity in the data. Following one or more large earthquakes the generating
process is likely to be changed in an attempt to prevent further such events, based on
the available data. These changes might be single or multiple, synchronous or asyn-
chronous. Their effect on seismicity might be smooth or sudden, instantaneous or
lagged, or only have a cumulative effect. Making good, data-driven decisions on how
to intervene requires high quality detection systems. Improvements to these systems
over time are therefore likely to increase the ability to detect small earthquakes. This
CHAPTER 1. INTRODUCTION 3
will result in further non-stationarity in the catalogue through an improvement in
both the quality and quantity of earthquake observations within the record of seismic
activity. Separating the developments in the earthquake generating and detection
processes is a challenge that is not faced when modelling tectonic earthquakes.
One substantial advantage when modelling induced earthquakes is that the human
activity which causes the earthquakes is often reliably recorded. When combined
with geological expertise, these records can be used to measure or estimate covariates
relevant to the earthquake generating process. These covariates are rarely available
in the tectonic setting and can help to complement the small amount of data in the
earthquake catalogue itself.
There are two main purposes for modelling induced seismicity. The first is to develop
understanding of the process that is generating earthquakes. The second is to draw on
this understanding to forecast earthquakes under different scenarios. Each of these,
in their own way, allows informed decision making on which actions or interventions
should be taken in order to keep seismic hazard at an acceptable level.
Modelling induced earthquakes presents many challenges. Some of these are unique
to a particular application and require solutions tailored to that application. Other
challenges are common to all earthquake catalogues and could be adapted to similar
data structures in other applications. This thesis presents solutions to several prob-
lems motivated by earthquakes caused by gas extraction in the Netherlands. The
remainder of this chapter gives the context for these problems and then describes the
thesis structure.
CHAPTER 1. INTRODUCTION 4
1.1.1 A brief history of the Groningen gas field
At its time of discovery in 1959, the Groningen gas field was the largest in the world.
Today it remains the largest natural gas field discovered in Europe and among the top
ten worldwide (TNO, 2017). It was estimated that the field initially contained close to
3000 billion cubic metres (bcm) of gas. Extraction of this gas commenced in 1963 and
by the beginning of 2015, 2115bcm of gas had been extracted from the field - around
75% of the initial volume (van Thienen-Visser and Breunese, 2015). This extraction
is operated by the Dutch Petroleum Society (Nederlandse Aardolie Maatschappij,
NAM), a joint venture between Royal Dutch Shell and Exxon Mobil.
The Groningen field is located in the north-east of the Netherlands. It is situated in
the geological region named the Groningen high. This region is tectonically inactive
and does not experience earthquakes caused by the motion of the Earths tectonic
plates. Following extraction of gas from the field, concerns were raised about the
possibility of induced earthquakes; earthquakes caused by human activity. Since 1986
the region has been monitored for the presence of low magnitude seismic events that
might occur as a result of the gas extraction. The network of geophones used for this
monitoring is owned and maintained by the Royal Netherlands Meteorological Insti-
tute (KNMI). This network extends across the Netherlands but repeated investment
in, and improvement of, the network were made in the Groningen region in order to
better detect and understand induced earthquakes around the Groningen gas field.
These improvements mean that this region of the Netherlands now has the highest
resolution geophone network globally.
The first recorded earthquake in the the Groningen region occurred in 1991. To date,
the largest recorded event occurred on the 16th of August 2012, with a local magnitude
of 3.6ML. While this is not a large magnitude in relation to tectonic seismic events,
these induced events occur at shallow depths which, combined with the soft surface
CHAPTER 1. INTRODUCTION 5
soil in the area, lead to housing damage (van Thienen-Visser and Breunese, 2015).
The gas from the Groningen field was used extensively both within the Netherlands
and as an export to Germany, Belgium and Northern France. Within these regions
adaptations were made to the majority of domestic and industrial gas appliances to
account for the relatively low calorific value of gas from the Groningen field (NAM,
2017; TNO, 2017). This meant that switching to gas from another source was a
prohibitively expensive option and extraction from the site was expected to continue
until around 2080 subject to safety and the recommendations of the state supervision
of mines (TNO, 2017).
Further earthquakes in the years that followed lead to cap on total annual extraction
from the Groningen field. In addition, changes were made to the method of gas
extraction in an attempt to extract more evenly across the field and through the year
in an attempt to mitigate the negative effects of gas extraction. Magnitude 3.4 events
occurring in January 2018 and May 2019 contributed to the decision to move away
from Groningen gas and transition to using more costly imported gas. The initial
deadline for this transition was then accelerated so that from 2022 gas will only be
extracted from the field to supply extreme demand caused by the most severe seasonal
weather.
The mechanism linking gas extraction to induced seismicity is highly complex and not
well understood. The central aim of this thesis is to contribute to the understanding
of this relationship and the models that are used to describe it. The original purpose
of this was to allow informed decision making when selecting between a set of future
gas extraction scenarios. This understanding is useful even when production is ter-
minated, because in the years that follow gas will continue to redistribute within the
reservoir and potentially cause further seismic events.
CHAPTER 1. INTRODUCTION 6
1.1.2 Geology of the Groningen gas field
The Groningen gas field is located on the Groningen High, a region that is fault-
closed and lies within a tectonically stable block. The gas reservoir is located at a
vertical depth that varies from 2.6-3.2km below surface level, in the Rotliegend layer
as shown in Figure 1.1.1. The fault closures around the field prevent movement of gas
out of the region, while a perfect top seal for the reservoir is provided by a layer of
Zechstein salt above (NAM, 2016a). The Rotliegend reservoir is composed of porous
sandstone and claystone. The pore space in the structure of this rock layer is filled
with gas emanated from the carboniferous layer below. The thickness of the porous
Rotliegend layer varies across the field, from its thinnest of 140m in the south-south-
east to thickest of 300m in the north-north-west of the field. As well as varying in
thickness, the composition of the layer also changes from predominantly sandstone to
predominantly claystone along this gradient, as shown in Figure 1.1.2. The porosity
of the sandstone rock is high while in the claystone it is low. The composition and
thickness of the reservoir partly determine the potential for compaction of the porous
layer on the removal of gas from its pore space. This is pertinent because pore pressure
depletion and the resulting compaction of the reservoir are thought to be drivers of
the seismicity observed in the Groningen region (NAM, 2016a).
Other reservoirs have been been observed with greater levels of compaction than seen
in Groningen without any earthquakes being induced (Davison et al., 2010). Earth-
quakes occur because the Groningen reservoir contains a great many pre-existing
faults. These faults are cracks in the reservoir rock structure that pre-date gas ex-
traction and are thought to be necessary for induced seismicity to occur. In excess
of 1700 faults have been interpreted in the Groningen field as a part of the adden-
dum to the 2013 Winingsplan Groningen (van Elk et al., 2013). Of the faults in the
Groningen reservoir, 707 are included within the static and dynamic reservoir models
CHAPTER 1. INTRODUCTION 7
Figure 1.1.1: Schematic cross-section through the Groningen field indicating the main
stratigraphic intervals. Source: TNO (2017)
Figure 1.1.2: Lithostratigraphic subdivision of the Rotliegend in the Groningen area.
Source: NAM (2016a)
CHAPTER 1. INTRODUCTION 8
of Shell. The selection of these faults was justified by the discarded faults adding
geometric detail but not contributing significantly to seismic risk at the scale being
considered. Even with this up-scaling, it is currently not computationally feasible to
include all 707 of these faults in geomechanical models of the reservoir. Simplifications
of this fault structure are used in the geomechanical models, with ongoing work to
lessen these simplifications. It should be noted that at the resolution of the available
data it is not possible to assign earthquakes to individual faults. These limitations
to a deterministic, physical model for induced earthquake activity further motives a
statistical approach to this problem.
1.1.3 From gas extraction to induced earthquakes
Gas is extracted from the Groningen reservoir via 258 wells, which are located across
the field as 22 production clusters. The gas at each of these wells comes from a
common connected source. As gas is removed at wells, the remaining gas will move
from areas of high to low concentration and equalise the pressure gradient caused
by extraction. However, due to hydraulic resistance across the reservoir this pressure
equalisation is far from instantaneous (van Thienen-Visser and Breunese, 2015).
The pore pressure in the reservoir is the pressure of the gas that is located in the
pore space of its rocky structure. Localised and field-wide reduction of pore pressure
are problematic. This is because the layers of rock and soil above the reservoir are
held in place by a combination of the normal force provided by the reservoir structure
and the pore pressure of the gas within that structure. As gas is removed from the
reservoir the pore pressure reduces because the number of molecules per unit volume
is decreased. The resultant force causes compaction of the reservoir structure until
the forces are again balanced. As an illustration of this concept, consider letting air
out of a balloon. The pressure inside the balloon reduces while the air pressure on the
CHAPTER 1. INTRODUCTION 9
outside of the balloon remains constant. This net force causes the balloon to contract
until the forces are once again in balance.
This analogy also introduces the idea of elastic deformation; the balloon is able to
stretch and compress, subject and proportional to the forces exerted upon it. Such
elastic deformations are often assumed when constructing physics-based models for
induced seismicity. An associated concept is the bulk modulus of a material. This
gives the relative change in volume of the material per unit of compressive force exerted
upon it. In the Groningen reservoir, it has been argued that pressure depletion brings
about compaction in two ways; not only does it cause the net force on the reservoir
but it is also thought to reduce the bulk modulus of the composite material. In other
words, the reservior compacting is thought to make it more susceptible to further
compaction (Bourne and Oates, 2017a).
This compaction alone is not necessarily problematic. Much of the 900 km2 area of
the Groningen field has historically experienced subsidence due to the compaction of
the overburden, comprised of the layers above the reservoir. This means that building
methods and interventions are already used and available in the area to cope with
compaction and the resulting subsidence. It is the pre-existing faults which present a
danger.
When compaction occurs in the reservoir, as opposed to the overburden, additional
shear stresses are placed across the pre-existing faults within this layer. Initially these
are accommodated by static friction and the walls of the fault remain in place. When
these stresses become large enough to overcome the static friction, the fault will slip,
releasing energy and causing an induced earthquake (van Thienen-Visser and Bre-
unese, 2015). While this general process is widely agreed upon, the specifics of how
this happens are not well understood. Additionally, there is not agreement upon which
reservoir properties and features of the extraction scheme that are most important
CHAPTER 1. INTRODUCTION 10
drivers of induced earthquake activity. For example, Bourne et al. (2014) and Bourne
and Oates (2017a) argue in favour of seismicity being dependent on compaction while
van Thienen-Visser et al. (2016) provides a counter-argument, favouring seismicity
being dependent on pressure depletion. Despite this lack of understanding and agree-
ment, predictions of subsidence and seismic hazard must be produced. These must be
provided to regulators for a proposed production plan and must be updated and plans
be amended as extraction proceeds (van Thienen-Visser and Breunese, 2015).
1.1.4 Shell seismic risk analysis
Extraction of gas from the Groningen field has in the past been an essential component
of the Dutch economy and continues to be an important reserve for times of crisis. It
is widely agreed that this process has led to induced earthquakes in the Groningen
region, but the specifics of this process are not well understood.
While operating and eventually closing the Groningen gas field, Shell and NAM are
invested in minimising the risk associated with the production of natural gas. The
production plans of NAM include predictions of the subsidence and seismicity caused
by the proposed gas extraction and the associated the hazards and risks. The mod-
elling and assessment of risks are covered in greater technical detail in the Technical
Addendum to the Winningsplan. This large document is split into sections covering
methods for: planning gas production, forecasting and reducing seismic risk NAM
(2016a); subsidence modelling NAM (2016b); hazard assessment NAM (2016c); risk
assessment NAM (2016d) and damage assessment NAM (2016e).
A schematic diagram of the risk assessment procedure is shown in Figure 1.1.3. The
process begins by forecasting pressure depletion and compaction for three levels of
production, at each level considering two production distributions across the reser-
voir. This gives six possible production scenarios for risk comparison. These models
CHAPTER 1. INTRODUCTION 11
Figure 1.1.3: Schematic diagram of seismic risk assessment procedure after modelling
of pressure depletion and reservoir compaction.
for pressure depletion and compaction of the reservoir then feed into the probabilistic
forecasting, under each production scenario, of seismic events within the reservoir.
These forecasts are then translated into the effects seen at surface level through the
use of ground motion prediction equations (GMPEs). These ground motions, along
with the density of population and development across the region, are used to cal-
culate the risk and hazard profiles of each production scenario. The assessment of
production scenarios can therefore be seen as a modular exercise, with each com-
ponent interchangeable with another method that performs the same function and
provides as output the required inputs of future modelling stages.
This framework is adaptable to changes in internal structure but also can affect ex-
ternal change. As a result of the first Winningsplan published in 2013, action was
taken in 2014 to reduce the risk associated with induced seismic activity. Production
was then reduced and redistributed within the field, with dramatic reductions in the
areas with greatest compaction (van Thienen-Visser and Breunese, 2015). This action
followed from the theory, later published in Bourne et al. (2014), that the amount of
seismic energy that can be released in an earthquake is an exponential function of the
cumulative compaction of the reservoir.
CHAPTER 1. INTRODUCTION 12
This thesis aims to provide improvements for the first component of this work-
flow: modelling the number, magnitude, mechanism and location of induced earth-
quakes.
1.2 Thesis outline
This thesis has two primary aims. The first of these is to improve the understanding
of the process by which earthquakes are induced in the Groningen gas field through
examination, comparison and deconstruction of existing models. The second aim is
to improve these models in terms of their ability to represent reality and in terms of
their statistical properties.
Chapter 2 introduces and describes in detail the data that are available for modelling
earthquakes in the Groningen gas field. Features of both the data and the collection
process are discussed.
Chapter 3 introduces the relevant theory on stochastic point processes and extreme
value theory that provide the building blocks of the statistical models used in this
thesis. In particular Poisson, self-exciting and marked point processes are described
along with the peaks-over-threshold and point process approaches to univariate ex-
treme value modelling. Also introduced are current approaches to modelling induced
and tectonic earthquakes, with a focus on statistical approaches.
In Chapter 4 earthquake locations are considered in isolation from their magnitudes.
A physical hybrid model of earthquake locations is deconstructed to identify important
features that it captures in the earthquake generating process, and features it fails to
capture that present potential areas for improvement.
In Chapter 5 the development of the earthquake detection network in Groningen
is considered. A statistical approach is developed for estimating the time-varying
CHAPTER 1. INTRODUCTION 13
magnitude threshold above which all earthquakes are detected. The benefits of using
these additional small magnitude events when modelling large magnitude event is
demonstrated using both simulated and Groningen data.
In Chapter 6 takes the popular epidemic type aftershock sequence (ETAS) model
for aftershocks and proposes a reparameterisation and extension of the model. The
reparameterisation simplifies inference by rendering the parameters near-orthogonal
while increasing flexibility in the model. The extension provides a simple way to
move the ETAS model beyond the assumption of independent, identically distributed
magnitudes and allows interaction between earthquakes.
Chapter 7 concludes this thesis with a summary of the contributions that have been
made, a discussion of the shortcomings of the presented models, and opportunities
for future work.
Chapter 2
2.1 Outline
This chapter will introduce the data available for modelling earthquake activity in
the Groningen region. The earthquake catalogue used in this thesis is publicly avail-
able from the Royal Dutch Meteorological Institute, (KNMI, 2020). The data on gas
extraction and subsidence were provided by Shell and are available as part of pub-
lished technical reports, e.g. Bourne and Oates (2017b). Supplementary information
concerning the development of the earthquake detection and the gas production net-
works were also provided by Shell. These datasets will be outlined in Sections 2.2 -
2.4, where descriptions are given on both the information recorded and how this was
collected or constructed. Following this, Section 2.5 provides an exposition of the key
features of these data sets.
CHAPTER 2. DATA 15
2.2 Earthquake catalogue
The earthquake catalogue is freely available from the Royal Dutch Meteorological
Institute KNMI (2020). It details the location, time and magnitude of all recorded
earthquakes across the European Netherlands since the 5th of December 1991.
Earthquake locations are given by Northing-Easting pairs in the RD coordinate sys-
tem. Earthquake locations are stated to be accurate to within 500m. This has previ-
ously been interpreted as locations having isotropic standard errors of 250m (Bourne
and Oates, 2015). The RD coordinate system is a planar projection of the European
Netherlands and the areas of land and sea around its borders. The errors in location
incurred because of this projection are up to 0.25m, which are inconsequential as com-
pared to the measurement error. In addition to planar locations, earthquake depths
are available but these measurements have low resolution compared to the depth of
the reservoir. Earthquakes within the Groningen field are therefore all assigned a
nominal depth of 3km and so earthquake locations are represented as points on a
plane.
Each recorded earthquake has an associated magnitude. This is a measure of the
energy released by the seismic event and is reported in units of local magnitude
(ML) to one decimal place. Local magnitude is a logarithmic scale used to measure
earthquake severity. It is important to note that an earthquake can only be included
in the catalogue if it is of sufficiently large magnitude for its location to be determined.
This requires that the earthquake to be detected by three or more geophones. The
magnitude of completion for a given region and period is the smallest magnitude
seismic event that can be detected at any location or time within that interval. The
magnitude of completion therefore varies in space and time according to the number,
location and quality of geophones within the interval.
CHAPTER 2. DATA 16
The magnitude of completion for the Groningen gas field is a property that must
be estimated. It is widely accepted that since 1995 a conservative estimate of the
magnitude of completion in the Groningen region is 1.5ML. Since then significant
investment in the geophone network has lead to an increased ability to detect small
magnitude events, reducing the magnitude of completion. The time at which this was
reduced and the value that is was reduced to are disputed. Statistical analyses of the
Groningen catalogue are therefore typically performed only using events of magnitude
1.5ML or greater to avoid the bias induced by incorrectly assuming that the catalogue
is complete.
Figure 2.3.1 shows the locations of town- and city-centres and the locations of all ob-
served earthquakes in the period 1991-2015. The magnitude of each observed earth-
quake is indicated by the area of the circle representing it. The highest density regions
of earthquakes and population are aligned, which makes induced earthquakes in the
region a particular concern.
2.3 Static covariates
2.3.1 Fault structure
The fault structure of the reservoir was determined using tomography (also known
as seismic imaging) and can be separated into closing faults and internal faults. Gas
cannot move across closing faults, which represent the perimeter of the gas field and
determine its spatial extent. A polygonal approximation of the perimeter of the
gas field is shown in Figure 2.3.1. Internal faults are cracks in the rock structure
of the reservoir and are represented by sets of connected line segments. The 707
internal faults that are included in the static and dynamic reservoir models of the
2013 Winningsplan Groningen van Elk et al. (2013) are shown in Figure 2.3.1. It
CHAPTER 2. DATA 17
can be seen that these major faults are not evenly distributed throughout the field,
being more heavily concentrated in the south, with a band running NW-SE across
the field.
This set of faults was chosen by expert opinion to be a representative and influential
collection of faults. However, the selection is not exhaustive and this is for two reasons.
Firstly, the fractal nature of faulting means that there will always be faults smaller
than the resolution of the chosen imaging technique. It is thought that the failure of
several of these undetectable faults could cause an earthquake that is not attributable
to any of the larger, detected faults. Secondly, the computational intensity of the
reservoir models limits the scale at which faults can be included while maintaining
reasonable computation times. The threshold for fault inclusion is therefore somewhat
arbitrary but should ideally be as low as possible within the constraints of the available
measurement and modelling techniques.
2.3.2 Reservoir thickness and topographic gradient
The initial thickness of the reservoir, measured in metres, is given on a 500500 metre
grid across the reservoir. The topographic gradient of the reservoir is the gradient
of the upper surface of the reservoir, and is provided on the same 500500 metre
grid. Topographic gradient is a dimensionless scalar value giving the absolute value
of the steepest gradient at each point in the reservoir. The top surface of the reservoir
is measured by tomography, from which the spatial gradients used to calculate to
topographic gradient are obtained by finite differencing methods.
The topographic gradient and initial reservoir thickness are static reservoir properties,
they do not change over time, and are plotted in Figure 2.3.2.
CHAPTER 2. DATA 18
235000 245000 255000 265000
Easting
235000 245000 255000 265000
Easting
Figure 2.3.1: Locations of towns and cities around the Groningen field, shown by red
triangles. [Left] Locations of active production clusters shown as blue circles. Major
mapped faults shown in grey. [Right] Locations of all Earthquakes since 1995 shown
as grey circles with areas proportional to magnitude.
230000 240000 250000 260000
Easting
230000 240000 250000 260000
Easting
Figure 2.3.2: [Left] Thickness of Rotliegend reservoir in metres. [Right] Topological
gradient, the gradient of the upper surface of the Rotliegend reservoir.
CHAPTER 2. DATA 19
2.4 Extraction covariates
Information on how gas extraction impacts the reservoir is also available in the form
of gridded covariate data. The spatial grid on which these covariates are available is
the same as for the static covariates. Temporally, these values are reported at each
location, interpolated based on model output to the start of each year or to the start
of each week.
Extraction covariates are outputs of physics-based reservoir models which interpo-
late and infer values on a regular grid, based on irregularly spaced measurements of:
pressures at well-heads, surface-level displacements, and seismic images of the reser-
voirs top surface. The scale of errors on these covariates is unknown, but is likely
to be large, and is frequently disregarded. Bourne and Oates (2017a) acknowledges
the spatial uncertainty in both earthquake location and covariate value by selecting
the level of spatial smoothing in the covariate to optimise predictive performance.
What follows is a brief description of four possible covariates and their methods of
estimation.
Pressure depletion (Bar): Pressure in the reservoir pore space is measured at
well-heads at infrequent, irregular intervals. The MoRes finite element reservoir model
interpolates these values to return pressure estimates at 3 dimensional points within
the reservoir (NAM, 2016b). These estimates are based on strong modelling assump-
tions about reservoir properties and are aggregated vertically and then laterally inter-
polated to give estimates on a regular lattice. Smoothing is applied to these lattices
before use as a covariate.
Compaction (Metres): Compressibility of the reservoir, which is assumed to be
temporally constant but spatially varying, is estimated on a 2500m grid using surface
CHAPTER 2. DATA 20
displacement measurements and estimated pressure depletion. Estimates of com-
paction are achieved on a 500 metre grid by multiplying the pressure depletion grids
by the compressibility grid and then smoothing. For further details see Bierman et al.
(2015).
Strain Thickness (Metres): The vertical integral of strain over the thickness of
the reservoir at a given point. When there is only vertical strain this reduces to
compaction, otherwise it is the product of compaction and the topographic gradient.
Topographic gradient is the absolute value of the slope of the reservoirs upper sur-
face, as measured using seismic imaging. To avoid over-fitting based on this image a
smoothing is usually applied. For further details see Bourne and Oates (2015).
Incremental Coulomb stress (Bar): The additional stress placed along a fault
by compaction across the fault. For steeply dipping faults, the confining stress will
be small as compared to shear stress. In this case incremental coulomb stress will be
equal to a material constant multiplied by vertical strain (compaction per unit original
length) multiplied by topographic gradient. Again smoothing is used to avoid over-
fitting to the seismic image.
The pressure depletion and the resulting derived values of compaction, strain thick-
ness and incremental Coulomb stress were initially only available aggregated to an
annual scale. Later a weekly version of pressure depletion was made available, which
presented issues that were not present in the aggregated data. These issues stem
from the historically variable extraction of gas to supply a strongly seasonal demand.
This presents as large amounts of gas being extracted during the winter months and
relatively little during the summer. During the period of high extraction pockets of
low pressure are created which gas then moves into, equalising the pressure gradi-
ent during the summer months. This leads to localised but extensive pore pressure
CHAPTER 2. DATA 21
increases in the weekly data. How these areas of pressure-up should be related to
earthquake activity is not well understood.
2.5 Exploratory analysis
2.5.1 Outline
The aim of this section is to provide an exploratory analysis of the Groningen earth-
quake catalogue in order to improve familiarity with the available data and identify po-
tential challenges developing models for this earthquake catalogue. This exploratory
analysis has been decomposed to consider spatial, temporal and magnitude features
separately.
As mentioned in Section 2.2 the magnitude of completion is not constant through
the observation period. This complicates the interpretation of many aspects of the
exploratory analysis, because not all events below the magnitude of completion are
recorded and therefore a naive use of all data will result in biased conclusions. To
demonstrate this, several of the following analyses are presented for all events since
1995, irrespective of their magnitudes, and also for events since 1995 which exceed a
conservative threshold magnitude of 1.5ML. This choice of threshold follows Bourne
and Oates (2017a) and Dost et al. (2012), where the magnitude of completion is taken
to be 1.5ML in the Groningen field for this period.
2.5.2 Exploratory spatial analysis
Following from Figure 2.3.1, which showed the location of events of all magnitudes,
we begin our exploratory analysis with a simple investigation of the spatial density of
events throughout the Groningen region. Figure 2.5.1 shows Gaussian kernel density
estimates of the spatial distribution of seismic events for all events since 1995 and those
CHAPTER 2. DATA 22
exceeding magnitude 1.5ML. Most earthquakes occur in the north-west of the gas field
with some evidence of another intensity mode in the south-west. The earthquakes in
the south tend to be smaller and this second mode is less distinct when considering
events above magnitude 1.5ML.
230000 240000 250000 260000 270000
Easting
230000 240000 250000 260000 270000
Easting
Figure 2.5.1: Contour plots of the spatial earthquake density estimated using: [left]
all events since 1995, [right] all events with magnitude of at least 1.5ML since 1995.
Although these plots show the spatial variability in the observed earthquake density
through the field, they do not indicate whether these peaks in earthquake activity
would be surprising if events were located at random across the gas field. As a mea-
sure of how unusual such a point pattern would be under complete spatial randomness,
we look at Ripleys K-function (described fully in Section 3.1.4) as a measure of spa-
tial clustering. This function describes the expected number of further events within
a distance r > 0 of an arbitrary event in the process. Figures 2.5.2 shows the K-
function estimates using the observed point patterns of all events and those above the
1.5ML threshold. For values of r that are above 200 m, the estimated K-function of
both point patterns exceeds the simulation envelope calculated under the assumption
of complete spatial randomness. This indicates that there are a greater number of
CHAPTER 2. DATA 23
events within a short distance of each other than would be expected under this very
simple model, confirming the initial appearance of clustering. Failure to accept com-
plete spatial randomness is unsurprising, but gives justification for investigating other
models which might better describe the clustering of events. This might be through
the use of either spatially inhomogeneous Poisson processes, self-exciting processes or
a combination of the two.
0 200 400 600 800
Kobs(r)
Ktheo(r)
Khi(r)
Klo(r)
0 2000 4000 6000 8000
Kobs(r)
Ktheo(r)
Khi(r)
Klo(r)
0 200 400 600 800
Kobs(r)
Ktheo(r)
Khi(r)
Klo(r)
0 2000 4000 6000 8000
Kobs(r)
Ktheo(r)
Khi(r)
Klo(r)
Figure 2.5.2: Estimated Ripley K-function for observed data as compared to simu-
lation envelope for 99 realisations of complete spatial randomness: [upper] all events
since 1995 and [lower] events with magnitude of at least 1.5ML since 1995. Left and
right plots show function estimates over short (800m) and moderate (8km) distances
relative to the size of gas field.
To introduce a temporal component to the exploratory analysis, a first question of
CHAPTER 2. DATA 24
interest is whether or not the intensity of events is changing in the same way through
time at all locations. In order to test this, we consider a simple division of the
field separating the two high density regions seen previously in Figure 2.5.1. This
is shown in Figure 2.5.3, with the spatial modes separated by the line y = 73500 
0.6x. The density of events in each of these regions through time is also shown in
Figure 2.5.3.
A Monte Carlo test was carried out to test the null hypothesis that the earthquake
intensity in each of these regions is changing in the same way through time. Each
earthquake was labelled as belonging to one of the two high density regions. Under
the null hypothesis, that each region is developing in the same way through time,
estimates of the temporal intensity should remain valid under permutation of these
labels.
The integrated squared error between temporal density estimates f(t) and g(t) on the
time interval [tmin, tmax] is
ISE(f, g) =
 tmax
[f(t) g(t)]2dt. (2.5.1)
The integrated squared error was used as a measure of discrepancy between the tem-
poral earthquake densities in each of the high density regions, which were each scaled
to have unit integral over the observation period. The integrated squared error of the
observed intensity estimates, shown in Figure 2.5.3, was compared to those obtained
from 10,000 random permutations of the region labels. The probability of an inte-
grated squared error at least as large as that observed was 0.005, strongly suggesting
that the intensity of events changes differently though time depending on location.
Although the significance of this difference will depend on the chosen time-frame and
measure of discrepancy, this provides motivation for further work investigating the
way in which spatial location interacts with changes in intensity through time.
CHAPTER 2. DATA 25
220000 230000 240000 250000 260000 270000 280000
Easting
1995 2000 2005 2010 2015
Event densities in spatialhighdensity regions 1 and 2
N = 315   Bandwidth = 1.404
Figure 2.5.3: [Left] Splitting of field into two high density regions HDR1 and HDR2.
[Right] Temporal event density estimates for HDR1 and HDR2 through time.
2.5.3 Exploratory temporal analysis
In the same way that the exploratory spatial analysis began by considering the distri-
bution of earthquakes over the region, we begin the temporal analysis by considering
their distribution through time. Figure 2.5.4 shows a histogram of the number of
events per year, which gives a simple first visualisation. While the rate of events
above the magnitude of completion of 1.5ML is potentially constant over time, the
overall rate of events appears to be increasing through time. Two points are of note
here. Firstly that the catalogue up until March 2017 is used in this exploratory anal-
ysis, which explains the low event counts for 2017. Secondly, annual aggregation of
event counts is motivated by the seasonal nature of gas extraction but the division
into calendar years is an arbitrary choice. From this plot we can see that the total
event count increases more dramatically over time than that of the largest events; that
is likely through a combination of smaller events being detected more consistently as
the sensor network developed and because small events occur with higher relative
frequency.
Rather than aggregating events by year, in Figure 2.5.5 a smoothed intensity estimate
for each dataset is considered instead. These are obtained using Gaussian kernel den-
CHAPTER 2. DATA 26
sity estimation and show a similar structure whether including or excluding small
magnitude events. Added to each plot are 95% simulation intervals for the estimated
intensity if the event rate were truly constant over the entire period. The observed
intensity estimates are both systematically below the simulation interval at the start
of the period and above at the end of the period. This indicates that the intensity of
earthquakes is increasing through time at both large and small magnitudes, though
this change is more apparent when considering all events than only those above mag-
nitude 1.5ML.
An artefact of this density estimation method is that the intensity estimates will
be lower at the beginning and end of the estimation interval, because the events that
might have occurred just before or after the interval do not contribute to the intensity
estimate. This edge effect could somewhat be overcome by using a density estimate
that uses boundary reflection methods (Silverman, 1986), but since our focus here is
on the values relative to a homogeneous process this is not strictly necessary.
1995 2000 2005 2010 2015 2020
All events
Events >1.5M
Figure 2.5.4: Histogram displaying the number of earthquakes each year since 1995.
In addition to the trend in intensity through time, the inter-arrival time between
events is of interest as an indicator of the degree of temporal clustering or regular-
ity. In such comparisons, the homogeneous Poisson process is typically used as a
comparator and has exponentially distributed inter-arrival times. To compare these
CHAPTER 2. DATA 27
1995 2000 2005 2010 2015
1995 2000 2005 2010 2015
Figure 2.5.5: Estimated intensity of events through time. Red lines indicate the 95%
simulation intervals under the assumption of constant intensity. [Left] All events since
1995. [Right] Events since 1995 of at least magnitude 1.5ML.
distributions, the observed inter-arrival times were first standardised to have mean
one. The quantiles of these standardised intervals were then compared to those of
the standard exponential distribution. This is shown in Figure 2.5.6. We consider
only events above magnitude 1.5ML in this plot, as the increasing capability to detect
small earthquakes leads to identification of clustering behaviour that is actually an
artefact of the detection method.
0 1 2 3 4 5
Theoretical Quantiles
Figure 2.5.6: Quantile-quantile plot of standardised observed interval lengths against
standard exponential distribution. Solid line shows y = x, dashed lines give 95%
simulation envelope.
CHAPTER 2. DATA 28
From Figure 2.5.6 we can see that the observed quantiles deviate significantly from
an exponential distribution. The largest quantiles are larger than expected and the
smallest are smaller than expected, which is consistent with temporal clustering of
events. The idea of temporal clustering has the intuitive appeal that in the context
of earthquakes it may be interpreted physically as aftershocks, earthquakes that are
triggered by another recent earthquake. However this is not the only possible ex-
planation, the increasing intensity over time seen in Figure 2.5.4 would also induce
clustering, though of a different type.
If the mainshocks could be well modelled as a homogeneous Poisson process, so that
the overall intensity was constant, then Figure 2.5.6 could be used to estimate the
proportion of aftershocks within the data. Assuming that delays between a main-
and aftershocks are small compared to the time between mainshocks, long inter-event
intervals would consist mainly of the intervals between main-shocks. Since these follow
an exponential distribution for a homogeneous Poisson process, the upper quantiles
of the inter-event time distribution would therefore form a straight line on the right
of the QQ-plot. The quantile at which this line begins would give an estimate of the
proportion of events which are aftershocks (Ferro and Segers, 2003).
In Figure 2.5.6, there appears to be three rather than two straight line sections. There
are several possible models which may yield such an interval distribution, for example
a combination of a clustered point process and a background intensity that changes
gradually over time. This suggests that inhomogeneous and self-exciting process mod-
els are both worth further investigating in the context of Groningen earthquakes.
2.5.4 Exploratory magnitude analysis
In addition to exploring the spatial and temporal structure of event occurrences, we
can also look at the structure of the marks associated with these events. In the
CHAPTER 2. DATA 29
assessment of seismic risk, being able to model accurately the markings of events as
well as their locations is of great importance. This is because the mark associated with
each event denotes the magnitude of the earthquake, indicating the amount of energy
released and therefore to some extent the potential of the event to cause damage at
surface level.
0 1 2 3 4
Magnitude
All Events
Events of at least 1.5M
Figure 2.5.7: Gaussian kernel density estimates of magnitude distributions using all
events (black) and only those of at least magnitude 1.5ML (red).
Figure 2.5.7 shows kernel density estimates of the magnitude distributions of all
recorded events since 1995, and the subset of those which reached at least magnitude
1.5ML. The Gutenberg-Richter law is a widely used descriptive model for magnitudes
of seismic events. It states that in any given earthquake catalogue, the number of
events N of magnitude of M or greater is given by
N = 10abM , (2.5.2)
for some constants a and b specific to the region and period. The term 10a is a
normalising constant equal to the total number of events in the catalogue Ntotal.
Reworking equation (2.5.2) to be in terms of probabilities and natural logarithms, the
CHAPTER 2. DATA 30
magnitude survivor function FM(m) is
FM(m) =
Ntotal
= 10aN = (10b)m = em, (2.5.3)
where  = b loge 10. This is the survivor function of an exponential distribution and
so the Gutenberg-Richter law can be restated as magnitudes following an exponential
distribution.
In many earthquake catalogues, the distribution of magnitudes deviates from an ex-
ponential at both the small and large magnitudes. At the lower end this is usually
because the network of geophones lacks the sensitivity to detect small events and so
under-counts as compared to the Gutenberg-Richter model. At the higher end, the
deviation occurs because the maximum potential energy that can be stored within a
region before an earthquake occurs has some finite limit. This effectively means that
there is a maximum magnitude of earthquake which can be observed, whereas there
is no upper limit on the tail of the exponential distribution (Vere-Jones, 2010).
In Figure 2.5.7, we can see that for events of at least magnitude 1.5ML, the mono-
tonic decreasing form of the Gutenberg-Richter model seems as though it may be
appropriate. When using the entire catalogue, however, the density estimate becomes
uni-modal. This is likely due to events with small magnitudes typically being missed
by the detection network. A change in this behaviour over time can be seen in Fig-
ure 2.5.8; small magnitudes events are detected more often in the later part of the
observation period, following improvements to the sensor network.
Figure 2.5.8 also demonstrates that the censoring of small magnitude events is not of
a simple cut-off form; some events below the magnitude of completion are detected.
These low magnitude events are increasingly detected at later times, which may be a
result of continued/changing gas production or else due to the increasing sensitivity
of the geophone network. It is thought that all earthquakes of this magnitude 1.5ML
CHAPTER 2. DATA 31
1995 2000 2005 2010 2015
Magnitude of completion
Figure 2.5.8: Magnitudes of earthquakes through time. Red line indicates the mag-
nitude of completion used by Bourne and Oates (2017a) for this period, 1.5ML.
or greater have been recorded in the Groningen region since 1995. The non-standard
censoring of this non-stationary process means that seismicity models for the Gronin-
gen region are typically fitted using only events of at least magnitude 1.5ML. This
avoids biasing in the resulting parameter estimates by using incomplete data but at
the cost of only using 27% of the available data, resulting in parameter estimates with
greater uncertainties.
Figure 2.5.7 provides high-level evidence in support of the Gutenberg-Richter law be-
ing a suitable model for the magnitude distribution in the Groningen region above the
nominal magnitude of completion. We therefore investigate the appropriateness and
the goodness-of-fit of this exponential model to events large enough to be completely
recorded.
As a first step, we consider whether the distribution of large magnitudes remains
constant through time. Since the intensity of events is increasing over time, when
making this assessment we do not split the Groningen catalogue into equal time
intervals but rather into 6 consecutive intervals that each containing the same number
CHAPTER 2. DATA 32
of events. This helps to ensure that our ability to identify the form of the magnitude
distribution is similar in each interval.
Figure 2.5.9 shows, for each of the 6 time intervals, Gaussian kernel density estimates
for the conditional distribution of magnitudes exceeding 1.5ML using two techniques.
In the first approach, only events exceeding 1.5ML are used to construct the density
estimate, using the boundary reflection technique of Silverman (1986) to account for
edge effects. This approach avoids the inclusion of data below the stated magnitude
of completion but also assumes that the density is symmetric about the boundary.
This is inconsistent with the monotonically decreasing Gutenberg-Richter model and
may be causing the flattening of density estimates near the boundary value of 1.5ML.
The second approach includes events below 1.5ML when estimating the conditional
distribution above 1.5ML. If many events are missing below the boundary, this risks
introducing a bias that reduces density estimates near the boundary.
Comparing the density estimates in Figure 2.5.9, including events below the magni-
tude threshold seems to cause less distortion than the boundary reflection method; the
density estimates for each interval using the second method retain an exponential-like
trend close to the threshold magnitude. This suggests that 1.5ML is a conservative
estimate for the magnitude of completion for the period since 1995, or that only a
small proportion of events just below this level are censored. Using either method,
the estimated magnitude distributions are in good agreement with one another across
all time intervals, indicating that above 1.5ML magnitudes follow the same distribu-
tion over time. We therefore have evidence that while the proportion of all recorded
events that are above the threshold is decreasing through time, the distribution of
events conditional on being above this threshold remains constant.
The Gutenberg-Richter law asserts not only that the exceedances follow the same
distribution through time, but that this is an exponential distribution. To assess this
CHAPTER 2. DATA 33
1.5 2.0 2.5 3.0 3.5 4.0
Magnitude
Interval
1.5 2.0 2.5 3.0 3.5 4.0
Magnitude
Interval
Figure 2.5.9: Gaussian kernel density estimates of the magnitude distribution in each
of six consecutive intervals, conditional on events exceeding magnitude 1.5 ML. [Left]
Density estimate excluding events below magnitude 1.5ML with boundary correction.
[Right] Density estimate including events below magnitude 1.5ML.
assumption we use a quantile-quantile plot for magnitudes exceeding 1.5ML, shown
in Figure 2.5.10. Issues arise here due to many sample quantiles taking the same
value because the magnitude data is reported to only one decimal place. Therefore,
both a standard (red) and a corrected (blue) tolerance interval are shown in Fig-
ure 2.5.10. The standard tolerance interval shows the typical range for quantiles of
unrounded exponential data. The corrected tolerance interval show similar ranges
but for exponential data that have been rounded to one decimal place. Including
rounding explains some, but not all, of the additional probability mass around the
threshold magnitude as compared to an exponential distribution. This is indicated
by the greater coverage of the data by the rounding-corrected interval as compared
to the standard exponential interval.
Taken collectively, these exploratory plots indicate that the Gutenberg-Richter model
provides a reasonable starting point for modelling Groningen earthquake magnitudes
but that further investigations in which rounding of the observations is properly ac-
counted for are also warranted.
CHAPTER 2. DATA 34
0 1 2 3 4 5 6
Theoretical quantiles
Figure 2.5.10: Quantile-quantile plot of standardised threshold exceedances against
standard exponential distribution. 95 % tolerance intervals for standard exponential
and rounded standard exponential quantiles are shown in red and blue respectively.
2.5.5 Review
While by no means exhaustive, this exploratory analysis has revealed many features
of the data and answered several of the most pressing question regarding its mod-
elling.
After exploratory analysis we have found that the spatial intensity of events is in-
homogeneous. This was confirmed by considering the probability, under complete
spatial randomness, of the originally apparent clustering of large magnitude events in
the north-west of the field and the secondary cluster of smaller events in the south-
west, by way of the Ripley K-function. The intensity of recorded events was also
found to be increasing through time, with the additional possibility of temporal clus-
tering which requires further investigation. Furthermore, the two high density regions
of recorded earthquake activity appear to have intensity functions that are changing
differently through time.
CHAPTER 2. DATA 35
An exploratory analysis of the earthquake magnitudes revealed issues relating to the
censoring of small magnitude events throughout the observation period. This cen-
soring is incomplete and varies through time, caused by the increasing sensitivity of
the geophone network. The apparent change in modal magnitude is perhaps also
an artefact of improved detection of small magnitude events because the conditional
distribution of magnitudes exceeding 1.5ML appears to be stationary. Despite this
stationarity, the Gutenberg-Richter law has limitations when modelling these large
magnitude events, assigning lower probability than one night expect to magnitudes
close to the threshold value.
Chapter 3
Literature review
3.1 Point process models
3.1.1 Overview
Point processes are a special case of stochastic processes. As such, they can be de-
scribed by a collection of scalar- or vector-valued random variables Y = {Y1, . . . , YN}.
Both the elements of Y and the number these elements N = |Y| are random. Each
element of Y represents a point in some mathematical space, typically this obser-
vation window W will be either all or a subset of Rd for d  1. One realisa-
tion of such process is a collection of point locations in the observation window
y = {y1, . . . , yn}  W n.
Point processes are useful models for localised events which occur across space, time or
both. Point process models have been used in forestry, epidemiology and neuroscience,
and form the basis for a popular class of earthquake models. This section aims to give
a brief introduction to important examples of point processes, building from simple to
specialised models. Further details on point process theory are given in Cox and Isham
CHAPTER 3. LITERATURE REVIEW 37
(1980), while Diggle (1983) covers statistical analysis of point process data.
3.1.2 Poisson point processes
Homogeneous Poisson processes A homogeneous Poisson process (HPP) is the
simplest type of point process; in a HPP the event locations and their count, N(W ) =
|{Yi  Y : Yi  W}|, are both random. In a HPP events are located independently of
one another and uniformly at random across W . The intensity of the point process 
determines the expected number of events per unit volume in W .
Consider events occurring only in time so that W = R. In this case, it is usual
to relabel the elements of Y so that event indices impose a temporal ordering and
Y1      YN . The history of the point process at time t is the set of events that
occur up to time t and is denoted by Ht = {Yi : Yi  t}. The intensity of the Poisson
process,  > 0, gives the expected number of events in the process per unit time. The
HPP with intensity  on R can be defined by the conditions that for all t  W , as
  0+:
P (N(t, t+ ) = 1|Ht) =  + o(); (3.1.1)
P (N(t, t+ ) > 1|Ht) = o(); (3.1.2)
P (N(t, t+ ) = 0|Ht) = 1  + o(). (3.1.3)
It follows from conditions (3.1.1) - (3.1.3) that:
 The waiting time until the next event from an arbitrary time t, Tt  Exp ();
 The intensity and waiting time distribution do not depend on the history of the
process;
 The counting measure N(A) on A  R is distributed Pois (|A|), where |A| is
the Lebesgue measure of A;
CHAPTER 3. LITERATURE REVIEW 38
 For disjoint sets A,B  W , N(A) and N(B) are independent.
These properties allow us to specify a HPP by giving one or more of the following
properties: the intensity for every point in W , the distribution of inter-event times,
or the joint distribution of the counting measure N on all subsets of W .
Inhomogeneous Poisson processes The HPP can be generalised by allowing the
intensity of events to vary as a function of time or covariates. This results in an
inhomogeneous Poisson process (IHPP) with intensity function (t) : W  R+0 . This
intensity function is defined as the instantaneous rate of events at t, i.e.,
(t) = lim
N(t, t+ )
. (3.1.4)
The survivor function of the waiting time Tt retains an exponential form but with
a rate parameter that now depends on both the starting point t and the intensity
function. Define the integrated intensity function on a set A as
(A) =
(a)da
and as (a, b) on open intervals of the form (a, b). Then the survivor function of Tt
has the form
FTt() = P (Tt  ) = exp{(t, t+ )}.
It follows that an IHPP, also satisfies the memoryless property and has indepen-
dent event counts on disjoint subsets. However, for an IHPP the distribution of
the counting measure N(A) depends on the set A that is being considered, where
N(A)  Pois ((A)).
Time rescaling theorem Through a transformation of the time axis it is possible
to transform between Y1, an IHPP with known intensity (t) : R+  R+, and Y2, a
CHAPTER 3. LITERATURE REVIEW 39
HPP on R+ with unit rate. The transformation from Y1 to Y2 is given by
Y2 = (Y1) = {(0, Yi) : Yi  Y1}. (3.1.5)
While the reverse the transformation from a HPP with unit rate Y2 to Y1, an IHPP
with known intensity (t) : R+  R+,is
Y1 = 1 (Y2) =
1(0, Yi) : Yi  Y1
, (3.1.6)
where 1(0, y) is the value of t which solves (0, t) = y.
The transformation (3.1.5) is provided by the time rescaling theorem (Brown et al.,
2002) and can be used to assess the fit of an IHPP model to a point pattern; the
observed point pattern is transformed using the fitted intensity function and the
properties of the transformed pattern can be compared to those of a homogeneous
Poisson process. For example, the inter-event time distribution can be compared to an
Exp (1) distribution. The reverse transformation (3.1.6) gives can be used to simulate
a one dimensional IHPP by transforming a HPP.
Random thinning and superposition In a random thinning of a point process,
each event is either retained in the process or removed with some stated probability.
A randomly thinned Poisson process remains a Poisson process. If the original process
has intensity 1(t) and each point is retained with probability p(t), then the thinned
process has intensity 2(t) = 1(t)p(t). This result gives a second method of gener-
ating IHPPs with intensity function 2(t): first simulate a HPP with rate 1(t) = m
where m  2(t) for all t, then retain each event in this process with probability
p(t) = 2(t)/m.
A related operation on point processes is superposition, where events from two or
more point processes are combined into a single point process. The superposition of
independent Poisson processes remains a Poisson processes and the resulting intensity
CHAPTER 3. LITERATURE REVIEW 40
is given by the sum of the component intensities. A limit theorem exists for point
process superposition, which states that for a set of k suitably well-behaved, non-
Poisson processes the process formed by their superposition, after sufficient scaling,
converges to a Poisson process as k  (Cox and Isham, 1980). This theorem helps
to explain the wide applicability of Poisson processes to natural phenomena, which
can often be thought of as the superposition of many sub-processes.
Spatial and spatio-temporal Poisson processes The definitions and proper-
ties of Poisson processes can be extended to spatial and spatio-temporal observation
windows. For models in two or more dimensions, the interval specification of the
point process becomes less useful because each inter-event distance corresponds to a
set of possible locations and points which are at a similar distance from a particular
location are not necessarily close to one another. The intensity specification remains
valid for point processes in more than one dimension, with slight alterations to the
definition.
For a spatial point process on W  R2 the intensity function at x  R2 is
(x) = lim
N(b2(x, ))
|b2(x, )|
where where bd(x, r) = {s  Rd : |s x| < r} is the d-dimensional ball centred at x
of radius r.
The spatial analogue of the waiting time is the radial contact distance, Rx. This is
the the `2-distance from planar location x to the nearest event. The radial contact
distance Rx has distribution function:
P (Rx  r) = 1 exp {(b2(x, r))} . (3.1.7)
When the Poisson process is homogeneous with rate , the distribution (3.1.7) no
longer depends on location x and simplifies to R  Exp (r2).
CHAPTER 3. LITERATURE REVIEW 41
For a spatio-temporal process on W  R2R the intensity function at (x, t)  R2R
is defined using a space-time cylinder surrounding that location:
(x, t) = lim
,0+
N(b2(x, ) (t, t+ ))
|b2(x, ) (t, t+ )|
For spatio-temporal processes, the waiting time distribution is defined by aggregating
events over spatial dimensions at each time point and the radial contact distance by
aggregating over time. Note that this does not preserve orderliness of the process
because multiple events can occur at the same location in space or time following
aggregation of the point process.
Inference for Poisson processes Let y = {y1, . . . , yn} be a point pattern on
observation window W , from which a parametric Poisson process intensity function
(w; ) : W  R+0 is to be inferred, where the parameters  are in some specified
parameter space. The event count and event locations are independent for a Poisson
process. Therefore, the likelihood function for  is
L(;y) = P (N(W ) = n|)
P (Yi = yi|) ,
(W ; )n exp{(W ; )}
(yi; )
(W ; )
= (n)1 exp{(W ; )}
(yi; ); (3.1.8)
where (W ; ) =
(w; )dw. The corresponding log-likelihood is
`(;y) =
{log (yi; )}  log(n) (W ; ). (3.1.9)
The parameters  of the intensity model can be estimated by maximum-likelihood or
Bayesian inference. For maximum likelihood estimation, closed form estimators can
only be obtained for very simple intensity functions and so numerical optimisation
routines are required. Similarly, in a Bayesian analysis conjugate prior distributions
CHAPTER 3. LITERATURE REVIEW 42
are not available except in the simplest of cases and Markov chain Monte Carlo
(MCMC) methods must be used to estimate the posterior distribution of .
One difficulty that can arise when fitting such models is the computational cost when
evaluating the integral (W ; ). If this integral does not have a closed form then
numerical integration is required at each step of the optimisation or Markov chain.
This can be very costly depending on complexity of the intensity model and the shape
of the observation window. Another difficulty is that point pattern data sets are often
small, making the intensity parameters difficult to estimate precisely. A Bayesian
approach to modelling can ease this problem if domain specific knowledge can be used
to help constrain parameter values through the choice of prior distribution.
3.1.3 Generalisations of the Poisson process
3.1.3.1 Renewal Processes
Renewal processes generalise the interval specification of homogeneous Poisson pro-
cesses in one dimension. For a one-dimensional HPP, the intervals between events
are independent and identically distributed exponential random variables (Cox and
Isham, 1980). Renewal processes generalise this to allow independent intervals with
some other distribution function G.
The dispersion of G controls the degree of regularity or clustering in the process.
If G is less dispersed than the exponential density then the renewal process will
be more regular than a Poisson process. If the dispersion is larger, then the interval
lengths in the renewal process are more irregular, leading to clustering of events in the
process. Renewal processes encompass Poisson processes but allow greater flexibility
in clustering behaviour. However, intervals lengths remain independent and the model
does not easily extend to higher dimensional spaces.
CHAPTER 3. LITERATURE REVIEW 43
3.1.3.2 Linear self-exciting processes
The intensity function definition of a point process (3.1.4), can be generalised to allow
dependence on the history of the process, so that
(t|Ht) = lim
E [N(t, t+ )|Ht] .
Poisson process intensities are independent of their history, and renewal process in-
tensities at time t depend only on the instant before t. Self-exciting processes extend
this dependence to allow some or all previous events to influence the intensity at time
t. A non-negative background intensity (t) is supplemented by contributions to the
integrated intensity by each previous event. The amount by which the integrated
intensity is increased by each previous event is denoted by r0  0, which gives the
expected number of further events triggered by each event. The allocation of this
additional intensity over time is determined by the kernel function, w(), which is
defined on   0 and integrates to 1 over this support. The amount and allocation of
additional intensity determines the level of clustering within the process.
A linear self-exciting process has an intensity function of the form
(t;Ht) = (t) +
r0w(t Yi). (3.1.10)
This intensity function can be viewed as the superposition of N + 1 IHPPs. Each
component represents one cause of events; the background intensity and each of the
N events themselves. The process can be therefore be seen as a composite arrival and
branching process, where each event generates a Poisson number of offspring. The
expected number of offspring per event, r0, must be below 1 for the process to be
sub-critical and have stable long-term properties.
The intensity specification of self-exciting processes allows the extension of the model
to spatial and spatio-temporal supports. Self-exciting processes can be made more
CHAPTER 3. LITERATURE REVIEW 44
flexible by allowing (t), r0 or w() to depend on covariates. In the modelling of
earthquakes, self-exciting models provide a way of jointly modelling earthquakes and
aftershock activity.
3.1.3.3 Doubly stochastic point processes
The final extension of a Poisson process treats the intensity as a random func-
tion. These point processes are called doubly stochastic point processes or Cox pro-
cesses.
Let {(t)} be a real-valued non-negative stochastic process of preassigned structure,
with history at time t given by Ht = {() :  < t}. Then the complete intensity
function of the doubly stochastic Poisson process is:
(t;Ht,Ht ) = lim
N(t, t+ ) | Ht,Ht
= (t).
Unlike the realisation of the events from the point process, the realisation of the inten-
sity function is not typically observed. Cox processes therefore belong to the class of
latent models and, depending on the specification of {(t)}, are capable of represent-
ing a variety of complex intensity functions. A common form is to suppose that the
log intensity is a Gaussian process (Baddeley, 2008). The flexibility of these models
means that very large datasets, repeated observations or strong prior knowledge are
required to constrain the model.
3.1.3.4 Multi-type and marked processes
All previous examples of point processes have considered events to consist only of a
location in space, time or both. Covariate information is often available on the events
themselves, as well as on the observation window. To distinguish between these, event
covariates are described as marks. A marked point process has one or more marks
CHAPTER 3. LITERATURE REVIEW 45
assigning to each event. These marks could be external covariates or they could be
used to extend point process models. For example, integer valued marks can be used
to allow multiple events to occur at the same location, where the mark denotes event
multiplicity. Multi-type point processes can be modelled by using categorical marks
denote the type of each event. This allows, for example, within-type clustering but
between-type independence.
3.1.4 Measures of clustering
To determine if models that allow clustering are appropriate for an observed point
pattern requires a method to measure the observed degree of clustering. This mea-
surement can then be compared to the expected behaviour of a Poisson process to
give evidence of relative clustering or regularity in the pattern.
The second order intensity function describes the level of clustering of events in a
point process. For a point process on W  R2 and arbitrary locations x1, x2  W the
second order intensity function is given by
2(x1, x2) = lim
,0+
E[N(b2(x1, ))N(b2(x2, ))]
|b2(x1, )||b2(x2, )|
The second order intensity function lacks the easy physical interpretation of the first
order intensity function. For this reason the reduced second moment function, or
Ripley K-function, was introduced. For a stationary, isotropic and orderly process
with intensity , the function 2 depends on x1 and x2 only through the distance
between them, r = |x1  x2|. The Ripley K-function is then
K(s) = 22
2(r)rdr. (3.1.11)
The K-function has a physical interpretation as the expected number of further events
within distance s of an arbitrary event, per unit intensity. At small distances this
CHAPTER 3. LITERATURE REVIEW 46
function will therefore have relatively high values for clustered patterns, and low for
regular ones. To make this comparison a homogeneous Poisson process is used as a
benchmark, with K(s) = s2.
The physical interpretation of the K-function suggests a means of its estimation from
a point pattern. The K-function may be estimated by first estimating the intensity
and then finding the sample mean number of further events within distance s of an
arbitrary event. When considering a finite observation window this estimate should
be corrected for the bias caused by not observing events outside that window. Let
dij be the distance between events i and j. Also let wij be the proportion of the
circumference of the circle centred at event i of radius dij that is contained in the
window. An edge corrected estimate for the Ripley K-function is then:
K(s) =
j 6=i
wijI{dij  s},
where  is an estimate of the intensity  and n is the number of events within the
window. Baddeley et al. (2000) and Marcon and Puech (2009) later considered gen-
eralisations of the K-function to inhomogeneous processes. The second order in-
tensity function and K-function may be extended to spatio-temporal point process.
Definitions and techniques for estimating these extensions are detailed in Dorai-Raj
(2001).
3.2 Extreme value methods
3.2.1 Overview
The usual aim of statistical modelling is to closely represent the centre of a probability
distribution or the typical values of a stochastic process. Standard statistical methods
were developed with this aim in mind, but in many applications it is not typical values
CHAPTER 3. LITERATURE REVIEW 47
that are of interest. Rather, it is the particularly high or low extreme values that are
of interest. A model is therefore required for the tails rather than the body of the
probability distribution. It is not appropriate to use standard modelling approaches in
these settings because they are driven primarily by the large number of non-extreme
observations.
Extreme value methods provide statistically rigorous models for the tails a probabil-
ity distribution or random process. These models are fitted exclusively to, or with
strong emphasis on, data from the tail of the distribution. They are therefore not
compromised by the abundance of central values in the observed data. Extreme value
models also provide a principled way of extrapolating beyond the observed levels of
the process. This is justified by deriving asymptotic models for extremal behaviour
and then using these models as approximations for the behaviour at high but finite
levels of a process.
The ability to extrapolate is particularly important when models are being used to
assess hazard and risk, which are strongly influenced by the extremal properties of
both the damage and protective mechanisms. As such, extreme value methods have
been used widely in areas where risk estimation is important, including: finance,
hydrology, and process control (Coles, 2001). To date, the use of these methods
within seismology has been limited.
This section provides an overview of the asymptotic motivation for commonly used
univariate extreme value models and their estimation.
3.2.2 Block maxima approach
Let X1, . . . , Xn be a sequence of independent, identically distributed (i.i.d.) random
variables with unknown distribution function F . Define the maximum of this sequence
CHAPTER 3. LITERATURE REVIEW 48
to be Mn = max(X1, . . . , Xn). The distribution function of Mn is then given by
Pr(Mn  x) = Pr(X1  x, . . . , Xn  x)
= Pr(X1  x) . . .Pr(Xn  x)
= {F (x)}n.
The distribution function of Mn could be estimated by constructing an estimator F
for F , but this approach is highly sensitive to changes in the estimated distribution
function F . An alternative approach is to consider the distribution function of Mn as
the length of the sequence X1, . . . , Xn grows. Unfortunately, the distribution of Mn
converges to a point mass on the upper end point of F :
Mn  xF as n where xF = sup {x : F (x) < 1}.
This issue can be overcome by obtaining a sequence of linear transformations on Mn
that result in a non-degenerate limit distribution. Define Mn to be
Mn =
Mn  bn
for sequences of constants an > 0 and bn  R, which stabilise the location and scale
of Mn to avoid the degeneracy of its distribution as n increases. The Extremal Types
Theorem (Fisher and Tippett, 1928) states that if these sequences of normalising
constants exist, then as n:
Mn  bn
 G(x), (3.2.1)
where G is distribution function of a Frechet, Gumbel or negative Weibull random
variable. These distributional forms are united in a single parameterisation by the
Unified Extremal Types Theorem. The resulting generalised extreme value (GEV)
family of distribution functions has the form
G(x) = exp
1 + 
]1/
, (3.2.2)
CHAPTER 3. LITERATURE REVIEW 49
where x+ = max(x, 0),   R+ and ,   R. The parameters ,  and  have
respective interpretations as location, scale and shape parameters. Positive values of
 correspond to a Frechet distribution and a heavy upper tail. When  = 0 the GEV
is equivalent to a Gumbel distribution and has an exponential upper tail. Negative
values of  correspond to a negative Weibull distribution, which is light tailed and
has a finite upper end point.
A linear transformation of a GEV random variable remains within the GEV family
but has different parameter values; this means that the if the distribution of Mn can
well approximated by a GEV distribution then so can the distribution of Mn. This
result motivates the use of the GEV distribution as an asymptotic model for finite
sample maxima, analogous to the central limit theorem motivating a Gaussian model
for finite sample means.
To estimate the parameters of the GEV distribution, the observed sequenceX1, . . . , Xn
is separated into m blocks of equal length as in Figure 3.2.1. The sample maxima
in each of these blocks may be treated as an approximate sample from the GEV dis-
tribution of interest and used to estimate its parameters in a frequentist or Bayesian
framework.
3.2.3 Peaks over threshold
An alternative approach to modelling extremal behaviour uses all observations that
exceed some suitably high threshold u. This approach can make use of more of the
available data when multiple extreme events occur within a single block, as shown in
Figure 3.2.1, and can also be used when data are not regularly sampled.
To derive an asymptotic model for this type of data, let X1, . . . , Xn be a sequence of
CHAPTER 3. LITERATURE REVIEW 50
0 500 1000 1500 2000 2500 3000
Figure 3.2.1: Simulated daily data. Red crosses show the values in an annual maxima
extreme value analysis. Red line indicates threshold value u = 3.0, all exceedances of
which are used in a peaks over threshold or point process approach to extreme value
analysis.
i.i.d. random variables with common distribution function F . Let
Nn(x) =
I{Xi > anx+ bn},
where the normalising constants an and bn satisfy the conditions of limit (3.2.1) and
I{A} is an indicator of event A. This random variable counts the exceedances of anx+
bn among the sequence X1, . . . , Xn. Since the Xi are i.i.d. random variables,
Nn(x)  Binom (n, 1 F (anx+ bn)) . (3.2.3)
Taking logs of the limiting result (3.2.2), we know that
n logF (anx+ bn) logG(x).
Applying a first-order Taylor expansion to the left hand side and then negating gives
that, for all values of x,
n [1 F (anx+ bn)]  logG(x) =
1 + 
]1/
. (3.2.4)
CHAPTER 3. LITERATURE REVIEW 51
It follows from (3.2.3) and (3.2.4) that as n the exceedance count
Nn(x) N(x), where N(x)  Pois
1 + 
]1/
Additionally, for values of x > u:
P (Xi > anx+ bn|Xi > anu+ bn)
logG(x)
logG(u)
= 1Hu(x),
where
Hu(x) =
1 +  xu
]1/
if  6= 0,
1 exp
if  = 0.
(3.2.5)
The distribution function (3.2.5) defines a generalised Pareto random variable with
parameters for the threshold u  R, scale u  R, and shape   R of the distribution.
The shape parameter is equal to that of the corresponding GEV distribution for block
maxima and does not depend on the choice of threshold. The scale parameter is
dependent on the choice of threshold and is linked to that of the corresponding GEV
distribution by u =  + (u  ) . Additionally, the threshold stability property of
the GPD (Davison and Smith, 1990) states that if X  u|X > u  GPDu(u, ) then
for a higher threshold v, X  v|X > v  GPDv(u + (v  u), ).
The GPD family has three sub-classes depending on the vale of .
 If  < 0 then the GPD is heavy-tailed and X  u|X > u is Pareto distributed
with scale parameter u/ and shape parameter 1/.
 If  = 0 then the GPD has an exponential tail and Xu|X > u is exponentially
distributed with expectation 1/u.
 If  < 0 then the GPD has a light tail and the distribution has a finite upper
end point xG = u u/.
The limit (3.2.5) motivates the use of the GPD as a model for the exceedances of
a high threshold u. This model does not depend on the generating distribution F ,
CHAPTER 3. LITERATURE REVIEW 52
and so is widely applicable. After choosing a threshold value, exceedances of this
level may be treated as approximate samples from a GPD and used to estimate its
parameters.
3.2.4 Point process representation
A further generalisation of the extreme value model is to consider exceedances of a
high threshold u as a point process and to examine the properties of this process as
the number of observations becomes large and the threshold approaches xF .
Let X1, . . . , Xn be a sequence of independent random variables with common distri-
bution function F . Assume that the conditions for the limit distribution (3.2.1) hold
and define the sequence of point processes P1, P2, . . . on [0, 1] R where
Xi  bn
: i = 1, . . . , n
. (3.2.6)
This sequence converges to a non-degenerate, inhomogeneous Poisson process P as
n. Large values of Xi are retained in P , while small values are normalised to a
common value bl. The limit process P has intensity function
(t, x) =
1 + 
]11/
, (3.2.7)
on (t, x)  [0, 1] [bl,). The link to the peaks over threshold model can be seen by
considering regions of the form A = (t1, t2) [u,). The integrated intensity function
on such regions is
(A) =
(t, x) dt dx = (t2  t1)
1 + 
)]1/
The Poisson distribution for the total exceedance counts (3.2.3) can be shown by
considering (t1, t2) = (0, 1). This limit process also demonstrates the independence
and identical distribution of exceedance sizes, which correspond to the generalised
Pareto survivor function (3.2.5).
CHAPTER 3. LITERATURE REVIEW 53
Parameters of the extreme value model can be estimated by assuming that the Poisson
process limit holds exactly above some high threshold u. Standard estimation methods
for parametric point process intensities may then be used to fit the extreme value
model to an observed point pattern.
Extreme value models are often reported in terms of the distribution for annual max-
ima. This can be easily incorporated into the point process model by introducing a
scaling factor m into the intensity function (3.2.7);
(t, x) =
1 + 
x m
]11/
. (3.2.8)
The resulting parameter estimates are equivalent to the GEV parameters for annual
maxima if m is chosen to be the duration (in years) of the data. If m is chosen to be
the number of threshold exceedances then the parameter estimates are equivalent to
those of the corresponding GPD. For any value of m the resulting parameter estimates
are not dependent on the choice of threshold u. The point process formulation is
therefore particularly useful when the threshold, size or rate of exeedances change
over time.
3.2.5 Inference for extreme value models
Each of the extreme value models can be fitted in a likelihood or Bayesian framework,
with a frequentist approach being most common. Closed forms are not available for
the maximum likelihood estimators in any of these models and so to find estimates
numerical optimisation routines are required. When the shape parameter  > 1/2
the maximum likelihood estimators are asymptotically Gaussian in the number of
block maxima or threshold exceedances (Smith, 1985). This property holds in the
majority of physical applications of extreme value models and allows standard confi-
dence interval construction using the delta method or the profile deviance function.
CHAPTER 3. LITERATURE REVIEW 54
The case where   1/2 corresponds to an upper tail that is very short and the
estimators converge at a greater rate.
Bayesian modelling requires a prior distribution to be specified on the model parame-
ters. Conjugate priors do not exist for these models and so Markov chain Monte Carlo
(MCMC) methods are required for inference. A Bayesian approach can therefore be
computationally costly, but does allow expert knowledge of the process to be included
through the choice of prior. This can be highly beneficial to inference in the low data,
extreme value setting. Bayesian methods also provide natural estimation of parameter
uncertainty, avoiding the theoretical complications of maximum likelihood.
A detailed description of the frequentist inference procedure for each of these models
is given in Coles (2001), and the Bayesian analogues in Coles and Tawn (1996) and
Sharkey and Tawn (2017).
In each of these models the threshold value, or equivalently block length, must be
chosen. This choice presents a trade-off between bias and variance. A low threshold
risks the asymptotic model being valid for only a portion of the data used, biasing
parameter estimates. Conversely, a high threshold reduces the amount of available
data and provides lower precision estimates of the parameters. There are no exact
methods for choosing the threshold value. Diagnostics used to guide this choice are
usually based on demonstrating deviation from the limit distribution and are reviewed
in Scarrott and MacDonald (2012).
CHAPTER 3. LITERATURE REVIEW 55
3.3 Earthquake modelling
3.3.1 Overview
The are a great many models for seismicity in either space or time, with far fewer
spatio-temporal models available. A reason for this is the large computational cost
of fitting and implementing spatio-temporal models and the relatively small amount
of information available to fit these. Earthquake models may be descriptive or con-
ceptual. Descriptive models aim only to capture the important properties of the
earthquake catalogue, while conceptual models aim to describe the process generat-
ing the earthquakes. The mechanics of geological systems are not fully understood
and so all models must fall somewhere between these two extremes. Appropriately
combining a descriptive model with physical insights is therefore a key component of
statistical seismicity modelling.
Point process models of seismicity can range from purely descriptive to very conceptual
depending on the intensity specification. Epidemic type aftershock sequence (ETAS)
point process models are toward the descriptive end of this scale and are widely
used to model tectonic earthquakes (Zhuang et al., 2012). These models are special
cases of the self-exciting processes introduced in Section 3.1.3.2 and will form the
focus of Section 3.3.2. Following this, Section 3.3.3 considers physically motivated
statistical models specific to the Groningen field, which aim to explain as well as
describe earthquake occurrences.
3.3.2 Epidemic Type Aftershock Sequence models
Labelling earthquakes as foreshocks, mainshocks and aftershocks is not a straight-
forward task, even retrospectively, and remains open area of research (van Stiphout
et al., 2012; Benali et al., 2020). ETAS models draw inspiration from epidemiology
CHAPTER 3. LITERATURE REVIEW 56
and avoid the need for this classification. Instead, all earthquakes are treated equally.
Each earthquake has the potential to trigger further earthquakes and the propensity
to do so is determined by its magnitude.
The initial ETAS model of Ogata (1988) is a temporal marked point process model
for earthquake occurrences times and their magnitudes. In this model, each event
represents a time-magnitude pair Yi = (Ti,Mi) for i = 1, . . . , N and the history of the
process includes the marks as well as times of previous events; Ht = {Yi = (Ti,Mi) :
Ti  t}. The point pattern and associated marks are modelled independently, with
marks assumed to be i.i.d. with probability density function f(m). This magnitude
distribution is often taken to be the Gutenberg-Richter model,
f(m) =
 exp{(mm0)} for  > 0 and m  m0,
0 otherwise,
(3.3.1)
where m0 is the minimum event magnitude. The Gutenberg-Richter model is equiva-
lent to an Exp () density translated by m0, where earthquakes with magnitudes less
than m0 are considered too small to cause a hazard or to induce further earthquakes.
When this magnitude model is used, Yi  W  (m0,) where W  R.
The intensity function of the ETAS model is given in equation (3.3.2). This inten-
sity is a superposition of a background process with constant rate, , and intensity
contributions from the previous events depending on their times ti and magnitudes
(t;Ht) =
i:ti<t
(mi)h(t ti)
, (3.3.2)
where  and h are functions defined as follows.
The productivity function (m) is a function giving the expected number of events
that are triggered directly by an earthquake of magnitude m. The expected number
of triggered events is dependent on the triggering event magnitude through a relation
CHAPTER 3. LITERATURE REVIEW 57
of the form:
(m) =
A exp{(mm0)} for m  m0 and A  0,
0 otherwise.
The allocation function h(t) is a probability distribution describing the time delay
between triggering and triggered events. The time delays until these aftershocks are
usually described by the Omori-Utsu law. When modelling the temporal intensity
of aftershocks t time units after a main-shock, (t), for the large Nobi earthquake of
1891, Omori (1894) found that a relation of the form:
(t) =
(t+ c)
I{t > 0} for c,K  0, (3.3.3)
provided a good fit to the observations. This was later generalised by Utsu (1957),
who suggested that the decay through time could vary across catalogues and proposed
a relation of the form
(t) =
(t+ c)p
I{t > 0} for c,K  0 and p  0. (3.3.4)
This relationship is known as the Omori-Utsu or modified Omori law. It was shown
by Utsu et al. (1995) to describe many aftershock sequences and that the temporal
decay in intensity was independent of the magnitude of the initial earthquake.
The temporal ETAS model was extended to space and time in Ogata (1998). The
intensity function of this process is given in equation (3.3.5), where x  R2. This
model allows for a spatially varying background intensity (x) and also for the spa-
tial distribution of triggered events to depend on the magnitude and location of the
triggering earthquake through the spatial kernel g(x,m). Again, the magnitudes are
assumed to be independent of time, location and cause of event. This formulation
gives the model
(x, t,m|Ht) = f(m)
(x) +
i:ti<t
(mi)h(t ti)g(x xi,mi)
. (3.3.5)
CHAPTER 3. LITERATURE REVIEW 58
The ETAS model is now well studied, both in theory and in practice. This model and
its extensions are commonly used to describe tectonic seismicity. A review of further
extensions to the ETAS model is given in Zhuang et al. (2011). These extensions
include models for (x) using splines, Gaussian processes, and adaptive piecewise-
constant functions (Kolev and Ross, 2020; Molkenthin et al., 2020; Ogata, 2011).
Models with spatially varying kernels and magnitude parameters have also been pro-
posed. Such model extensions are only feasible for large earthquake catalogues. The
ETAS model is not the only descriptive model of seismicity available. Zhuang et al.
(2011) reviews alternative models, with references to more detailed descriptions.
3.3.3 Physics-based modelling
3.3.3.1 Elastic thin-sheet models
Physics-based seismicity models must be developed in the context of the geological
structures and the mechanism driving seismicity in a particular study region. In this
review, we therefore focus on the elastic thin-sheet models for the Groningen reservoir
developed in Bourne and Oates (2017a). These models build upon previous work in
Bourne et al. (2014) by incorporating additional assumptions about reservoir proper-
ties to better link the gas extraction and earthquake processes. In Bourne and Oates
(2017a), the reservoir is modelled as a porous, elastic, thin sheet which deforms due
to the observed pore pressure depletion. This deformation causes additional stress on
a heterogeneous network of faults, which each have some initial stress state. When
the combined initial and added stress exceeds a critical value, the faults will slip
and release the stored potential energy as an earthquake. Several inhomogeneous
Poisson process models are constructed to describe resulting spatio-temporal inten-
sity of induced earthquakes based on variations in the assumptions about reservoir
properties.
CHAPTER 3. LITERATURE REVIEW 59
The simplest form of the model considers a uniform pressure change within an homo-
geneous, isotropic, linear-elastic reservoir of infinite extent, which contains a network
of pre-existing faults with i.i.d. initial stresses. Since the reservoir is assumed to be
linear-elastic, the additional stresses acting on faults are proportional to pressure de-
pletion. Under this model a fault will slip and cause an earthquake after a pressure
depletion of p if its initial stress C was within mp of its critical stress ccrit, where
m is a material constant. In this way, induced earthquakes correspond to those faults
with the highest initial stresses. The fraction of faults which fail at a particular level
of reservoir depletion therefore depends on the upper tail of the initial stress distribu-
tion. This initial stress distribution is unknown, but extreme value theory provides
an asymptotically motivated form for its tail.
If C is the random initial stress on a particular fault and u is a high quantile of the
initial stress distribution, then Cu|C > u  GPDu(, ) provides an asymptotically
motivated model for extreme initial stresses. Since a fault fails if it has initial stress
within mp of ccrit, the probability of failure is
Pf = P (C +mp  ccrit)
= P (C  ccrit mp|C > u)P (C > u)


1 + 
(ccrit mp u)
)1/
P (C > u) if  6= 0,
(ccrit mp u)
P (C > u) if  = 0.
Fault failure probabilities are derived in a similar way for cases where the properties
of the reservoir are less restricted; these allow for heterogeneous reservoir thickness
and for there to be pre-existing vertical offsets across faults. Using these failure
probabilities, a Poisson point process for extreme threshold failures may be specified.
The intensity function (t) at time t for this process can be expressed in terms of the
CHAPTER 3. LITERATURE REVIEW 60
fault failure probability:
(t) = h
(p)
(p)
where  is the volume density of faults and h is reservoir thickness.
This type of intensity function was constructed for a range of reservoir models, the
properties of which are detailed in Table 3.3.1. In these models the bulk modulus
of the reservoir may be homogeneous or may vary laterally, the deformation of the
reservoir may be elastic or plastic (elastic deformation models are based on pressure
depletion while plastic deformation models are based on vertical compaction), and the
failure probability may respond to depletion or compaction in a linear, exponential or
generalised Pareto relationship.
Heterogeneity
Model Geometric Elastic Covariate Pf response
Homogeneous None None None Constant
PT None None Pressure Depletion Linear
EPT None None Pressure Depletion Exponential
CT None X Compaction Linear
ECT None X Compaction Exponential
EST X X Strain Exponential
GPST X X Strain Generalised Pareto
Table 3.3.1: Reservoir models considered by Bourne and Oates (2017a)
Each of these models were fitted to the earthquake catalogue for the Groningen reser-
voir. To avoid the issues associated with incomplete earthquake detection, the models
were fitted using events of magnitude 1.5ML or greater in the period between 1995
CHAPTER 3. LITERATURE REVIEW 61
and 2017. The models were fitted in a Bayesian framework using independent uni-
form prior distributions for the parameters. Exponential prior distributions were also
considered, and achieved the same model rankings as the uniform prior.
For evaluation of the models, the data were split into training and test periods. The
division between these was taken as the 1st of January 2012, to match with the five
year predictions required for the Groningen production plans. The models were then
evaluated by likelihood- and simulation-based testing. For likelihood-based testing,
the posterior predictive distribution of the likelihood of the test data was calculated
for each model in Table 3.3.1. These distributions were then used for model compar-
ison, where models with larger modal values and small variability about this value
are preferred. Simulation testing was also performed, to compare the properties of
catalogues simulated under each fitted model to in the test and training portions of
the catalogue. These properties included the temporal intensity of events, and the
spatial distribution of events.
The likelihood-based testing revealed the following order of model performances using
the same model codes as in Table 3.3.1, where A < B and A  B show strong and
weak preferences for model B over model A:
PT < homogeneous < CT < EPT < ECT < GPST  EST.
Simulation testing revealed that the exponential and generalised Pareto trend models
were better able to describe the temporal developments in earthquake intensity. It also
revealed that strain-based models were better able describe the spatial distribution
of seismic events, which gives some reasoning for the observed ordering of events.
The exponential strain model (EST), was selected to be the preferred model because
of its parsimony and slightly better performance in the likelihood testing. From
the relative performance of these models, Bourne and Oates (2017a) conclude that
CHAPTER 3. LITERATURE REVIEW 62
there is strong evidence for including the inhomogeneous reservoir properties when
modelling seismicity. Further, it was suggested that additions to the model might
include stress-transfer within the reservoir, the possibility of self-excitation of the
point process and covariate dependent event magnitudes. Later model developments
(Bourne and Oates, 2017c; Bourne et al., 2018; Bourne and Oates, 2020) investigate
these additional features using the exponential trend intensity model, or one of similar
derivation, for mainshocks.
The effects of long-term gas extraction or cessation of extraction are the same under
each of these models. Reducing or stopping production would reduce the rate of
earthquakes, but not stop them entirely, because pore pressure would continue to
change across the reservoir as spatial pressure gradients equalise. The effect of long-
term gas production is that fault failures will begin on faults that are in the body
of the initial stress distribution, rather than the tail. The failure trend would then
be expected to fall from exponential to linear (Bourne and Oates, 2017a) and so the
model would over-predict seismicity. This is not necessarily negative, but could lead
to the implementation of overly conservative production plans.
3.4 Recent work on Groningen seismicity
We conclude this chapter by giving a brief overview of the breadth of current research
topics that focus on the Groningen gas field and the earthquakes that occur there.
The induced earthquakes in the Groningen gas field have received much and varied
attention by the statistical seismology community in recent years. The reasons for
this are at least threefold; the detection of earthquakes in this region is second to
none, providing a world-leading earthquake catalogue in terms of completeness; the
high quality covariate information on gas extraction presents new opportunities that
are not present when modelling tectonic earthquakes; and finally, the possibility for
CHAPTER 3. LITERATURE REVIEW 63
human intervention in the earthquake generating process means that work in this area
has a potentially huge impact.
There is continuing research to further improve the completeness of the earthquake
catalogue in the Groningen region as well as the quality of the covariate information,
which describes how the gas field is changing due to gas extraction. This has led to
active research in signal processing to better detect earthquakes from the background
vibrations of the Earths surface (Paap et al., 2020; Waheed et al., 2020) and in
remote sensing to better measure the compaction of the gas field (Hol et al., 2018;
Hadi Mehranpour et al., 2020).
Physics-based statistical models continue to be a popular approach to modelling
the times, locations and magnitudes of induced earthquakes (Dempsey and Suckale,
2017; Richter et al., 2020; Smith et al., 2020). The application of machine learning
techniques is a novel approach to this same task, which is taken by Limbeck et al.
(2021).
Finally, a major focus of research regarding the Groningen gas field is to characterise
the earthquake magnitudes in the region. This is particularly important in assessing
the risk posed to buildings overlying the gas field. Attention is often given to esti-
mating the largest possible earthquake within the region or the largest earthquake
expected during a given time interval, with a workshop having been dedicated to
addressing these challenges (Zoller and Holschneider, 2016). A recent treatment of
this problem is given by Beirlant et al. (2019) who compare a range of approaches.
Estimating these largest magnitudes links closely with the extreme value techniques
introduced earlier in this chapter; Shcherbakov et al. (2019) uses associated methods
to address this same problem in the tectonic setting.
Chapter 4
Covariate-based models for
induced earthquake locations
4.1 Introduction
4.1.1 Induced earthquakes
Catalogues of earthquakes induced by human activity differ in several important ways
from those caused by the motion of the Earths tectonic plates. Induced earthquake
catalogues are typically composed of fewer, smaller earthquakes that occur closer to
the Earths surface. Their proximity to the surface means that they pose a hazard
to infrastructure despite their relatively small magnitudes because their effect is dis-
persed over a smaller spatial extent. Appropriate modelling of earthquake occurrences
and magnitudes is foundational to the appropriate protection of infrastructure against
seismic hazards. When modelling either type of seismicity it is common to assume
that earthquake counts and locations may be modelled separately from their magni-
tudes (Zhuang et al., 2011). Here, we focus on models for the locations and counts of
CHAPTER 4. MODELS FOR INDUCED EARTHQUAKE LOCATIONS 65
induced earthquakes.
In the tectonic setting, the earthquake generating process is often modelled as hav-
ing reached steady-state. Fluctuations from this are then attributed to aftershock
activity: earthquakes that are triggered by slip rather than drift of tectonic plates.
The combination of large catalogue sizes and the assumption of temporal stationarity
facilitate the use of highly flexible semi-parametric occurrence models for tectonic
earthquakes (Kolev and Ross, 2020; Molkenthin et al., 2020). When modelling in-
duced earthquakes, the steady-state assumption is rarely appropriate because the
human activity that causes earthquakes changes over time. Additionally, the small
number of earthquakes available in catalogues of induced seismicity makes the use of
such highly flexible models particularly challenging.
Despite the above challenges, specific opportunities exist that are accessible only in
the context of induced earthquakes. Notably, the seismic process is driven by human
actions, and so if these have been sufficiently monitored then there is the potential
to include these actions as covariates within an earthquake model. The small size of
the earthquake catalogue necessitates a structured modelling approach. This addi-
tional model structure can be beneficial as it can allow us to focus on interpretable
model forms that can increase our understanding of the earthquake triggering pro-
cess, relative to a purely descriptive modelling approach. Increased understanding
is important here, because it can potentially support informed intervention into the
human activities that are driving the earthquake activity.
4.1.2 Motivation and aims
This research focuses on earthquakes that are induced by extraction of natural gas
from the Groningen gas field. This gas field lies approximately 3 km below a densely
populated region in the north-east of the Netherlands that does not experience any
CHAPTER 4. MODELS FOR INDUCED EARTHQUAKE LOCATIONS 66
other form of seismic activity (NAM, 2016a; van Thienen-Visser et al., 2016). As a re-
sult, the structures overlying the gas field were not designed to withstand such events.
Understanding the link between gas extraction and the locations of induced earth-
quakes is therefore important for at least two reasons. Firstly, it can help to inform
which areas should be prioritised when retrofitting buildings to withstand induced
earthquakes. Secondly, it allows investigation of whether the number or location of
earthquakes could be influenced by following different gas extraction scenarios.
The ground and structures above the gas field are collectively known as the overbur-
den. The overburden is supported by the reservoir, which is comprised of porous rock
where the pore space is filled with natural gas. The overburden is supported by both
the structural integrity of the porous rock and also by the pressure exerted by the
gas within the pore space. Extracting gas reduces the pressure in the pore space and
increases the load on preexisting faults within the reservoir structure. When the shear
force on these faults becomes sufficiently large to overcome static friction the fault
will slip, releasing the potential energy as an induced earthquake (van Thienen-Visser
and Breunese, 2015; Bourne and Oates, 2017c).
Detailed information on gas extraction from the Groningen field is available along
with other key reservoir properties (Bourne and Oates, 2017b). Such covariate in-
formation is rarely available in the tectonic setting and is central to the modelling
approach for small catalogues of induced earthquakes. Rather than considering very
flexible model forms, parsimonious parametric models can be constructed based on
the physical process that is causing earthquakes. This model structure supports lim-
ited earthquake data within a physically motivated framework, which can be adapted
to answer questions of interest about the earthquake generating process.
Bourne and Oates (2017a) developed such a physically-motivated point process model
for earthquakes within the Groningen gas field. In this chapter, we take this as our
CHAPTER 4. MODELS FOR INDUCED EARTHQUAKE LOCATIONS 67
baseline model with two primary aims. Firstly, we assess the parsimony of the base-
line model. We do so by considering in turn each component in the baseline point
process intensity function and exploring its relative importance to model fit. Sec-
ondly, we incorporate a range of additional physical and mathematical features into
the baseline model, for example a lagged effect of gas extraction or spatially varying
model parameters. We investigate whether incorporating these additional features
of the earthquake generating process into the baseline model leads to significant im-
provements in model fit. This modelling addresses questions of practical and scientific
interest. Specifically, we assess whether there is sufficient evidence to suggest answer
the following questions:
 Is there spatial variation in the parameters of the baseline model?
 Does the level of smoothing applied to the gas extraction activities limit the
baseline model?
 Is there a temporal lag or spatial displacement between gas extraction and
induced earthquake activities?
 Does the gas extraction rate influence the resulting induced earthquake count?
Full descriptions are given in Chapter 2 for the available covariates on gas extraction
when answering these questions.
4.1.3 Outline
The rest of this chapter is organised as follows: Section 4.2 introduces covariate-based
point process models and describes the baseline model; Section 4.3 gives descriptions
of model simplifications and extensions that will be investigated; Section 4.4 gives the
results and a discussion of fitting these models; Section 4.5 summarises our findings
and proposes potentially fruitful areas of further research.
CHAPTER 4. MODELS FOR INDUCED EARTHQUAKE LOCATIONS 68
4.2 Background
4.2.1 Point process models for earthquakes
A point process is a stochastic process that provides a statistical model for the count
and locations of localised events within a fixed region. As such, point processes are
frequently used as a stochastic model for earthquake epicentres. Poisson processes are
one of the simplest, most well studied and most widely used point process models.
A Poisson process, defined on a region A, may be defined completely by its intensity
function (x) : A  [0,). The number of point events in the process and their
locations on A are random. The number of events on A is denoted by N(A) and
follows a Poisson distribution where the expected event count is equal to the intensity
function integrated over A:
N(A)  Poisson((A)) where (A) =
(x)dx.
Each event in a Poisson process is located independently of all other events and of
the event count. The locations of events within a particular realisation of the point
process are distributed over A in proportion to the intensity function . Throughout,
X = {Xi : i = 1, . . . , N} denotes the event locations in a stochastic point process,
where Xi  A and N  R+0 . The observed point pattern on A, which is a particular
realisation of the point process, will be denoted by x = {x1, . . . , xn}.
Point process models can be fitted to an observed point pattern using standard meth-
ods from either frequentist or Bayesian inference. In either case, it is usual to select
a flexible parametric or semi-parametric model for the intensity function and to esti-
mate the vector of parameter values  for (x; ) under the chosen framework. In a
similar manner to generalised linear modelling, covariate effects may be incorporated
into the intensity function; an equivalent of the link function may be used to ensure
that the non-negativity condition on  is not violated by the inclusion of covariates.
CHAPTER 4. MODELS FOR INDUCED EARTHQUAKE LOCATIONS 69
Letting z denote one or more covariates that are measured on A, the intensity function
may then be specified as (x; , z). The corresponding integrated intensity function
on the entire region is given by
(A; , z) =
(x; , z)dx.
The likelihood function for a Poisson process with intensity function (x; , z) and
integrated intensity function (A; , z) is then given by
L(;x, z) = Pr(N(A) = n X = x|, z)
= Pr(N = n|, z)
Pr(Xi = xi|, z)
n(A; , z) exp{(A; , z)}
(xi; , z)
(A; , z)
The log-likelihood of the Poisson process is therefore given by
`(;x, z) =  log(n) (A; , z) +
log (xi; , z).
For particularly simple choices of the intensity function it is possible to find the maxi-
mum likelihood estimator for  analytically or to specify a conjugate prior distribution
for . For most parametric forms for  that are useful in practice, numerical optimi-
sation routines or Markov chain Monte Carlo methods must be used to estimate the
parameter values.
Specifying a Poisson process thorough its intensity function allows the same modelling
framework to be used for point patterns observed on regions A of one, two or many
dimensions. In the context of the Groningen earthquakes, the region A is the spatial
extent of the gas field, W  R2, over the time interval (0, tmax), so that A = W 
(0, tmax). Earthquakes are then described as point events (x, t) in three dimensions,
where x represents the planar location and t represents occurrence time.
CHAPTER 4. MODELS FOR INDUCED EARTHQUAKE LOCATIONS 70
4.2.2 Baseline intensity model
We use the physically motivated Poisson process model developed in Bourne and
Oates (2017a) as our baseline model. This model was derived based on the physics
of the gas extraction process and a thin-sheet approximation of the reservoir struc-
ture to describe vertical reservoir compaction due to gas extraction. Based on weak
assumptions about the initial distribution of stresses on pre-existing faults within
the reservoir, a covariate-driven intensity model was derived based on the additional
stresses applied to these faults due to gas extraction.
The baseline model was constructed based on the earlier observation in Bourne et al.
(2014) that the earthquake count (per unit reduction in reservoir volume) increases
exponentially with cumulative reservoir compaction. The covariates z in the resulting
intensity function are c(x, t), the cumulative compaction until time t at location x,
and c(x, t) = d
c(x, t) the instantaneous compaction at time t and location x. Con-
sider a small spatial extent around a point x  W in which reservoir compaction is
approximately constant and denote this region by B(x, ) = {x  W : ||xx||2 < }
for  > 0. By letting |B(x, )| be the spatial area of B(x, ), the observed relationship
between compaction and earthquake counts leads to an integrated intensity function
of the form:
(B(x, ) (0, t);, z) = 0|B(x, )|c(x, t) exp{1c(x, t)}, (4.2.1)
where  = (0, 1) is a vector of model parameters to be estimated. To ensure
non-negativity of the corresponding intensity function, it is required that 0  0
and 1  maxA c(x, t). It is useful to consider the integrated intensity function
on regions of this form because covariate values are given annually on a spatial grid
with fine spatial resolution. The integrated intensity over W  (0, t) can then be
well approximated by a temporal interpolation of the cumulative covariate value in
CHAPTER 4. MODELS FOR INDUCED EARTHQUAKE LOCATIONS 71
each pixel to time t and then summing expressions of the form (4.2.1) over all pixels.
The integrated intensity function (4.2.1) corresponds to an intensity function of the
form:
(x, t;, z) = 0c(x, t)[1 + 1c(x, t)] exp{1c(x, t)}. (4.2.2)
The later modelling approach Bourne and Oates (2017a) uses an intensity function
of the same form but using strain thickness, the product of reservoir compaction
and the topographic gradient of the reservoir surface, in place of compaction. In
their approach, the strain thickness covariate is spatially smoothed using an isotropic
Gaussian kernel where the bandwidth is chosen to optimise model performance.
Work done by Shell subsequent to the initial development of this material has re-
sulted in further changes to the covariates used during model fitting (Bourne et al.,
2018). There has been a shift to using the smoothed incremental Coulomb stress
(ICS) in place of smoothed strain thickness. The ICS is the product of compaction,
topographic gradient and a spatially variable reservoir property (a poroelastic mod-
ulus) that describes the proportion of reservoir compressibility attributable to each
of the reservoir rock structure and remaining gas pressure. When constructing the
incremental Coulomb stress covariate, three properties are selected to optimise model
performance: the largest fault offset, as a proportion of reservoir thickness, on which
induced earthquakes can occur (termed the maximum fault throw); the value of the
poroelastic modulus; and the bandwidth of the spatial smoothing kernel. In the most
recent implementation, uncertainty in these values is reflected within a Bayesian infer-
ence framework. This is a thorough but computationally expensive approach that is
not suitable for the exploratory nature of the work presented in this chapter; namely
to investigate a wide range of physical processes which might improve the model
formulation.
CHAPTER 4. MODELS FOR INDUCED EARTHQUAKE LOCATIONS 72
In this research we address the above developments by select the smoothing length
scale  applied to gas extraction covariates as part of our inference procedures. To
emphasise this, we denote (for example) the cumulative ICS at location x  W and
time t  (0, tmax) by s(x, t;). The model extensions that we consider here are already
extensive in number and complexity, and so to limit the computational intensity of
this exploratory work we use the incremental Coulomb stress for fixed values of the
maximum fault throw and poroelastic modulus.
4.3 Alternative models
4.3.1 Approach outline
We have two aims when developing alternative forms for the intensity function :
firstly to determine which mathematical components of the physically motivated in-
tensity model (4.2.2) are most important to model fit, and secondly to investigate
whether there are physical features that could be included to improve the intensity
model. This section is therefore divided into an investigation of sub-models to ad-
dress the former question and model extensions to address the latter. Here, we discuss
model motivations and formulations while the results of fitting these models to the
Groningen earthquake catalogue are presented in Section 4.4.
Table 4.3.1 gives the intensity function corresponding to each model, while Table A.1.1
in Appendix A.1 gives the corresponding integrated intensity function for an individual
spatial pixel. The baseline model introduced in Section 4.2.2 is described by model
B0 in each of these tables.
CHAPTER 4. MODELS FOR INDUCED EARTHQUAKE LOCATIONS 73
4.3.2 Model simplifications
A Poisson process model with intensity of the form (4.2.2) was proposed by Bourne
and Oates (2017a) for earthquake locations in the Groningen gas field. Up to pro-
portionality, this intensity function has three terms: the instantaneous covariate rate,
a linear term in the cumulative covariate and an exponential term in the cumulative
covariate. Since the model was derived based on physical considerations, we aim to
assess the importance of each of these terms to the overall model fit. To do this we
consider four sub-models.
The first sub-model that we consider, S1 in Table 4.3.1, has an intensity proportional
to the smoothed topographic gradient of the upper surface of the gas field, g(x). Since
this covariate does not change in time, the model does not allow for any temporal
variation in earthquake locations and counts. This is a deliberately over-simplified
model that we do not expect to perform well. The purpose of including this model is
to provide a comparison for the second sub-model, S2 in Table 4.3.1. This is again
a very simple model, where earthquake intensity is proportional to the ICS. This is
likely a more meaningful model as it allows the changes in gas extraction over time
to be represented. Models S1 and S2 represent the most parsimonious inclusion of
covariates within the point process intensity.
We also consider the two sub-models, S3 and S4, formed by respective elimination of
the exponential and linear terms from the intensity function. This is motivated by
the the linear term providing a first order approximation to the exponential term for
small covariate values. If this approximation is good then this may allow a more par-
simonious representation of the intensity function. Otherwise, these models provide
a means to assess the relative contributions of these two terms to the overall model
CHAPTER 4. MODELS FOR INDUCED EARTHQUAKE LOCATIONS 74
Evaluation and comparison of these models is somewhat complicated by the small
number of observed events. Additionally, these sub-models are not formally nested
within the baseline model B0. This means that likelihood ratio tests are not ap-
propriate. For these reasons we do not split the earthquake catalogue into training
and evaluation sets. Instead, we use the AIC as a metric to reward goodness-of-fit
while penalising model complexity. Other metrics or information criteria such as the
BIC might also have been considered. In this exploratory work it is preferable to
identify potentially promising model forms to then be put forward for more thorough
examination, and so the less conservative AIC metric is used for model comparison
(Pawitan, 2001). In subsequent discussions comparing model performance, we deem
a better fitting model to be one with a lower AIC value.
4.3.3 Model extensions
In addition to understanding the most important components of the baseline model,
we want to investigate whether the inclusion of a range of additional physical features
into the intensity function might improve model fit.
Spatial variation in model parameters. The first feature we investigate is whether
there is spatial variability in the model coefficients; that is, we wish to investigate
whether the effect of gas extraction on induced seismicity is the same throughout the
gas field. There are two spatial modes of earthquake activity within the gas field,
as shown in Figure 4.3.1. A question of interest is therefore whether the seismicity
in these two regions is better described by a unified model or two distinct mod-
els. As an exploratory approach to this problem we use a simple linear partitioning
of the gas field into lower and upper regions, each containing one mode of earth-
quake activity. We allow distinct coefficients in the parametric intensity function
between regions but maintain a shared smoothing scale across the entire gas field.
CHAPTER 4. MODELS FOR INDUCED EARTHQUAKE LOCATIONS 75
Model (x, t|, )
S1 0g(x;)
S2 0s(x, t;)
S3 0s(x, t;)[1 + 1s(x, t;)]
S4 0s(x, t;) exp{1s(x, t;)}
B0 0s(x, t;)[1 + 1s(x, t;)] exp{1s(x, t;)}
E1 (0IxWL + 1IxWU )s(x, t;)
E2 0s(x, t;)[1 + 1s(x, t;)] exp{1s(x, t;)}IxWL+
2s(x, t;)[1 + 3s(x, t;)] exp{3s(x, t;)}IxWU
E3 0s(x, t;1)[1 + 1s(x, t;2)] exp{1s(x, t;2)}
E4 0[2s(x, t;) + (1 2)s(x, t 1;)][1 + 1s(x, t;)] exp{1s(x, t;)}
E5 0s(x, t;)s(x, t;)
1[ + 1s(x, t;)
] exp{1s(x, t;)}
E6 0 [s(x, t;)(1 + 1s(x, t;)) + 2s(x, t;)s(x, t;)]
exp{1s(x, t;) + 2s(x, t;)}
Table 4.3.1: Intensity functions for sub-models (S1-S4), the baseline model (B0)
and model extensions (E1 - E6). The topographic gradient is denoted by g(x),
while s(x, t;) denotes the cumulative incremental Coulomb stress smoothed using
an isotropic Gaussian kernel with standard deviation . The first and second tem-
poral derivatives of cumulative ICS are given by s(x, t;) and s(x, t;). Regions WL
and WU for models E1 and E2 are defined in Section 4.3.3.
CHAPTER 4. MODELS FOR INDUCED EARTHQUAKE LOCATIONS 76
Easting (m)
 5e10 
 1e09 
 1.5e09 
 2e09 
 2.5e09 
 3e09 
 3.5e09 
  4.5e09 
230000 240000 250000 260000 270000
Figure 4.3.1: Field outline with superposed Gaussian kernel density estimate of spatial
earthquake distribution. The red line shows the line y = 735000 0.6x, which is used
to separate lower and upper modes of earthquake activity.
Formally, we partition the gas field into lower and upper regions, W = WL  WU ,
where WU = {(x, y) = w  W : y > ax + b} and WL = W \ WU for a = 0.6
and b = 735000. Alternative partitioning methods are of course possible, but we
limit ourselves here to testing sensitivity to the choices of a and b which separate the
spatial modes of earthquake activity. This restriction to the boundary forms can be
motivated by the principle of parsimony; more complex boundary forms are no less
arbitrary than a linear division unless they have a sound physical motivation, for ex-
ample using sealing faults across which gas pressure gradients can not equalise. Since
this type of fault information is not available, we proceed with the simplest form of
boundary. We investigate the advantages of this approach for the baseline model and
also for the intensity model that is proportional to ICS, which are respectively named
models E1 and E2 in Table 4.3.1.
Rate smoothing. The second type of model extension we investigate is related to
the smoothing of the rate and cumulative ICS covariates. It has been shown in the
context of kernel density estimation that the optimal kernel bandwidths for estimation
of a function and its derivative are not the same (Ramsay, 2006). This suggests that
CHAPTER 4. MODELS FOR INDUCED EARTHQUAKE LOCATIONS 77
230000 240000 250000 260000 270000
2005: Observed = 10, Expected = 7.61
Easting
230000 240000 250000 260000 270000
2006: Observed = 15, Expected = 8.16
Easting
230000 240000 250000 260000 270000
2007: Observed = 8, Expected = 7.37
Easting
Figure 4.3.2: Maps of expected event counts under the fitted baseline model for years
2005-2007. Observed event locations are overlaid.
for an intensity function that includes both a covariate and its cumulative value, using
a single smoothing level might be sub-optimal in both cases. We therefore investigate
whether selecting separate smoothing parameters for ICS and cumulative ICS can
provide a better representation of these covariates and improve model performance.
This is represented by model E3 in Table 4.3.1.
Lag and displacement effects. The next proposed model extensions arise from
observed discrepancies between the fitted baseline model and the observed earthquake
locations. The number and location of events observed in each year are shown against
the fitted baseline model in Figures A.2.1 and A.2.2 of Appendix A.2. The observed
events appear to be displaced relative to peaks in the fitted intensity. This is high-
lighted in Figure 4.3.2 which focuses on the years 2005-2007. Spatially, events appear
to surround peaks in intensity rather than occurring at the apex. The years 2005 and
2006 demonstrate this, particularly around the northern mode of earthquake activity.
There also appears to be several observed events in regions with low expected counts
that were higher in the previous time period. This potential lagged effect can be
observed for events in the south-west of the field in 2007.
The above observations may correspond to two physical phenomena that may not have
CHAPTER 4. MODELS FOR INDUCED EARTHQUAKE LOCATIONS 78
been properly represented in the model. Firstly, there may be some delay between gas
extraction causing an increase in ICS and this presenting as induced seismicity. This
has important implications, particularly when trying to establish whether changes in
gas extraction methods are effective in influencing the number or location of induced
earthquakes. If there is a time lag then it will take longer for these changes to become
apparent. Secondly, earthquakes may be triggered not at peak values of ICS but
where the spatial gradient of this covariate is large. This might suggest that induced
earthquake activity could be decreased by extracting gas more equally across wells to
reduce spatial stress gradients.
To investigate the evidence for a lagged effect, we replace the current incremental
Coulomb stress in the intensity function by a weighted combination of the current
value and the value one year previously. The relative weight given to the current and
previous covariate values is determined as a part of model fitting. This is model E4
in Table 4.3.1.
To investigate the clustering of events around (but not directly upon) the peaks of
the fitted intensity, we fit a model in which incremental Coulomb stress values are
raised to a fractional power. This has the effect of flattening peaks in the intensity
function so that more intensity is allocated to areas surrounding the peak. Since it
is not clear whether this should be applied to the ICS, cumulative ICS or both, we
use a model form which allows for each of these possibilities. Model E6 of Table 4.3.1
considers a fractional power transformation applied to both the instantaneous and
cumulative covariates, where the same or different exponents may be applied to each
covariate.
Effect of extraction rate. Under the baseline model the rate at which gas is ex-
tracted does not influence the total number of earthquakes triggered; this is influenced
CHAPTER 4. MODELS FOR INDUCED EARTHQUAKE LOCATIONS 79
only by the total amount of gas extracted. This is known as a film-rate effect : it is the
total amount of gas extracted and not the time frame over which extraction occurs
that determines the total amount of induced seismic activity. In contrast, we con-
sider a non-film-rate effect model, where the total number of induced events depends
on the rate at which gas is extracted. If there is evidence for such a non-film-rate
model, this clearly has important repercussions when deciding between different gas
extraction plans; it determines whether scenarios with greater extraction rates should
be viewed negatively. To incorporate this into the baseline model we include both
the cumulative and instantaneous ICS within the exponential term of the integrated
intensity function. This model is described as E9 in Table 4.3.1.
Summary. By fitting the proposed sub-models we establish which terms within
the baseline intensity function are most important to model fit. This may allow a
more parsimonious representation of the baseline model and will certainly increase
understanding of this model.
The proposed model extensions aim to identify or exclude future areas of model devel-
opment. This is done through the use of simple statistical tests to establish the merit
of including additional physical features within the baseline intensity model. The
physical features that we investigate include spatial variation in model parameters,
a time-lag between gas extraction and induced seismicity, displacement of induced
earthquakes from the area of greatest stress and the extraction rate impacting the
total number of induced earthquakes.
CHAPTER 4. MODELS FOR INDUCED EARTHQUAKE LOCATIONS 80
4.4 Results
4.4.1 Outline
For each model simplification or extension the resulting AIC value relative to the
baseline model B0 is given in Table 4.4.1 along with the maximum likelihood esti-
mate and standard error of the associated smoothing parameter, . Interpretation
and discussion of the results are organised as follows: Section 4.4.2 addresses the
choice of smoothing parameter, ; Section 4.4.3 addresses model simplifications; and
Section 4.4.4 addresses model extensions.
4.4.2 Covariate smoothing
Our first finding based on Table 4.4.1 is that the estimated smoothing parameter
does not differ significantly across models. It should be noted that for all models the
standard error of the estimated smoothing scale  is large and the point estimate is
close to the lower boundary of the parameter space, where  = 0. Under none of
the models does the smoothing of the covariate produce a significant improvement in
model fit when tested at the 5% level. The standard errors of the estimated smoothing
parameter are included in the table to illustrate this point, but should be interpreted
with caution since the sample size on which they are based is small and the point
estimate to which they pertain is close to the boundary of the parameter space. This
finding suggests that it is not necessary to separately optimise the smoothing scale for
each model and that, for the range of models considered, optimising the smoothing
scale does not significantly improve model fit.
CHAPTER 4. MODELS FOR INDUCED EARTHQUAKE LOCATIONS 81
Model AIC  [std error]  [std error]
S1 160.30 0.33 [2.3102] 49 [750]
S2 119.87 9.6102 [6.6103] 252 [213]
S3 76.98 (7.0104, 21.8) [2.0106, 1.5] 228 [157]
S4 -0.82 (4.0103, 0.45) [7.3104, 2.0102] 407 [250]
B0 0.00 (2.4103, 0.36) [2.0104, 1.2102] 484 [273]
E1 94.15 (5.3102, 0.12) [8103, 9.5103] 251 [216]
E2 -27.83 (2.8103, 2.7101, 2.6103, 3.8101) 511 [401]
  [6.5104, 2.8102, 2.7104, 1.4102] 
E3 9.93 (14) (3.1103, 0.33) (487, 484)
  [3.7104, 1.4102] [674, 223]
E4 5.86 (12) (3.3103, 0.33, 0.59) 489 [210]
  [4.3104, 1.5102, NA] 
E5 3.88 (2.4103, 0.36, 0.95) 494 [271]
  [2.0104, 1.1102, 1.3] 
E6 1.58 (2.5103, 0.36, 1.1105) 126 [320]
  [1.3104,7.4103, 2.0106] 
Table 4.4.1: Summaries of fitted sub-models (S1-S4), baseline model (B0) and model
extensions (E1 - E6). The second and third columns give the numerically maximised
log-likelihood value and the change in AIC relative to the baseline model. The fourth
and fifth columns give point estimates for the model parameters and covariate smooth-
ing scales; approximate standard errors are given in square brackets.
CHAPTER 4. MODELS FOR INDUCED EARTHQUAKE LOCATIONS 82
4.4.3 Model reductions
When considering model reductions, the sub-models S1-S3 each resulted in a worse
model fit relative to the baseline model. This is to be expected, particularly for
models S1 and S2, which were deliberate oversimplifications. Within the sequence of
sub-models S1-S3 we have increasing goodness of fit. The improvement of S2 over S1
demonstrates what might reasonably be expected: that including information on gas
extraction over time leads to a better model of induced seismicity than using only the
topographic gradient, which is a reservoir property that does not change over time.
The improvement when moving from S2 to S3 then demonstrates that the number
and location of induced earthquakes is not influenced only by gas extraction (through
the ICS) but also depends on the cumulative amount of gas that has been extracted.
Finally, the lower AIC value of model S4 over S3 shows that this dependence on the
cumulative ICS is better described by an exponentially increasing trend than by a
linear approximation to this trend.
It should be noted that model S4, which removes the linear term from the baseline
intensity function, provides a slight improvement in model fit over the baseline model
B0. Model S4 improves parsimony in terms of the expression for the intensity function,
but leads to more complicated interpretation of the model parameters. This is because
under S4 both 0 and 1 scale the integrated intensity function, as can be seen in
Table A.1.1 of Appendix A.1. Since the improvement in model fit by S4 is modest, we
choose to develop our model extensions on the baseline model B0 and prioritise the
desire for ease of interpretation and physical derivation over the desire to use strictly
the most parsimonious model.
CHAPTER 4. MODELS FOR INDUCED EARTHQUAKE LOCATIONS 83
4.4.4 Model extensions
The majority of the proposed model extensions did not provide a significant improve-
ment to model fit. In particular, it should be noted that the models E3 and E4 had
maximised log-likelihood values that were less than that of the baseline model B0,
which they extend; this is counter-intuitive but is explained as follows. These models
respectively extend the baseline model by including separate smoothing parameters or
a lag between gas extraction and induced seismicity. The decrease in the maximum
log-likelihood value for these models can be attributed to the integrated intensity
function for these models not having a closed form. This necessitates the use of nu-
merical integration to calculate the integrated intensity term of their log-likelihoods.
From the available information, the cumulative ICS value is known for each spatial
pixel on the first of January each year. Since the evolution of ICS within each year
is not known, the mean of the initial and final values is used to construct an approx-
imation of the integrated intensity. Bounds on the likelihood value can be obtained
by using the initial or final value of ICS within each year, these bounds correspond
to a step change in cumulative ICS at the end or beginning of each year. The AIC
values for models E3 and E4 are less than or greater than that of the baseline model
B0, depending on which approximation to the integral is used. There is therefore
not enough information available using these covariate grids to definitively establish
whether including either separate smoothing parameters or a lag between gas extrac-
tion improves the model of induced seismicity. However, when using the numerical
integration scheme under which the AIC is reduced, the size of the reduction is mod-
est. This suggests that these features do not influence strongly the induced seismic
activity.
Model E5 considered raising either the cumulative or instantaneous ICS value to a
fractional power, in order to address the observation that earthquakes cluster around
CHAPTER 4. MODELS FOR INDUCED EARTHQUAKE LOCATIONS 84
peaks in the fitted baseline intensity, rather than occurring at the peaks. This model
did not reduce the AIC relative to the baseline model. This is perhaps because such
flattening of peaks in the intensity function is also achievable through increasing the
length scale of the smoothing kernel. This could be further explored by investigating
models which include the spatial gradient of the covariates, d
s(x, t;) or d
s(x, t;),
in some way.
Model E6 investigated whether there was significant evidence that the ICS rate in-
fluenced the total number of induced earthquakes. From the available data, there
was insufficient evidence to suggest a non-film-rate effect. It should be noted that
there were some difficulties when fitting of this model, due to the presence of second
derivative of cumulative ICS within the intensity function. This covariate is difficult
to obtain accurately due to the coarse temporal resolution of the cumulative ICS data,
which might be a limiting factor in assessing this modelling approach. Additionally,
the second derivative of ICS is negative in many locations and periods (roughly cor-
responding to areas or periods where where the gas depletion slows down). This
presented problems when ensuring non-negativity of the fitted intensity everywhere
on A. In principle, this can be achieved through constraints on the coefficient val-
ues , but this caused the numerical optimisation routines used to fit the model to
fail. To work around this issue, we approximated the model by taking the pointwise
maximum of zero and the intensity term that depends on s(x, t;) (the term given in
square brackets) at each time and location to ensure non-negativity. Then numerical
integration was used to calculate the integrated intensity term of the log-likelihood.
Alternative methods of ensuring non-negativity were also investigated, where s(x, t;)
was replaced by |s(x, t;)| or max{s(x, t;), 0}; neither approach led to a significant
reduction in AIC.
The models E1 and E2 allow the model parameters to vary spatially across the gas
CHAPTER 4. MODELS FOR INDUCED EARTHQUAKE LOCATIONS 85
field while maintaining the same model forms as S1 and B0. Both model extensions
led to reduced AIC values relative to their equivalent spatially constant models. In
particular, the extension of the baseline model to have separate parameters in the
north-east and south-west of the gas field, E2, was the only model extension to pro-
vide a marked improvement in model fit over the baseline model. This suggests that
the two spatial modes of earthquake activity within the gas field are responding differ-
ently to gas extraction. The sensitivity of this finding was tested by considering four
alternative boundary lines that separated the to modes of intensity. Maps of annual
expected earthquake counts under this model are given in Figures A.2.3 and A.2.4 of
Appendix A.2.
The improvement of this simple spatial model over the baseline suggests that a more
in-depth investigation might be worthwhile, for example, variations of the baseline
model with parameters that vary smoothly over space. This will require a careful
balance between achieving flexibility in the model form and the ability to fit the model
using the limited available data. Our linear partition clearly leans toward the latter
consideration but serves as motivation for a model using either a physically motivated
reservoir partitioning or smoothly varying parameters as further work.
4.5 Conclusions and further work
The first aim of this work was to establish which sub-components of the baseline inten-
sity function were the most important drivers of model fit. Through the investigation
of four sub-models, we found that the terms concerning the ICS and exponential trend
in cumulative ICS are the main contributors to model fit. The linear term in cumula-
tive ICS is dominated by the exponential term to the extent that a small improvement
in model fit may be achieved by removing the linear term from the intensity function.
However, removing this term complicates the interpretation of model parameters since
CHAPTER 4. MODELS FOR INDUCED EARTHQUAKE LOCATIONS 86
this leads to both 0 and 1 scaling the integrated intensity function.
The second aim of this work was to investigate a range of extensions to the baseline
intensity model. The extensions investigated were selected based on physical phenom-
ena that might be missing from the model, or mathematical transformations motivated
by potential shortcomings of the baseline model. From the data used, there is insuf-
ficient evidence to conclude that there is a lagged effect of ICS on induced seismicity
or that this can be better described by using some fractional power of ICS. There is
also insufficient evidence to conclude that model fit can be significantly improved by
using separate smoothing levels, either within or across the models considered.
It does, however, appear that the relationship between induced seismicity and ICS
is not constant across the gas field. This finding was tested using a simple linear
partitioning of the gas field to separate modes of earthquake activity. The finding
was not highly sensitive to the choice of boundary line, however, the use of a linear
form for the boundary lacks a physical basis and was chosen for reasons of parsimony.
A potentially fruitful area of further work might therefore be to investigate models
that rely on a physically motivated partitioning of the gas field (for example using
sealing faults across which pressure gradients can not easily equalise), or indeed a
model where parameters vary smoothly over space. The challenge in each of these
cases will be striking a balance between model complexity and the small number of
available data.
We identified that induced events appear to cluster around modes of fitted intensity
in the baseline model, but that model fit can not be significantly improved by using
a power transformation of the ICS. In future work, this phenomenon could be further
investigated by including the spatial gradient of ICS within the intensity or integrated
intensity function - although how best to include this covariate is not obvious. Another
route to extending the work here would be to use covariates on a finer temporal grid,
CHAPTER 4. MODELS FOR INDUCED EARTHQUAKE LOCATIONS 87
for example using weekly or monthly values. This would have the benefit of allowing
variations in gas extraction within each year to be represented within the models and
might allow first and second temporal derivatives of ICS to be better represented.
However, using monthly values comes with an increased computational burden and
leads to extensive issues related to ensuring non-negativity of the fitted intensity. For
example, seasonal or reduced gas extraction can lead to areas of pressure-up as gas
pressure equalises across the field resulting in widespread areas where the ICS rate is
negative.
One model extension which is not considered in this work is the inclusion of aftershock
activity, such as within an epidemic type aftershock sequence model (Ogata, 1988,
2011). While this was considered by Bourne et al. (2018), we were unable to replicate
such a model extension without imposing strong constraints on aftershock parameter
values. There are a range of issues associated with fitting this type of aftershock
model, such as the likelihood function having locally flat regions, being costly to
evaluate and difficulty in separating parameter effects (Veen and Schoenberg, 2008;
Schoenberg, 2013; Ross, 2016). In this work we have restricted focus to models for
independent events but in Chapter 6 of this thesis we go some way to addressing these
issues.
Chapter 5
Inference for extreme earthquake
magnitudes accounting for a
time-varying censoring process
5.1 Introduction
5.1.1 Aims and motivation
The observational nature of environmental data can lead to challenges during statisti-
cal modelling and inference. In particular, improved measurement of an environmental
process within a dataset should be acknowledged as part of any inference. Failing to
do so leads to biased inference, while including data based only on the initial qual-
ity of measurements is overly conservative, leads to inefficient inference, and makes
financial investment into the measurement process redundant. We consider how to
include changing data quality in an extreme value analysis where low data quality is
present as the partial censoring of rounded data. Here and throughout, censored data
CHAPTER 5. INFERENCE FOR EXTREME EARTHQUAKES 89
are values that are missing-not-at-random (Little and Rubin, 2019). This chapter is
motivated by the modelling of earthquake catalogues, but results in a method that
is applicable more widely where these data features are present. This new threshold
selection method should also be of value in more general extreme value analyses.
5.1.2 Earthquake data
Earthquakes are recorded if their locations and magnitudes can be inferred from
ground vibrations at sensor locations; this requires an earthquake to be detected by
multiple sensors. An earthquake is detected or missed depending on its magnitude and
location relative to the sensor network. A low sensitivity network of sensors therefore
leads to the partial or complete censoring of small magnitude seismic events. As the
network is extended or upgraded over time the censoring of small events is reduced. It
is usual in earthquake catalogues for magnitudes to be reported to one decimal place;
this data feature is often overlooked during statistical analyses (Marzocchi et al.,
2019). Using these rounded, incomplete observations we seek to understand the tail
behaviour of the magnitude distribution.
Since 1991 the Groningen region of the Netherlands has experienced induced earth-
quakes. These seismic events are caused by gas extraction and have relatively small
magnitudes compared to tectonic events. However, they also occur at much shallower
depths than their tectonic equivalents. This means that for equal magnitudes they
pose a greater hazard than their tectonic counterparts because their impact is spread
over a smaller spatial extent. These small earthquakes are therefore both hazardous
and difficult to detect. This has led to continued investment in the geophone network
around the Groningen gas field to increase detection of small earthquakes and to better
understand earthquake activity in the region. Estimating high quantiles of the mag-
nitude distribution, and quantifying their uncertainty, is instrumental to appropriate
CHAPTER 5. INFERENCE FOR EXTREME EARTHQUAKES 90
design and improvement of buildings to withstand these earthquakes.
5.1.3 Magnitude of completion
The magnitude of completion mc is the lowest magnitude above which all earthquakes
are certain to be recorded in a given area and time interval. The magnitude of
completion therefore depends on the density and sensitivity of the sensor network as
well as the local geology. When a sensor network changes substantially over time t,
the magnitude of completion in that region can be considered as a function of time,
denoted mc(t). The magnitude of completion is not a quantity that can be determined
experimentally, it must be inferred from the set of observed event magnitudes.
Existing methods for statistical estimation of a constant mc use parametric or non-
parametric methods to detect deviations from the assumed monotonicity of the magni-
tude distribution (Mignan and Woessner, 2012). Parametric methods typically assume
an exponential magnitude distribution, based on the empirical magnitude-frequency
relationship of Gutenberg and Richter (1956). Heuristic techniques are used to detect
deviations from this model based on maximum curvature, goodness-of-fit, or param-
eter stability.
Several methods exist to estimate a spatially varying magnitude of completion (Wiemer
and Wyss, 2000; Mignan et al., 2011). In contrast, little attention has been given to
estimating a changing magnitude of completion over time. Where it has been con-
sidered, focus has been on temporary increases in mc(t) due to residual vibrations
following large earthquakes (Woessner and Wiemer, 2005; Utsu et al., 1995). Long-
term changes in mc(t) have been considered by assuming a constant value within a
pre-determined temporal partitioning (Hutton et al., 2010) or a locally constant value
estimated using a rolling window (Mignan and Woessner, 2012).
CHAPTER 5. INFERENCE FOR EXTREME EARTHQUAKES 91
5.1.4 Extreme value methods
To specify a model for earthquake magnitudes we adapt a model from extreme value
theory. An asymptotic argument justifies the use of the generalised Pareto distribu-
tion (GPD) to model the excesses of a continuous random variable over a suitably
chosen threshold, under weak assumptions on the distribution of that random variable
(Pickands, 1975). The distribution function of a random variable Y that follows a
GPD, given that it is above the threshold u, is
F (y;, ) =
1 [1 + (y  u)/]1/+ for  6= 0, y  u,
1 exp[(y  u)/] for  = 0, y  u;
(5.1.1)
where the shape parameter   R, scale parameter  > 0 and y+ = max(0, y). The
distribution is exponential when  = 0, heavy-tailed when  > 0 and decays to a
finite upper end point y+ = u  / when  < 0 (Davison and Smith, 1990). The
GPD generalises the Gutenberg-Richter model, in which magnitudes are independent
and identically distributed (i.i.d.) exponential random variables, by allowing greater
flexibility in the tail behaviour of the distribution.
Standard extreme value modelling deals with i.i.d. data, observed at regular inter-
vals without rounding or censoring. The standard approach is to select a constant
threshold u that is a fixed, high quantile of the empirical distribution. Heuristic meth-
ods are used to select an appropriate quantile, see Scarrott and MacDonald (2012)
for a review. These methods can be based on the stability of parameter estimates,
goodness-of-fit measures, or the mean threshold exceedance size (Coles, 2001). When
interest lies in estimating a particular extreme value property, such as the shape pa-
rameter, an alternative strategy is to select the threshold that optimises inference for
that property (Danielsson et al., 2001).
Using a constant threshold is inefficient when the data distribution changes over time.
This type of change is likely to alter the quantile value above which a GPD is ap-
CHAPTER 5. INFERENCE FOR EXTREME EARTHQUAKES 92
propriate and cause the GPD parameters to change over time. To avoid this issue,
quantiles can be estimated locally as a function of time and a global decision can be
made on which quantile to use as a time-varying threshold u(t) (Eastoe and Tawn,
2009; Northrop and Jonathan, 2011).
5.1.5 Shortcomings of current methods
Estimating the magnitude of completion and selecting an extreme value threshold
are closely linked problems. Both aim to select a value (possibly time-varying) above
which a probability model is appropriate. Standard methods from either setting do
not meet our modelling needs, for the reasons that follow.
Methods assuming an exponential magnitude distribution are problematic for two
reasons. Firstly, an exponential tail model can lead to bias and false confidence in
quantile estimates. Coles and Pericchi (2003) demonstrated in a hydrological context
the benefits of using the encompassing generalised Pareto model to properly represent
uncertainty in the tail shape. Secondly, the exponential distribution does not account
for rounding of the data, resulting in biased parameter estimates (Marzocchi et al.,
2019; Rohrbeck et al., 2018). Failing to acknowledge this rounding can therefore also
cause bias in threshold selection.
Methods to select a static threshold are also unsuitable for our problem. To obtain
precise estimates of the GPD parameters and high quantiles, as much data as possible
should be used in the analysis. However, this must be balanced by the need to
represent model uncertainty and avoid bias from incorrectly including small magnitude
events. This bias has two sources: using either data values for which the extreme value
model does not apply or values that are below the magnitude of completion at the
time of their occurrence. The optimal choice of time-varying threshold is therefore
v(t) = max{mc(t), u(t)}. Methods for selecting a static modelling threshold v are
CHAPTER 5. INFERENCE FOR EXTREME EARTHQUAKES 93
inefficient when the true threshold varies with time; the static threshold must satisfy
v  maxt v(t) and so excludes viable data from the analysis.
Finally, current approaches to selecting or estimating time-varying thresholds are also
unsuitable for our problem; methods for estimating mc(t) consider only a small portion
of the data at once, while the selection of u(t) by a local quantile approach is impeded
by the temporal development of the censoring process.
5.1.6 Contributions and outline
In this chapter we develop an automated method to select a dynamic threshold for
rounded GPD data. This is, to our knowledge, the first time that data rounding has
been considered during threshold selection. Our proposed threshold selection method
uses as much data as possible while guarding against the use of values where a tail
model is not appropriate or observations are not complete. This threshold choice
leads to more precise estimation of high magnitude quantiles, properly represents
their uncertainty, and can also suggest how the magnitude of completion changes
over time. The selection method is developed for earthquake data, but the core idea
of the method can be applied to extreme value threshold selection more generally.
We demonstrate, via simulation, the benefits of including additional, small magni-
tude events in an extreme value analysis to both parameter recovery and return level
estimation. We go on to select dynamic thresholds for partially censored earthquake
catalogues and investigate the impact of this threshold when estimating high quantiles
of the magnitude distribution.
This chapter is structured as follows. Section 5.2 describes the Groningen earthquake
catalogue that motivates the proposed methodology, the model for observed magni-
tudes, and the novel inference for the underlying parameters. Section 5.3 demonstrates
the benefits of including small magnitude events into an extreme value analysis. Sec-
CHAPTER 5. INFERENCE FOR EXTREME EARTHQUAKES 94
tion 5.4 introduces our proposed method of threshold selection. The method is applied
to simulated earthquake catalogues in Section 5.5 and to the Groningen catalogue in
Section 5.6. Concluding remarks are given in Section 5.7.
5.2 Motivating data and model formulation
5.2.1 Data description
We study the induced earthquakes in the Groningen region of the Netherlands from
January 1st 1995 to December 31st 2019. Compared to tectonic earthquakes, these are
close to the surface and can cause damage despite their relatively small magnitudes.
This has led the Royal Dutch Meteorological Institute (KNMI) to invest heavily in
the earthquake detection infrastructure in the Groningen region. Over time, more and
better sensors have been added in the region to increase the detection and reporting
of small earthquakes. The resulting earthquake catalogue is publicly available and
magnitudes are reported in units of local magnitude (ML) to one decimal place (KNMI,
2020).
Figure 5.2.1 shows Groningen earthquake magnitudes against both occurrence time
and earthquake index, along with smoothed estimates of their mean using a gen-
eralised additive model with cubic-spline basis. Assuming that magnitudes are i.i.d.
(which is supported by the exploratory analysis in Appendix B.1 for Groningen earth-
quakes exceeding 1.5ML) and that departures from this are due to the partial cen-
soring of small magnitude events, the reduction in mean magnitude indicates that
fewer small magnitude events were censored at later times. It is unclear whether this
change in detection was sudden or gradual. The KNMI report that mc(t)  1.5ML
for the entire period (Dost et al., 2012). Paleja and Bierman (2016) and Dost et al.
(2017) used a fixed temporal partitioning and conclude that for the period 2014-09-
CHAPTER 5. INFERENCE FOR EXTREME EARTHQUAKES 95
24 to 2016-09-27 the magnitude of completion was likely to be below 1.0ML. Since
sensors have not been removed from the network, this suggests that the magnitude of
completion should be less than or equal to this in the period following their analysis
(i.e. to 2020 in Figure 5.2.1).
Figure 5.2.1: Full Groningen earthquake catalogue, with magnitudes reported in ML
and smoothed mean estimate; shown using natural- [left] and index-times [right].
5.2.2 Data model and inference
This section introduces our notation and data model for threshold selection and in-
ference on extreme earthquake magnitudes. We define an earthquake catalogue to
be the set of n recorded time-magnitude pairs {(ti, xi) : i = 1, . . . , n} where the
recorded magnitudes x = (x1, . . . , xn) are given rounded to the nearest 2 ( > 0) and
the event times t = (t1, . . . , tn) are each within the observation interval (tmin, tmax).
The unrounded magnitudes associated with each event are represented by the vector
y = (y1, . . . , yn). An event (ti, xi) therefore corresponds to an earthquake of magni-
tude yi  (xi  , xi + ] that occurred at time ti and that was not censored.
Recall from Figure 5.2.1 that earthquake intensity is not constant over the observation
period. To separate exposition of our threshold selection method from estimation of
this temporally-varying earthquake rate, we map each event time to its correspond-
ing index. This transforms event times t from an irregular sequence on the natural
CHAPTER 5. INFERENCE FOR EXTREME EARTHQUAKES 96
timescale t to a regular sequence  on the index scale  , where observed events occur
at  = 1, . . . , n. A modelling threshold v() is then specified for the transformed
observation period   (0, max) and the threshold values at each event time are
given by the vector v = (v(1), . . . , v(n)) = (v1, . . . , vn). The threshold function v()
and threshold vector v will be initially treated as known until threshold selection is
discussed in Section 5.4.
The probability (, y) that an event is detected by the sensor network and included
in the earthquake catalogue is an unknown function of its time and magnitude. It
is expected that for the Groningen catalogue (, y) is a non-decreasing function in
each of  and y; larger or later earthquakes are more likely to be detected. We make
two assumptions on (, y): firstly that observation is complete above the modelling
threshold, so that (, y) = 1 for y  v(); secondly that censoring begins gradually
so that for all  , (, y)  1 when y  [v()  , v()]. This allows rounded mag-
nitudes within  of the modelling threshold to be included during inference without
constructing a full model for the censoring process.
In constructing our model for magnitudes exceeding v(), we assume that the un-
rounded magnitudes y may be modelled as i.i.d. GPD random variables (Y1, . . . , Yn)
with parameters  = (u, ) when they exceed a constant, lower threshold of u <
min v() . Formally, we assume Yiu|Yi > u  GPD(u, ). Since events exceed-
ing v() are never censored, excess magnitudes of v() may also be modelled using
a GPD but with threshold dependent scale parameters vi = u + (vi  u), so that
Yi  vi|Yi > vi  GPD(vi , ).
When using this probability model to construct a likelihood function for the GPD
parameters, rounded magnitudes xi should contribute only if the latent value yi > vi.
Events with xi > vi + , should certainly contribute to the likelihood function and
events with xi < vi   certainly should not. When |xi  vi| <  it is uncertain
CHAPTER 5. INFERENCE FOR EXTREME EARTHQUAKES 97
whether yi > vi and whether event i should contribute to the likelihood. Each event
is therefore weighted in the log-likelihood by wi = Pr(Yi > vi|xi,), the probability
that it truly exceeds v(). This is equivalent to using the expected likelihood over all
possible unrounded magnitude vectors. The resulting log-likelihood function for the
the parameters  = (u, ) of F , the GPD (5.1.1) is:
`(|x,v) =
wi log Pr(Xi = xi|Yi > vi,)
wi log Pr(max(vi, xi  ) < Yi < xi + |) (5.2.1)
wi log [F (xi +   vi;vi , ) F (max(vi, xi  ) vi;vi , )] ,
where
Pr(max(vi, xi  ) < Yi < xi + |)
Pr(xi   < Yi < xi + |)
F (xi +   u;u, ) F (max(vi, xi  ) u;u, )
F (xi +   u;u, ) F (xi    u;u, )
. (5.2.2)
The maximum likelihood estimate  can be found using numerical optimisation of
this function. Confidence intervals may be obtained based on asymptotic normality,
but this approximation can be poor for the estimated shape parameter  and quantile
values. To avoid this and to ensure that confidence bounds on u are positive, we use
a parametric bootstrap approach to describe parameter uncertainty, as described in
Appendix B.2.
5.3 Motivating the inclusion of small magnitudes
5.3.1 Simulation study overview
Here we show that using a non-constant threshold to include additional, small mag-
nitude earthquakes in an extreme value analysis can be beneficial to both parameter
CHAPTER 5. INFERENCE FOR EXTREME EARTHQUAKES 98
and quantile estimation. We compare three approaches to inference on 1000 simulated
earthquake catalogues that have a known, stepped threshold. Each catalogue is sim-
ulated by first generating 1000 latent magnitudes as independent GPD exceedances
of u = 1.05ML with parameters  = (u, ) = (0.4, 0.1). Each event i = 1, . . . , 1000
is censored if i  500 and yi < 1.65ML. The retained magnitudes are then rounded
to the nearest 2 = 0.1ML, resulting in a catalogue of the form shown in Figure 5.3.1
(left). The size of the retained catalogue depends on the simulated magnitudes, and
so varies between catalogues.
A GPD model is fitted to each of the simulated catalogues by maximising the log-
likelihood (5.2.1) under each of three approaches. The first, conservative approach
to inference uses only exceedances of the flat modelling threshold v() = 1.65ML for
0    1000. The second approach uses exceedances of the stepped threshold where
v() = 1.65ML for 0    500 and v() = 1.05ML for 500 <   1000. The number
of data points used by the stepped approach will be at least as large as by the con-
servative approach. A third approach, possible in simulation but not practice, is also
considered. In this third approach, additional earthquakes are simulated above the
conservative level to extend the simulated catalogue until the number of exceedances
of 1.65ML matches the number of events used by the stepped approach. A GPD
model is then fitted to the extended set of earthquakes that exceed 1.65ML.
We compare the three approaches to inference in terms of parameter and quantile
estimation. The conclusion of each comparison can differ because of the non-linear
relationship between GPD parameters and quantiles, which are also sensitive to small
changes in the estimated shape parameter . Parameter estimates are compared using
their bias and variance over the 1000 simulated catalogues. To be able to compare
quantile estimates across modelling thresholds we consider the conditional quantiles
above the conservative threshold level, using conditional return levels. The conditional
CHAPTER 5. INFERENCE FOR EXTREME EARTHQUAKES 99
p-quantile above some magnitude c > u is the magnitude yp,c that satisfies
Pr(Y  yp,c|Y > c) = p.
Letting c = Pr(Y > c|Y > u) = 1  F (c;), where F is the distribution function
(5.1.1), yp,c can be expressed as a function of :
yp,c() =
u+ u
(cp)
  1
for  6= 0,
u+ log(cp) for  = 0.
(5.3.1)
An alternative representation of conditional quantiles, more in-keeping with the ex-
treme value approach, is the m-event conditional return level above c. This can be
found by setting p = 1  1/m in equation (5.3.1) and interpreted as the magnitude
exceeded (on average) by one in every m events that exceed c. We compare point esti-
mates and confidence intervals of conditional return levels under the three approaches
to inference.
5.3.2 Simulation study results
Figure B.4.1 in Appendix B.4 shows the sampling distribution of parameter estimates
and an error decomposition under each approach to inference. The stepped threshold
is best for parameter estimation, with the smallest bias and variance of the three
approaches. The mean squared error of the stepped estimator is 9.6 times smaller
than that of the conservative estimator, mainly due to its increased precision. For
comparison, artificially extending the earthquake catalogue gives a reduction factor of
only 4.2. In this example, each small magnitude event added by lowering the threshold
is more than twice as valuable to parameter estimation than an additional observation
above the conservative level.
Figure 5.3.1 (right) shows the conditional return levels for magnitudes above c =
1.65ML under each approach. Point estimates are qualitatively similar in each case,
CHAPTER 5. INFERENCE FOR EXTREME EARTHQUAKES 100


0 200 400 600 800 1000
event index
1 5 10 50 100 500 1000
return period
Conservative
Extended
Stepped
Figure 5.3.1: [Left] Simulated catalogue structure: events are censored (grey dots)
if in the first 500 and below 1.65ML. For this catalogue, the conservative threshold
(dashed red line) includes 181 events, while the stepped threshold (solid black line) in-
cludes 582 events. [Right] Magnitude conditional return level estimates in ML against
return period in number of earthquakes exceeding 1.65ML. Point estimates and 95%
confidence intervals are given under conservative, extended and stepped approaches
to inference, along with the true values.
but confidence intervals are narrowed by using the stepped rather than constant
threshold. Confidence intervals are further narrowed by artificially extending the
observation period. This is because of the additional large values in the extended
data, which have a strong influence over the estimated return levels (Davison and
Smith, 1990).
These results show clearly the benefits for parameter and quantile inference that can
be achieved by using a dynamic modelling threshold to include additional small mag-
nitude events in an extreme value analysis. Using a conservative constant threshold
leads to wasteful inference and the squandering of these potential gains.
CHAPTER 5. INFERENCE FOR EXTREME EARTHQUAKES 101
5.4 Threshold selection
5.4.1 Overview
In practice, the true modelling threshold v() is always unknown. To choose between
potential thresholds, we must define what it means for one threshold to be preferred
over another. A generalised likelihood ratio test is not appropriate for this comparison
because it compares nested models on the same data, rather than comparing the same
model on nested data (Wadsworth and Tawn, 2012; Wadsworth, 2016).
To select a model that is robust to sampling variability, v() should include as much
data as possible in the model and therefore be chosen to be as low as possible. How-
ever, selecting v() < max(u(),mc()) for any 0 <  < max will cause bias in the
fitted model, making it incapable of obtaining an asymptotically consistent estima-
tor of the true parameter values. The best choice of v() is therefore the threshold
that includes the most data while maintaining a good agreement between observed
threshold exceedances and the fitted GPD.
For i.i.d. continuous valued data, the distributional agreement with a probability
model can be assessed graphically by using a PP- or QQ-plot and adding tolerance
intervals to show expected behaviour under that model. Alternatively, the distribu-
tional fit can be summarised using a metric, such as the Anderson-Darling or Cramer-
von Mises distances (Laio, 2004). Both graphical- and metric-based approaches can
be adapted for data y that are independent and continuous valued, but which do
not have a shared distribution. This is achieved by using the probability integral
transform and the fitted distribution to transform the data to have a shared marginal
distribution before using methods for i.i.d. data to produce plots or metric values
(Heffernan and Tawn, 2001).
We further adapt these methods to handle both rounded data and parameter un-
CHAPTER 5. INFERENCE FOR EXTREME EARTHQUAKES 102
certainty, before showing how they can be used to inform selection of a modelling
threshold. In doing so, we transform to standard Exponential margins because this
distribution is central within the GPD family and follows the precedent of Heffernan
and Tawn (2001). Alternative marginal distributions could be used; we additionally
consider PP-plots, which correspond to the special case of uniform margins.
5.4.2 Graphical assessment
The observed magnitudes x that exceed v() do not have a shared marginal distri-
bution when v() is non-constant and they are not continuous-valued due to their
rounding. This presents challenges when trying to create a PP- or QQ-plot for ex-
ceedances of the modelling threshold v(). Firstly, constructing these plots using
rounded values can lead to many probabilities or quantiles of equal value and the
plots being difficult to interpret. The second challenge relates to observed, rounded
values close to the modelling threshold, {xi : |xi  vi| < , i = 1, . . . , n}; it is not
known which, or how many, of these events satisfy yi  vi and so should be included
when constructing the plot.
To overcome these challenges we use simulation to construct Monte Carlo confidence
intervals for the sample quantiles (or probabilities) of the unrounded threshold ex-
ceedances transformed onto shared exponential margins. The process is described
in Appendix B.3 and leads to a modified plot with two sets of intervals; tolerance
intervals show the expected variability of sample quantiles (or probabilities) under
the fitted model while confidence intervals show the uncertainty about the observed
sample quantile values. Confidence and tolerance intervals that do not overlap suggest
that the distribution of the rounded exceedances is not coherent with the fitted GPD
model.
Examples of such PP- and QQ-plots are shown in Figure 5.4.1. These use the simu-
CHAPTER 5. INFERENCE FOR EXTREME EARTHQUAKES 103
lated catalogue shown in Figure 5.3.1 (left) and constant modelling thresholds of v()
= 1.85ML and 1.15ML. For this catalogue, exceedances of a flat threshold should
be consistent with a GPD model only if that threshold is of 1.65ML or greater. For
the higher threshold v() = 1.85ML, the confidence intervals on sample probabilities
and quantiles overlap with the tolerance intervals, indicating that exceedances of this
threshold are consistent with the fitted GPD model. For the lower threshold v()
= 1.15ML this is not the case, with the large sample quantiles bigger than expected
under the fitted model. Notice the shape of the tolerance intervals in Figure 5.4.1;
the largest deviations from the line y = x are expected at central probabilities in the
PP-plots and at the largest quantiles of the QQ-plots. This feature is important in
Section 5.4.3 where we propose metrics to summarise these plots.
5.4.3 Metric-based assessment
Using a metric rather than a graphic to assess the distributional coherence of modelled
and observed threshold exceedances facilitates the comparison of many thresholds. We
therefore aim to summarise the PP- and QQ-plots using metrics that reward accurate
estimation of the magnitude distribution function. An unbiased estimate results in
a plot that covers the line y = x, while a precise estimate results in plots that are
stable between sampled values for the mle  and unrounded data y. Our approach
to creating a metric that summarises these plots is novel in its design, which rewards
large sample sizes through their effect to increase the precision of the distribution
estimate.
We propose four metrics to summarise deviation from the line y = x in PP- and QQ-
plots using the mean absolute distance and mean squared distance in what follows.
The calculation of these metrics is described below for a single sampled vector of
threshold exceedances on exponential margins z. Let d0 be the realised metric value
CHAPTER 5. INFERENCE FOR EXTREME EARTHQUAKES 104
0.0 0.2 0.4 0.6 0.8 1.0
model probability
1 0 1 2 3 4 5 6
model quantiles
0.0 0.2 0.4 0.6 0.8 1.0
model probability
0 1 2 3 4 5 6
model quantiles
Figure 5.4.1: PP-plots [left] and QQ-plots [right] for threshold exceedance sizes
shown on Exp(1) margins for constant modelling thresholds v() = 1.85ML [top]
and v() = 1.15ML [bottom]. 95% tolerance intervals are shown as grey regions,
while 95% confidence intervals on each probability or quantile are shown as vertical
lines. These are coloured red (blue) where the confidence interval is entirely above
(below) the tolerance interval.
CHAPTER 5. INFERENCE FOR EXTREME EARTHQUAKES 105
for an arbitrary dataset using one of the four methods, and d = EY ,|x,v(d0) be the
expected value of d0 over the joint distribution of Y , |x,v, thus accounting for the
rounding and parameter uncertainties that are represented by the confidence intervals
of Figure 5.4.1. We select a modelling threshold by minimising d and investigate which
choice of d0 is best. Here the expected values of the metrics are calculated by a Monte
Carlo approximation.
Smaller values of each metric are preferable, with large values caused by the quantiles
of the fitted model being either highly uncertain or incoherent with the observed
data. Minimising these metrics provides a new approach to threshold selection, which
rewards thresholds that give low sampling variability and small bias in the resulting
estimator. The remainder of this section covers the calculation of these metrics, while
Section 5.5 explores their relative performance on simulated data.
In the following, z(i) is the ith parametric-bootstrapped vector of threshold exceedances
transformed onto exponential margins for independent, replicated samples i = 1, . . . , k.
An algorithm to obtain these is given in Appendix B.3. Also let H(i)(y) : R+  [0, 1]
and Q(i)(p) : [0, 1] R+, respectively, be the empirical distribution function and the
sample quantile function of z(i) for i = 1, . . . , k. The sample quantile functions are
defined as linear interpolations of the points
n(i)1 , z
: j = 1, . . . , n(i)
, where
n(i) is the length of z(i) and z
is the jth order statistic of z(i).
The quantile based distance metrics d(i)(q, 1) and d(i)(q, 2) summarise the expected
deviation in the QQ-plot of z(i) from the line y = x at a set of m  N+ equally spaced
evaluation probabilities {pj = j/(m+1) : j = 1, . . . ,m}. The two metrics respectively
give the mean absolute distance and mean squared distance between model and sample
quantiles over the set of evaluation probabilities. They are given by
d(i)(q, 1) =
|  log(1 pj)Q(i)(pj)|
CHAPTER 5. INFERENCE FOR EXTREME EARTHQUAKES 106
d(i)(q, 2) =
( log(1 pj)Q(i)(pj))2.
In a PP-plot the variance of deviations from the line y = x is greatest when pj = 0.5
and shrinks to 0 as pj approaches 0 or 1. In the PP-based metrics we therefore weight
the sum of the deviations to account for large discrepancies being less surprising for
central probabilities. The metrics d(i)(p, 1) and d(i)(p, 2) are therefore calculated using,
respectively, the weighted-absolute and weighted-squared errors:
d(i)(p, 1) =
pj(1 pj)
)1/2 pj H(i)( log(1 pj))
d(i)(p, 2) =
pj(1 pj)
)1/2 (
pj H(i)( log(1 pj))
These deviations are again measured at m equally spaced evaluation probabilities
denoted by p1, . . . , pm. In the quantile-based metrics the weighting is handled implic-
itly by choosing equally spaced evaluation probabilities, which gives dense evaluation
where discrepancies from y = x are expected to be small and sparse evaluation where
they are expected to be large. In this way, the weights reflect the width of the tolerance
intervals in Figure 5.4.1.
Uncertainties in the estimated GPD parameters, the size of the exceedance set and the
values of the unrounded exceedances should all be accounted for when using a metric
to select a modelling threshold. This can be achieved by calculating the distance
metrics for each of k realisations of the vector z, where each uses one of k bootstrap
parameter estimates of . The expected metric values over these realisations are
denoted by d(a, b), where a  {p, q} and b  {1, 2}. The expected distance metric
d(q, 1) is defined as:
d(q, 1) =
d(i)(q, 1),
with the other expected distance metrics defined similarly.
CHAPTER 5. INFERENCE FOR EXTREME EARTHQUAKES 107
5.4.4 Minimisation procedure
To select the most appropriate threshold, the threshold parameters which minimise the
selected metric d must be found. Standard, gradient-based optimisation procedures
are not well suited to this task because the censoring mechanism can cause multiple
local minima and the Monte Carlo evaluation leads to local roughness over parameter
values. When using a simple parametric form for the threshold, such as a constant
or stepped threshold (where the change location is known), a simple grid search can
be used to overcome these issues and find the threshold parameters that minimise
the metrics. For more complex threshold forms, with a higher dimensional parameter
space to optimise over, a grid search becomes prohibitively expensive.
To find the threshold parameter set for more complicated thresholds we explore the
threshold parameter space in a more principled manner. To do this we use Bayesian
optimisation (Snoek et al., 2012) as implemented in the ParBayesianOptimization R
package (Wilson, 2020). The optimisation procedure begins by evaluating d at a small
initial collection of randomly chosen parameter vectors within a bounded search space.
Based on the resulting metric values, future evaluation points are selected sequentially
as the parameter vector with the greatest expected reduction in d as compared to the
current best value. This search method balances evaluations between parts of the
parameter space where the metric is known to have low values and parts where it is
most uncertain.
Bayesian optimisation is a heuristic search method but has been shown in other ap-
plications to find good parameter combinations using a relatively small number of
function evaluations (Shahriari et al., 2015). To establish its suitability in our setting
we compared Bayesian optimisation to a grid search for two sub-problems; catalogues
with a flat threshold and catalogues with a stepped threshold with known change
location. In both cases Bayesian optimisation performed favourably compared to grid
CHAPTER 5. INFERENCE FOR EXTREME EARTHQUAKES 108
search, selecting thresholds close to the true value at a lower computational cost.
We do not claim that Bayesian optimisation is the best method for optimising the
proposed metrics over threshold parameters, only that it appears to be an efficient
method of finding good thresholds.
5.5 Threshold selection on simulated catalogues
5.5.1 Simulation study overview
We consider the performance of the proposed threshold selection metrics on a col-
lection of simulated data sets with either constant or stepped threshold forms. This
simulation study illustrates the effectiveness of our method and establishes which of
the distance metrics proposed in Section 5.4 is best.
We attempt to select the most appropriate threshold from a set of candidate thresh-
olds. Two censoring types (hard and phased) are considered for magnitudes that are
below the modelling threshold. For hard censoring, all simulated continuous magni-
tudes below the modelling threshold are undetected. In phased censoring the detection
probability of each event, (yi, vi) = exp([vi  yi]+), decreases as the simulated
continuous magnitude falls further below the threshold, as controlled by the parame-
ter  > 0. The particular choices of exponential decay and the value of  are arbitrary
but were chosen to reflect, in a broad sense, the censoring observed in the Gronin-
gen earthquake catalogue. Note that either of these censoring types can result in
some rounded magnitudes that are below the threshold even though their simulated
continuous values are above the threshold.
CHAPTER 5. INFERENCE FOR EXTREME EARTHQUAKES 109
5.5.2 Constant threshold, hard censoring
We first use the four proposed metrics to select a constant threshold for 1500 simulated
i.i.d. GPD exceedances of the constant threshold v() = 0.32ML, hard-censored below
v() and rounded to the nearest 0.1ML. We first consider the metrics for a single
dataset. Expected metric values are calculated at the 41 equally spaced, constant
candidate thresholds shown in Figure 5.5.1. The candidate threshold selected by
minimising d(q, 1) is the closest threshold on the grid to the true value. This metric
also appears to provide the most clearly defined minimum, indicating that it penalises
both thresholds that are too low and too high. All four metrics show clear increases
in metric value for candidate thresholds that are too low, but not when the candidate
threshold is too high. The probability-based metrics do not increase greatly when the
candidate threshold is too high, and so fail to adequately reward the inclusion of valid
events with smaller magnitudes. This is presumably because they do not sufficiently
penalise the increased uncertainty in the estimated parameters when using a higher
threshold.
When selecting a constant threshold, the standard approach is to exploit the well-
established property that the GPD shape parameter is invariant to threshold choice
(Coles, 2001). Point estimates and 95% confidence intervals for  were obtained using
exceedances of each candidate threshold, accounting for the rounding of observations.
The confidence intervals for  overlap for all candidate thresholds above 0.275ML,
and so by the parameter stability method any greater threshold is also valid. The
thresholds chosen by our proposed method are therefore consistent with the parameter
stability method, but are preferable in that the selected thresholds are not below the
true level. Our proposed selection method is also more general; it allows comparison
of many non-constant thresholds without the need for subjective and time-consuming
interpretation of parameter stability plots.
CHAPTER 5. INFERENCE FOR EXTREME EARTHQUAKES 110

0.0 0.2 0.4 0.6 0.8 1.0
threshold

0.0 0.2 0.4 0.6 0.8 1.0
threshold

0.0 0.2 0.4 0.6 0.8 1.0
threshold

0.0 0.2 0.4 0.6 0.8 1.0
threshold
Figure 5.5.1: Flat threshold selection on a simulated catalogue. Top row: expected
mean absolute [left] and expected mean squared [right] QQ-distances against threshold
value. Bottom row: expected PP-distance metrics based on absolute [left] and squared
[right] errors against threshold. Selected and true thresholds are indicated by solid
black and dashed red lines.
CHAPTER 5. INFERENCE FOR EXTREME EARTHQUAKES 111
0.0 0.2 0.4 0.6 0.8 1.0
d(q,1), RMSE = 0.04
threshold
0.0 0.2 0.4 0.6 0.8 1.0
d(q,2), RMSE = 0.07
threshold
Figure 5.5.2: Sampling distribution of threshold selection methods for quantile-based
metrics over 500 simulated catalogues with constant threshold and hard censoring.
The true threshold is shown by a dashed red line and the root mean squared error
(RMSE) for each method is given in plot titles.
Figure 5.5.2 presents the sampling distribution and RMSE of the thresholds selected
from the candidate set by each of the QQ-based metrics over 500 replicated datasets,
simulated as previously described. The thresholds chosen by the PP-based metrics
are shown in Figure B.4.2 of Appendix B.4 and are frequently much higher than the
true value, resulting in higher RMSE values of 0.34 for d(p, 1) and 0.12 for d(p, 2).
The metric d(q, 1) has the lowest RMSE and so appears to be the best of the proposed
metrics in this case. All metrics have a tendency to overestimate the threshold value;
this is likely to be attributable to the hard censoring process. We therefore also
consider the performance of each metric using catalogues with phased censoring.
5.5.3 Constant threshold, phased censoring
To assess the performance of each metric on simulated catalogues with phased cen-
soring, we consider the thresholds selected by each metric for each of 500 simulated
catalogues. For each catalogue, 2400 i.i.d. GPD exceedances of 0ML were simulated.
Each exceedance was retained with probability (yi, vi), as defined in Section 5.5.1
with v() = 0.32ML and  = 7. This combination of simulated catalogue size and
CHAPTER 5. INFERENCE FOR EXTREME EARTHQUAKES 112
censoring parameter gave an average catalogue size of 1500 recorded values, similar
to those in Section 5.5.2.
The resulting RMSEs in threshold selection over these 500 catalogues were: 0.06 for
d(q, 1), 0.08 for d(q, 2), 0.35 for d(p, 1), and 0.12 for d(p, 2). For all metrics the RMSE
is slightly increased compared to hard censoring, as threshold selection is made more
difficult by the retention of some events that are truly below the threshold. As with
hard censoring, the metrics d(p, 1) and d(p, 2) were prone to selecting conservative
threshold values and d(q, 1) resulted in the lowest RMSE. Unlike for hard censoring,
the sampling distributions of selected thresholds now cover the true threshold values,
this is shown in Figure B.4.4 of Appendix B.4. Similar selection properties for each
metric were seen when considering more complex threshold forms and so further ex-
position is limited to the metric d(q, 1), and we subsequently refer to d = d(q, 1).
5.5.4 Non-constant threshold selection
Here catalogues are simulated by generating 4000 i.i.d GPD exceedances of 0ML and
censoring (either hard or phased) based on a threshold with v() = 0.83ML for 0 <
  2000 and v() = 0.42ML for 2000 <   4000, see Figure 5.5.3 where  = 7.
We considered threshold selection behaviour over 500 earthquake catalogues simu-
lated using the above change-point threshold for each of hard and phased censoring.
Note that the number of retained events and the threshold change location   within
these will vary between simulations because they both depend on the simulated event
magnitudes and on how many of these are retained. However, in each case the true
value of   is known.
For each simulated catalogue we selected a threshold of the form v() = v(1) for
CHAPTER 5. INFERENCE FOR EXTREME EARTHQUAKES 113
0 200 400 600 800 1000 1200
event index
0 500 1000 1500
event index
Figure 5.5.3: Example simulated catalogues with hard censoring [left] and phased
censoring [right] for stepped thresholds of (v(1), v(2)) = (0.83,0.42), shown as a red
line, and phasing parameter  = 7.
0 <     and v() = v(2) for   <  < max, where the threshold parameters
(v(1), v(2),  ) are unknown. Threshold parameters were selected using the Bayesian
optimisation method of Section 5.4.4 to minimise the metric d. The sampling distribu-
tion of the errors in the selected threshold parameters are shown in Figure 5.5.4, where
it can be seen that our threshold selection method regularly recovers the non-constant
modelling threshold to within /2 of it true value.
0.6 0.4 0.2 0.0 0.2 0.4 0.6
v(1) error
0.1 0.0 0.1 0.2 0.3 0.4 0.5 0.6
v(2) error
600 200 0 200 400 600
* error
0.6 0.4 0.2 0.0 0.2 0.4 0.6
v(1) error
0.1 0.0 0.1 0.2 0.3 0.4 0.5 0.6
v(2) error
600 200 0 200 400 600
* error
Figure 5.5.4: Marginal sampling distributions of errors in the selected values of v(1)
(left), v(2) (center) and   (right) for 500 simulated catalogues with change-point type
thresholds and hard (top row) or phased (bottom row) censoring.
CHAPTER 5. INFERENCE FOR EXTREME EARTHQUAKES 114
Specific findings vary by censoring type. For hard censoring, as would be expected,
the threshold levels v(1) and v(2) are rarely selected to be below the true values. The
error distribution of   has, in both cases, a mode close to 0 but with large variance.
As expected, the sampling variability of the error in each parameter is larger for
phased censoring than for hard censoring, though it is reassuring to see that the
distributions of selected threshold parameters are now centered on the true values.
This demonstrates that the tendency to select threshold values too high for catalogues
with hard censoring is a consequence of the censoring mechanism, not a bias in the
selection method.
5.6 Application to Groningen earthquakes
5.6.1 Validating data model for Groningen catalogue
We compare GPD and exponential models for Groningen earthquake magnitudes.
Rohrbeck et al. (2018) and Marzocchi et al. (2019) demonstrated the importance of
acknowledging rounding of observations, and so this is accounted for within the infer-
ence for both models. We focus on earthquakes exceeding the constant conservative
threshold of 1.45ML, subsequently referred to as vC . This is the magnitude of comple-
tion stated by the KNMI (Dost et al., 2012), adjusted to account for rounding.
Both the GPD and exponential models assume that magnitudes are i.i.d.; this is
supported by our exploratory analysis of the Groningen catalogue above 1.5ML in
Appendix B.1. The two models may be compared by considering the sampling distri-
bution of the estimated shape parameter under a GPD model, because the exponential
model is a special case of the GPD where  = 0. Fitting a GPD to the 311 exceedances
of vC leads to point estimates of (1.45, ) = (0.448,0.018) with respective 95% boot-
strap confidence intervals of (0.399, 0.501) and (0.147, 0.086). Since the confidence
CHAPTER 5. INFERENCE FOR EXTREME EARTHQUAKES 115
interval for  covers 0, the exponential model cannot be discounted at the 5% sig-
nificance level using only exceedances of vC . A second method of comparison is to
fit both an exponential and GPD model to exceedances of vC and, appealing to the
asymptotic distribution of the MLE, perform a likelihood ratio test. This produces a
likelihood ratio of 1.04 and associated p-value of 0.214, leading us to draw the same
conclusion in both comparisons: that there is insufficient evidence to conclude that
the Groningen magnitudes deviate from the Gutenberg-Richter law when using only
exceedances of vC .
However, if an exponential magnitude model is assumed then the uncertainty about
 is ignored. This has the effect of dramatically, but artificially, narrowing the con-
fidence intervals on the estimated magnitude quantiles, as shown in Figure B.4.3 of
Appendix B.4. The potential repercussions of ignoring this uncertainty are described
in detail in Coles and Pericchi (2003). A GPD model should therefore be used for
the underlying magnitudes, to properly represent this uncertainty when selecting a
modelling threshold for the Groningen gas field.
If the rounding of observations had been ignored in the fitting of the GPD model,
the point estimates of the GPD parameters would be (1.45, ) = (0.453,0.027)
with respective standard errors of (0.039, 0.066). The parameter estimates are not
significantly different to those using the correct likelihood because the small number
of threshold exceedances means that parameter uncertainty obscures the bias induced
by neglecting to account for rounding.
Finally, in Figure 5.6.1 we check that the fitted GPD model is consistent with the
empirical distribution of exceedances of 1.45ML through the use of the modified QQ
and PP plots introduced in Section 5.4.2. Since the tolerance intervals and confidence
intervals overlap for both the sample quantiles and sample probabilities, we conclude
that a GPD model is appropriate for Groningen earthquake rounded magnitudes
CHAPTER 5. INFERENCE FOR EXTREME EARTHQUAKES 116
0.0 0.2 0.4 0.6 0.8 1.0
model probability
0 2 4 6
model quantiles
Figure 5.6.1: Modified PP (left) and QQ (right) plots for Groningen magnitudes
exceeding 1.45ML under the GPD model. Grey regions show 95% tolerance intervals
while vertical lines show 95% confidence intervals on sample probabilities / quantiles.
All confidence intervals overlap with the associated tolerance intervals.
exceeding 1.45ML.
5.6.2 Parametric threshold forms
Now we select thresholds of two parametric forms for the Groningen catalogue and
explore the results of the subsequent inference. The first is a constant threshold,
v() = v, where the level v is to be chosen. This will allow us to assess the level of
conservatism in the conventional modelling threshold where v = 1.45ML. The second
form is a sigmoid-type threshold v() = vR + (vL vR) ([  ]/) , with parameters
(vL, vR, , )  R3  R+ and where  is the standard Gaussian distribution function.
This extends the idea of the change-point threshold to accommodate smooth change
in the threshold value centred on . The threshold parameters may be interpreted
as follows. The left and right asymptotic levels of the threshold are given by vL and
vR,  is the index-time at which the threshold takes the value (vL + vR)/2, and 
controls how rapidly the threshold changes about , with   0 corresponding to a
step change. In the context of the Groningen catalogue we expect that vR < vL.
CHAPTER 5. INFERENCE FOR EXTREME EARTHQUAKES 117
5.6.3 Threshold selection
5.6.3.1 Constant threshold
A grid search was used to find the flat threshold that minimises the metric d, as shown
in Figure 5.6.2. There are two local minima at v = 0.85ML and v = 1.07ML, the
latter being the global minimum. For thresholds greater than 1.07ML, including the
conservative threshold of 1.45ML, the metric values are increasing as not all viable
data are utilised. For thresholds less than 0.85ML the metric also increases as the
validity of the tail model breaks down. The small peak between these minima is
likely attributable to the reduction of the mc over time. In Figure 5.2.1 we saw
that fewer small magnitude events are censored at later times. The minimum at
1.07ML uses less data to achieve good distributional agreement for the entire period,
while the minimum at 0.85ML compromises on the distributional agreement at early
times to retain a larger proportion of the data. As the threshold is lowered between
magnitudes 0.95ML and 0.85ML, enough additional data are added to more than
compensate for the reduced goodness-of-fit in the early part of the observation period
and so the metric value reduces. Since the global minimum corresponds to the more
conservative threshold, we select 1.07ML as our constant modelling threshold.
5.6.3.2 Sigmoid threshold
Bayesian optimisation was used to find the sigmoid threshold parameters (vL, vR, , )
that minimise the metric d, where the search space was constrained to the region
[0.4, 1.7]2  [200, 1100]  [1, 500]. For an initial set of 20 randomly selected thresh-
old parameter combinations, d was evaluated. A further fixed budget of 100 metric
evaluations was allocated and the thresholds with the smallest metric value retained
for further inspection. To assess the sensitivity of the selected threshold to the set
of initial evaluation points, this was repeated for five initial parameter combination
CHAPTER 5. INFERENCE FOR EXTREME EARTHQUAKES 118
0.5 1.0 1.50
threshold value, v

1 5 50 500
return period


Figure 5.6.2: [Left] Grid search to minimise d(q, 1) over threshold values v. Metric
values are shown on log-scale and vertical lines mark the edges of magnitude rounding
intervals. [Right] Point estimates (solid lines) and 95% confidence intervals (dashed
lines) for the conditional return levels for exceedances of 1.45ML, using the conserva-
tive (black) and selected thresholds (red). Sample conditional return levels are shown
in blue.
sets.
The thresholds with the lowest values of d based on each initialisation are shown
in Figure 5.6.3 (left). The selected threshold values at the ends of the observation
interval appear to be stable across initialisations, but the transition between these
levels is not. Further investigation supports the stability of the end levels; the blue and
turquoise thresholds have significantly greater metric values than the other thresholds,
suggesting that these initialisations had too few evaluations to explore beyond a local
minimum. These conclusions are consistent with the simulation study of Section 5.5.4,
illustrating that threshold levels are more easily estimated than the change between
those levels.
A second Bayesian optimisation was performed, fixing the end levels of the sigmoid
threshold to the those shared by the best performing thresholds in the previous opti-
misation, namely (vL, vR) = (1.15, 0.76). This reduces the dimension of the parameter
space and simplifies the optimisation task. Using the same procedure as for the un-
CHAPTER 5. INFERENCE FOR EXTREME EARTHQUAKES 119
0 200 400 600 800 1000 1200
index
0 200 400 600 800 1000 1200
index
1995 2000 2005 2010 2015 2020
A B C
Figure 5.6.3: Selected sigmoid thresholds using Bayesian optimisation from 5 ran-
dom initial parameter sets. [left] Optimising over all thresholds parameters. [centre,
right] Optimising over (, ) and fixing (vL, vR) = (1.15, 0.76) on index- (centre) and
natural- (right) timescales. Colours are comparable only between centre and right
plots. Dashed horizontal lines show the conservative threshold value. Important dates
relating to the development of the Groningen seismic detection network are shown as
vertical lines: (A) development begins, (B) first additional sensors activated, (C)
upgrade complete.
constrained optimisation, the resulting selected thresholds from each initialisation are
shown in Figure 5.6.3 (centre). Upon repeated Monte Carlo evaluation of the met-
ric value for each of these thresholds, there is insufficient evidence to select one over
the others. When transformed onto the natural time scale, as shown in Figure 5.6.3
(right), the selected thresholds are all consistent with the known dates at which sen-
sor installation occurred. This shows that from the earthquake catalogue alone our
method is able to detect the starting and ending threshold levels and the period
in which it changed. However, we cannot identify precisely the way in which the
threshold changed during the installation period. This is not a major setback, since
between the most and least conservative of the chosen thresholds (turquoise and red
in the centre and right panels of Figure 5.6.3) the expected number of observations
above the threshold differs by only 50 earthquakes. We fitted the GPD model using
each of these five threshold functions, reaching similar conclusions, and so present
CHAPTER 5. INFERENCE FOR EXTREME EARTHQUAKES 120
further results for only the turquoise threshold.
5.6.3.3 Threshold comparison
We compare the conservative, selected constant, and selected sigmoid thresholds,
which are referred to as vC , v and vS respectively. Comparisons are made on: the
expected metric value, the number of events used to fit the GPD model, the estimated
GPD parameter values, and the estimated return levels.
Metric evaluations are subject to Monte Carlo noise and so the metric value was eval-
uated 100 times for each threshold. The mean metric value and 95% Monte Carlo
noise intervals were calculated to be: 0.091 (0.088, 0.096) for vC , 0.054 (0.053, 0.055)
for v, and 0.041 (0.039, 0.043) for vS. This suggests that the model fit using vS fits
the observed data best, with v being preferred over vC . These improvements in model
fit may be attributable to the increased data usage of the selected thresholds. The
threshold vC is at the edge of a rounding interval and so utilises 311 threshold ex-
ceedances in the resulting model. For thresholds v and vS, the rounding of magnitudes
means that the exact number of exceedances is unknown. The expected number of
exceedances under the fitted magnitude models are 629 and 702 for v and vS, respec-
tively. By using either of the selected thresholds, we have more than doubled the size
of usable catalogue as compared to the conservative threshold.
Figure 5.6.4 (left) shows the estimated parameter values under the fitted GPD model
using each threshold. The uncertainty in both parameters is reduced when using v
rather than vC , and further reduced when using vS. To give a sense of scale in this
reduction, we can calculate the number of additional exceedances of vC to which they
are equivalent, under the assumption that the standard error of parameter estimates
scales with exceedance count n as n1/2. In doing this, the additional 318 and 391
small magnitude earthquakes included by, respectively, using v or vS are equivalent
CHAPTER 5. INFERENCE FOR EXTREME EARTHQUAKES 121
to 363 or 509 additional events above vC . Therefore, point-for-point, the small mag-
nitude earthquakes are at least as valuable as additional data above vC for parameter
estimation.
When modelling exceedances of v or vS the respective point estimates and 95% confi-
dence intervals for the shape parameter are -0.084 (-0.168, -0.017) and -0.069 (-0.144,
-0.008). Using exceedances of v or vS leads to only 0.5% or 1.5% of the sampling
distribution for  being above 0. This provides empirical evidence that the Groningen
magnitude distribution has a finite upper endpoint, unlike the conventional Guten-
berg Richter magnitude model. This dramatic conclusion could not be reached using
the smaller dataset exceeding vC , where 33% of the sampling distribution for  lay
above 0.
Similar conclusions can be reached by using likelihood ratio tests to compare Expo-
nential and GPD models for exceedances each of vC , v and vS; the respective p-values
are 0.78, 0.046, and 0.064. By using more of the available data, we have increased our
ability to discern between an exponential model and the observed magnitude distribu-
tion. The conclusions that can be drawn from this test are in agreement with, but are
less strong than, those of the previous comparison: a Gutenberg Richter magnitude
model is likely inferior to a GPD. The discrepancy in conclusion strength between the
two comparisons is likely due to the asymptotic assumptions of the likelihood ratio
test not being met by our finite sample size.
The estimated conditional return levels above 1.45ML are shown using each threshold
in Figure 5.6.4 (right). The estimated return levels are similar when using v and vS,
but confidence intervals for large return periods are narrower when using vS. In either
case, the return levels have both smaller point estimates and uncertainties by using
our threshold selection method than when using the conservative threshold. This is an
important finding when deciding what measures to take when designing or retrofitting
CHAPTER 5. INFERENCE FOR EXTREME EARTHQUAKES 122
1 5 50 500
return period
Figure 5.6.4: Bootstrap GPD parameter estimates based on exceedances of the con-
servative (black), flat (red) and sigmoid (blue) thresholds [left]. Estimated return
levels in ML and 95% confidence intervals for magnitudes exceeding 1.45ML [right].
earthquake defences for buildings.
5.7 Discussion / Conclusion
This chapter introduced a principled method to select a time-varying modelling thresh-
old for an extreme value analysis. The effectiveness and value of using this method
to include additional, less extreme events in the analysis were demonstrated through
simulation studies. Although the method was developed in the context of earthquake
catalogues, and to accommodate the additional challenges to inference that these
pose, the core of our method is applicable to extreme value threshold selection more
generally and we anticipate it having a much broader impact.
Using the new threshold selection method, we have been able to identify the period
in which the Groningen sensor network was being improved by using the earthquake
catalogue alone. Our threshold selection method more than doubled the usable size
of the Groningen earthquake catalogue compared to using the conservative threshold
given by the KNMI, whilst also improving model fit. This has several important
implications beyond the direct improvement to statistical inference.
CHAPTER 5. INFERENCE FOR EXTREME EARTHQUAKES 123
The use of these additional small magnitude earthquakes leads to greater precision
in the estimates of high magnitude quantiles, which is potentially a huge benefit
by reducing the cost of designing, constructing or retrofitting earthquake defences.
Following threshold selection, a Bayesian modelling approach would allow quantile
uncertainty to be included naturally when designing defences against natural hazards
(Coles and Tawn, 1996; Fawcett and Green, 2018; Jonathan et al., 2021) and estimates
with greater precision can reduce the cost required to provide protection with equiv-
alent confidence. The gain we have made in the efficiency of statistical inference can
be translated to a tangible economic benefit of using the additional data recorded by
improving the censor network. The more efficient use of the available data has allowed
us to conclude, for the first time based on empirical evidence alone, that Groningen
earthquake magnitudes are likely to have a light-tailed distribution. Using the conser-
vative threshold level this conclusion could only have been achieved by waiting many
years to observe additional large magnitude earthquakes. Finally, using a less conser-
vative modelling threshold provides a return on the substantial investment into the
earthquake detection network around the Groningen gas field. When a non-constant
threshold is selected, the added value of the network improvements is exploited fully
and the subsequent modelling threshold can also offer insights into the reduction of
mc over time.
A limitation of the work is that the computational effort required to select a modelling
threshold is relatively high. We do not view this as a large drawback since threshold
selection must be performed only once through the modelling process. An area for
further development would be to investigate alternative, exact methods to optimise the
expected selection metric over the threshold parameters. One possible extension to our
approach would be to adapt the data model to account for magnitude measurement
error causing events to be recorded within incorrect rounding intervals. Another,
CHAPTER 5. INFERENCE FOR EXTREME EARTHQUAKES 124
more ambitious, extension might consider a selection of spatio-temporal threshold
function to describe spatial variability as well as the temporal evolution of event
detection. Finally, an extensive comparison of our proposed and standard extreme
value threshold selection methods would be a valuable piece of further work, given its
critical importance in extreme value methods.
Chapter 6
Improving and extending the
ETAS formulation
6.1 Introduction
6.1.1 The ETAS model
The epidemic-type aftershock sequence (ETAS) model describes marked, clustered
point processes. In the simplest form of the ETAS model, each event is a time-mark
pair (t,m) where typically t  (0, tmax) and m  R+. A single realisation of the point
process is then a collection of n  N0 = N  {0} such pairs {(ti,mi) : i = 1, . . . , n},
where the number of events n is a random variate. The ETAS model was developed for
earthquake catalogues by Ogata (1988), but has since found applications in finance,
the natural- and social-sciences (Reinhart, 2018). In the context of seismicity, the
ETAS model can be used descriptively or predictively. Descriptive modelling can
be used to address questions of scientific interest about the earthquake generating
process, while predictive modelling can inform earthquake defence policies that help
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION126
to safeguard against future hazards.
A point process model can be defined by its intensity function, which determines the
expected number of events in the process and how those events are located across
time or space. In the ETAS model this intensity function is increased locally after
each event, resulting in a clustered point process. The size of the additional intensity
contribution from each event is determined by the value of its mark, where larger
mark values are expected to yield a larger number of additional events. In the con-
text of seismicity modelling, each event represents an earthquake and the associated
mark represents its magnitude. In this way, each event in an ETAS point process
may be considered as either a background or triggered event; background events are
attributable to the initial intensity component and triggered events are attributable
to an intensity component caused by a previous event. Within the standard ETAS
model, event magnitudes are independent of one another and of event type. As with
background events, triggered events provide an additional intensity contribution and
so can also trigger further events. The ETAS model may therefore be interpreted as
either the superposition of point processes or as a branching process.
In the simplest form of the ETAS model, background events come from a Poisson pro-
cess with constant intensity . This intensity is then augmented by each of the events,
indexed by i, according to their magnitude and occurrence time. The magnitude pro-
ductivity function  : R R+0 determines the expected number of events triggered at
a given magnitude. The probability density function h : R+  R+ characterises the
delays between triggering and triggered events. The resulting conditional intensity
function depends on the history of the process at time  , H = {(ti,mi) : ti < } and
is parameterised by the vector  = (, , h). Specifically, the conditional intensity 
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION127
at time  is defined as:
( |H ; ) = +
i:ti<
(mi; )h(  ti; h). (6.1.1)
In seismological applications of the ETAS model the most common parametric forms
for  and h are the empirical earthquake laws which will be introduced in Sec-
tion 6.3.1. Broadly, these forms for  and h impose that the increment in intensity
following each event should increase with the magnitude of the event and should di-
minish with time since that event. The first contribution of this chapter is to show
that these empirical earthquake laws may be represented by the single, encompassing
model provided by the generalised Pareto distribution (GPD). We show that using
this encompassing model better represents epistemic uncertainty (by broadening the
class of models which can be represented) and increases the statistical efficiency of
parameter inference.
6.1.2 Magnitude modelling
In the standard ETAS formulation, event marks are modelled as an independent
component of the point process. That is, marks are modelled as independent and
identically distributed (i.i.d.) random variables with a common probability density
function f(m;).
Within the seismology literature, previous studies have investigated the validity of
earthquake magnitudes being identically distributed, i.e., that there is a single mag-
nitude distribution for all earthquakes. These studies rely on declustering the earth-
quake catalogue into mainshock and aftershock events and then testing for differences
in the distributions of the two samples (Stallone and Marzocchi, 2019). This clustering
is frequently done using a window- or distance-based approach following the events of
largest magnitude, in which case mainshocks and aftershocks are not coherent with
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION128
the complex triggering structure of the ETAS model (Varini et al., 2020). An alter-
native approach to declustering is to use point estimates of the ETAS parameters to
perform a model-based stochastic declustering (Zhuang et al., 2002). This approach
yields, for each event, a probability distribution that gives the probability that the
event is attributable to each of the previous earthquakes or is a background event.
This stochastic declustering respects the branching structure of ETAS point processes
and gives some measure of the uncertainty on the the estimated cluster allocation.
However, this approach importantly fails to capture the joint uncertainty across all
events or to include the additional uncertainty arising from the ETAS parameters
themselves being estimated.
There have also been previous investigations into the assumed independence between
earthquake magnitudes (Gulia et al., 2018; Stallone and Marzocchi, 2019; Cai et al.,
2021). These rely on the previously mentioned declustering methods and so suffer from
the same limitations. Additionally, attention is often restricted to dependence between
the magnitudes of the main-shock and largest aftershock within each identified cluster.
This focus may be motivated by mathematical convenience or else by an empirical
relation known as Baths law (Bath, 1965), which relates these quantities. However,
this approach fails to use all available data and the motivation based on Baths law
is unsound; Baths law can been shown to arise as an artefact of the window-based
declustering method when magnitudes are truly i.i.d. (Lombardi, 2002; Helmstetter
and Sornette, 2003).
A different dependence structure was investigated by Chavez-Demoulin et al. (2005),
who incorporated auto-regressive dependence into a financial application of the ETAS
model. In this setting, events correspond to financial losses exceeding a given threshold
and marks give the size of this exceedance. The event times are modelled using an
ETAS point process, modified so that each mark has a generalised Pareto conditional
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION129
distribution (as given in equation (6.3.6)) when the value of the previous mark is
known. The scale parameter of this conditional distribution is linked to the previous
mark so that Mi|Mi1 = mi1  GPD(exp{a + bmi1}, ), where a, b and  are
parameters to be estimated. This is a relaxation of the assumption that marks are
i.i.d. under the standard ETAS model that leads to marginal mark distributions that
are not within the GPD family.
A third dependence structure, which addresses the branching structure of the ETAS
process, was investigated by Zugec (2019). Their approach assumes that the marginal
distribution of all magnitudes can be described by a single exponential distribution.
The dependence between the magnitudes of triggering and triggered events is incorpo-
rated through the use of a Farlie - Gumbel - Morgenstern copula, constrained to only
allow positive dependence. This dependence structure has the benefit of preserving
the marginal magnitude distributions and respecting the branching structure of the
ETAS process. However, the choice of copula used to model dependence does not
allow magnitudes to be dependent at extreme levels, allowing only near extremal
independence (Ledford and Tawn, 1997). Additionally, Zugec (2019) presents only
theoretical results for their model and does not provide an inference method with
which to fit their model to an observed earthquake catalogue.
In Section 6.4 we introduce an alternative relaxation of the i.i.d. mark assumption.
This is designed to ensure that: dependence is based on the branching structure of
the process, rather than the temporal ordering of events; the marginal mark distribu-
tions are in the GPD family; and strong dependence at extreme magnitudes may be
represented. We begin Section 6.4 by introducing an extension of the ETAS model
to allow separate magnitude distributions for background and triggered events. This
is based on the estimated branching structure of the ETAS process and accounts for
all sources of uncertainty within this model. We then consider a further extension by
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION130
allowing dependence between the magnitudes of triggered events and the magnitude
of the event that triggered them. We present inference methods for each model. These
methods allow for simple hypothesis tests on the presence of each of these magnitude
features, while properly reflecting uncertainties within the ETAS paradigm.
6.1.3 Bayesian ETAS modelling
When fitting the ETAS model to an observed catalogue of earthquakes, one must
estimate the parameter vectors  and  that describe the point process intensity
(t; ) and magnitude distribution f(m;). Direct maximisation of the likelihood
function is the most common approach to parameter estimation for the ETAS model.
This approach is described in Section 6.2.1. There are three main issues with the direct
approach to inference for the standard ETAS model. Firstly, parameter uncertainty is
difficult to propagate into earthquake forecasts. This means that in many cases only
the point estimates of parameters are retained (Ogata, 1988; Veen and Schoenberg,
2008). This is a particular issue when the model is used to predictively simulate
an ensemble set of futures to aid decision-making; using only point estimates will
lead to an overly narrow set of possible outcomes being represented. Secondly, the
likelihood function requires numerical maximisation. Direct numerical maximisation
is unreliable for the ETAS likelihood because the parameters are not orthogonal, the
likelihood function can have multiple modes and local regions may be almost flat
(Veen and Schoenberg, 2008). These features also mean that measures of parameter
uncertainty based on asymptotic standard errors can be unsuitable or unreasonable.
Finally, each evaluation of the likelihood for the ETAS model is computationally
expensive, and this becomes the increasingly prohibitive factor as the number of events
in the observed catalogue grows. This final issue can be compensated for through
use of the expectation-maximisation algorithm introduced by Veen and Schoenberg
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION131
(2008). An additional, fourth issue arises when the i.i.d. magnitude assumption is
relaxed: the likelihood becomes intractable making evaluation, and therefore direct
maximisation, of the likelihood function impossible.
The aforementioned challenges presented by the ETAS likelihood motivate a Bayesian
approach to modelling. This avoids reliance on asymptotic results for parameter un-
certainty and makes propagating these uncertainties into predictions straightforward.
It also offers the possibility to incorporate expert knowledge into the fitting procedure,
which could be particularly helpful when modelling point processes with relatively few
observed events. Fitting the ETAS model in a Bayesian framework requires Markov
Chain Monte Carlo (MCMC) sampling methods, which makes it more computation-
ally challenging than a frequentist approach. However, this cost can be greatly reduced
through the use of the Gibbs sampling scheme introduced by Ross (2016).
Ross (2016) interprets the ETAS model as a branching process and introduces a latent
vector to describe the branching structure. This vector defines the graph represen-
tation of triggering and triggered events and is described fully in Section 6.2. In-
troducing this branching vector facilitates inference for the ETAS model by allowing
alternate estimation of the branching structure and ETAS parameters. This provides
a method of probabilistically declustering earthquake sequences that fits naturally
into the Bayesian paradigm. The approach was shown to reduce dependence between
groups or blocks of the ETAS parameters and was shown experimentally to reduce
the cost of evaluating the likelihood function. A remaining issue, which we address
through our proposed reparameterisation in Section 6.3, is that parameters remain
highly dependent on one another within these blocks.
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION132
6.1.4 Contributions and chapter outline
We first show that the earthquake laws used in Ross (2016) and many other applica-
tions of the ETAS model are restricted forms of the generalised Pareto distribution.
We reformulate the ETAS intensity in terms of this model, which reduces within-
block parameter dependence and leads to more efficient inference. The reformulation
properly accounts for both model-uncertainty and parameter-uncertainty, leading to
conclusions and predictions that are more robust.
We then propose two extensions of the ETAS model that build explicitly on its rep-
resentation as a branching process. Our first extension allows distinct magnitude
distributions for background and triggered earthquakes. The second extension al-
lows magnitudes of triggered earthquakes to be dependent on the magnitude of the
triggering event. These extensions allow simple, structured tests for the additional
magnitude properties that could not be included in the ETAS model when taking a
direct approach to fitting the model.
The temporal ETAS model (Ogata, 1988) has many extensions, including those which
allow a variable background rate and those which model events over space and time
(Ogata, 2011; Kolev and Ross, 2020). The ideas presented in this paper extend readily
to these settings and so for simplicity of exposition we restrict our attention to the
temporal ETAS model.
The rest of this chapter is set out as follows. In Section 6.2 we outline in detail both
the direct and latent variable inference procedures for the ETAS model. We identify
the source of the efficiency gain that was demonstrated numerically by Ross (2016)
and calculate the order of this efficiency gain. In Section 6.3 we propose a more general
parameterisation of the empirical laws that are commonly used for the productivity
function, delay and magnitude distributions. The proposed extension broadens the
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION133
class of models that can be represented within the ETAS framework and aims to make
inference more efficient by reducing the dependence within groups of parameters. In
Section 6.4, we introduce two extensions to the standard ETAS magnitude model
and describe estimation procedures for these extensions. The first extension utilises
the latent branching structure in an ETAS point process to allow distinct magnitude
distributions for background and triggered events. The second extension moves be-
yond the assumption of independence to allow triggered magnitudes to depend on the
magnitude of their triggering event. Section 6.5 demonstrates the benefits of these ap-
proaches using simulated earthquake catalogues. Finally, Section 6.6 gives concluding
remarks and suggests areas for further work.
6.2 Estimation of ETAS parameters
6.2.1 Direct estimation: ETAS as a point process
Use of the ETAS model requires estimation of the parameter vectors  and  of the
intensity function ( ; ) and mark distribution f(m;). This section describes how
these parameter vectors can be estimated for the standard ETAS formulation (with
i.i.d. magnitudes) using a direct point process representation. This is accomplished
using a single, observed catalogue of n events y = (y1, . . . , yn) = ({t1,m1}, . . . , {tn,mn}),
which is a realisation of the marked point process Y = (Y1, . . . , YN) where each ele-
ment of Y is a time-magnitude pair and the number of events N is random. In order
to be included in the catalogue, event magnitudes must exceed a minimum value of
m0 and must occur before the end of observation at time tmax. The support for each
element of Y is therefore the time-magnitude window [0, tmax] [m0,).
To find the joint posterior distribution of  and , we first require the likelihood
function Y |,(y). The corresponding log-likelihood has the same form as that of an
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION134
inhomogeneous Poisson process with independent marks and the history-dependent
intensity function from expression (6.1.1), namely:
log Y |,(y) =
log f(mi;)
 tmax
( |H ; )d +
log (ti|Hti; ). (6.2.1)
The log-likelihood (6.2.1) is separable in  and  and so these vectors may be estimated
separately. We focus in this section on estimation of , which is the more challenging
aspect when magnitudes are independent and identically distributed. Expression
(6.2.2) gives the log-likelihood of  in expanded form, whereH( ; h) is the distribution
function associated with the aftershock delay density h( ; h).
log Y |(y) = tmax 
(mi|)H(tmax  ti|h)
+ 
j:tj<ti
(mj|)h(ti  tj|h)
 . (6.2.2)
There is no conjugate form for this model and so Monte Carlo methods are required to
obtain approximate samples from the posterior distribution. The standard and most
direct approach is to use a random walk Metropolis algorithm to do so; Ross (2016)
describes how this can be implemented for the ETAS model. As with likelihood-
based inference, this approach suffers from the local flatness of the likelihood and
strong, complex parameter dependence pointed out by Veen and Schoenberg (2008).
The direct approach to inference requires a large number of evaluations of the log-
likelihood (6.2.2). Each evaluation is an O(n2) operation due to double summation
in the final term, which makes the sampling algorithm very slow for even moderately
sized catalogues. These issues can be alleviated by instead considering the ETAS
model as a branching process.
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION135
6.2.2 Latent estimation: ETAS as a branching process
Self-exciting point processes such as the ETAS model may be interpreted as branching
processes (Kirchner, 2017). These processes can be represented as a graph, where
events are nodes and directed edges connect an event to those that it triggers. This
branching structure produces a collection of trees, which are unobserved but can be
modelled by a latent n-vector, B = (B1, . . . , Bn). Element i of this vector, Bi 
{0, 1, . . . , i 1}, is equal to zero if event i is a background event (that is not triggered
by a previous event) and otherwise gives the index the triggering event. The branching
vector therefore identifies the set of any child events triggered by each parent event
i  1, . . . , n, which we index by the sets Ci = {j  {i+ 1, . . . , n} : bj = i}.
Figure 6.2.1 shows the graph representation and associated branching vector B of a
toy example from the ETAS model, where Y1, . . . , Y7 are a temporally ordered set of
events. The root of each tree is formed by a background event. In this example
there are three trees initiated by the background events Y1, Y2 and Y5. The example
shows that an event may directly trigger zero, one, or multiple further events, as
demonstrated by the child sets C2 = , C1 = {3} and C3 = {4, 6}. Note that triggered
events may in turn trigger further triggered events, as demonstrated by Y3. Finally,
notice that triggering and triggered events are not necessarily consecutive or even
contiguous in time. This is because events can trigger multiple further events and
distinct trees can overlap in time. These effects can be seen in the toy example where
Y6 is separated in time from its triggering event Y3 by both the background event Y5
and also by Y4, another child event of Y3.
An alternative approach to estimation of the ETAS parameters utilises this branch-
ing process interpretation and in particular the latent branching vector, B. This
method was proposed by Veen and Schoenberg (2008) and brought to the Bayesian
setting by Ross (2016). In this approach, the conditional intensity function (6.1.1)
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION136
Y1 Y2
Figure 6.2.1: Graphical representation of a toy example from the ETAS model with
branching vector B = (0, 0, 1, 3, 0, 3, 5). Note that an event may trigger multiple fur-
ther events, that event sequences may overlap temporally, and that both background
and triggered events may trigger further events. Each event Yi = (Ti,Mi) represents
a time-magnitude pair where T1 < T2 < . . . < T7.
can be regarded as the superposition of n + 1 Poisson process intensities. These
Poisson processes are indexed 0, . . . , n, with intensity functions 0, . . . , n such that
( |H ; ) =
i=0 i( |yi; ). Each Poisson process represents one source of seis-
micity: the background events, those triggered by the first earthquake, by the second
earthquake and so on. The intensities of these Poisson processes, 0 to n, are defined
i( |H ; ) =


 if i = 0 and   0,
(mi; )h(  ti|ti; h) if i  {1, . . . , n} and  > ti,
0 otherwise.
(6.2.3)
Under this interpretation, the branching vector B specifies to which Poisson compo-
nent of the ETAS process each event belongs. Given B, the full ETAS point pro-
cess is simply the superposition of independent Poisson processes and the conditional
log-likelihood given B is the sum of Poisson process log-likelihoods. By defining
y0 = (t0,m0) = (0, 0) and letting ybi = (tbi ,mbi) denote the parent of event i, the
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION137
conditional ETAS log-likelihood given B is:
log Y |,B(y)
log bi(ti; )
 tmax
i()d
{log (mbi |)h(ti  tbi |h)}  tmax 
{(mi|)H(tmax  ti|h)}. (6.2.4)
The conditional log-likelihood (6.2.4) no longer contains a double summation and so
is much less costly to evaluate than the full likelihood (6.2.1). This reduced com-
putational cost motivates a conditional approach to estimation of |Y . This can be
done by using a Metropolis-within-Gibbs sampler to alternately draw samples from
the conditional posteriors B|Y,(b) and |Y,B() at the present value of the other pa-
rameter set. Implementing this sampler then requires a tractable form for B|Y,(b).
This distribution has a particularly simple form if the priors on each Bi are pair-wise
independent and discrete uniform on their respective supports. This choice of prior
can be motivated by the inter-event times and magnitudes being unknown a priori.
In that case:
B|Y,(b) =
Bi|Y,(bi) where Bi|Y,(bi) =
bi(ti;Hti, )i1
j=0 j(ti;Hti, )
. (6.2.5)
6.2.3 Benefits of the latent estimation approach
The computational benefits of the latent estimation approach were demonstrated
empirically by Ross (2016). Here, we make some further comments to clarify the
source of this improvement.
To sample values from |Y (), the direct estimation procedure requires one O(n
evaluation of the full log-likelihood at each step in the Markov chain. The latent
estimation procedure requires, at each step in the Markov chain, one evaluation of
the conditional likelihood |Y,B() and one sample from B|Y,(b). From expression
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION138
(6.2.4), it is evident that evaluating |Y,B() is an O(n) operation. To draw the
full vector B from this conditional posterior, we must sample each of its n elements.
Sampling the element Bi draws a sample from a discrete distribution with i possible
outcomes; this is an O(i) operation (Walker, 1977) that must be done for each of
the n earthquakes. Sampling the full vector B is therefore an O(log(n)) operation.
Therefore, for large n, sampling the branching vector from B|Y,(b) is the most costly
operation when obtaining each sample from the posterior distribution of . Sam-
pling from ,B|Y (, b) by the latent variable method is therefore also an O(log(n))
operation, which has been reduced from O(n2) by the direct method.
This conditional estimation approach has two further benefits. The first is that sam-
pling B alongside  provides a stochastic declustering of the catalogue as part of
the model fitting process. This declustering allows the branching structure to be
estimated while fully accounting for parameter uncertainty. Being able to estimate
the branching structure then permits generalisations of the ETAS model that are
based on its representation as a branching process. The second benefit of the ap-
proach is that parameter vector  can be decomposed into the near-orthogonal blocks
 = (, , h) that may be updated separately (Ross, 2016). These blocks are exactly
orthogonal if all aftershock activity is within the observation period, meaning that
H(tmax   |h) = 1 for all   [0, tmax) (Schoenberg, 2013).
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION139
6.3 Extreme value reparameterisation of empirical
6.3.1 Empirical laws
The functions , h and f are usually chosen to coincide with empirical laws from
the seismology literature (Hainzl and Christophersen, 2017). The productivity func-
tion (mi;K, a), is frequently chosen to be an exponentially increasing function of
magnitude above some fixed threshold magnitude u:
(mi;K, a) = K exp{a(mi  u)}Imi>u, (6.3.1)
where K, a  0 and IA is an indicator function of the event A. The threshold u is
typically taken to be the magnitude of completion, above which all earthquakes are
recorded in the catalogue. The modified-Omori law is typically used for the aftershock
delay distribution h( ; c, p). This is a heavy tailed, power-law distribution that decays
more slowly than an exponential distribution:
h( ; c, p) = (p 1)cp1
I>0, (6.3.2)
where c > 0 and p > 1. The (truncated) Gutenberg-Richter law is used for the mag-
nitude distribution f(m; ). This is a shifted and truncated exponential distribution
with rate parameter  > 0. The support of the distribution is truncated to be in the
range (mmin,mmax) where mmax  , resulting in the density:
f(m; ) =
 exp{(mmmin)}
1 exp{(mmax mmin)}
Imminmmmax . (6.3.3)
There are two main issues that arise from using these empirical laws in the point
process intensity model (6.1.1).
When using the modified Omori law (6.3.2), the delay distribution is constrained to
be heavy tailed. This means that each event influences the intensity at all future times
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION140
and that this influence decays very slowly. The modified-Omori law is restrictive in
that it does not allow the influence to decay rapidly or to have a finite extent. The
second issue with using this empirical law is that its heavy-tailed nature can lead
to a large proportion of the intensity from each event being placed outside of the
observation window. This increases dependence between the ETAS parameter blocks
{}, {K, a} and {c, p} and makes inference more challenging.
There are also issues with the choice of the Gutenberg-Richter magnitude model.
Firstly, if mmax = then the fitted magnitude distribution will always be unbounded,
which is not physically reasonable. Secondly, if mmax <  then there exists some
greatest possible magnitude but the density is discontinuous there, because the distri-
bution is constrained to have an exponential form. This form of constraint has been
shown in other environmental applications to result in underestimation of the severity
of the largest events (Coles et al., 2003). When a maximum magnitude is used, it is
estimated through a separate extrapolation and the point estimate is used, ignoring
crucial uncertainty about the events with greatest potential to cause damage (Kijko
and Singh, 2011; Beirlant et al., 2019).
6.3.2 Proposed parameterisation
We propose alternative parametric forms for the functions , h, and f , which maintain
or extend the class of functions which can be represented by the model. The proposed
forms remove the restrictions placed on the shape and upper endpoint of both the
delay and magnitude distributions, while dramatically reducing dependence within
parameter blocks.
Productivity function. The productivity function  links the magnitude of an
event to the expected number of additional events that it triggers. In the latent
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION141
approach to inference, the parameters of  are estimated conditional on knowing the
branching vector B. As discussed in Section 6.2, when B is known the ETAS process
can be decomposed into n + 1 independent, inhomogeneous Poisson processes. The
observed event counts of these processes, {Ni = |Ci| : i = 0, . . . , n}, are independent
observations from a Poisson generalised linear model, where the linear predictor is a
function of the parent magnitude.
With this interpretation, the productivity function in expression (6.3.1) can be rewrit-
ten as the generalised linear model where the parameters K and a are to be estimated:
Ni  Poisson((mi)), where
(m;K, a) = elog(K)+a(mm0) = e(log(K)am0)+am. (6.3.4)
This view of the productivity function highlights that the effect of magnitude on
productivity is described relative to the threshold magnitude m0. The intercept term
in model (6.3.4) involves both K and a, which can result in strong dependence between
these parameters and poor mixing when MCMC is used to sample from their joint
posterior. To alleviate this issue, we can centre the magnitude effect at the mean
observed magnitude m:
(mi;K, a) = e
log(C)+a(mim), (6.3.5)
where C = K exp{a(mm0) > 0 and a > 0.
In model (6.3.5) the interpretation of a is unchanged from the previous model but the
parameter C now represents the expected aftershock productivity of a mean magni-
tude event. The parameters C and a have distinct interpretations in relation to the
observed counts. This reparameterisation reduces dependence between the parameters
in the same way as centring a linear model and should lead to more efficient MCMC
sampling behaviour. One slight disadvantage is that the interpretation of C is now
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION142
catalogue dependent. However, sampled parameter values may be back-transformed
onto the original parameter space to avoid this issue.
Delay distribution and Magnitude distribution. For both the delay distribu-
tion and the magnitude distribution, we propose the use of the generalised Pareto
distribution. This two parameter distribution is commonly used in extreme value
theory as an asymptotically motivated model for the distribution of exceedances of a
threshold u (Coles, 2001). Under mild regularity conditions, the density function of
a random variable (X  u|X  u)  GPDu(, ) is given by:
fXu|X>u(x u;, ) =


(xu)
 6= 0, x  u;
 = 0, x  u;
0 otherwise;
(6.3.6)
where x+ = max(x, 0). The shape parameter   R controls the tail decay behaviour
while the scale parameter  > 0 describes the typical size of excesses. When  > 0,
the distribution is a heavy tailed power-law distribution, when  = 0 the distribution
is exponential, and when  < 0 the distribution has a finite upper endpoint at xmax =
. Expression (6.3.6) gives the standard parameterisation of the generalised Pareto
distribution, in which the parameters  and  are correlated. Chavez-Demoulin and
Davison (2005) introduced a parameterisation of the generalised Pareto distribution
using the alternative scale parameter  = 
fXu|X>u(x u; , ) =


(1+)(xu)
 6= 0, x  u;
 = 0, x  u;
0 otherwise.
(6.3.7)
Under this parameterisation, the parameters  and  are orthogonal when  > 1/2
and if  < 0, the upper endpoint of the distribution is xmax = u [(1 + )]1.
The modified-Omori and Gutenberg-Richter laws of models (6.3.2) and (6.3.3) can
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION143
each be seen as restricted forms of the generalised Pareto distribution with thresholds
u = 0 and parameters (t, t) and (m, m), respectively. The modified-Omori law of
aftershock delay times is a power-law distribution and so is equivalent to the distribu-
tion GPD0(t =
p1(1 +
p1), t =
p1), constrained so that t > 0. Similarly, when
mmax = , the Gutenberg-Richter law is a special case of the GPD0(m = 1 , m)
distribution constrained so that m = 0. In the case where m < 0 the GPD provides
a similar model to the truncated Gutenberg-Richter law, but with the added benefit
of the density function being continuous at mmax.
The previously described empirical law models are all nested within the generalised
Pareto model. This means that using a generalised Pareto distribution in place of
each empirical law will allow a broader class of models to be represented and better
represent epistemic uncertainty. Additionally, the orthogonal representation of the
generalised Pareto distribution (6.3.7) is likely to improve inference because of more
efficient MCMC sampling. The empirical laws being nested within the GPD model
allows this to be demonstrated by imposing parameter restraints on the the more flex-
ible model and comparing the sampling efficiency under each parameterisation.
6.3.3 Comparing parameterisations
In this section, we compare the properties of MCMC chains for estimating the ETAS
model parameters under two parameterisations. As described in the previous section,
the conventional parameterisation of the ETAS model conv = (,K, a, c, p) is based
on empirical laws, uses a power-law distribution for the aftershock delay distribution
and describes the effect of magnitude on aftershock productivity relative to a threshold
magnitude. The novel proposed parameterisation prop = (,C, a, t, t) centres the
effect of magnitude on productivity at the mean magnitude and uses the encompassing
generalised Pareto distribution for aftershock delay distribution. To compare the
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION144
models fairly, the parameters of the GPD are constrained so that t > 0. This ensures
that the two parameterisations represent the same class of models, namely those with
heavy tailed waiting-time distributions. By imposing this restriction there is a one-
to-one mapping g between the two parameterisations so that
prop = g(conv) =
, K exp{a(mm0)}, a,
conv = g
1(prop) =
, C exp{a(m0  m)}, a,
t(1 + t)
, 1 +
We base our comparison of the two parameterisations on two properties of the MCMC
chain: recovery of the the true parameter values from a simulated catalogue and
efficient exploration of the parameter space. Effective parameter recovery is indicated
by the posterior mode being close to the true parameter values and the posterior
being concentrated around that true value. Efficient exploration of the parameter
space by an MCMC chain is indicated by a moderate acceptance probability during
Metropolis steps and sampled parameter values having auto-correlation functions that
decay quickly as the lag increases.
Parameter recovery can be evaluated visually by overlaying true parameter values on
contour plots of the univariate or bivariate marginal posteriors of each parameter.
Good parameter recovery is then indicated by the combination of tightly spaced con-
tours and the posterior probability density being high at the true value. This can be
summarised numerically by using the mean squared error (MSE) across all sampled
parameter values from their joint posterior distribution.
The exploration of the parameter space or mixing of an MCMC chain can be mea-
sured using the effective sample size of each parameter chain. The sampled parame-
ter values obtained using a Metropolis-Hastings MCMC scheme are usually positively
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION145
auto-correlated; a chain of s sampled values therefore conveys less information about
the posterior distribution than would s independent samples from that posterior.
The effective sample size gives the number of independent samples to which the auto-
correlated MCMC chain is equivalent and a greater effective sample size is therefore
preferable (OHagan and Forster, 2004).
Let i denote the s-vector of sampled values for the i
th parameter of the ETAS model
under a particular parameterisation and represent the lag-j sample auto-correlation
of i by acf(i, j). The effective sample size of the vector i is then defined to be
ESS(i) =
1 + 2
j=1 acf(i, j)
where in this expression k is the lag beyond which the sum of all auto-correlations
is negligible. The MSE and ESS are useful tools for comparing two MCMC schemes
on the same parameter space but are not particularly useful when comparing MCMC
schemes that are implemented on separate parameter spaces. In particular, it is not
meaningful to compare the MSE for posterior samples of conv and prop because these
distances are calculated on different parameter spaces. To avoid this issue, both chains
can be transformed onto a single parameter space and the ESS can then be compared.
When proposing a new parameterisation of an existing model, the comparison is often
made on the parameter space of the original model by applying g1 to posterior
samples of prop. However, when the mapping onto the original parameter space is
non-linear, this transformation can alter the auto-correlation of the transformed chain
and the resulting ESS of transformed chain.
To address the above issues we begin by comparing the chains on both the conven-
tional and proposed parameter spaces. We additionally compare the chains using a
third, more meaningful representation. This representation is a collection of physical
properties of the earthquake process that would be measurable were the branching
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION146
structure known. We do this because when fitting the ETAS models it is usually
physical proprieties such as these that are of interest, rather than the model param-
eters themselves. We consider the recovery and mixing of the collection of physical
properties described in the following paragraph, each of which can be calculated from
a vector of sampled parameter values under either parameterisation.
The first property we use is already present in each parameterisation: the seeding rate
. The second property we consider is h, the distribution of aftershock delay times.
We investigate h through its 10th, 50th and 90th percentiles, which we denote by h10, h50
and h90. The third property we consider is the magnitude-productivity relationship,
. We consider this relationship through the expected number of aftershocks triggered
by events with magnitudes equal to the 10th, 50th and 90th sample percentiles. We
denote these expected aftershock counts by 10, 50 and 90. This gives a vector of
physically meaningful properties that we would like to be able to recover from an
observed catalogue: phys = (, h10, h50, h90, 10, 50, 90).
6.3.4 Demonstration on simulated catalogue
6.3.4.1 Description of simulated catalogue
To compare parameterisations, we consider a simulated earthquake catalogue on the
time window [0, 2000] and magnitude range [3,). This catalogue has the con-
ventional ETAS parameters conv = (,K, a, c, p) = (0.2, 0.2, 1.5, 0.5, 2) and mag-
nitude parameters  = (m, m) = (0.42, 0). The catalogue has n = 863 events
and is displayed in Figure 6.3.1. The mean magnitude of the simulated catalogue
is m = 3.43 and so the simulation parameters in the proposed parameterisation are
prop = (,C, a, t, t) = (0.2, 0.38, 1.5, 1, 1). The model parameters were estimated
from this catalogue using a Metropolis-within-Gibbs sampling scheme as described in
Section 6.3 with independent flat priors and each of the conv and prop parameterisa-
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION147
0 500 1000 1500 2000
Figure 6.3.1: Simulated earthquake catalogue of 863 events used to compare conv and
prop parameterisations of the ETAS model.
tions. In this section, we compare the resulting chains of sampled ETAS parameters
and branching vectors from each parameterisation of the model.
6.3.4.2 Estimation of ETAS parameters
Marginal distributions. Figure 6.3.2 summarises, for each parameterisation conv
and prop, the sampled values from the approximate joint posterior of the ETAS
parameters. The plots on the diagonal show histograms of the univariate marginal
posteriors, while the off-diagonal plots give contour plots and correlations for the pair-
wise marginal posterior distributions, where orange dots indicate the true parameter
values. These plots reveal several important points.
Firstly, from the contour plots of Figure 6.3.2, we see that by using the prop param-
eterisation the posteriors are better conditioned: the contours of pair-wise marginal
distributions are elliptical around the modes and the posterior distributions are less
concentrated at the edges of the parameter space. This is particularly apparent for the
parameters of the delay distribution h. Secondly, the correlation within the parameter
blocks controlling  and h are both reduced by the reparameterisation. The corre-
lations between the parameter blocks {, , h} have been increased, but recall that
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION148
0.16 0.20 0.24
0.15 0.25
0.057
1.2 1.4 1.6 1.8
0 2 4 6 8
2 4 6 8 10
(a) conv parameterisation
0.14 0.20 0.26
0.25 0.40 0.55
0.041
1.2 1.4 1.6 1.8
0.00074
0.5 1.5 2.5
0.0 1.0 2.0 3.0
(b) prop parameterisation
Figure 6.3.2: Contour plots of pair-wise marginal posterior distributions of ETAS
parameters  using the conv and prop parameterisations. True parameter values are
indicated by orange dots.
these blocks of parameters are conditionally independent given the branching vector.
Since these parameter blocks are updated conditional on both the other blocks and
the branching vector, this should not impede effective inference. Finally, in the conv
parameterisation the posterior modes of the background event rate  and the produc-
tivity intercept parameter K are misaligned with the true values; respectively these
parameters are over- and under-estimated relative to their true values. This issue
appears to be reduced for  and C when the prop parameterisation of is used.
Within-block parameter recovery. Figure 6.3.3 focuses on the posterior distri-
butions of  and h under each parameterisation. As discussed in Section 6.3.3, each
posterior is shown on both its original parameter space and transformed onto the
alternative parameter space.
Under both parameterisations the intercept, a, and gradient terms, K or C, of the
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION149
0.2 0.3 0.4 0.5 0.6
0.10 0.15 0.20 0.25 0.30
(a) Productivity function parameters, .
0.0 0.5 1.0 1.5 2.0
0.0 0.5 1.0 1.5 2.0
(b) Delay distribution parameters, h.
Figure 6.3.3: Estimated posterior distributions using samples from the prop (orange)
and conv (blue) parameterisations. Both parameter spaces are shown and true values
are given by black crosses.
magnitude-productivity relationship  are overestimated. This suggests that this is a
feature specific to this simulated catalogue. On both parameter spaces, the contours
of  are wider and have a less steeply angled major axis for the prop chain than for
the conv chain. These properties reflect, respectively, the better exploration of the
parameter space of a and the reduced dependence between parameters when using
the prop chain. This results in the the posterior density being greater at the true
value for the prop chain than the conv chain in both parameter spaces.
The parameters of the aftershock delay distribution h are poorly recovered by the
conv chain but successfully by the prop chain, which has a posterior mode close to
the true values in each parameter space. Each chain appears to explore its native
parameter space more effectively than the transformed space, but the conv chain does
not well-explore heavy-tailed delay distributions (which correspond to large values of
t or small values of p) as effectively as the prop chain does in either space. This
could be related to the over-estimation of the background rate  by the conv chain:
long-term aftershock activity is being attributed to the background process because
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION150
RRMSE  a K C c p t t h10 h50 h90 10 50 90
conv 117 71 143 112 2954 1066 181 515 202 130 442 9 11 18
prop 99 71 136 110 835 300 253 435 166 245 202 8 10 21
Table 6.3.1: Element-wise relative root mean squared errors (103) of conv, prop and
phys for MCMC chains on the prop and prop parameter spaces.
heavy-tailed delay distributions are not being properly explored.
To compare parameter recovery across the joint distribution of all parameters, we
calculate the relative root mean squared errors (RRMSE) of each chain on each pa-
rameter space. If {1, . . . , m} is a set of m sampled s-dimensional parameter vectors
from the posterior of , given data simulated with true parameter values , then the
RRMSE is given by:
RRMSE({1, . . . , m}, ) =
 1
i  i
This metric gives a combined measure describing how close to, and concentrated
about, the true parameter values are the sampled parameter vectors. Smaller values
of the metric therefore represent more accurate and precise parameter recovery. On
the conv parameter space, the conv chain had a RRMSE of 4.350 and the prop chain
had a RRMSE of 1.442. On the prop parameter space, the conv chain had a RRMSE
of 0.995 and the prop chain had a RRMSE of 0.969. In each parameter space the
RRMSE is smaller for the prop chain and so we conclude that overall, this chain is
better able to recover the simulation parameters. The RRMSE values are decomposed
into the contributions from each parameter in Table 6.3.1, from which it appears that
most benefit comes from the improved estimation of the delay distribution h.
In addition to recovering the underlying model parameters, we would also like to
be able to recover the physical quantities phys, which would be measurable if the
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION151
0.04 0.06 0.08 0.10
0.4 0.6 0.8 1.0
2 4 6 8 10 12 14
0.15 0.20 0.25 0.30
0.20 0.25 0.30 0.35 0.40
0.6 0.7 0.8 0.9 1.0 1.1
Figure 6.3.4: Posterior distributions of physically meaningful quantities phys using
estimated using samples from the prop (orange) and conv (blue) parameterisations.
Upper plots show the 10th, 50th and 90th percentiles of the aftershock delay distribu-
tion. Lower plots show expected aftershock count at the 10th, 50th and 90th percentiles
of the empirical magnitude distribution. True quantiles shown as dashed lines.
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION152
ESS  a K C c p t t h10 h50 h90 10 50 90
conv 1173 2451 1820 1573 516 478 551 413 1994 1103 419 1800 1635 1198
prop 462 2212 827 593 813 622 343 372 2376 394 471 798 643 449
Table 6.3.2: Element-wise effective sample sizes of conv, prop and phys for MCMC
chains of 10,000 sampled values on the prop and prop parameter spaces.
true branching vector was known. Figure 6.3.4 shows the posterior distributions for
quantiles of  and h, based on the conv and phys chains, with the true values overlaid.
The posterior modes for the prop chain are closer to the true phys values and, for all
but the median of h, the posterior density is greater at the true value for the prop
chain. For all but h10, the prop posteriors have heavier right tails indicating that they
better explore less optimistic regions of the parameter space, where triggered events
form a greater proportion of the catalogue and can occur much later than their parent
event. These effects are combined in the RRMSE values for each quantile, which are
given in Table 6.3.1. According to the RRMSE values, the prop chain better recovers
all of the physical properties phys except for h50 and 90.
Effective sample sizes. The effective sample size of each parameter in each pa-
rameter space is given in Table 6.3.2. For all parameters and all quantities, except
h10 and h90, the effective sample size of the conv chain exceeds that of the prop chain.
This is surprising given that, based on the marginal plots considered previously, it ap-
peared that the prop chain was better exploring most parameter and quantile spaces.
This might be explained by the ESS being a measure of local dependence within the
chains; the conv chain has weaker local dependence but this does not ensure the chain
moves around the entire parameter space efficiently. The low ESS relative to chain
lengths highlights the difficulty in efficiently sampling from the joint posterior of the
ETAS parameters.
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION153
6.3.4.3 Branching vector recovery and mixing.
In addition to facilitating the estimation of the ETAS parameters , the conditional
approach to inference also allows the branching structure B to be estimated as part
of the inference procedure. The recovery of the true branching vector and proper
representation of uncertainty in the estimated vector was not considered by Ross
(2016). In this section, we introduce plots to diagnose branching vector recovery
graphically and to assess effective mixing of the sampled chains of branching vector.
We then use these to compare the performance of the conv and prop chains.
We begin by considering the marginal posterior of a single branching vector element
Bi for a range of values of i. For a given event index i, this is a probability mass
function (pmf) giving the posterior probability that event i was triggered by each
previous earthquake, Pr(Bi = j) for j  {1, . . . , i1}, or comes from the background
process (i.e. Pr(Bi = 0)). The majority of probability mass will usually be allocated
to recent previous events or the background process because the delay function h is
monotonically decreasing over time. For large values of i there are many values of
j (which have low probability mass) separating the most likely sources of event i,
which can make reading the pmf difficult. To rectify this issue we therefore display
Pr(Bi = 0) at i instead of at 0, so that the background process is the rightmost bar
in the pmf. Figure 6.3.5 shows these posterior distributions along with trace-plots for
a selection of events using each chain. The branching elements B51, B81 and B199 are
shown, which have true values of b51 = 45, b81 = 0 and b199 = 0.
From Figure 6.3.5 we can see that some events (such as event 199) are classified as
a background event with high probability, while others (such as event 51) are more
likely to be triggered events. Additionally, events which are labelled as triggered with
a high probability might be attributable to a few recent events with high probability,
as with event 81, or this might be split more evenly between many previous events, as
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION154
Figure 6.3.5: Trace plots and posterior pmfs for branching vector elements B51, B81
and B199. The conv chain is shown on the left and the prop chain on the right. The
branching elements have true values of b51 = 45, b81 = 0 and b199 = 0.
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION155
0 200 400 600 800
Event index, i
0 200 400 600 800
Event index, i
Figure 6.3.6: Proportion of sampled Bi values equal to the true value in the conv
(left) and prop (right) chains.
with event 51. The interaction of the event magnitude and delay time can also be seen
in these plots: considering event 51, we see that the most likely parent is not always
the previous event and the triggering probability does not decrease monotonically
with lag. Comparing the plots between parameterisations, it can be seen that for all
three events, the prop chain samples a greater proportion of bi values for which |i bi|
is large. This again indicates that this chain better explores heavy-tailed distributions
for h.
Since the catalogue is simulated, we know the true branching vector and can use this
to assess the estimation of each element under each model parameterisation. To do
this, we can consider the proportion of samples for which each branching element
was correctly identified. This is shown in Figure 6.3.6. From this we can see that
the source of each event is not equally easy to identify; some events are allocated
correctly with high probability while others with very low probability. By considering
the event-wise difference in the probability of correct allocation we can compare the
two parameterisations; the correct parent is chosen on average 2% more often by the
conv chain, suggesting that this better recovers the true branching structure.
We can also assess the mixing of the branching vector chains by looking at the propor-
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION156
0 200 400 600 800
event index, i
0 200 400 600 800
event index, i
Figure 6.3.7: Proportion of updates at which each branching vector element was
unchanged for the conv chain (left) and prop chain (right). Branching vector elements
that change on less than 5% of updates are highlighted in red.
tion of updates of each branching element at which its value was unchanged. A high
proportion of unchanged values indicates that either the chain is not mixing well or
that the event is clearly attributable to a single source. Such events are highlighted in
red in Figure 6.3.7, which shows the proportion of updates for which each branching
element is unchanged for each of the conv and prop chains. By comparing these we
can identify some common structures that are features of the data, such as the events
around index 250 that cannot be attributed to a single source with high probability.
This is contrasted by the number of highlighted elements, which is much lower for
the prop chain. This indicates that many of the elements of B were not mixing well
using the conv parameterisation, rather than it being the case that these events have
a clear source.
Summary. Combining the previous findings we can see that the conv chain recovers
the true branching structure well. However, the chain does not mix as well as the prop
chain and does not reflect the full uncertainty about the branching structure. In this
way it suffers, to a lesser extent, from the limitations of a deterministic declustering
that were discussed in Section 6.1. This is likely linked to the chain not exploring
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION157
heavy-tailed aftershock delay distributions well. If heavy-tailed delay distributions are
not being explored, then a greater proportion of probability mass will be allocated
to branching elements corresponding to recent events or the background process.
This in turn leads to overconfidence about which previous event was the parent event,
overestimation of , and the consequent underestimation of K because of the negative
correlation between these two parameters. This means that conclusions based on the
conv parameterisation risk under representing the possibility of long-term dependence
in earthquake occurrences rates and overestimate the baseline level of seismicity.
Note that the conclusions here are based on a catalogue for which the delay distri-
bution was truly a power-law distribution (t > 0). This class of distribution can
be represented by both parameterisations and the less general conv parameterisation
can only represent these heavy-tailed distributions. The proposed parameterisation
was constrained to this class of models to facilitate comparison. More generally the
proposed parameterisation is also able to represent delay distributions that are expo-
nentially decreasing over time or that have a finite upper end point (where t  0).
We have seen that the restricted prop parameterisation better explores the space of
heavy-tailed models. Further to this, the unconstrained prop parameterisation is able
to account for models that cannot be represented in the conventional framework. The
unconstrained model would therefore represent model uncertainty more comprehen-
sively in resulting the conclusions.
6.4 Extensions of the magnitude model
6.4.1 Dual magnitude extension
In the standard formulation of the ETAS model, introduced in Section 6.1, the marks
associated with each event are assumed to be i.i.d. with a common probability density
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION158
function f(m;). Our first proposed extension to the magnitude model is to allow
marks to be drawn independently from one of two possible distributions. Events from
the background process have mark distribution f0(m;0); these are the events that
are attributable to the intensity component 0(t) and whose corresponding branching
vector element is zero. All remaining events are triggered; they are attributable to
one of the previous events and have a separate mark distribution that we denote by
f1(m;1). The combined vector of magnitude parameters is now given by  = (0, 1)
and must be estimated from the observed, marked point pattern together with the
ETAS parameters and branching vector. Estimation of these parameters is simple
when the branching vector B is known, since the distribution to which each magnitude
belongs is also known.
In the i.i.d. mark formulation of the ETAS model, the mark distribution may be
estimated independently of the intensity parameters  and branching vector B. To fit
the dual magnitude model, the magnitude parameters must be estimated jointly with
the intensity parameters and the branching vector as 0 and 1 are not identifiable
without knowledge of B. We therefore extend the Metropolis-within-Gibbs sampling
scheme for the standard formulation to include the parameter blocks 0 and 1;
each block of parameters {, , h, B1, . . . , Bn, 0, 1} is updated in each iteration
conditional on the values of all other blocks. Assuming independent, flat priors on the
magnitude parameter blocks, the conditional posterior of the magnitude parameters
(|Y, , B) =
i:bi=0
f0(mi|0)
i:bi>0
f1(mi|1). (6.4.1)
Under the dual magnitude model, the conditional log-likelihood (and therefore the
conditional posterior) of the ETAS intensity parameters  in expression (6.2.4) remains
unchanged. With dual magnitude distributions, the elements of the branching vector
B remain independent of one another, but the conditional posteriors are changed from
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION159
those in expression (6.2.5). This is because the observed magnitude mi now carries
information on whether Bi = 0 or Bi > 0. This modification leads to the magnitude-
based weights in the conditional posterior of each branching vector element (6.4.2).
These weights impart the additional information carried by the magnitudes and are
obtained in Appendix C.1.
Pr(Bi = bi|Y, , ) =
i  fmin(1,bi)(mi|)
f0(mi|0) + (i 1)f1(mi|1)
bi(ti|Hti, ti, )i1
j=0 j(ti|Hti, ti, )
. (6.4.2)
This dual magnitude model permits simple testing of the hypothesis that background
and triggered events have different mark distributions through the use of Bayes factors.
When f0 and f1 are nested, this becomes particularly simple because the standard
ETAS formulation is then nested within the dual magnitude model. The evaluation
can then be made directly using the values sampled from the posterior distribution
of .
6.4.2 Correlated magnitude extension
Our second proposed extension to the magnitude model allows for triggered mag-
nitudes to be dependent on the magnitude of the event that triggers them. This
dependence is modelled using a bivariate copula. This extension may be applied sep-
arately or together with the dual magnitude extension. In this section we outline the
more general, combined model extension. In the following, we respectively denote
the marginal distribution and probability density functions of background event mag-
nitudes by F0(m;0) and f0(m;0), respectively. The corresponding functions for
triggered events are denoted by F1(m;1) and f1(m;1). Correlated magnitudes may
be introduced with a single distribution by fixing f0 = f1. We begin by describing the
dependence model in detail before explaining how this alters the inference procedure
from that of the dual magnitude extension. We introduce here an extension to the
ETAS model in which magnitudes are not independent and where the magnitudes of
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION160
background and triggered events have separate distributions. A first-order Markov
dependence structure is imposed on the graphical representation of the ETAS pro-
cess. Under this dependence model, the magnitudes of events in the same tree are
dependent on one another and the magnitudes of events in separate trees are inde-
pendent. Additionally, events that are in the same tree but that are not adjacent
to one another are conditionally independent given the magnitudes of their adjacent
events: their families. Formally, the family of event i = 1, . . . , n is indexed by the
set Fi = Ci = {j  {i + 1, . . . , n} : bj = i} when event i is a background event and
when event i is a triggered event then the family also includes the parent event of
event i so that Fi = Ci  {bi}.
To give a concrete example of this Markov dependence structure, we return to the toy
example in Figure 6.2.1. In this example, the magnitude of event Y2 is independent
of the magnitudes of both of Y5 and Y7 because they belong to a different tree. The
magnitudes of events Y1, Y3, Y4 and Y6 are all dependent on one another because they
are in the same tree. However, given the magnitude of Y3 is M3 = m3 the remain-
ing magnitudes are pair-wise conditionally independent. This Markov dependence
structure means that the joint density of all magnitudes may be written in terms of
only marginal densities and bivariate joint densities. For the toy example we have
f(m1, . . . ,m7|B,)
= f(m1,m3,m4,m6|B,)f(m2|B,)f(m5,m7|B,)
i{1,2,5}
{f0(mi|0)} f(m3|m1, )f(m4|m3, )f(m6|m3, )f(m7|m5, )
i{1,2,5}
{f0(mi|0)}
f(m1,m3|)
f0(m1|0)
f(m3,m4|)
f1(m3|1)
f(m3,m6|)
f1(m3|1)
f(m5,m7|)
f0(m5|0)
, (6.4.3)
where the first two equalities follow from the independence between trees and condi-
tional independence within trees. The final equality expresses the conditional densities
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION161
as the ratio of joint and marginal densities. Similarly, the conditional density of a
single magnitude may also be written using only marginal densities and bivariate joint
densities, as follows for M3 in the toy example:
f(m3|m1,m2,m4, . . . ,m7, B, ) = f1(m3|1)
jF3={1,4,6}
f(mj,m3, )
f1(m3|1)
. (6.4.4)
This Markov dependence structure can be achieved while preserving the marginal
magnitude distributions of background and triggered events by using a copula to
construct the joint distribution f(, |) of triggering and triggered magnitudes from
their marginal distributions. A d-dimensional copula C : [0, 1]d  [0, 1] is a multi-
variate distribution function with uniform margins; it is used to couple or link d  2
marginal distributions to give a joint distribution (Joe, 2014). The use of a copula al-
lows the joint density of triggered and triggering magnitudes to be represented as the
product of the marginal densities and the copula density. When d = 2 and the copula
function C is parameterised by , the copula density is given by c(u, v; ) =
2C(u,v;)
The joint density of two magnitudes Mi and Mj may therefore be expressed as
f(mi,mj|, ) = c(F (mi|), F (mj|)|)f(mi|)f(mj|),
where f(mi|) is either f0(mi|0) or f1(mi|1), depending on whether event i is a
background or triggered event, and F (mi|) is used equivalently. The conditional
distribution of Mi given Mj may be written as the product of the copula density and
a single marginal density:
f(mi|mj, , ) = c(F (mi|), F (mj|)|)f(mi|).
The joint and conditional distributions (6.4.3) and (6.4.4) may therefore be expressed
as the product of marginal densities and bivariate copula densities.
Here, we use a bivariate Gaussian copula to link the marginal distributions of triggered
and triggering magnitudes. The Gaussian copula defines the dependence between
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION162
triggering and triggered magnitudes when each is transformed to have a standard
Gaussian marginal distribution. The copula has a single parameter   [1, 1] that
defines the correlation between these transformed variables. The effect of the param-
eter  on the copula can be understood by considering three cases: if the magnitude
of a triggering event is in a high quantile of its distribution and  > 0 then the trig-
gered magnitude is likely to also be in a high quantile of its distribution; if  < 0
then the triggered magnitude is likely to be in a low quantile; finally if  = 0 then
the two magnitudes are independent. This choice of copula has the practical benefit
that associated conditional distributions can be found in closed form and with relative
ease. This copula also has the benefit of being able to represent independence, posi-
tive or negative association, or extremal dependence between large magnitude events
depending on the value of  (Ledford and Tawn, 1997).
Incorporating Markov dependence into the ETAS model using a Gaussian copula
extends the vector of magnitude parameters, which is now  = (0, 1, ). By fixing
 = 0, event magnitudes can be made independent and the correlated magnitude
ETAS model reduces to the dual magnitude extension.
Conditional posterior distribution of the magnitude parameters. To fit the
correlated magnitude model, we again extend the Metropolis-within-Gibbs sampling
scheme to include the correlation parameter . To do this, we require the conditional
posteriors under the correlated magnitude model for each of: the magnitude param-
eters, |Y,B,; the branching vector, B|Y,, and the ETAS parameters, |Y,B,. To
define these conditional posteriors, we introduce the indicator variables di = I{bi > 0}
for i = 1, . . . , n, which give the index of the magnitude distribution associated with
each event. We also define gi = 
1(Fdi(mi)) to be the magnitude of event i trans-
formed to have a standard Gaussian marginal distribution, which has distribution
function  and density function .
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION163
Appendix C.2 derives the conditional posterior of the magnitude parameter vector
when using independent flat priors on each parameter. The resulting conditional
posterior of the magnitude parameter vector is
|Y,B, =
i:bi=0
f0(mi|0)
i:bi>0
gi  gbi
1 2
f1(mi;1)
(gi)
. (6.4.5)
The joint conditional posterior in (6.4.5) allows all magnitude parameters to be up-
dated as a block using a single Metropolis step within the MCMC sampling scheme.
This block can be further decomposed to sample from the conditional posteriors of
0, 1 and  sequentially. Sampling from these smaller parameter blocks can improve
mixing in the Metropolis component of the MCMC scheme and (when particularly
simple magnitude distributions are used) may permit direct sampling from the con-
ditional posteriors.
Conditional posterior distribution of the branching vector. When event
magnitudes are i.i.d., the conditional posterior B|Y,, can be factorised into terms
corresponding to each event. The elements of the branching vector remained con-
ditionally independent with dual magnitude distributions, given the magnitudes of
each event. However, when correlation between magnitudes is introduced, elements
of the branching vector are no longer conditionally independent given only Y,  and
. However, we note that during the Metropolis-within-Gibbs sampling for the dual
magnitude model, each element of B is sampled separately. This does not require
the conditional distribution of the full branching vector, only the distribution of
Bi|Y,Bi, , ; where Bi = {Bj : j 6= i}. In a similar way as in the earlier case
when magnitudes were independent, we can find the conditional posterior of Bi but
now also conditioning on all other elements of B. This distribution has a closed form
which is given in Appendix C.3.
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION164
Conditional posterior distribution of the ETAS parameters. The conditional
distribution |Y,B, is unchanged by the introduction of correlated magnitudes. We
now have closed forms for all required distributions to implement a Metropolis-within-
Gibbs sampler for the joint posterior ,B,|Y . In this scheme each parameter block
is sampled from its conditional posteriors (either directly or using a Metropolis step)
given the current value of all other parameter blocks. The block structure for the
correlated, dual magnitude model is {, , h, 0, 1, , B1, B2, . . . , Bn}.
6.5 Application of extended ETAS models to sim-
ulated catalogues
6.5.1 Dual magnitude extension
Section 6.4.1 introduced the dual magnitude extension of the ETAS model and de-
scribed a Bayesian approach to inference for this model extension. In this section we
use a simulated catalogue with dual magnitudes to demonstrate that the proposed
inference method is able to recover the true parameter values used in the simula-
tion. We also consider a second simulated catalogue, which has i.i.d. magnitudes,
to investigate how estimation of the branching vector B and the magnitude distri-
bution f are impacted by the false assumption of either a single or dual magnitude
distribution.
6.5.1.1 Simulated catalogue with dual magnitude distribution
We consider a catalogue of earthquakes simulated on the time-magnitude interval
[0, 50000] (3,) which has true ETAS parameters prop = (, C, a, t, t) = (0.02,
0.2, 0.1, 0.1, 0.1) and magnitude parameters  = (m0, m0, m1, m1) = (0.6, 0, 0.1,
0). The catalogue contains n = 1205 earthquakes of which 239 are triggered events
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION165
0.017 0.019 0.021
0.16 0.18 0.20 0.22 0.24
0.0 0.1 0.2 0.3 0.4 0.5 0.6
0.08 0.10 0.12 0.14
0.2 0.0 0.2 0.4 0.6
Figure 6.5.1: Marginal posterior distributions of ETAS parameters prop given a dual
magnitude earthquake catalogue, estimated using a dual magnitude model (blue) or
single magnitude model (orange). True parameter values are shown as dashed lines -
note that the true value of m0 = m1 = 0.
and where the mean magnitude is m = 1.99. Both the dual magnitude and the single
magnitude ETAS models were fitted to this catalogue using the prop parameterisation,
as outlined in Section 6.4; the former allows us to demonstrate recovery of the true
parameters while the latter illustrates the bias that can occur from failure to account
for dual magnitudes.
ETAS parameter recovery. Figure 6.5.1 shows the estimated marginal posterior
distribution for each of the ETAS parameters prop. For each of the ETAS parameters
the true values are recovered well, falling within the 95% highest posterior density
regions. Differences between true and estimated values of , C and a here are due to
properties of the particular simulated catalogue and the marginal posteriors of these
parameters are robust to mis-specification of the magnitude model. The parameters of
the delay distribution, t and t, are also well recovered from the simulated catalogue
by both chains, but their marginal posterior distributions are more sensitive to mis-
specification of the magnitude model.
Magnitude distribution recovery. Figure 6.5.2 shows the marginal and joint
posterior distributions of the magnitude parameters estimated using a dual or single
magnitude model along with the true values; for the dual magnitude model these
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION166
have been separated by event type. From these plots, we can see that the magnitude
parameters for both the background and triggered events have been successfully re-
covered, as they are close to the posterior modes. We can also see that by falsely
assuming a single magnitude distribution, the single scale parameter is estimated to
be between those of the background and triggered distributions, while the single shape
parameter is greater than that of either component distribution. Since the posteriors
for background and triggered event magnitude parameters have very little overlap,
we can conclude from these plots that the dual magnitude model is the more ap-
propriate for this catalogue. While it is reassuring that we are able to recover the
magnitude distribution of each event type, the aim of modelling is typically to es-
timate the combined distribution of all events. This combined distribution depends
on each component magnitude distribution and also on the proportion of each event
type.
In order to compare fitted and observed magnitude distributions we use return level
plots for background, triggered and all magnitudes. The r-event magnitude return
level mr (with an associated return period of r events) is the quantile of the magnitude
distribution that is exceeded in expectation once every r events. A return level plot
shows, typically on a log-log scale, the return level against the return period. A
return level plot allows assessment of model fit which emphasises the most extreme
magnitudes by overlaying the fitted, empirical and (for simulated data) true return
levels.
Figure 6.5.3 shows return level plots for the background event magnitudes, triggered
event magnitudes and the combined magnitude distribution. Posterior mean and
point-wise 95% credible intervals are shown for the fitted return levels along with the
empirical values. Note that the empirical return levels for background and triggered
events are obtained using the true branching vector, which is known only because
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION167
0.0 0.2 0.4 0.6 0.8
0.3 0.1 0.1 0.3
0.0 0.2 0.4 0.6
Figure 6.5.2: Marginal (left and centre) and joint (right) posterior distributions of
magnitude parameters based on a simulated dual magnitude earthquake catalogue.
Distributions are shown by event type under a dual or single magnitude model: dis-
tributions shown in black and red correspond to background and triggered event
magnitude parameters in the dual magnitude model, (m0, m0) and (m1, m1). Dis-
tributions shown in orange correspond to the mis-specified single magnitude distribu-
tion parameters (m, m). True parameter values are indicated by dashed lines / blue
points.
5 50 500 5000
return period
5 50 500 5000
return period
2 5 20 100 500 2000
return period
Figure 6.5.3: Return level plots for background magnitudes (left), triggered mag-
nitudes (centre), and all magnitudes (right). Point-wise posterior means and 95%
credible intervals for return levels are indicated by solid blue lines for the dual magni-
tude model and dashed orange lines for the single magnitude model. Empirical return
level estimates are shown as points and true return levels as dotted lines.
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION168
this is a simulated catalogue. In recorded earthquake catalogues, only the combined
empirical distribution would be known. In these plots we can see that for the dual
magnitude posteriors, shown in blue, the empirical return levels are within the relevant
credible regions and point estimates are close to the true values. This is not the case
when a single magnitude distribution is falsely assumed. In that case, the fitted return
levels of the combined magnitude distribution, shown in orange, are overestimated for
events with long return periods; assuming a single magnitude distribution leads to
inflation of the fitted return levels.
Inflated estimates for high return levels is not unique to this simulated catalogue,
but are to be expected when fitting a single generalised Pareto distribution to data
that truly comes from a mixture of generalised Pareto distributions. This effect can
be linked to the fact that the generalised Pareto distribution is not sum stable; the
weighted sum of two GPD random variables does not have a generalised Pareto dis-
tribution apart from in the trivial cases where all weight is allocated to one of the
variables or where the parameters of the two distributions are equal. This means that,
in general, the magnitude mixture distribution cannot be represented using the single
magnitude model. This results in the parameter biases observed in Figure 6.5.2. The
fitted scale parameter compromises between that of the background and triggered
events, depending on their proportions and to compensate for this the shape param-
eter is overestimated. Since return levels are strongly driven by the shape parameter,
particularly for long return periods, this leads us to the conclusion that falsely assum-
ing a single mark distribution will inflate the estimated return levels. This reasoning
suggests why this overestimation can be expected to occur across catalogues and gives
further motivation for the use of our dual magnitude model extension, so as to avoid
drawing overly conservative conclusions.
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION169
0.18 0.19 0.20 0.21 0.22
proportion of events triggered
0.90 0.92 0.94 0.96 0.98 1.00
proportion of B correct
Figure 6.5.4: Posterior proportions of triggered events (left) and correctly allocated
elements of B (right), based the dual magnitude (blue) and single magnitude (orange)
models. Dashed lines represent perfect recovery.
Branching vector recovery. Figure 6.5.4 shows, for both a single and dual magni-
tude model, the posterior distributions for the proportion of triggered events and for
the proportion of elements of B that are correctly recovered. This simulated catalogue
has a low seeding rate and so has relatively few overlapping aftershock sequences. This
means that a large proportion of events are correctly identified with high probability
under both models - even when a single magnitude distribution is falsely assumed.
Despite this, both properties of the branching vector are better recovered by the dual
magnitude model, which has a posterior root mean squared error in the true pro-
portion of triggered events of 0.30%, as compared to 0.42% for the single magnitude
model. The dual magnitude model also better recovers the true proportion of trig-
gered events: the posterior proportion of B correct has both a greater expectation
and lower variance when using the dual magnitude model. We can also consider the
recovery of the branching vector through the posterior probability that each branch-
ing element is allocated correctly. For events with different probabilities under each
model, using the dual magnitude model leads to an average increase of 2.5% in the
probability of correct allocation.
In all, we may therefore conclude that when there are truly two magnitude distri-
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION170
butions, we can effectively recover the ETAS, magnitude and branching parameters
from a simulated catalogue using the proposed inference method. We have also shown
that by acknowledging the two magnitude distribution within the inference procedure
over-estimation of large return levels in the combined magnitude distribution can be
avoided and estimates the branching structure can be improved.
6.5.1.2 Simulated catalogue with i.i.d. magnitudes
We also consider a simulated catalogue which has i.i.d. magnitudes. We aim show that
when a dual magnitude model is used the parameters and distributions of magnitudes
for background and triggered events are not significantly different from either one
another or that obtained using the true, single magnitude model. We also investigate
how falsely assuming a dual magnitude model impacts the estimation of the branching
structure.
We use a catalogue that is simulated using the same parameters as in the previous
simulated catalogue, but now with a single magnitude distribution with parameters
 = (m, m) = (0.4, 0). This simulated catalogue contains n = 1206 earthquakes of
which 240 are triggered events and where the mean magnitude is m = 1.89. Again,
both the dual magnitude and single magnitude ETAS models were fitted to this
catalogue using the prop parameterisation. As can be seen in Figure 6.5.5, the ETAS
intensity parameters were again successfully recovered under each model specification
and so we focus attention on the magnitude model and the branching vector.
Magnitude distribution recovery. Figure 6.5.6 shows the marginal and joint pos-
terior distributions of the magnitude parameters estimated under each model. There
is substantial overlap between the posterior distributions of the magnitude parameters
for background and triggered events, suggesting that the additional model complexity
from allowing separate magnitude distributions is not necessary. Additionally, each
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION171
0.017 0.019 0.021
0.16 0.18 0.20 0.22 0.24
0.0 0.2 0.4 0.6 0.8 1.0
0.08 0.10 0.12 0.14
0.2 0.0 0.2 0.4 0.6
Figure 6.5.5: Marginal posterior distributions of ETAS parameters prop given an
earthquake catalogue with i.i.d. magnitudes, estimated using a dual magnitude model
(blue) or single magnitude model (orange). True parameter values are shown as
dashed lines.
of these posterior distributions has substantial overlap with that of the single magni-
tude model parameters, showing that we have still been able to recover the underlying
magnitude distribution for this simulated catalogue, despite the magnitude model be-
ing over-parameterised and unnecessarily flexible. This comparison is formalised by
Figure 6.5.7, which shows joint posteriors for the difference between parameters of
the background magnitudes (m0, m0), triggered magnitudes (m0, m0) and a single
magnitude distribution (m, m). Since the origin lies within the 95% highest poste-
rior density regions in each of these plots we have demonstrated that our ability to
recover the magnitude parameters for each event type was not impaired by the model
over-specification and that we can conclude that a single magnitude distribution is
sufficient for this catalogue.
Figure 6.5.8 shows the estimated, empirical and true magnitude return level plots
based on this single magnitude catalogue for background, triggered and all event
magnitudes. Note that the credible intervals for background and triggered magnitude
return levels would overlap if overlaid and that the point estimates and credible regions
for the combined and single magnitude return levels are very similar. These features
provide another route to the previous conclusion that a single magnitude distribution
is sufficient and is not distorted by fitting the over-specified dual magnitude ETAS
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION172
0.25 0.30 0.35 0.40 0.45
0.3 0.1 0.1 0.2 0.3
0.30 0.35 0.40 0.45
Figure 6.5.6: Marginal (left and centre) and joint (right) posterior distributions of
magnitude parameters shown by event type under a dual or single magnitude model
fitted to a catalogue with i.i.d. magnitudes. Distributions shown in black and red
correspond to background and triggered event magnitude parameters in the over-
specified dual magnitude model, (m0, m0) and (m1, m1). Distributions shown in
orange correspond to the single magnitude distribution parameters (m, m). True
parameter values are indicated by dashed lines/ blue points.
model. This is in contrast to the previous conclusion from Figure 6.5.8, where falsely
assuming a single magnitude distribution lead to overestimation of high return levels
of the combined magnitude distribution.
Branching vector recovery. In Figure 6.5.9 we again consider the posterior pro-
portion of events that are triggered and that are allocated to the correct parent
process when using a single or dual magnitude model, this time for a catalogue with
i.i.d. magnitudes. From this we can see that both models have very similar posterior
distributions: fitting a model with two magnitude distributions does not detrimentally
impact our ability to recover the branching structure. For the events which have a
different probability of correct allocation under the two models, the expected increase
in probability of correct allocation under the dual magnitude model is only 0.006%.
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION173
m0  m1
0.05 0.00 0.05 0.10 0.15
m0  m
0.02 0.00 0.02 0.04
m1  m
0.10 0.05 0.00 0.05
Figure 6.5.7: Joint posterior distributions of the difference in magnitude parameters
for: (left) background and triggered events in the dual magnitude model, (centre)
background events in the dual magnitude model and all events in the single magnitude
model, (right) triggered events in the dual magnitude model and all events in the single
magnitude model. 95% highest posterior density regions are shown in red and the
origin is shown as a blue dots.
5 50 500 5000
return period
5 50 500 5000
return period
2 5 20 100 500 2000
return period
  
  
  
Figure 6.5.8: Return level plots for background magnitudes (left), triggered magni-
tudes (center), and all magnitudes (right) fitted using a catalogue with i.i.d. mag-
nitudes. Point-wise posterior means and 95% credible intervals for return levels are
indicated by lines: shown in solid blue for the dual magnitude model and dashed
orange for the single magnitude model. Empirical return levels are shown as points
and true return levels as dotted lines.
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION174
0.18 0.19 0.20 0.21 0.22
proportion of events triggered
0.90 0.92 0.94 0.96 0.98 1.00
proportion of B correct
Figure 6.5.9: Posterior proportions of triggered events (left) and correctly allocated
elements of B (right), based the dual magnitude (blue) and single magnitude (orange)
models and a simulated catalogue with i.i.d. magnitudes. Dashed lines represent
perfect recovery.
6.5.1.3 Summary
In this section we have demonstrated that our proposed inference procedure for the
dual magnitude ETAS model is capable of recovering the ETAS, magnitude and
branching parameters the proposed model extension. We have also shown that falsely
assuming a single magnitude distribution inflated the estimated return levels for our
simulated catalogue and have provided an outline argument for why this can be ex-
pected to occur more generally. We have shown that, conversely, the fitted magnitude
distribution is robust to the false assumption of a dual magnitude model and also that
simple testing procedures may be used to select between the dual and single magnitude
ETAS models. In this case we used highest density posterior regions to inform our
model selection but alternative approaches may have been used, for example Bayes
factors or Bayesian model averaging (Gelman et al., 2013). Using these alternative ap-
proaches is less straightforward due to the conditional approach to inference; sampled
joint posterior values are required but only conditional posterior values are calculated
as part of the MCMC sampling scheme.
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION175
6.5.2 Correlated magnitude extension
Section 6.4.2 introduced an extension to the ETAS model in which the magnitudes of
triggered events were allowed to depend on the triggering event magnitude through
the introduction of a Gaussian copula. In this section, we demonstrate that the
inference methods presented in Section 6.4.2 for the correlated ETAS model are able
to recover the true values of the ETAS parameters, the magnitude parameters and the
branching vector. We show that this is the case for a catalogue with either correlated
or independent magnitudes. We also investigate how estimation of the branching
vector B and the model parameters  and  are impacted when magnitudes are
assumed falsely to be independent.
In this section we use two simulated earthquake catalogues. The first simulated cat-
alogue has independent magnitudes, so that  = 0, while the second has positive
magnitude dependence with  = 0.6. We shall subsequently refer to these as the inde-
pendent catalogue and the correlated catalogue. Both catalogues are simulated using
the same ETAS parameters prop = (,C, a, t, t) = (0.02, 0.21, 0.1, 0.1, 0) and dual
marginal magnitude distributions with parameters  \ {} = (m0, m0, m1, m1) =
(0.6, 0, 0.1, 0). Both the independent catalogue and correlated catalogue are simulated
on the time-magnitude interval [0, 50000] (1.5,).
The dual-magnitude and correlated-magnitude ETAS models are both fitted to each
of simulated catalogues using the conditional Bayesian inference approach described
in Section 6.4.2. The remainder of this section compares the resulting estimates of
the model parameters and branching vector.
6.5.2.1 Simulated catalogue with dual, independent magnitudes
ETAS and marginal magnitude parameter recovery. Figure 6.5.10 shows the
marginal posterior distributions of the ETAS parameters prop given the indepen-
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION176
dent catalogue, obtained using the dual and correlated ETAS models. Figure 6.5.11
shows equivalent plots for the magnitude parameters . From these plots we can see
that both the dual magnitude model and the correlated magnitude model are able
to recover the ETAS parameters and the parameters of the marginal magnitude dis-
tributions for background and triggered events. The posterior distributions are very
similar under each model, which suggests that the greater flexibility afforded by the
correlated ETAS model is not adversely impacting the estimation of the other model
parameters.
The true correlation between triggered and triggering magnitudes in this catalogue
is  = 0 and so the dual magnitude model recovers this parameter trivially. The
posterior mode of  is close to zero under the correlated ETAS model, demonstrating
that when the correlated model is applied to independent magnitudes it is capable of
recovering that independence. Additionally, the posterior distribution of  using the
correlated model is not entirely concentrated at 0, as it is under the dual model. This
indicates that models with weak dependence (e.g.   0.1) are not implausible based
on information conveyed by the independent catalogue. A risk-assessment using the
dual magnitude model makes a hard assumption of independence and would therefore
ignore all possibility of positive dependence, which can result in under-estimation of
the largest earthquake magnitudes in the region and interval under assessment. A risk
assessment based on the fitted correlated model would incorporate this possibility of
mild positive dependence and should therefore be the preferred approach.
Branching vector recovery. Figure 6.5.12 shows plots summarising the recovery
of the branching structure of the independent catalogue when using the dual and
correlated ETAS models. The posterior distribution of the proportion of the branching
vector that is correctly identified is similar under each model. Both models are able to
recover the true proportion of events, with the dual magnitude model giving a lower
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION177
0.017 0.019 0.021
0.16 0.20 0.24
0.0 0.2 0.4 0.6
0.08 0.10 0.12 0.14
0.2 0.0 0.2 0.4 0.6
Figure 6.5.10: Marginal posterior distributions of ETAS parameters prop given an
earthquake catalogue with dual magnitudes, estimated using a dual magnitude model
(blue) and correlated magnitude model (purple). True parameter values are shown
by vertical lines.
0.55 0.60 0.65
0.15 0.05 0.05 0.15
0.07 0.09 0.11 0.13
0.3 0.1 0.1 0.2 0.3
0.3 0.1 0.1 0.2
Figure 6.5.11: Marginal posterior distributions of magnitude parameters  given a
earthquake catalogue with dual magnitudes, estimated using a dual magnitude model
(blue) and correlated magnitude model (purple). True parameter values are shown
by vertical lines.
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION178
0.18 0.19 0.20 0.21 0.22
Proportion of events triggered
0.95 0.96 0.97 0.98 0.99
Proportion of B correct
Figure 6.5.12: Posterior proportions of triggered events (left) and correctly allocated
elements of B (right), based the dual magnitude (blue) and correlated magnitude (or-
ange) models for a simulated catalogue with dual magnitudes. Vertical line represent
true proportion of triggered events.
RMSE of 0.0030 compared to 0.0153 for the correlated model. This difference RMSE
is due to slight over-estimation of the proportion of triggered events by the correlated
ETAS model.
6.5.2.2 Simulated catalogue with dual, correlated magnitudes
ETAS and magnitude parameter recovery. Figure 6.5.13 shows the marginal
posterior distributions of the ETAS parameters prop given the correlated catalogue,
under the dual and correlated ETAS models. Figure 6.5.14 shows equivalent plots for
the magnitude parameters . From these plots we can see that, as for the independent
catalogue, both models are able to recover the ETAS parameters and the parameters
of the marginal magnitude distributions for background and triggered events. There
are some notable differences in the posteriors of m1 and m1 between the two models,
which we explore further using the ratio of root mean squared errors (RMSE) for each
parameter under the correlated and dual models.
The elementwise ratios of RMSEs for the magnitude parameters are (1.03, 0.93, 1.03,
0.81, 0.07). This indicates that while the marginal scale parameters are slightly better
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION179
recovered by the dual model, this is more than offset by the marginal shape parame-
ters, which are better recovered by the correlated magnitude model. The small value,
0.07, for the ratio of RMSEs for  should not be surprising; under the dual magnitude
model  is effectively fixed at zero, making it impossible to recover the true correlation
parameter  = 0.6. This should not discount the fact that the correlation parameter
is accurately recovered by the correlated ETAS model with a RMSE of 0.044. As pre-
viously mentioned, falsely assuming independence of magnitudes can lead to overly
optimistic assessments of the most extreme future magnitudes.
For the correlated catalogue, the RMSE of the marginal shape parameters is reduced
when using the correlated (as opposed to the dual) ETAS model. This is because this
model can fully utilise the additional information imparted by these correlations. To
gain an intuition for this, consider (X1, X2) following a bivariate normal distribution
with standard Gaussian margins and correlation . When the dependence between
X1 and X2 is ignored the standard deviation of X1 is 1, but when the dependence
is acknowledged this reduces to
1 2. Acknowledging correlation reduces the
standard deviation by a factor of
1 2. Additionally, (Genest et al., 1995) showed
that, when taking a step-wise approach to copula estimation by first estimating the
marginal distributions, the estimated marginal distributions will be unbiased even
when the copula is misspecified. Combining these two results, we might expect the
RMSE of the marginal parameters, which combines the bias and standard deviation
of their posterior distributions, to decrease by a factor of approximately
1 0.62 =
0.8 when dependence in the correlated catalogue is acknowledged. Since there are
two parameters in each marginal magnitude distribution, we cannot know a priori
how this reduction will be attributed between those parameters. In this example,
it appears that acknowledging dependence mainly benefits estimation of the shape
parameters.
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION180
0.017 0.019 0.021
0.16 0.18 0.20 0.22 0.24
0.0 0.2 0.4 0.6
0.08 0.10 0.12 0.14
0.2 0.0 0.2 0.4
Figure 6.5.13: Marginal posterior distributions of ETAS parameters prop given an
earthquake catalogue with correlated magnitudes, estimated using a dual magnitude
model (blue) and correlated magnitude model (purple). True parameter values are
shown by vertical lines.
0.55 0.60 0.65
0.15 0.05 0.05 0.15
0.07 0.08 0.09 0.10 0.11 0.12
0.2 0.0 0.1 0.2 0.3
1.0 0.5 0.0 0.5 1.0
Figure 6.5.14: Marginal posterior distributions of magnitude parameters  given an
earthquake catalogue with correlated magnitudes, estimated using a dual magnitude
model (blue) and correlated magnitude model (purple). True parameter values are
shown by black vertical lines.
The ratio of RMSEs for the 50th, 90th and 99th quantiles of the background and trig-
gered magnitude distributions are (0.98,1.03, 0.98) and (0.92, 0.93, 0.67) respectively.
This indicates that acknowledging the correlation makes little difference to the esti-
mation of low and moderate magnitude quantiles but greatly improves estimation of
large aftershock magnitudes.
Branching vector recovery. In Figure 6.5.15 we consider again the posterior dis-
tributions for proportion of events that are triggered and the proportion that are
allocated to the correct parent process; these distributions are examined when apply-
ing the dual or the correlated magnitude ETAS models to the correlated catalogue.
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION181
0.18 0.19 0.20 0.21 0.22
Proportion of events triggered
0.95 0.96 0.97 0.98 0.99
Proportion of B correct
Figure 6.5.15: Posterior proportions of triggered events (left) and correctly allocated
elements of B (right), based the dual magnitude (blue) and correlated magnitude
(orange) models for a simulated catalogue with correlated magnitudes. Vertical line
represent true proportion of triggered events.
Both models are able to recover the true proportion of triggered events, with the
correlated model resulting in a smaller posterior variance and reduces the RMSE by
a factor of 0.84 relative to the dual model, which falsely assumes that magnitudes are
independent. Additionally, the expectation of the posterior proportion of the branch-
ing vector correct is increased when correlation is accounted for, while the variance
of this posterior is slightly reduced. These effects combine to reduce the RMSE by a
factor of 0.88 on the posterior proportion of B correctly allocated, as compared to the
dual model. This demonstrates that the correlation between event magnitudes can
be used to better estimate the branching structure of the point process, even when
a large proportion of the branching structure can already be clearly identified when
using the overly simplified dual model.
6.5.2.3 Summary
In this section we have demonstrated that, when magnitudes are truly independent,
inference based on our correlated magnitude ETAS model is capable of identifying
this. Testing for magnitude dependence is simple and equivalent to testing  = 0
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION182
against  6= 0 under our correlated ETAS model. Additionally, we showed that fitting
the correlated ETAS model to such a catalogue (rather than the true, dual magni-
tude model) does not prevent recovery of the ETAS parameters, marginal magnitude
parameters or branching vector.
We have also shown that when magnitudes are truly dependent, our conditional
Bayesian inference for the correlated ETAS model is capable of recovering the de-
gree this dependence in addition to the ETAS, marginal magnitude and branching
parameters. When magnitudes are truly dependent and independence is falsely as-
sumed, then prop, 0 and 1 may still be well estimated. However, making this false
assumption is detrimental to the estimation of both  and B. This gives particular
cause for concern. In simulated catalogues the true values of B and  are known but in
recorded earthquake catalogues B and  are latent quantities, which must be inferred
and cannot be measured directly. This makes validation of the independence assump-
tion difficult and violations of the assumption more likely to go undetected.
Unlike the i.i.d. or dual magnitude ETAS models, the correlated ETAS model is
capable of representing the possibility of weak magnitude dependence that cannot
be excluded based the simulated dual-magnitude catalogue. Ignoring this potential
dependence in a risk analysis can to lead to underestimation of the true risk. (This is
analogous to the underestimation of risk when assuming that a marginal magnitude
distribution has a shape parameter exactly equal to zero when its credible interval
contains both positive and negative values.) An assessment of risk based on the
correlated magnitude model therefore more comprehensively reflects the epistemic
uncertainty about whether earthquake magnitudes are truly independent.
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION183
6.6 Conclusions and further work
This chapter has focused on the Epidemic Type Aftershock Sequence model, which is
used pervasively in the statistical modelling of both induced and tectonic earthquakes.
In this final section, we summarise the contributions made by this chapter and outline
potential areas for further work.
We first showed that the empirical laws that are conventionally used as components
of the ETAS model are each nested within the generalised Pareto distribution. This
led us to propose an orthogonal reparameterisation of the ETAS model in which
each of these empirical laws is replaced by the encompassing GPD and the relation-
ship between earthquake magnitudes and aftershock productivity is centred at the
mean observed magnitude. This reparameterisation allows greater representation of
model uncertainty by extending the representable class of ETAS models relative to
the conventional parameterisation. Through simulation we showed that, even when
constrained to the empirical-law model space, our reparameterised ETAS model leads
to more efficient parameter inference, better exploration of heavy-tailed distributions
and more accurate recovery of the latent branching structure.
The second contribution of this chapter was to introduce two extensions of the ETAS
model, with dual and correlated magnitudes. These extensions were developed to
ensure that their necessity over the standard ETAS model, with i.i.d. magnitudes,
could be easily tested. Care was also taken to ensure that that a range of dependence
strengths could be represented by our formulation of the correlated ETAS model
(at both moderate and extreme values) and to ensure that the marginal magnitude
distributions remained within the GPD family.
Efficient methods of conditional Bayesian inference were developed for both the dual-
and correlated-magnitude ETAS models. We demonstrated that both the presence
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION184
and absence of these additional features can be identified through the use of simulated
catalogues and considered, for the first time, the recovery of the latent branching
structure of the ETAS model. Through these simulations we found that the addi-
tional model complexity does not impede inference when it is not required. Where
non-identical margins or correlation are truly present, incorporating these into the
inference lead to improved estimation of the branching structure and magnitude pa-
rameters.
There are many ways in which the new methodology and models presented in this
chapter may be taken further. This chapter has focused on the temporal ETAS
model with homogeneous background rate . This simplifies the exposition of our
proposed methodology and models but this is likely to be overly simplified for direct
use as a model for earthquake catalogues, as we saw in Chapter 2. A first extension
might therefore consider a temporally varying background rate, estimated using a
parametric or semi-parametric model. This would allow these models to be applied
to induced earthquake catalogues, in which the seeding rate cannot reasonably be
assumed constant because of changes in the human activity driving the seismicity.
Care will be required to ensure that the background intensity model is sufficiently
flexible to capture variations in the background event rate, but not so flexible as to
obscure aftershock activity.
A second, and more challenging piece of further work would be to develop analogous
extensions to the ETAS model in the spatio-temporal setting. This presents several
challenges. Firstly, the dimensions of both the observation and parameter spaces
are increased while the number of available data remains fixed. This could lead to
less precise estimation of model parameters or require stronger prior distributions
to be used to guide inference. Secondly, edge effects occur if additional intensity
is allocated outside of the observation region and this is not accounted for during
CHAPTER 6. IMPROVING AND EXTENDING THE ETAS FORMULATION185
inference. These edge effects have been shown to be highly influential on the estimated
ETAS parameters (Schoenberg, 2013). These effects are easily handled in the one-
dimensional temporal setting but calculating the amount of intensity allocated outside
of a spatio-temporal region with an irregular spatial boundary would be a significant
challenge.
A final piece of further work might consider how dual or correlated event magnitudes
within the ETAS model influence the distribution of the largest magnitude in a given
period, referred to as Mmax(t1, t2) where t1 < t2 are the start and end of the period.
This quantity is of particular interest in a seismological setting as it is often used to
inform policy decisions relating to earthquake defences. Estimation of Mmax(t1, t2)
from an earthquake catalogue has been considered for i.i.d. magnitudes (Shcherbakov
et al., 2019). Properties related to Mmax have also been considered for ETAS-type
models used in financial applications; for example through the distribution of the
sum or maximum of i.i.d. marks when the point process parameters are known (Bas-
rak et al., 2019; Zugec, 2019). What has not been established is how a mixture of
magnitude distributions or dependence between earthquake magnitudes will impact
these results. Theoretical or simulation based approaches to understanding this effect
would be valuable further work.
Chapter 7
Conclusions and further work
In this final chapter, we summarise the contributions to the area of statistical seismic-
ity that result from Chapters 4 - 6 of this thesis. We outline potential developments
to each of these works individually and also comment on how these may be drawn
together in future research.
In Chapter 4, we investigated simplifications and extensions to a state-of-the-art,
physically motivated model for induced earthquake locations in the Groningen gas
field, which we took as our baseline model. Model simplifications were motivated by a
desire for parsimony in the statistical description of earthquake locations, while model
extensions were proposed based on the addition of a range of physical characteristics
that were not included in the baseline model.
From this work we found that, based on the available data, there was insufficient
evidence to suggest that many of these model alterations provided a significant im-
provement to model fit. However, there was one notable exception to this finding:
allowing the parameters of the baseline model to take different values in the upper
and lower regions of the gas field resulted in a marked improvement to the baseline
CHAPTER 7. CONCLUSIONS AND FURTHER WORK 187
model. In our model extension, spatial variability was permitted through a simple,
linear partitioning of the gas field by dividing the field in such a way as to separate
two modes of earthquake activity.
Although the improvement in model fit was not found to be overly sensitive to the
choice of boundary line, the choice of a linear form for the boundary was arbitrary -
as would have been any other division of the gas field that was not based on physical
properties. Further work might investigate variations of the baseline model where
parameters are allowed to vary smoothly over space, for example through the use
of thin-plate splines. The challenge in successfully doing so will be balancing model
flexibility against the small size of the Groningen earthquake catalogue.
In Chapter 5, we considered the development of the sensor network for detecting
earthquakes in the Groningen gas field. In particular, we considered how investment
in this network impacted the detection of small magnitude events and developed a
method to select a time-varying threshold above which to model the magnitude dis-
tribution of earthquakes. This allowed small earthquakes, which would not have been
detected at the start of the catalogue, to be included within the inference procedure
thus reducing uncertainty in the estimated magnitude return levels as compared to a
standard modelling approach.
When applying our threshold selection method to the Groningen earthquake catalogue
we found that the constant threshold that is currently used as standard is overly con-
servative. Using the additional information provided by small magnitude earthquakes
allowed us to conclude, for the first time based on empirical evidence alone, that the
magnitude distribution of Groningen earthquakes has a finite upper end point.
This work and its conclusions have generated interest from stakeholders for the Gronin-
gen gas field. This interest lies both in the demonstrable return on investment this
CHAPTER 7. CONCLUSIONS AND FURTHER WORK 188
gives for the developments to the sensor network and also in the implications about
the magnitude return levels. The work presented in Chapter 5 prompted further work
from Shell to investigate whether the measurement scale used to record earthquakes
influences the selected threshold and resulting conclusions about the extreme value
properties of the magnitude distribution. This work is inspired by Wadsworth et al.
(2010), where Box-Cox transformations are used to represent the uncertainty in the
measurement scale that leads to the most efficient extreme value analysis. Prelimi-
nary results from this further work suggest that the conclusions of Chapter 5, made
on the local magnitude scale, are robust to moderate transformations of the chosen
measurement scale.
There are many other possible avenues of additional further work leading from the
methodology developed in Chapter 5. This work introduced a new method for se-
lecting a time-varying threshold for a univariate extreme value analysis. This was
motivated and explored in the case where observation is incomplete below the mod-
elling threshold, but the method is applicable more generally. An important piece of
further work would therefore be to perform an extensive simulation study to assess
how the method compares to other approaches for selecting a time-varying thresh-
The method we have presented focuses the temporal evolution of earthquake detection.
A second extension would be to develop a method to select a threshold that varies over
space instead of or as well as over time, basing this on the same underlying principle of
quantifying deviation from a fitted GPD model. This development is important since
the spatial element in the development of the sensor network has not been accounted
for in this first work.
In Chapter 6 we considered a reparameterisation and two extensions of the ETAS
model for earthquakes and aftershock activity. We showed that inference for the
CHAPTER 7. CONCLUSIONS AND FURTHER WORK 189
standard ETAS model could be improved while broadening the class of models that
can be represented; this was achieved by centring the effect of earthquake magnitudes
on aftershock productivity and by using generalised Pareto distributions in place of
the empirical laws conventionally used to describe the aftershock-delay and magni-
tude distributions. We also introduced two extensions to the ETAS model: the dual
magnitude extension, which allows background and triggered events to have distinct
magnitude distributions, and the correlated magnitude extension which allows for
dependence between triggered and triggering magnitudes. Each of these model exten-
sions respects the complex branching structure of ETAS earthquake catalogues and
efficient, conditional approaches to inference were developed for each extension.
A limitation in the current work is that we have restricted attention to the one dimen-
sional case where background events come from a homogeneous Poisson process. The
reparameterisation of the standard ETAS model could be extend readily to spatio-
temporal point patterns with non-constant background rates, and could be of im-
mediate practical benefit when fitting ETAS models to earthquake data from the
Groningen field and beyond.
In order to be properly applied to catalogues of induced earthquakes, the model ex-
tensions must also be extended to accommodate a non-constant rate of background
events. Translating the ETAS model extensions to this more general setting is con-
ceptually simple, but will likely present practical difficulties in distinguishing between
temporal (or indeed spatial) variability in the background event rate from clusters of
aftershock activity. This same problem arises for the standard ETAS model when a
flexible background intensity component is used and this identifiability issue is only ex-
acerbated when catalogue sizes are small. Applying these model extensions to induced
earthquake catalogues will likely be challenging without imposing strong assumptions
on the form of the background event intensity to ensure sufficient smoothness in space
CHAPTER 7. CONCLUSIONS AND FURTHER WORK 190
and/or time. Catalogues of tectonic earthquakes, where temporal stationarity may
reasonably be assumed, might provide a suitable starting point for extensions of this
type.
A final area of further work based on Chapter 6 would be to investigate the effects
of a dual magnitude distribution or correlated earthquake magnitudes on the esti-
mated values for the largest earthquake in a stated space-time interval, Mmax. This
quantity is often of interest when planning earthquake defences and is dependent on
all components of the ETAS model: the rate of background events, the recent event
history, aftershock productivity, and of course the distribution and dependence struc-
ture of earthquake magnitudes. Because of these complex dependencies, Mmax is best
estimated though the use of Monte Carlo simulations. It would be a worthwhile un-
dertaking to conduct a thorough investigation on how estimates of Mmax are changed
when, for example, dual or dependent magnitudes are truly present but are neglected
in the modelling framework.
In addition to extending the work of each chapter of this thesis individually, there is
also much further work that could be done to unite the methods and models developed
in Chapters 4-6 of this thesis.
The work in Chapters 4 and 5 of this thesis could be combined to allow the use
of later, small magnitude events when modelling earthquake locations. This addi-
tional information might materially alter the conclusions in Chapter 4 about which
additional physical processes are detectable using the available data. The increased
quantity of usable data might also facilitate the fitting of more complicated location
models, such the proposed extension with smoothly varying parameters. When com-
bining location modelling with selection of a variable magnitude threshold, it will be
important to proceed with great care because lowering the magnitude of completion
will also increase the apparent rate at which events occur. A sensible starting point
CHAPTER 7. CONCLUSIONS AND FURTHER WORK 191
for this work might be to first extend the models of Chapter 4 to consider the mark
distribution as an additional dimension to the observation window and to allow the
mark intensity to also vary according to a suitable covariate.
A limitation of the model extensions presented in Chapter 6 is the restriction of focus
to the case where background events come from a homogeneous Poisson process with
rate . This restriction was made to facilitate the presentation and implementation
of these new model extensions, but in theory could be relaxed to allow parametric
or semi-parametric modelling of the intensity of background events, such as those
investigated in Chapter 4. This presents the additional challenge of selecting an ap-
propriate level of smoothness in the intensity function of background events so that it
may capture medium and long-term variation in the rate of background events with-
out masking any short-term aftershock activity. A first attempt at this might assume
a known background event rate, which could be estimated by spatially aggregating
the fitted intensity of the baseline model from Chapter 4. Given this point estimate
of baseline intensity, estimates of the ETAS, magnitude and branching parameters
could be estimated from the observed catalogue. This might give a good starting
point from which to relax the assumption that the background intensity is fixed and
known.
Further work on modelling induced seismicity is not limited to modifications or ex-
tensions of the work presented in Chapters 4-6. The ETAS model is the dominant
statistical model for aftershock activity, but when this model is used to augment a
covariate-driven model of background events, such as those investigated in Chapter 4,
it leads to inconsistent treatment of background and triggered events. The Poisson
process describing background events is driven by the gas extraction process but the
counts, locations and magnitudes of ETAS aftershocks are dependent on gas extraction
only though the magnitude and locations of those background events. This is because
CHAPTER 7. CONCLUSIONS AND FURTHER WORK 192
in the ETAS model earthquakes augment the intensity of the point process, rather
that the covariate surface that is thought to be driving earthquake activity.
An alternative model formulation could be developed around a covariate-driven inten-
sity model in which each earthquake may bring about further seismic events by locally
altering the covariate values in the surrounding area and at subsequent times. This
could reflect, for example, the additional stresses on the reservoir structure caused by
that earthquake. The way in which the covariate surface is altered by seismic events
should be chosen carefully, based on the form of the intensity model, to lead to a
physically sensible local increase the covariate-driven intensity surface. Some initial
ideas and discussions of this modelling approach are outlined Appendix D.1.
The main benefit of this type of combined model formulation for background and
triggered events would be the increased coherence of the modelling framework for these
two event types; each occurs when stresses across fault planes are increased beyond
some critical value and this commonality should ideally be reflected in the point
process model. However, such a combined model is not without its own challenges.
The model would suffer from similar computational challenges to the direct fitting of
the standard ETAS model, which could no longer be alleviated by a decomposition into
simpler model components given a latent branching vector. There would also be the
additional challenge of ensuring that changes to the covariates caused by earthquakes
are coherent with subsequent, measured values of the covariates. These challenges
could make implementation and application of such a combined aftershock model
challenging.
At this point we hope that it is evident that the statistical modelling of earthquake
activity is anything but straightforward. This modelling task is made yet more chal-
lenging in the setting of induced earthquakes, due to the small amount of available
data and the variations in human activity that are driving earthquake occurrences.
CHAPTER 7. CONCLUSIONS AND FURTHER WORK 193
This thesis has made several contributions to better our understanding and modelling
of induced earthquakes, and toward overcoming some of the associated challenges.
As evidenced by the further work proposed in this chapter, many new questions and
challenges about the modelling of induced earthquakes have been uncovered as the
work in this thesis developed. In this way, statistical seismology remains an fecund,
important and promising area for future research.
Appendix A
Supplementary materials to
Chapter 4
A.1 Integrated intensity functions
APPENDIX A. SUPPLEMENTARY MATERIALS TO CHAPTER 4 195
Model (B(x, ) (0, t)|,)
S1 0|B(x, )|g(x;)
S2 0|B(x, )|s(x, t;)
S3 0|B(x, )|s(x, t;)[1 + 12 s(x, t, ;)]
S4 0|B(x, )|11 exp{1s(x, t;)}
B0 0|B(x, )|s(x, t;) exp{1s(x, t;)}
E1 (0|B(x, ) WL|+ 1|B(x, ) WU |)s(x, t;)
E2 0|B(x, ) WL|s(x, t;) exp{1s(x, t;)+
2|B(x, ) WU |s(x, t;) exp{3s(x, t;)
E3 numerical integration required
E4 numerical integration required
E5 0|B(x, )|s(x, t;) exp{1s(x, t;)}
E6 0|B(x, )|s(x, t;) exp{1s(x, t;) + 2s(x, t;)}
Table A.1.1: Integrated intensity functions for sub-models (S1-S4), the baseline model
(B0) and model extensions (E1 - E6). The topographic gradient is denoted by g(x),
while s(x, t;) denotes the cumulative incremental Coulomb stress smoothed using an
isotropic Gaussian kernel with standard deviation . The first and second temporal
derivatives of ICS are given by s(x, t;) and s(x, t;). Regions WL and WU for models
E1 and E2 are defined in Section 4.3.3.
APPENDIX A. SUPPLEMENTARY MATERIALS TO CHAPTER 4 196
A.2 Maps of annual expected earthquake counts
The following pages show maps of the annual expected earthquake counts on a
500500 m grid over the Groningen gas field, under the fitted baseline model B0
and the fitted model extension E2.
APPENDIX A. SUPPLEMENTARY MATERIALS TO CHAPTER 4 197
230000 250000
1995: Observed = 4, Expected = 4.74
Easting
230000 250000
1996: Observed = 2, Expected = 6.1
Easting
230000 250000
1997: Observed = 5, Expected = 5.47
Easting
230000 250000
1998: Observed = 5, Expected = 5.45
Easting
230000 250000
1999: Observed = 4, Expected = 4.4
Easting
230000 250000
2000: Observed = 6, Expected = 4.26
Easting
230000 250000
2001: Observed = 2, Expected = 4.88
Easting
230000 250000
2002: Observed = 3, Expected = 5.42
Easting
230000 250000
2003: Observed = 9, Expected = 5.98
Easting
230000 250000
2004: Observed = 5, Expected = 6.79
Easting
230000 250000
2005: Observed = 10, Expected = 7.61
Easting
230000 250000
2006: Observed = 15, Expected = 8.16
Easting
Figure A.2.1: Observed and expected event counts in each year (1995-2006) under
the fitted baseline model B0.
APPENDIX A. SUPPLEMENTARY MATERIALS TO CHAPTER 4 198
230000 250000
2007: Observed = 8, Expected = 7.37
Easting
230000 250000
2008: Observed = 6, Expected = 10.52
Easting
230000 250000
2009: Observed = 15, Expected = 10.58
Easting
230000 250000
2010: Observed = 11, Expected = 14.87
Easting
230000 250000
2011: Observed = 19, Expected = 14.63
Easting
230000 250000
2012: Observed = 13, Expected = 16.27
Easting
230000 250000
2013: Observed = 22, Expected = 20.23
Easting
230000 250000
2014: Observed = 15, Expected = 18.39
Easting
230000 250000
2015: Observed = 19, Expected = 13.21
Easting
230000 250000
2016: Observed = 10, Expected = 12.61
Easting
Figure A.2.2: Observed and expected event counts in each year (2007-2016) under
the fitted baseline model B0.
APPENDIX A. SUPPLEMENTARY MATERIALS TO CHAPTER 4 199
230000 250000
1995: Observed = 4, Expected = 5.31
Easting
230000 250000
1996: Observed = 2, Expected = 6.59
Easting
230000 250000
1997: Observed = 5, Expected = 5.86
Easting
230000 250000
1998: Observed = 5, Expected = 5.75
Easting
230000 250000
1999: Observed = 4, Expected = 4.85
Easting
230000 250000
2000: Observed = 6, Expected = 4.72
Easting
230000 250000
2001: Observed = 2, Expected = 5.18
Easting
230000 250000
2002: Observed = 3, Expected = 5.44
Easting
230000 250000
2003: Observed = 9, Expected = 5.84
Easting
230000 250000
2004: Observed = 5, Expected = 6.99
Easting
230000 250000
2005: Observed = 10, Expected = 7.93
Easting
230000 250000
2006: Observed = 15, Expected = 8.19
Easting
Figure A.2.3: Observed and expected event counts in each year (1995-2006) under
the fitted model extension E2.
APPENDIX A. SUPPLEMENTARY MATERIALS TO CHAPTER 4 200
230000 250000
2007: Observed = 8, Expected = 7.57
Easting
230000 250000
2008: Observed = 6, Expected = 10.61
Easting
230000 250000
2009: Observed = 15, Expected = 10.9
Easting
230000 250000
2010: Observed = 11, Expected = 14.21
Easting
230000 250000
2011: Observed = 19, Expected = 15.15
Easting
230000 250000
2012: Observed = 13, Expected = 16.56
Easting
230000 250000
2013: Observed = 22, Expected = 19.55
Easting
230000 250000
2014: Observed = 15, Expected = 16
Easting
230000 250000
2015: Observed = 19, Expected = 12.62
Easting
230000 250000
2016: Observed = 10, Expected = 12.31
Easting
Figure A.2.4: Observed and expected event counts in each year (2007-2016) under
the fitted model extension E2.
Appendix B
Supplementary materials to
Chapter 5
B.1 Assessing i.i.d. assumption for Groningen earth-
quakes
B.1.1 Connection to main text
This appendix supports the claim made in Sections 5.2 and 5.6 of the main text
that Groningen earthquakes exceeding 1.45ML may be modelled as independent and
identically distributed.
B.1.2 Exploratory analysis
Here we examine the validity of the assumption, common to both GPD and exponen-
tial models, that magnitudes are i.i.d. above 1.45ML. In the case of continuous-valued
data that are completely observed, this assumption implies that inter-arrival times of
threshold exceedances should approximately follow an exponential distribution. Due
APPENDIX B. SUPPLEMENTARY MATERIALS TO CHAPTER 5 202
0 2 4 6 8 10
interexceedance delay (# events)
      
     
     
0 2 4 6 8 10
interexceedance delay (# events)
      
    
     
0 2 4 6 8 10
interexceedance delay (# events)
     
    
     
Figure B.1.1: Frequency plots of the number of earthquakes exceeding 1.45ML that
separate earthquakes exceeding exceeding 1.65ML (left), 1.75ML (centre), and 1.85ML
(right) for the Groningen earthquake catalogue. Observed frequencies (black lines)
fall within the 95% confidence intervals under the fitted models (grey lines).
to incomplete observation below 1.45ML and the transformed time scale, we instead
consider the inter-arrival times of events exceeding magnitude c  vC measured in
terms of the number of events with magnitudes between 1.45ML and c. If events are
i.i.d. then these inter-arrival times are geometrically distributed. It is important to
investigate a range of values for c, since lower values lead to more (shorter) observed
interval lengths but these are more concentrated about 0 making assessment of the
geometric distribution more difficult. This trade-off can seen by considering the edge-
cases: if c = 1.45ML then each interval is of length 0, and if c is between the second
and third largest observed magnitudes then there is a single observed interval.
The empirical distributions of interval lengths in the Groningen catalogue are shown in
the panels of Figure B.1.1 for exceedances of c = 1.65ML, 1.75ML, and 1.85ML. These
are consistent with the 95% confidence interval for the fitted geometric distribution
at each value of c. The same conclusion was found for 1.55ML < c < 2.25ML. Events
larger than this show mild evidence of clustering, but overall this suggests that it is
reasonable to model magnitudes as i.i.d. above c = 1.45ML.
APPENDIX B. SUPPLEMENTARY MATERIALS TO CHAPTER 5 203
B.2 Bootstrap datasets and parameter estimates
B.2.1 Connection to main text
This section supports the material presented in Section 5.2 of the main text. To
represent the sampling variability in the maximum likelihood estimate  associated
with the log-likelihood (5.2.1) of the main text we take a parametric bootstrapping
approach. This appendix describes the simulation of bootstrap catalogues and how
they may be used to obtain bootstrap estimates of . Bootstrap parameter estimates
of this type are used throughout the main text.
B.2.2 Generating bootstrapped data-sets
B.2.2.1 Threshold exceedances and a point process
In bootstrap realisations of the earthquake catalogue, the number, timing and mag-
nitudes of events exceeding v() are all variable. In an alternate catalogue, events
remain within the transformed observation interval (0, max) but no longer form a
regularly spaced sequence. Rather, the events in the region Av = {(, y) : 0   
max, y  v()}, as shown in Figure B.2.1, are approximated by a Poisson process.
The Poisson process intensity on Av is determined by the GPD parameters  and u,
where u is the expected number of events exceeding u  min0<<max v() per unit
 (Coles, 2001). Since it is assumed that no censoring occurs on Av and that the
underlying magnitude distribution is identical over t, it follows that u is constant
over  because time has been transformed so that earthquakes occur at a constant
rate. The resulting intensity function on Av is:
(, y) =
1 + 
y  u
]1/1
for (, y)  Av. (B.2.1)
APPENDIX B. SUPPLEMENTARY MATERIALS TO CHAPTER 5 204
Figure B.2.1: Threshold exceedances as point process.
Bootstrap catalogues of earthquakes with magnitudes exceeding v() can be obtained
as realisations from the point process with intensity function (B.2.1) using the esti-
mated parameters . We first describe how to generate such bootstrap catalogues
and then how these can be used to obtain bootstrap estimates of .
B.2.2.2 Simulating the exceedance count
The first step in generating a bootstrap catalogue is to sample the number of events
that occur on Av. The number of events on Av is Poisson distributed with expecta-
(Av) =
(, y) d dy, (B.2.2)
which must be estimated from the observed catalogue. This is complicated by the
rounding of observed magnitudes x to the nearest 2. Recall that for borderline events
{xi  x : |xi  vi| < } it is not known whether or not the corresponding unrounded
magnitudes exceed v() and place those events on Av. Therefore nv, the observed
event count on Av, is unknown and must itself be estimated.
Given the rounded magnitudes x = (x1, . . . , xn) and the estimated GPD parameters
APPENDIX B. SUPPLEMENTARY MATERIALS TO CHAPTER 5 205
(u, ), events i = 1, . . . , n each exceed their magnitude threshold and are on Av
independently with probability wi as defined in expression (5.2.2) of the main text.
Uncertainty in the observed event count due to magnitude rounding can be included
in the generation of bootstrap sample sizes by simulating a value mv for nv, as the
sum of n independent Bernoulli random variables with expectations w1 to wn.
This simulated value for the observed event count can be used as a point estimate
(Av) = mv for (Av). Since (Av) is an estimate of based on a single observed
count, it is important to also include uncertainty in the inferred value of (Av) when
generating bootstrap catalogue sizes. This can be done using a bootstrapped value
for the value of the estimator (Av). Since inference is based on a single Poisson
count, the bootstrap estimator (Av) is obtained simply as a sampled value from the
Poisson((Av)) distribution.
Finally, the event count n to be used for the bootstrapped catalogue can be sampled
from a Poisson((Av)) distribution. Doing so properly represents rounding, estima-
tion and sampling uncertainties in the bootstrapped event counts.
B.2.2.3 Simulation of event times
Having simulated the exceedance count nv, the times  = (1, . . . , nv) of the events
on Av can sampled according to the marginal temporal intensity (). The marginal
temporal intensity is found by integrating the joint intensity (B.2.1) over magnitudes.
Noticing that (, y) is proportional to the GPD density, () can be stated in terms
of the GPD survivor function F (y;, ) = [1 + y/]
+ to give:
() =
(, y) dy = uF (v() u;u, ) for 0    max. (B.2.3)
Sampling exceedance times from this intensity can be achieved through reverse appli-
cation of the time-rescaling theorem (Brown et al., 2002). In general, this will require
APPENDIX B. SUPPLEMENTARY MATERIALS TO CHAPTER 5 206
numerical integration and can be computationally intensive. Depending on the form
of v() more efficient methods may be available. When v() is a step function the
step-wise integral of the temporal intensity has a simple form and event times can
be simulated very efficiently; the nv events are allocated independently to steps with
probability proportional to the temporal intensity integrated over each step. Events
are then be located uniformly at random within their allocated step. When a more
complex form is used for v(), alternative sampling approaches may reduce the com-
putational cost of sampling event times. For example, exact samples may be obtained
through rejection sampling and approximate samples by approximating v() as a step
function.
B.2.2.4 Conditional simulation of event magnitudes
The magnitude of each event in the bootstrap catalogue may then be simulated con-
ditional on its occurrence time. Given a simulated occurrence time   (0, max),
the conditional magnitude intensity for magnitudes exceeding v() is simply the
GPD density function with shape parameter  and time-dependent scale parameter
() = u + (v() u):
(y|) =
( , y)
()
()
1 + 
y  v()
()
]1/1
, for y  v(). (B.2.4)
This allows easy simulation of magnitudes y = (y1, . . . , ynv) conditional on their
occurrence times  , by generating random variates from the appropriate GPD. The
simulation of a bootstrap earthquake catalogue is completed by rounding these to the
nearest multiple of 2, to obtain x = (x1, . . . , xnv). Note that in a bootstrap catalogue
yi > vi for i = 1, . . . , nv, but following rounding it is possible that some of the xi are
below their associated threshold value. The simulation of bootstrapped datasets is
summarised in Algorithm 1, where () = u + (v() u).
APPENDIX B. SUPPLEMENTARY MATERIALS TO CHAPTER 5 207
Algorithm 1: Simulation of GPD data with variable threshold and rounding.
Result: A bootstrapped dataset of rounded GPD observations x, based on the
threshold function v(), observations x and parameter estimates .
Sample mv as the sum of independent Bernoulli(wi) realisations where i = 1, . . . , n ;
Sample the estimate of (Av), (Av) from the Poisson(mv) distribution;
Sample the bootstrap number of exceedances nv from the Poisson((Av))
distribution;
for i = 1 to i = nv do
sample ui from a Uniform(0,1) distribution;
find the bootstrap occurrence time i which satisfies i
( ; u, , v()) d = ui(Av);
sample the bootstrap magnitude exceedance zi from the GPD((i), )
distribution;
calculate the bootstrap latent magnitude yi = v(i) + zi;
round yi to the nearest 2 to get the rounded bootstrap magnitude xi (which
may be less than v(i)).
APPENDIX B. SUPPLEMENTARY MATERIALS TO CHAPTER 5 208
B.2.3 Generating bootstrap maximum likelihood estimates
In the bootstrapped earthquake catalogues, some magnitudes x may be less than their
respective threshold values. However, unlike in the original catalogue, each of the
corresponding unrounded magnitudes y exceeds the respective modelling threshold.
Therefore, an unweighted log-likelihood should be used when obtaining bootstrap
maximum likelihood estimates. Letting vi = v(i) and vi = u + (vi  u), the
unweighted log-likelihood function is
`(|x, v) =
log [F (xi +   vi;vi , ) F (max(vi, xi  ) vi;vi , )] .
The maximum likelihood estimates resulting from a collection of bootstrap catalogues
can be used to represent the sampling uncertainty of the original maximum likelihood
point estimate . This is done in the main text when calculating confidence intervals
on parameter values, conditional quantiles and return levels. The bootstrap parameter
estimates are also used in the construction of adapted PP and QQ plots and when
evaluating metric values.
B.3 Sampling standardised threshold exceedances
B.3.1 Connection to main text
This appendix describes how to sample a vector z of unrounded threshold exceedances
transformed to have a common Exp(1) marginal distribution. This used in Sections 5.3
- 5.6 of the main text and requires: a single bootstrap estimate  of the estimated
GPD parameters  (obtained as described in Appendix B.2), the n-vector of rounded
observations x and the corresponding threshold vector v. The process for sampling a
single vector z is described and is formalised in Algorithm 2.
APPENDIX B. SUPPLEMENTARY MATERIALS TO CHAPTER 5 209
B.3.2 Sampling unrounded threshold exceedances
It is unknown which, if any, of the borderline values {xi  x : |xivi| < } correspond
to unrounded values yi  y that exceed the modelling threshold v(). The first step
in sampling z is therefore to sample the set I that exceed the modelling threshold.
This is done by simulating independent Bernoulli trials for each event i = 1, . . . , n
with success probabilities wi = Pr(Yi > vi|xi, ) as defined in equation (5.2.2) of the
main text. The vector z will therefore have a randomly sampled length m = |I|  n,
where the distribution of m depends on x,v and .
The unrounded magnitude values for events in I are then simulated from their con-
ditional distribution given: their rounded values, the estimated GPD parameters and
that they are threshold exceedances. Letting F be the GPD distribution function as
in Equation (5.1.1) of the main text, the required conditional distribution function of
Yi|xi,, Yi  vi is given by:
GYi|xi,,Yivi(y) =


0 for y < bi,
F (yu;)F (biu;)
F (xi+u;)F (biu;)
for bi  y  xi + ,
1 for y > xi + ,
(B.3.1)
where i  I and bi = max(xi, vi) is the smallest value above the modelling threshold
that results in the rounded observation xi. The sampled values are combined to create
y, an m-vector of continuous-valued sampled threshold exceedances. Note that the
length of this vector is a random variate when there are one or more borderline values.
By repeated simulation using k bootstrap parameter estimates {(1), . . . , (k)}, the
resulting vectors of unrounded exceedances {y(1), . . . , y(k)} reflect uncertainty about
both the GPD parameter values and the number of threshold exceedances.
APPENDIX B. SUPPLEMENTARY MATERIALS TO CHAPTER 5 210
B.3.3 Transformation onto common margins
When v() is non-constant, the elements of y are random variates from the GPD
family, but they do not share a common set of parameters. To resolve this issue, the
probability integral transform can then be used to give each element of y an Exp(1)
marginal distribution using its fitted GPD parameters. This results in a vector z of
standardised, sampled exceedances of the modelling threshold v(). The bootstrap
simulation of one such vector is formalised in Algorithm 2.
Note that the transformation onto exponential margins is dependent on the estimated
GPD parameters. The effect of parameter uncertainty on this transformation is rep-
resented across bootstrap catalogues by transforming each collection of bootstrapped
threshold exceedances {y(1), . . . , y(k)} using their respective bootstrapped parameter
estimates {(1), . . . , (k)}. This yields a set of k sampled vectors of threshold ex-
ceedances on exponential margins, {z(1), . . . , z(k)}. As with y, the length of z is a an
m-vector and so is a random variate when there are one or more borderline events in
These sampled vectors of standardised threshold excesses can be used to calculate
expected metric values or to construct modified PP- and QQ-plots, as in Figures 5.4.1
and 5.6.1 of the main text. In those plots, the variability between the empirical
quantiles (or probabilities) of each {z(1), . . . , z(k)} is shown by the confidence intervals
on sample quantile values (or probabilities). The expected range of values for each
sample quantile (or probability) is shown by the tolerance intervals. The uncertainty
in the number of threshold exceedances is also incorporated when calculating the
tolerance intervals; they are constructed using k sets of Exp(1) random variates of
lengths dim z(1), . . . , dim z(k).
APPENDIX B. SUPPLEMENTARY MATERIALS TO CHAPTER 5 211
Algorithm 2: Simulation of standardised threshold exceedance sets.
input : A bootstrap estimate  = (, ) of the GPD parameters, the n-vector of
rounded observed values x, and their corresponding thresholds v.
output: A vector z of length m  n of sampled unrounded values, transformed to
have an Exp(1) distribution under the fitted model.
for i = 1 to n do
calculate wi = Pr(Yi > vi|xi, ), the probability that each rounded observation
corresponds to an unrounded value on Av, as in Equation (3) of the main text;
Generate n independent Uniform[0,1] random variates u1, . . . , un ;
Sample the indexing set of events that are on Av, I = {i  (1, . . . , n) : ui  wi} and
let m = |I| ;
Store the elements of I in the vector  = (1, . . . , m) so that yi > vi for
i = 1, . . . , m and initialise y and z as vectors of length m;
for j = 1 to m do
Let a = j ;
Sample the jth unrounded exceedance yj from its conditional distribution
Ya|x=xa,=,Yava,(y) as in equation (B.3.1) ;
Let va = ( + (va  u), ) be the bootstrapped GPD parameters for
exceedances of va;
Transform yj onto Exp(1) margins under this fitted model by letting F be the
GPD distribution function (1) in the main text and setting
zj =  log
yj  va; va
APPENDIX B. SUPPLEMENTARY MATERIALS TO CHAPTER 5 212
B.4 Supplementary figures
The following pages of this appendix contains supplementary figures relating to Chap-
ter 5 of this thesis.
APPENDIX B. SUPPLEMENTARY MATERIALS TO CHAPTER 5 213
0.0 0.2 0.4 0.6 0.8 1.0
d(p,1), RMSE = 0.34
threshold
0.0 0.2 0.4 0.6 0.8 1.0
d(p,2), RMSE = 0.12
threshold
Figure B.4.1: Sampling distribution of threshold selection methods for PP-based met-
rics over 500 simulated catalogues with constant threshold and hard censoring. The
true threshold is shown by a dashed red line and the root mean squared error (RMSE)
for each method is given in plot titles.
0.1 0.2 0.3 0.4 0.5 0.6
  
 
 
Conservative
Extended
Stepped
bias2(u) bias2() Var(u) Var()
0.000
0.002
0.004
0.006
0.008 Conservative
Extended
Stepped
Figure B.4.2: [Left] Plot of maximum likelihood estimates of GPD scale and shape
parameters for 1000 simulated catalogues obtained using a conservative, stepped and
extended approach. [Right] Plot of mean squared error decomposition for maximum
likelihood estimates by each approach. The error is decomposed into squared bias and
variance terms for each of the GPD parameters, with variance terms having larger
contributions.
APPENDIX B. SUPPLEMENTARY MATERIALS TO CHAPTER 5 214
1 5 10 50 100 500
return period
Figure B.4.3: [left] Bootstrap maximum likelihood estimates for GPD parameters for
Groningen magnitudes above 1.45ML assuming GPD (black) and exponential (red)
models. [right] Resulting conditional return level plot with return period measured
in number of events exceeding 1.45 ML.
0.0 0.2 0.4 0.6 0.8 1.0
d(q,1), RMSE = 0.06
threshold
0.0 0.2 0.4 0.6 0.8 1.0
d(q,2), RMSE = 0.08
threshold
0.0 0.2 0.4 0.6 0.8 1.0
d(p,1), RMSE = 0.35
threshold
0.0 0.2 0.4 0.6 0.8 1.0
d(p,2), RMSE = 0.12
threshold
Figure B.4.4: Sampling distribution of threshold selection methods for QQ-based (top
row) and PP-based (bottom row) metrics over 500 simulated catalogues with constant
threshold and phased censoring. The true threshold is shown by a dashed red line
and the root mean squared error (RMSE) for each method is given in plot titles.
Appendix C
Supplementary materials to
Chapter 6
C.1 B conditional posterior, dual magnitude model
In the dual magnitude ETAS model there are two magnitude distributions: one for
background events and another for triggered events. Each magnitude M1, . . . ,Mn
remains independent of all other magnitudes, as in the standard ETAS model. How-
ever, introducing type-dependent magnitudes means that each element of the branch-
ing vector, Bi, is no longer independent of its corresponding magnitude, Mi for
i = 1, . . . , n. This is because the observed magnitude of event i, mi, carries in-
formation to discern whether Bi = 0 or Bi > 0 when the two magnitude distributions
are not equal. Conversely, knowing that bi = 0 or bi > 0 is informative about the
likely values of Mi.
Let f0(m;0) and f1(m;1) denote, respectively, the probability density functions
for magnitudes of background events and triggered events, where  = (0, 1) is
the vector of all magnitude parameters. Additionally, define di = min(1, bi) to be the
APPENDIX C. SUPPLEMENTARY MATERIALS TO CHAPTER 6 216
index of the magnitude distribution to which event i belongs, where i = 1, . . . , n.
The conditional posterior for the full branching vector B|Y,,(b) may still, as for
the standard ETAS model, be written as the product of element-wise conditional
posteriors:
B|Y,,(b) = Pr(B = b|Y, , ) =
Pr(Bi = bi|Y, , ). (C.1.1)
In particular this means that the elements of the branching vector may be updated
sequentially as part of the Gibbs sampling scheme used to fit the dual magnitude
ETAS model. The element-wise conditional posteriors that constitute the product
terms in equation (C.1.1) are less simple than in the case of i.i.d. magnitudes, but
still have a closed form, which we derive presently.
Let Hi = {Yj  Y : j = 1, . . . , i 1}, be the history of the point process up to event i
for i = 1, . . . , n. The conditional posterior for a single element of the branching vector
does not depend on events {Yi+1, . . . , Yn} and so
Pr(Bi = bi|Y, , ) = Pr(Bi = bi|Hi, Yi, , )
= Pr(Bi = bi|Hi, Ti,Mi, , ).
Applying Bayes rule and the law of total probability, we have that
Pr(Bi = bi|Y, , ) =
f(mi|Bi = bi,Hi, Ti, , )Pr(Bi = bi|Hi, Ti, )i1
j=0 fI{j>0}(mi|I{j>0})Pr(Bi = j|Hi, Ti, )
We may simplify the terms of both the numerator and denominator as follows. Con-
sider the first term of the numerator, f(mi|Bi = bi,Hi, Ti, , ). Since the branching
element Bi is known, we know that Mi has density function fdi(m;di). Therefore
conditioning of the remaining terms Hi, Ti and  contribute no further information
about the distribution of Mi and these may be dropped. Now consider the second
APPENDIX C. SUPPLEMENTARY MATERIALS TO CHAPTER 6 217
term of the numerator, Pr(Bi = bi|Hi, Ti, , ). Since the Mi is unknown the mag-
nitude parameters  are not informative about Bi and may be dropped from the
conditioning. Similar arguments apply to the denominator and so
Pr(Bi = bi|Y, , ) =
fdi(mi|di)Pr(Bi = bi|Hi, Ti, )i1
j=0 fI{j>0}(mi|I{j>0})Pr(Bi = j|Hi, Ti, )
fdi(mi|di) bi(ti;Hi, )
[i=1
k=0 k(ti;Hi, )
j=0 fI{j>0}(mi|I{j>0})j(ti;Hi, )
[i1
k=0 k(ti;Hi, )
]1 .
The term in square brackets in the denominator may be brought out of the summation
over j to cancel with the term in square brackets in the numerator to give
Pr(Bi = bi|Y, , ) =
fdi(mi|di) bi(ti;Hi, )i1
j=0 fI{j>0}(mi|I{j>0})j(ti;Hi, )
Separating the normalising constant in the denominator into terms where j = 0 and
where j > 0, since 0(ti,Hi, ) =  for all i = 1, . . . , n, we find that
Pr(Bi = bi|Y, , ) =
fdi(mi|di) bi(ti;Hi, )
f0(mi;0) + f1(mi;1)
j=1 j(ti;Hi, )
, (C.1.2)
for i = 1, . . . , n, where the summation in the denominator is defined to be 0 when
i = 1.
Expression (C.1.2) is similar to that found in (Ross, 2016) for i.i.d. magnitudes, but
the probability mass function is now weighted to reflect the additional information
that is conferred by the magnitude of event i about whether Bi = 0 or Bi > 0. Indeed,
it is clear to see that when f0(m) = f1(m), the conditional branching posterior (C.1.2)
reduces to that of the i.i.d. case:
APPENDIX C. SUPPLEMENTARY MATERIALS TO CHAPTER 6 218
bi(ti;Hi, )i1
j=0 j(ti;Hi, )
We can therefore draw the vector B directly from its conditional posterior as part of
a Metropolis-within-Gibbs scheme, by drawing each element B1, . . . , Bn according to
its conditional posterior distribution, as given by expression (C.1.2). Note that for
each event i = 1, . . . , n the denominator terms in expression (C.1.2) are normalising
constants for the probability mass function, taking the same value for each potential
parent process bi = 0, . . . , i  1. Since sampling from this posterior distribution
requires the probability mass function only up to a constant of proportionality, these
denominator terms need not be calculated explicitly for each value of bi.
C.2  conditional posterior, correlated magnitudes
In the correlated magnitude ETAS model, triggering and triggered events may have
distinct marginal magnitude distributions, which are coupled using a Gaussian copula.
This extends the vector of magnitude parameters to  = (0, 1, ). The elements of
 give parameters of the marginal distributions for background and triggered events
as well as , the correlation of triggered and triggering magnitudes when transformed
onto standard Gaussian margins.
In the following, let f0(m;0) and f1(m;1) denote the marginal magnitude distri-
butions of background and triggered events and (m) denote the standard Gaus-
sian density. Let the corresponding cumulative distribution functions be denoted by
F0(m|0), F1(m|1) and (m). Finally, define di = min(1, bi) to be the index of the
magnitude distribution to which event i belongs, where i = 1, . . . , n.
Note that if the branching vector B is known, then the graphical representation of the
point pattern as a collection of trees is also known. The likelihood of the magnitude
APPENDIX C. SUPPLEMENTARY MATERIALS TO CHAPTER 6 219
parameters given the magnitudes in a single tree may be calculated by multiplying the
marginal density of the root magnitude (which corresponds to a background event) by
the conditional densities of each triggered magnitude in the tree given the distribution
and value of its parent, working from root to leaves. Since magnitudes in separate trees
are independent, the likelihood of the magnitude parameters given all magnitudes is
the product of such expressions over all trees. This combined likelihood may be written
most clearly by collecting terms for background (root) and triggered (non-root) events.
The conditional posterior of the magnitude parameters, assuming improper uniform
priors, is then given by
|Y,B,() =
i:Bi=0
f0(mi;0)
i:Bi>0
f(mi|mbi , bi, bbi , ), (C.2.1)
where the terms of the second product are the conditional densities of triggered events,
given the magnitude, index and marginal distribution of the corresponding parent
events, in addition to the magnitude parameters. We can express this conditional
density in terms of the marginal and copula densities.
To simplify notation, we will consider a parent-child magnitude pair pair of events
(mp,mc) so that 1  p < c  n and bc = p. These magnitudes are transformed from
their marginal distributions fdp(m;) and fdc(m;) onto standard Gaussian margins
to give (gp, gc). (Note that since event c is a child of event p we therefore know that
bc = p and dc = 1.) The transformation onto standard Gaussian margins can be
achieved by application of the probability integral transform, where
gp = 
1  Fdp(mp;) and gc = 
1  Fdc(mc;). (C.2.2)
Then the conditional density that we require is
f(mc|mp, bp, bc = p, ) = f(gc|gp, bc = p, )
 dgcdmc
= f(gc|gp, bc = p, )
  1  Fdc(mc;)
fdc(mc;).
APPENDIX C. SUPPLEMENTARY MATERIALS TO CHAPTER 6 220
By our choice of Gaussian copula linking the marginal distributions of triggering and
triggered events it follows that Gc|gp, bc = p,   N(gp, 1 2) and therefore
f(mc|mp, bp, bc = p, )
1 (Fdc(mc;)) 1
Fdp(mp;)
1 2
  1  Fdc(mc;)
f1(mc;)
gc  gp
1 2
fdc(mc)
(gc)
We can therefore rewrite the conditional posterior of the magnitude parameters (C.2.1)
in terms of the observed magnitudes transformed onto standard Gaussian margins.
These transformed magnitudes are denoted by {gi = 1(Fdi(mi;)) : i = 1, . . . , n}
and the conditional posterior of the magnitude parameter vector is:
|Y,B,() =
i:Bi=0
f0(mi;0)
i:Bi>0
gi  gbi
1 2
f1(mi;1)
(gi)
. (C.2.3)
Using this conditional posterior distribution, the magnitude parameters  may be up-
dated within the Gibbs sampling scheme used to fit the correlated ETAS model.
C.3 B conditional posterior, correlated magnitudes
In the ETAS model with correlated magnitudes, the elements of the branching vector
are no longer conditionally independent of one another given the ETAS and mag-
nitude parameters  and . This makes the conditional posterior distribution for
the full branching vector intractable. However, during the Metropolis-within-Gibbs
sampling used to fit the model, each element of B is updated separately. Updating
a single branching element Bi requires only the distribution of Bi|Y,Bi, ,  (up to
proportionality) where Bi = {Bj  B : j 6= i} is the branching vector B without its
ith element. Fortunately, the conditional posterior of each branching element given
APPENDIX C. SUPPLEMENTARY MATERIALS TO CHAPTER 6 221
the rest of the branching vector,  and  may be obtained in a closed form, which is
derived in this appendix.
As in Appendix C.2, we begin by simplifying the conditioning on subsequent events
{Yi+1, . . . , Yn}. We may drop dependence on subsequent events which are not directly
triggered by event i, but we must retain those child events directly triggered by
event i. These child events are denoted by the set YCi = {Yj  Y : j  Ci} where
Ci = {j  {i+ 1, . . . , n} : Bj = i}. This gives
Pr(Bi = bi|Y, , ,Bi) = Pr(Bi = bi|Hi, Yi, YCi , , ,Bi). (C.3.1)
Applying Bayes rule and and the law of total probability to bring focus onto the joint
conditional distribution of {Mi,MCi} we have that
Pr(Bi = bi|Y, , ,Bi) =
f(mi,mCi |Bi = bi,Hi, Ti, TCi , , ,Bi)Pr(Bi = bi|Hi, Ti, TCi , , ,Bi)i1
j=0 f(mi,mCi |Bi = j,Hi, Ti, TCi , , ,Bi)Pr(Bi = j|Hi, Ti, TCi , , ,Bi)
. (C.3.2)
We will consider each of the numerator terms and the denominator of equation (C.3.2)
individually.
In the following, let (g) and (g) be the standard Gaussian probability density and
cumulative distribution functions. Also let f0(m) and f1(m) be the marginal proba-
bility density functions of background and triggered magnitudes with corresponding
distribution functions F0(m) and F1(m). Finally, let g(i,j) = 
1(FI{j>0}(mi)) be
the magnitude of event i transformed onto standard Gaussian margins assuming the
corresponding branching element Bi = j, where 0  j < i  n.
APPENDIX C. SUPPLEMENTARY MATERIALS TO CHAPTER 6 222
The first numerator term in equation (C.3.2) may be written as
f(mi,mCi |Bi = bi,Hi, Ti, TCi , , ,Bi)
= f(mi,mCi |Bi = bi,Mbi , ,Bbi , BCi)
= f(mi|Bi = bi,Mbi , Bbi , )
f(mc|Mi, Bi = bi, Bc = i, )
g(i,bi)  g(bi,bbi )
1 2
f1(mi)
g(i,bi)
)]I(bi>0) [f0(mi)]I(bi=0)
g(c,i)  g(i,bi)
1 2
f1(mc)
g(c,i)
)} . (C.3.3)
The second numerator term in equation (C.3.2) may be written as
Pr(Bi = bi|Hi, Ti, TCi , , ,Bi) = Pr(Bi = bi|Hi, Ti, ) =
bi(ti;Hi, )i1
k=0 k(ti;Ht, )
(C.3.4)
By similar arguments used to obtain (26) and (27), the denominator of equation
(C.3.2) is equal to:
g(i,j)  g(j,bbj )
1 2
f1(mi)
g(i,j)
)]I(j>0) [f0(mi)]I(j=0)
g(c,i)  g(i,j)
1 2
f1(mc)
g(c,i)
)} j(ti;Hi, )i1
k=0 k(ti;Hi, )
. (C.3.5)
The conditional posterior Bi|Y,Bi,,(bi) may be obtained by combining expressions (C.3.3),
(C.3.4) and (C.3.5). When this is done, the summations indexed by k in (C.3.4) and
(C.3.5) cancel to give, up to proportionality:
APPENDIX C. SUPPLEMENTARY MATERIALS TO CHAPTER 6 223
Pr(Bi = bi|Y, , ,Bi) 
g(i,bi)  g(bi,bbi )
1 2
f1(mi)
g(i,bi)
)]I(bi>0) [f0(mi)]I(bi=0)
g(c,i)  g(i,bi)
1 2
f1(mc)
g(c,i)
)}bi(ti;Hi, ) . (C.3.6)
Using this conditional posterior distribution, the elements of the branching vector
B1, . . . , Bn may be sequentially updated within the Gibbs sampling scheme used
to fit the correlated ETAS model. As was noted in Appendix C.1, this requires
Bi|Y,,,Bi(bi) only up to a constant of proportionality, and so the normalising denom-
inator term (C.3.6) (which for a given event i = 1, . . . , n is equal for all bi = 0, . . . , i1)
need not be evaluated.
Note that when  = 0 the conditional posterior of each branching element C.3.6
simplifies to
Pr(Bi = bi|Y, , ,Bi)  fI(bi>0)
mi;I(bi>0)
{f1(mc)}bi(ti;Hi, )
 fI(bi>0)
mi;I(bi>0)
bi(ti;Hi, ),
which is the corresponding expression for the dual magnitude model. As noted previ-
ously, this further simplifies to the standard ETAS model when the same distribution is
used for magnitudes of background and triggered events so that f0(m;0) = f1(m;1)
for all m.
Appendix D
Supplementary materials to
Chapter 7
D.1 Outline for combined covariate and aftershock
model
D.1.1 Connection to main text
In Chapter 7, one suggestion given for further work was to develop a combined model
for induced earthquakes and aftershocks inspired by the ETAS model. In this sug-
gested model, earthquakes do not induce further events by altering the point process
intensity function directly, but instead by altering the covariates (such as incremental
Coulomb stress) which feed into a parametric intensity model. In this appendix we
give some initial thoughts to motivate such a model and consider how a model of this
type might be constructed.
APPENDIX D. SUPPLEMENTARY MATERIALS TO CHAPTER 7 225
D.1.2 Motivation
To describe clustering of observed events in the Groningen earthquake catalogue, the
epidemic type aftershock sequence (ETAS) model can be used. The intensity function
for this point process has two components: a background intensity that changes in
space and time based on gas extraction and additional intensity that is added around
each past event. The background intensity is currently modelled as a function of the
stress state of the reservoir. The additional intensity is allocated isotropically about
the past events, where the amount of added intensity is an increasing function of event
magnitude and a decreasing function of both distance and time since the event.
An issue with this model is that the occurrence of background and triggered events are
treated differently. The background event intensity is driven by the stress state of the
reservoir, but the additional aftershock intensity does not depend on the stress state
of the reservoir at all. Here we describe a model in which all seismic activity is driven
by the stress state of the reservoir. We propose to achieve this by having earthquakes
increase the stress state around previous events, and therefore indirectly increase the
point process intensity function, rather than increasing the intensity function around
past events directly.
In this appendix we explore a few simple forms for how the additional stress might
be allocated over the space and time around each event. We compare these sugges-
tions based on their implications for the intensity function. We then state the generic
likelihood function for the proposed model, without restricting to one choice of ad-
ditional strain allocation, and discuss its potential issues. When giving an outline of
this combined aftershock model, we consider a background intensity function where
earthquake count increases exponentially with cumulative stress c, which motivates
APPENDIX D. SUPPLEMENTARY MATERIALS TO CHAPTER 7 226
an intensity model of the form:
1(x, t) = c(x, t)0(1 + 1c(x, t)) exp(1c(x, t)), (D.1.1)
where c(x, t) is the cumulative stress at location x and time t caused by gas extraction,
c(x, t) is the stress rate caused by gas extraction and (0, 1) are model parameters to
be estimated. Where it clarifies exposition, the explicit dependence of the covariates
on location and time will be dropped, so that equation (D.1.1) is equivalent to:
1 = c0(1 + 1c) exp(1c). (D.1.2)
D.1.3 Suggested model
D.1.3.1 Description
We suggest that rather than previous earthquakes directly adding to the intensity
function, that they should instead add to the covariate that is driving seismic activity
in the region of the event. This will contribute to the intensity through the same
model as the stress caused by gas extraction, providing a model that is more cohesive
and coherent with the physical process that is thought to be driving earthquake
activity.
In the proposed model, the intensity function from equation (D.1.1) would be replaced
2 = s(x, t,Ht)0(1 + 1s(x, t,Ht)) exp(1s(x, t,Ht)) (D.1.3)
where s(x, t,Ht) is the total stress, which is a function of location (x), time (t)
and now also depends on previous earthquakes (Ht). The total stress s(x, t,Ht) is
composed of the stress caused by gas extraction c(x, t) and and the stress caused by
previous earthquakes (x, t,Ht):
s(x, t,Ht) = c(x, t) + (x, t,Ht). (D.1.4)
APPENDIX D. SUPPLEMENTARY MATERIALS TO CHAPTER 7 227
The total stress rate s(x, t,Ht) is the sum of the stress rates from extraction and
previous earthquakes:
s(x, t,Ht) = c(x, t) + (x, t,Ht). (D.1.5)
Figures D.1.1 and D.1.2 graphically present the differences in dependence structure
between the current ETAS model and the proposed combined model. In the ETAS
model, induced events and aftershocks form separate model components, while under
the combined model these are modelled together.
Figure D.1.1: Schematic of ETAS model inputs, component models, and outputs.
Figure D.1.2: Schematic of combined model inputs, component models, and outputs.
APPENDIX D. SUPPLEMENTARY MATERIALS TO CHAPTER 7 228
D.1.3.2 Modelling assumptions
In Sections D.1.6 and D.1.7 we will propose functional forms for the total addi-
tional stress at location x and time t caused by all previous earthquakes earthquake
(x, t,Ht). We then discuss the implications of these forms on the point process
intensity function. In doing so, we will make the following assumptions:
1. The additional stress from all previous earthquakes is small relative to the cu-
mulative stress caused by gas extraction, (x, t,Ht) c(x, t). This means that
the cumulative total stress is approximated well by the cumulative stress from
gas extraction:
s(x, t,Ht)  c(x, t). (D.1.6)
2. The total additional stress is the sum of contributions from all of the previous
earthquakes. The size of the contribution from the ith event should depend the
proximity of its space-time location (xi, ti), as well as that events magnitude
mi. The form of this dependence is common to all earthquakes so that
(x, t,Ht) =
i:ti<t
f(x xi, t ti,mi) (D.1.7)
for some function f : R2R+[Mc,Mmax] R+, where the Mc is the magnitude
of completion of the catalogue and Mmax is the upper end point of the magnitude
distribution.
3. The effects of distance, time and magnitude on additional stress are independent,
so that
f(x, t,m) = x(x)t(t)m(m). (D.1.8)
where x : R2  R+, t : R+  R+ and m : [Mc,Mmax] R+.
APPENDIX D. SUPPLEMENTARY MATERIALS TO CHAPTER 7 229
D.1.3.3 Intensity function
Under the assumptions D.1.6 to D.1.8, we may rewrite the intensity function for the
proposed model in equation (D.1.3) as:
2 = (c+ )0(1 + 1(c+ )) exp(1(c+ ))
 (c+ )0(1 + 1c) exp(1c)
= c0(1 + 1c) exp(1c) + 0(1 + 1c) exp(1c)
= c0(1 + 1c) exp(1c)+
0(1 + 1c) exp(1c)
i:ti<t
x(x xi)m(mmi)t(t ti) . (D.1.9)
The similarities differences between the proposed and ETAS models are apparent
from inspection of this intensity function. As in the ETAS model, we have a baseline
intensity driven by gas extraction (the first term in equation (D.1.9)) and additional
contributions to the intensity function from previous events (the second term in equa-
tion (D.1.9)). However, in the combined aftershock model, the allocation of additional
intensity through space is controlled by the amount of added stress at that distance
and the allocation of additional intensity through time is controlled by the added
stress rate.
A second difference from the ETAS model is that, under the combined aftershock
model, additional intensity contribution from an earthquake is dependent on the cu-
mulative stress in the area surrounding (and time period following) that event. This
means that the expected number of aftershocks produced by an earthquake of known
magnitude will be greater if that event occurs occurs in a region or at a time where
the cumulative stress is higher. This follows from the logic that in areas of higher
stress a greater number of faults are close to slipping and so a given increase in stress
(whether caused by gas extraction or other earthquakes should result in a greater num-
ber of induced earthquakes in these areas, as compared to areas with lower stresses.
APPENDIX D. SUPPLEMENTARY MATERIALS TO CHAPTER 7 230
A model that includes this property is highly desirable, because it is leads to a co-
herent description of the physical process which triggers both background events and
aftershocks.
An additional benefit of the combined model form is that it leads to low aftershock
productivity in regions or time periods with low cumulative stresses. This may help
to mitigate against window- and edge-effects. This is because additional aftershock
intensity will be better contained within the spatial extent of the gas field if the
cumulative stresses in the area surrounding the field are comparatively low.
D.1.4 Likelihood function
In order to fit the proposed model, whether by maximum likelihood or Bayesian in-
ference, the likelihood of the parameters of the model will have to be repeatedly
calculated. Since the proposed model is still a Hawkes process, the overall form of
the likelihood function is similar to that of the ETAS model. Let the parameters
determining x, t and m be represented by the vector , and let  = (0, 1,) be
the vector of all parameters in the proposed model. For a catalogue of earthquakes
Y = {(xi, ti,mi) : i = 1, . . . , n)} occurring in the spatial region A and time period
[0, T ], which we call the observation window W = A  [0, T ], and given the extrac-
tion related stresses on this window c, then the likelihood function for the combined
aftershock model is:
`(|x, t,m, c) =
log (2(xi, ti|Hti ,, c))
2(,  |H ,, c)dd. (D.1.10)
The double integral in this expression can be decomposed into two double integrals,
respectively describing the expected event count from gas extraction and aftershocks.
APPENDIX D. SUPPLEMENTARY MATERIALS TO CHAPTER 7 231
Specifically,
(W ) =
2(,  |H ,, c)dd
c(, )0(1 + 1c(, )) exp(1c(, ))dd +
0(1 + 1c(, )) exp(1c(, ))
i:ti<
x(  xi)t(  ti)m(mi)dd
0 [c(, T ) exp(1c(, T )) c(, 0) exp(1c(, 0))] d +
0(1 + 1c(, )) exp(1c(, ))
i:ti<
x(  xi)t(  ti)m(mi)dd
(D.1.11)
The first of the double integrals has (by design) a closed form in time, but still requires
numerical integration over space. The second double integral must be evaluated nu-
merically over both time and space. If all parameters are updated together then this
integration will have to be performed at each evaluation of the log-likelihood, which
will be computationally very demanding.
If parameters are updated sequentially as part of an MCMC scheme then by storing
the value of integrals that are unchanged by the update we can reduce the compu-
tational burden of evaluating the log-likelihood. To clarify this point we can rewrite
equation (D.1.11) as
(W ) = 0
[c(, T ) exp(1c(, T )) c(, 0) exp(1c(, 0))] d +
m(mi)
x(  xi)
(1 + 1c(, )) exp(1c(, ))t()dd
(D.1.12)
In this form we can see that in order to update 0 or the parameters of m, no
additional numerical integration is required. When updating 1 or the parameters of
x, then numerical integration is only required in two dimensions. It is only when
APPENDIX D. SUPPLEMENTARY MATERIALS TO CHAPTER 7 232
updating the parameters of t that numerical integration over space and time will
be required. Exploiting this structure might lead to significant reductions to the
computational burden of evaluating the combined model.
D.1.5 Condition for sub-criticality
As with the ETAS model, the proposed model can be viewed as a branching process.
In order for this process to be sub-critical, the expected number of aftershocks per
triggering event must be strictly less than one. Let g(m) denote a probability density
function that represents the distribution of earthquake magnitudes. We will assume
here that this distribution is independent of the stress state at the location of the event.
This simplifying assumption can later be relaxed if the combined model improves upon
the ETAS model with the same assumptions.
Conditional on the location and time of the triggering event, the expected number of
aftershocks from a triggering event is:
E [Nafter|x, t] =
0(1 + 1c(, )) exp(1c(, ))x(  x)t(  t)m(m)g(m)dddm
= E [m(M)]
0(1 + 1c(, )) exp(1c(, ))x(  x)t(  t)dd.
To get the unconditional expectation, we would need to integrate the product of this
conditional expectation and the probability of an event occurring at each point over
the observation window:
E [Nafter] =
E [Nafter|,  ]
2(, )
(W )
dd. (D.1.13)
To ensure sub-criticality of the branching process, it is required that E [Nafter] < 1.
This condition that is much more difficult to verify than that for the ETAS model,
which can be found by a similar derivation and is stated in Zugec (2019).
APPENDIX D. SUPPLEMENTARY MATERIALS TO CHAPTER 7 233
D.1.6 Parametric forms for the temporal change in stress
Figure D.1.3 shows six possibilities for how additional stress might be allocated over
time following an earthquake. Each black curve shows a suggested additional stress
allocation, and the additional stress rate associated with each of these is shown in
red. It is the red curves that show the additional stress rate which will impact the
intensity function, and which should be the focus here.
The first option, in Figure D.1.3a, is an immediate step change in stress. This is
likely not reasonable because it implies a delta function peak in the stress rate, adding
an infinite peak to the intensity at the instant of the triggering event but leaving it
otherwise unchanged. The second option, in Figure D.1.3b is a more gradual transition
to the higher stress state. The stress is initially loaded slowly before reaching some
maximum loading rate, after which the loading rate decreases as the final stress state
is approached. This leads to the additional intensity peaking at some time after
the event and then diminishing again so that the intensity is not effected at long lead
times. Figure D.1.3c shows a similar change in the stress state, an increase to plateau,
but with a stress rate that is initially high and then reduces as the stress reaches its
maximum. This option provides the closest analogue to the ETAS model.
Options four through to six, in Figures D.1.3d - D.1.3f, all suppose that at very long
times after an event, the stress state is unchanged by the occurrence of the event.
This causes these models to have a common deficiency in that they all require the
stress state to decrease, which requires the stress rate to become negative. This has
the effect of reducing the intensity function, possibly after some delay. This does
not match with the idea that earthquakes should cause more seismic activity in the
future, not less.
It seems that the additional stress function shown in Figure D.1.3c is the one that
APPENDIX D. SUPPLEMENTARY MATERIALS TO CHAPTER 7 234
(a) (b)
(c) (d)
(e) (f)
Figure D.1.3: Proposed forms for additional stress (black) and stress rate (red) as
functions of time.
APPENDIX D. SUPPLEMENTARY MATERIALS TO CHAPTER 7 235
is most similar to current practice, but that Figure D.1.3b may also be a feasible
option. One possible parameterisation of this function in figure D.1.3c would take the
stress rate to be the normalised upper-half of a Gaussian distribution, and the stress
function be its integral. If () is the density function of standard Gaussian random
variable and () is its cumulative distribution function, then the additional stress and
stress rate functions are controlled by one parameter  and are stated below.
t(t) = 2
for t  0 and  > 0. (D.1.14)
t(t) = 2
 0.5
for t  0 and  > 0. (D.1.15)
Following feedback from Shell on these model forms, it was suggested that if additional
stress rate varied according to an inverse power law in time, then this would be
consistent with the rate and state friction model. Motivated by this and the parallels
that this form gives to the current modelling approach, we might process by a more
flexible version of the inverse power law model. For example could describe the
additional stress rate using a generalised Pareto density function and the additional
stress by its cumulative density function.
D.1.7 Parametric forms for the spatial change in stress
In the previous section, we considered several functional forms for how additional
stress might be allocated through time. In this section we will consider how additional
stress might be allocated over space. As a fist step we will consider the additional
stress to be isotropic, so that it is a function of distance from a point source, the
location of a previous event. Figure D.1.4 shows several potential functional forms
for additional stress as a function of distance from the triggering earthquake. Over
space, unlike for time, it is the amount of additional stress that impacts the intensity
APPENDIX D. SUPPLEMENTARY MATERIALS TO CHAPTER 7 236
function not the stress rate.
In the first function, shown in Figure D.1.4a, the stress is increased equally at all
distances. In the second function, shown in Figure D.1.4b, the stress is increased
equally but only within a fixed radius. The first of these does not seem sensible,
because we would expect that the additional stress should be negligible at very large
distances. The second seems more reasonable but we would also expect that additional
stress to be a smooth, decreasing function of distance. The second function does
not satisfy either of these, as it is piece-wise constant and contains a discontinuity.
Figures D.1.4c and D.1.4d present functional forms for the additional stress which
lead to intensity responses that are similar to the ETAS model. In these functions,
additional stress is a continuous function of distance and is strictly decreasing when the
additional stress is non-zero. At very large distances these functions respectively allow
the additional stress to either be very small or zero. Both of these functional forms
would be possible if the additional stress were allocated over space proportional to a
generalised Pareto density. The functions presented in Figures D.1.4e and D.1.4f have
additional stress peak some distance away from the previous event and then deplete as
distance increases beyond this value. At very short distances these models respectively
leave the stress unchanged or reduce stress. A justification for the decentralised
additional stress might be that earthquakes releases some local stress which is either
expressed in the earthquake or passed on to neighbouring faults.
Of these options, the generalised Pareto form seems like the least controversial choice
as it is the most similar in structure to the currently used ETAS model. Following
discussions with Shell, it was advised that a heavy-tailed generalised-Pareto allocation
of the additional stress over space is consistent with the idea of previous events causing
elastic dislocations within the reservoir. The generalised Pareto form seems to provide
a sensible model for initial development of a combined aftershock model.
APPENDIX D. SUPPLEMENTARY MATERIALS TO CHAPTER 7 237
(a) (b)
(c) (d)
(e) (f)
Figure D.1.4: Proposed forms for additional stress as a function of distance x.
APPENDIX D. SUPPLEMENTARY MATERIALS TO CHAPTER 7 238
D.1.8 Parametric forms for the effect of magnitude on addi-
tional stress
In the previous sections, we considered how additional stress is added through space
and time. Another question of interest is how the magnitude of an earthquake im-
pacts the total amount of additional stress that it causes. This is the purpose of the
function m. Since both x and t are density functions, these both integrate to one
over their respective supports and the total additional stress from an earthquake is
completely controlled by m. Figure D.1.5 shows six suggestions for the form of this
function.
If small magnitude earthquakes are not thought to add to the local stress state then
there may be a threshold involved in the relationship; this is included in models shown
in red in Figure D.1.5. This type of threshold is assumed in the ETAS model, where
the threshold is chosen to be equal to the magnitude of completion but it could equally
be chosen as a greater magnitude. Indeed, this will be the case in the latter portion of
the Groningen catalogue since the magnitude of completion has been reduced. Above
this chosen threshold level, increasing magnitude could change the stress state in a
number of ways. Magnitude might have no effect as in Figure D.1.5a, a linear effect
as in Figure D.1.5b, or exponential effect on the total additional stress as in Figure
D.1.5c.
The exponential increase seems most sensible, since magnitudes are measured on a
logarithmic scale. Since magnitude is a measure of the energy released by an earth-
quake and stress has units Nm2 = Jm3 it seems reasonable that a ten-fold increase
in the energy released would lead to a ten-fold increase in additional stress. As with
the other model components, using a generalised Pareto form here rather than an
exponential would allow for more flexibility, in particular it can account for the pos-
APPENDIX D. SUPPLEMENTARY MATERIALS TO CHAPTER 7 239
sibility of a sub-exponential relationship, which may arise if not all energy that is
released is propagated as additional stress.
A similar energy based argument could be used in support support an inverse power-
law style of decay of additional stress with distance. If this set amount of stress is
dissipated radially, then it would seem reasonable that it decrease in an inverse-square
or inverse-cube relation to distance. Again, this can be generalised by using an inverse
power-law, which would allow the power to be selected as part of model fitting, or
using a generalised Pareto form which allows more localised allocation.
Following discussions with Shell, we were advised that an exponentially increasing
form for m is equivalent to the constant stress drop model for slipping faults. For
this reason and the similarities to the ETAS model it would be prudent to initial
develop a model using an exponential form. At a later point the need for a more
flexible model could be investigated.
D.1.9 Summary
This appendix has introduced some initial ideas about the form and implementa-
tion of a ETAS-style model which provides a coherent treatment for the source of
both induced earthquakes and their aftershocks. While the combined modelling ap-
proach for all earthquakes is conceptually appealing, the resulting model is likely to
presents many of the same challenges as direct estimation of the ETAS model: highly
correlated parameters, issues with numerical optimisation routines, and a likelihood
function that is computationally expensive to evaluate. Additionally, this combined
aftershock model requires extensive use of numerical integration, which further in-
creases computational burden, and the model is not amenable to the conditional
inference approaches which facilitate estimation of the ETAS model.
APPENDIX D. SUPPLEMENTARY MATERIALS TO CHAPTER 7 240
(a) (b)
Figure D.1.5: Proposed forms for the relationship between magnitude and total ad-
ditional stress. Additional stress added by events above the magnitude of completion
Mc (black) or some higher magnitude u (red).
APPENDIX D. SUPPLEMENTARY MATERIALS TO CHAPTER 7 241
It is for the reasons listed above that we have chosen not to develop this model
further, instead to focus on extending and improving inference for the ETAS model
in Chapter 6 of this thesis. We present these initial ideas here in the hope that they
might be informative to anyone aiming to develop a similar model in the future.
Bibliography
Aki, K. (1965). Maximum likelihood estimate of b in the formula logn= a-bm and its
confidence limits. (http://hdl.handle.net/2261/12198).
Baddeley, A. (2008). Analysing spatial point patterns in R. Technical report, CSIRO,
2010. Version 4. (https://research.csiro.au/software/r-workshop-notes).
Baddeley, A., Barany, I., and Schneider, R. (2007). Spatial Point Processes and their
Applications. Springer.
Baddeley, A., Mller, J., and Waagepetersen, R. (2000). Non-and semi-parametric
estimation of interaction in inhomogeneous point patterns. Statistica Neerlandica,
54(3):329350.
Basrak, B., Wintenberger, O., and Zugec, P. (2019). On the total claim amount for
marked poisson cluster models. Advances in Applied Probability, 51(2):541569.
Bath, M. (1965). Lateral inhomogeneities of the upper mantle. Tectonophysics,
2(6):483514.
Beirlant, J., Kijko, A., Reynkens, T., and Einmahl, J. H. (2019). Estimating the
maximum possible earthquake magnitude using extreme value methodology: the
Groningen case. Natural Hazards, 98(3):10911113.
http://hdl.handle.net/2261/12198
https://research. csiro. au/software/r-workshop-notes
BIBLIOGRAPHY 243
Benali, A., Peresan, A., Varini, E., and Talbi, A. (2020). Modelling background
seismicity components identified by nearest neighbour and stochastic declustering
approaches: the case of northeastern italy. Stochastic Environmental Research and
Risk Assessment, 34:775791.
Bierman, S., Kraaijeveld, F., and Bourne, S. (2015). Regularised direct inversion to
compaction in the Groningen reservoir using measurements from optical leveling
campaigns. NAM report SR, 15:11194.
Bourne, S. and Oates, S. (2015). An activity rate model of seismicity induced by
reservoir compaction and fault reactivation in the Groningen gas field. NAM report.
Bourne, S. and Oates, S. (2017a). Extreme threshold failures within a heterogeneous
elastic thin sheet and the spatial-temporal development of induced seismicity within
the Groningen gas field. Journal of Geophysical Research: Solid Earth, 122(12):10
Bourne, S. and Oates, S. (2017b). Induced seismicity within the Groningen gas field
[data set]. Zenodo. https://doi. org/10.5281/zenodo, 103522.
Bourne, S., Oates, S., and Van Elk, J. (2018). The exponential rise of induced seis-
micity with increasing stress levels in the Groningen gas field and its implications
for controlling seismic risk. Geophysical Journal International, 213(3):16931700.
Bourne, S., Oates, S., van Elk, J., and Doornhof, D. (2014). A seismological model
for earthquakes induced by fluid extraction from a subsurface reservoir. Journal of
Geophysical Research: Solid Earth, 119(12):89919015.
Bourne, S. J. and Oates, S. (2020). Stress-dependent magnitudes of induced earth-
quakes in the Groningen gas field. Journal of Geophysical Research: Solid Earth,
125(11):e2020JB020013.
BIBLIOGRAPHY 244
Bourne, S. J. and Oates, S. J. (2017c). Development of statistical geomechanical
models for forecasting seismicity induced by gas production from the Groningen
field. Netherlands Journal of Geosciences, 96(5):s175s182.
Brown, E. N., Barbieri, R., Ventura, V., Kass, R. E., and Frank, L. M. (2002).
The time-rescaling theorem and its application to neural spike train data analysis.
Neural Computation, 14(2):325346.
Cai, J.-J., Wan, P., and Ozel, G. (2021). Parametric and non-parametric estimation of
extreme earthquake event: the joint tail inference for mainshocks and aftershocks.
Extremes, 24(2):199214.
Carter, D. and Challenor, P. (1981). Estimating return values of environmental pa-
rameters. Quarterly Journal of the Royal Meteorological Society, 107(451):259266.
Chavez-Demoulin, V. and Davison, A. C. (2005). Generalized additive modelling
of sample extremes. Journal of the Royal Statistical Society: Series C (Applied
Statistics), 54(1):207222.
Chavez-Demoulin, V., Davison, A. C., and McNeil, A. J. (2005). Estimating value-
at-risk: a point process approach. Quantitative Finance, 5(2):227234.
Coles, S., Pericchi, L. R., and Sisson, S. (2003). A fully probabilistic approach to
extreme rainfall modeling. Journal of Hydrology, 273(1-4):3550.
Coles, S. G. (2001). An Introduction to Statistical Modeling of Extreme Values.
Springer, New York.
Coles, S. G. and Pericchi, L. (2003). Anticipating catastrophes through extreme value
modelling. Journal of the Royal Statistical Society: Series C (Applied Statistics),
52(4):405416.
BIBLIOGRAPHY 245
Coles, S. G. and Tawn, J. A. (1996). A Bayesian analysis of extreme rainfall data.
Journal of the Royal Statistical Society: Series C (Applied Statistics), 45(4):463
Cox, D. R. and Isham, V. (1980). Point Processes, volume 12. CRC Press.
Danielsson, J., de Haan, L., Peng, L., and de Vries, C. G. (2001). Using a boot-
strap method to choose the sample fraction in tail index estimation. Journal of
Multivariate Analysis, 76(2):226248.
Davison, A. C. and Smith, R. L. (1990). Models for exceedances over high thresholds
(with discussion). Journal of the Royal Statistical Society: Series B (Methodologi-
cal), 52(3):393425.
Davison, M., Ita, J., Hofmann, R., Seli, P., and Ong, P. (2010). Ensuring production
in the Malaysian F6 field using field monitoring and geomechanical forecasting.
Proceedings of the International Oil and Gas Conference and Exhibition in China.
Dempsey, D. and Suckale, J. (2017). Physics-based forecasting of induced seismicity
at Groningen gas field, the Netherlands. Geophysical Research Letters, 44(15):7773
7782.
Diggle, P. (1983). Statistical Analysis of Spatial Point Patterns. Mathematics in
Biology. Academic Press.
Dorai-Raj, S. (2001). First-and second-order properties of spatiotemporal point pro-
cesses in the space-time and frequency domains. Doctoral Dissertation, Virginia
Polytechnic Institute and State University.
Dost, B., Goutbeek, F., van Eck, T., and Kraaijpoel, D. (2012). Monitoring induced
seismicity in the North of the Netherlands: Status Report 2010. KNMI scientific
report.
BIBLIOGRAPHY 246
Dost, B., Ruigrok, E., and Spetzler, J. (2017). Development of seismicity and prob-
abilistic hazard assessment for the Groningen gas field. Netherlands Journal of
Geosciences, 96(5):235245.
Eastoe, E. F. and Tawn, J. A. (2009). Modelling non-stationary extremes with ap-
plication to surface level ozone. Journal of the Royal Statistical Society: Series C
(Applied Statistics), 58(1):2545.
Fawcett, L. and Green, A. C. (2018). Bayesian posterior predictive return levels for
environmental extremes. Stochastic Environmental Research and Risk Assessment,
32(8):22332252.
Ferro, C. A. and Segers, J. (2003). Inference for clusters of extreme values. Journal
of the Royal Statistical Society: Series B (Statistical Methodology), 65(2):545556.
Fisher, R. A. and Tippett, L. H. C. (1928). Limiting forms of the frequency distri-
bution of the largest or smallest member of a sample. Mathematical Proceedings of
the Cambridge Philosophical Society, 24:180190.
Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., and Rubin, D. B.
(2013). Bayesian Data Analysis. CRC press.
Genest, C., Ghoudi, K., and Rivest, L.-P. (1995). A semiparametric estimation proce-
dure of dependence parameters in multivariate families of distributions. Biometrika,
82(3):543552.
Gulia, L., Rinaldi, A. P., Tormann, T., Vannucci, G., Enescu, B., and Wiemer, S.
(2018). The effect of a mainshock on the size distribution of the aftershocks. Geo-
physical Research Letters, 45(24):13277.
Gutenberg, B. and Richter, C. F. (1956). Earthquake magnitude, intensity, energy,
BIBLIOGRAPHY 247
and acceleration: (second paper). Bulletin of the Seismological Society of America,
46(2):105145.
Hadi Mehranpour, M., Hangx, S. J., and Spiers, C. J. (2020). Discrete element method
modelling of Groningen reservoir compaction using a new contact model describing
elastic and inelastic grain-scale interactions. In EGU General Assembly Conference
Abstracts, page 13941.
Hainzl, S. and Christophersen, A. (2017). Testing alternative temporal aftershock de-
cay functions in an etas framework. Geophysical Journal International, 210(2):585
Heffernan, J. E. and Tawn, J. A. (2001). Extreme value analysis of a large designed
experiment: a case study in bulk carrier safety. Extremes, 4(4):359378.
Helmstetter, A. and Sornette, D. (2003). Baths law derived from the Gutenberg-
Richter law and from aftershock properties. Geophysical Research Letters, 30(20).
Hol, S., Van der Linden, A., Bierman, S., Marcelis, F., and Makurat, A. (2018).
Proxies for quantifying depletion-induced reservoir compaction in the Groningen
field. In 52nd US Rock Mechanics/Geomechanics Symposium. OnePetro.
Hutton, K., Woessner, J., and Hauksson, E. (2010). Earthquake monitoring in south-
ern California for seventy-seven years (19322008). Bulletin of the Seismological
Society of America, 100(2):423446.
Joe, H. (2014). Dependence Modeling with Copulas. CRC press.
Jonathan, P., Randell, D., Wadsworth, J., and Tawn, J. (2021). Uncertainties in
return values from extreme value analysis of peaks over threshold using the gener-
alised Pareto distribution. Ocean Engineering, 220,107725.
BIBLIOGRAPHY 248
Kijko, A. and Singh, M. (2011). Statistical tools for maximum possible earthquake
magnitude estimation. Acta Geophysica, 59(4):674700.
Kirchner, M. (2017). Perspectives on Hawkes Processes. PhD thesis, ETH Zurich.
https://doi.org/10.3929/ethz-b-000161487.
KNMI (2020). Aardbevings catalogus. (https://www.knmi.nl/
kennis-en-datacentrum/dataset/aardbevingscatalogus).
Kolev, A. A. and Ross, G. J. (2020). Semiparametric Bayesian forecasting of spatial
earthquake occurrences. arXiv preprint arXiv:2002.01706.
Kostrov, V. (1974). Seismic moment and energy of earthquakes, and seismic flow of
rock. Physics of the Solid Earth, 1:1321.
Laio, F. (2004). Cramervon Mises and Anderson-Darling goodness of fit tests for
extreme value distributions with unknown parameters. Water Resources Research,
40(9).
Ledford, A. W. and Tawn, J. A. (1997). Modelling dependence within joint tail
regions. Journal of the Royal Statistical Society: Series B (Statistical Methodology),
59(2):475499.
Limbeck, J., Bisdom, K., Lanz, F., Park, T., Barbaro, E., Bourne, S., Kiraly, F., Bier-
man, S., Harris, C., Nevenzeel, K., et al. (2021). Using machine learning for model
benchmarking and forecasting of depletion-induced seismicity in the groningen gas
field. Computational Geosciences, 25(1):529551.
Little, R. J. and Rubin, D. B. (2019). Statistical Analysis with Missing Data, volume
793. John Wiley & Sons.
Lombardi, A. M. (2002). Probabilistic interpretation of Baths law. Annals of Geo-
physics, 45(3-4):455472.
https://www.knmi.nl/kennis-en-datacentrum/dataset/aardbevingscatalogus
https://www.knmi.nl/kennis-en-datacentrum/dataset/aardbevingscatalogus
BIBLIOGRAPHY 249
Marcon, E. and Puech, F. (2009). Generalizing Ripleys k function to inhomogeneous
populations. HAL (https://halshs.archives-ouvertes.fr/halshs-00372631).
Marzocchi, W., Spassiani, I., Stallone, A., and Taroni, M. (2019). How to be fooled
searching for significant variations of the b-value. Geophysical Journal International,
220(3):18451856.
McGarr, A. (1976). Dependence of magnitude statistics on strain rate. Bulletin of
the Seismological Society of America, 66(1):3344.
Mignan, A., Werner, M., Wiemer, S., Chen, C., and Wu, Y. (2011). Bayesian es-
timation of the spatially varying completeness magnitude of earthquake catalogs.
Bulletin of the Seismological Society of America, 101(3):13711385.
Mignan, A. and Woessner, J. (2012). Estimating the magnitude of completeness
for earthquake catalogs. Community Online Resource for Statistical Seismicity
Analysis.
Molkenthin, C., Donner, C., Reich, S., Zoller, G., Hainzl, S., Holschneider, M.,
and Opper, M. (2020). GP-ETAS: semiparametric Bayesian inference for the
spatio-temporal Epidemic Type Aftershock Sequence model. arXiv preprint
arXiv:2005.12857.
NAM (2016a). Technical addendum to the Winningsplan Groningen 2016, Document
1: Summary and Production. Technical report, NAM.
NAM (2016b). Technical addendum to the Winningsplan Groningen 2016, Document
2: Subsidence. Technical report, NAM.
NAM (2016c). Technical addendum to the Winningsplan Groningen 2016, Document
3: Hazard. Technical report, NAM.
https://halshs.archives-ouvertes.fr/halshs-00372631
BIBLIOGRAPHY 250
NAM (2016d). Technical addendum to the Winningsplan Groningen 2016, Document
4: Risk. Technical report, NAM.
NAM (2016e). Technical addendum to the Winningsplan Groningen 2016, Document
5: Damage and Appendices. Technical report, NAM.
NAM (2017). English information site. https://www.nam.nl/
english-information.html. (26-07-2017).
Northrop, P. J. and Jonathan, P. (2011). Threshold modelling of spatially depen-
dent non-stationary extremes with application to hurricane-induced wave heights.
Environmetrics, 22(7):799809.
Ogata, Y. (1988). Statistical models for earthquake occurrences and residual analysis
for point processes. Journal of the American Statistical Association, 83(401):927.
Ogata, Y. (1998). Space-time point-process models for earthquake occurrences. An-
nals of the Institute of Statistical Mathematics, 50(2):379402.
Ogata, Y. (2011). Significant improvements of the space-time ETAS model for fore-
casting of accurate baseline seismicity. Earth, Planets and Space, 63(3):6.
OHagan, A. and Forster, J. J. (2004). Kendalls Advanced Theory of Statistics,
volume 2B: Bayesian Inference. Arnold.
Omori, F. (1894). On the aftershocks of earthquakes. Journal of the College of
Science, Imperial University of Tokyo, 7:111120.
Paap, B., Carpentier, S., van Maanen, P.-P., and Meekes, S. (2020). A neural net-
work approach for improved seismic event detection in the Groningen gas field, the
Netherlands. arXiv preprint arXiv:2001.07027.
https://www.nam.nl/english-information.html
https://www.nam.nl/english-information.html
BIBLIOGRAPHY 251
Paleja, R. and Bierman, S. (2016). Measuring changes in earthquake occurrence rates
in Groningen. Shell Global Solutions International BV (Amsterdam).
Pawitan, Y. (2001). In All Likelihood: Statistical Modelling and Inference using Like-
lihood. Oxford University Press.
Pickands, J. (1975). Statistical inference using extreme order statistics. Annals of
Statistics, 3(1):119131.
Ramsay, J. (2006). Functional Data Analysis. Springer, 2nd ed. edition.
Reinhart, A. (2018). A review of self-exciting spatio-temporal point processes and
their applications. Statistical Science, 33(3):299318.
Richter, G., Hainzl, S., Dahm, T., and Zoller, G. (2020). Stress-based, statistical
modeling of the induced seismicity at the Groningen gas field, the Netherlands.
Environmental Earth Sciences, 79:115.
Rohrbeck, C., Eastoe, E. F., Frigessi, A., and Tawn, J. A. (2018). Extreme value
modelling of water-related insurance claims. Annals of Applied Statistics, 12(1):246
Ross, G. (2016). Bayesian estimation of the ETAS model for earthquake occurrences.
Technical Report.
Scarrott, C. and MacDonald, A. (2012). A review of extreme value threshold estima-
tion and uncertainty quantification. REVSTATStatistical Journal, 10(1):3360.
Schoenberg, F. P. (2013). Facilitated estimation of ETAS. Bulletin of the Seismological
Society of America, 103(1):601605.
BIBLIOGRAPHY 252
Shahriari, B., Swersky, K., Wang, Z., Adams, R. P., and De Freitas, N. (2015). Taking
the human out of the loop: A review of Bayesian optimization. Proceedings of the
IEEE, 104(1):148175.
Sharkey, P. and Tawn, J. A. (2017). A Poisson process reparameterisation for Bayesian
inference for extremes. Extremes, 20(2):239263.
Shcherbakov, R., Zhuang, J., Zoller, G., and Ogata, Y. (2019). Forecasting the mag-
nitude of the largest expected earthquake. Nature communications, 10(1):111.
Silverman, B. (1986). Density Estimation for Statistics and Data Analysis, volume 26.
CRC press.
Smith, J. D., White, R. S., Avouac, J.-P., and Bourne, S. (2020). Probabilistic
earthquake locations of induced seismicity in the Groningen region, the Netherlands.
Geophysical Journal International, 222(1):507516.
Smith, R. L. (1985). Maximum likelihood estimation in a class of nonregular cases.
Biometrika, 72(1):6790.
Smith, R. L. (1989). Extreme value analysis of environmental time series: an appli-
cation to trend detection in ground-level ozone. Statistical Science, 4(4):367377.
Snoek, J., Larochelle, H., and Adams, R. P. (2012). Practical Bayesian optimization of
machine learning algorithms. In Advances in neural information processing systems,
pages 29512959.
Stallone, A. and Marzocchi, W. (2019). Empirical evaluation of the magnitude-
independence assumption. Geophysical Journal International, 216(2):820839.
TNO (2017). Geological survey of the Netherlands, Groningen gasfield. (http://
nlog.nl/en/groningen-gasfield).
http://nlog.nl/en/groningen-gasfield
http://nlog.nl/en/groningen-gasfield
BIBLIOGRAPHY 253
Utsu, T. (1957). Magnitude of earthquakes and occurrence of their aftershocks. Zisin,
Ser. 2, 10:3545.
Utsu, T., Ogata, Y., and Matsuura, R. S. (1995). The centenary of the Omori formula
for a decay law of aftershock activity. Journal of Physics of the Earth, 43(1):133.
van Elk, J., Doornhof, D., Bourne, S., Oates, S., Bommer, J., Visser, C., van Eijs, R.,
and van den Bogert, P. (2013). Technical addendum to the Winningsplan Groningen
2013. Technical report, NAM.
van Stiphout, T., Zhuang, J., and Marsan, D. (2012). Seismicity declustering. Com-
munity Online Resource for Statistical Seismicity Analysis.
van Thienen-Visser, K. and Breunese, J. (2015). Induced seismicity of the Groningen
gas field: History and recent developments. The Leading Edge, 34(6):664671.
van Thienen-Visser, K., Sijacic, D., van Wees, J., Kraaijpoel, D., and Roholl, J.
(2016). TNO report R10425. SodM.
Varini, E., Peresan, A., and Zhuang, J. (2020). Topological comparison be-
tween the stochastic and the nearest-neighbor earthquake declustering meth-
ods through network analysis. Journal of Geophysical Research: Solid Earth,
125(8):e2020JB019718.
Veen, A. and Schoenberg, F. P. (2008). Estimation of spacetime branching pro-
cess models in seismology using an EMtype algorithm. Journal of the American
Statistical Association, 103(482):614624.
Vere-Jones, D. (2010). How to educate yourself as a statistical seismologist. Commu-
nity Online Resource for Statistical Seismicity Analysis.
Wadsworth, J., Tawn, J., and Jonathan, P. (2010). Accounting for choice of measure-
ment scale in extreme value modeling. Annals of Applied Statistics, 4:15581578.
BIBLIOGRAPHY 254
Wadsworth, J. L. (2016). Exploiting structure of maximum likelihood estimators for
extreme value threshold selection. Technometrics, 58(1):116126.
Wadsworth, J. L. and Tawn, J. A. (2012). Likelihood-based procedures for thresh-
old diagnostics and uncertainty in extreme value modelling. Journal of the Royal
Statistical Society: Series B (Statistical Methodology), 74(3):543567.
Waheed, U. b., Shaheen, A., Fehler, M., and Fulcher, B. (2020). Winning with sim-
ple learning models: Detecting earthquakes in Groningen, the Netherlands. arXiv
preprint arXiv:2007.03924.
Walker, A. J. (1977). An efficient method for generating discrete random variables
with general distributions. ACM Transactions on Mathematical Software (TOMS),
3(3):253256.
Wiemer, S. and Wyss, M. (2000). Minimum magnitude of completeness in earthquake
catalogs: Examples from Alaska, the western United States, and Japan. Bulletin
of the Seismological Society of America, 90(4):859869.
Wilson, S. (2020). ParBayesianOptimization: Parallel Bayesian Optimization of Hy-
perparameters. R package version 1.2.1.
Woessner, J. and Wiemer, S. (2005). Assessing the quality of earthquake catalogues:
Estimating the magnitude of completeness and its uncertainty. Bulletin of the
Seismological Society of America, 95(2):684698.
Zhuang, J., Harte, D. S., Werner, M. J., Hainzl, S., and Zhou, S. (2012). Basic
models of seismicity: Temporal models. Community Online Resource for Statistical
Seismicity Analysis.
Zhuang, J., Ogata, Y., and Vere-Jones, D. (2002). Stochastic declustering of space-
BIBLIOGRAPHY 255
time earthquake occurrences. Journal of the American Statistical Association,
97(458):369380.
Zhuang, J., Werner, M. J., Hainzl, S., Harte, D., and Zhou, S. (2011). Basic models
of seismicity: Spatiotemporal models. Community Online Resource for Statistical
Seismicity Analysis.
Zoller, G. and Holschneider, M. (2016). The maximum possible and the maximum
expected earthquake magnitude for production-induced earthquakes at the gas field
in Groningen, the Netherlands. Bulletin of the Seismological Society of America,
106(6):29172921.
Zugec, P. (2019). Marked Poisson Cluster Processes and Applications. PhD thesis,
University of Zagreb. Faculty of Science. Department of Mathematics.
	Abstract
	Acknowledgements
	Declaration
	Contents
	List of figures
	List of tables
	Introduction
	Motivation
	A brief history of the Groningen gas field
	Geology of the Groningen gas field
	From gas extraction to induced earthquakes
	Shell seismic risk analysis
	Thesis outline
	Outline
	Earthquake catalogue
	Static covariates
	Fault structure
	Reservoir thickness and topographic gradient
	Extraction covariates
	Exploratory analysis
	Outline
	Exploratory spatial analysis
	Exploratory temporal analysis
	Exploratory magnitude analysis
	Review
	Literature review
	Point process models
	Overview
	Poisson point processes
	Generalisations of the Poisson process
	Measures of clustering
	Extreme value methods
	Overview
	Block maxima approach
	Peaks over threshold
	Point process representation
	Inference for extreme value models
	Earthquake modelling
	Overview
	Epidemic Type Aftershock Sequence models
	Physics-based modelling
	Recent work on Groningen seismicity
	Covariate-based models for induced earthquake locations
	Introduction
	Induced earthquakes
	Motivation and aims
	Outline
	Background
	Point process models for earthquakes
	Baseline intensity model
	Alternative models
	Approach outline
	Model simplifications
	Model extensions
	Results
	Outline
	Covariate smoothing
	Model reductions
	Model extensions
	Conclusions and further work
	Inference for extreme earthquake magnitudes accounting for a time-varying censoring process
	Introduction
	Aims and motivation
	Earthquake data
	Magnitude of completion
	Extreme value methods
	Shortcomings of current methods
	Contributions and outline
	Motivating data and model formulation
	Data description
	Data model and inference
	Motivating the inclusion of small magnitudes
	Simulation study overview
	Simulation study results
	Threshold selection
	Overview
	Graphical assessment
	Metric-based assessment
	Minimisation procedure
	Threshold selection on simulated catalogues
	Simulation study overview
	Constant threshold, hard censoring
	Constant threshold, phased censoring
	Non-constant threshold selection
	Application to Groningen earthquakes
	Validating data model for Groningen catalogue
	Parametric threshold forms
	Threshold selection
	Discussion / Conclusion
	Improving and extending the ETAS formulation
	Introduction
	The ETAS model
	Magnitude modelling
	Bayesian ETAS modelling
	Contributions and chapter outline
	Estimation of ETAS parameters
	Direct estimation: ETAS as a point process
	Latent estimation: ETAS as a branching process
	Benefits of the latent estimation approach
	Extreme value reparameterisation of empirical laws
	Empirical laws
	Proposed parameterisation
	Comparing parameterisations
	Demonstration on simulated catalogue
	Extensions of the magnitude model
	Dual magnitude extension
	Correlated magnitude extension
	Application of extended ETAS models to simulated catalogues
	Dual magnitude extension
	Correlated magnitude extension
	Conclusions and further work
	Conclusions and further work
	Supplementary materials to Chapter 4
	Integrated intensity functions
	Maps of annual expected earthquake counts
	Supplementary materials to Chapter 5
	Assessing i.i.d. assumption for Groningen earthquakes
	Connection to main text
	Exploratory analysis
	Bootstrap datasets and parameter estimates
	Connection to main text
	Generating bootstrapped data-sets
	Generating bootstrap maximum likelihood estimates
	Sampling standardised threshold exceedances
	Connection to main text
	Sampling unrounded threshold exceedances
	Transformation onto common margins
	Supplementary figures
	Supplementary materials to Chapter 6
	B conditional posterior, dual magnitude model
	 conditional posterior, correlated magnitudes
	B conditional posterior, correlated magnitudes
	Supplementary materials to Chapter 7
	Outline for combined covariate and aftershock model
	Connection to main text
	Motivation
	Suggested model
	Likelihood function
	Condition for sub-criticality
	Parametric forms for the temporal change in stress
	Parametric forms for the spatial change in stress
	Parametric forms for the effect of magnitude on additional stress
	Summary
	Bibliography
