K-means landscapes: exploring
clustering solution spaces using
energy landscape theory
Luke Dicks
Department of Chemistry
Corpus Christi College, University of Cambridge
This dissertation is submitted for the degree of Doctor of Philosophy
May 11, 2021
Declaration
This dissertation is the result of my own work and includes nothing which is the
outcome of work done in collaboration except where specifically indicated in the text.
It is not substantially the same as any that I have submitted for a degree or diploma
or other qualification at any other University and no part has already been, or is
concurrently being, submitted for any degree, diploma, or other qualification. It does
not exceed 60,000 words, including tables, footnotes, bibliography and appendices.
Signature: Luke Dicks
Date: 29/03/21
Acknowledgments
First and foremost, I would like to thank my family for their continued support and
encouragement, without which none of this would have been possible. I would also
like to thank my partner for her support and patience throughout the most stressful
parts of writing this thesis.
I am very grateful to my supervisor, Prof. David J. Wales FRS, for allowing
me the freedom to explore many strange and new ideas, whilst providing valuable
input when needed. Additionally, I would like to thank the other members of the
theoretical chemistry RIG for many useful scientific discussions, and the members
of my office, for many useful discussions that had nothing to do with science.
I would like to gratefully acknowledge the Engineering and Physical Sciences
Research Council (EPSRC) for PhD funding through grant EP/L015552/1.
Abstract
K-means, one of the simplest clustering algorithms, is ubiquitous in every scientific
field. Its cost function supports many possible clustering solutions, and location of
low-valued clustering solutions can be challenging. Hence, the topography of the cost
function surface is crucial to understanding K-means performance. We present the
application of energy landscape theory to the K-means cost function to elucidate its
topography, which we deduce from K-means landscapes described in terms minima
and transition states.
We analyse K-means landscapes for Fishers Iris dataset, the glass identification
dataset, and many variations in which we alter their properties. For K-means the
number of clusters must be prespecified, and we consider the effect of that choice
on the Iris landscapes. K-means landscapes are also constructed for the glass and
Iris datasets with varying numbers of outliers, and different feature standardisa-
tions. Both the removal of outliers and standardisation are common procedures
during dataset preprocessing. Moreover, we systematically change the cluster over-
lap and cluster populations for the two datasets, both of which are crucial to K-
means accuracy. The K-means landscapes for all these modified datasets allows us
to understand the effect of the most important dataset features on cost function
topography.
In all cases we observe that the cost function topography changes independently
of K-means accuracy; a dataset modification that increases clustering accuracy can
also make the cost function surface harder to explore. Therefore, the maximum
accuracy is not always sufficient to understand K-means success, and we must also
consider the feasibility of obtaining these accurate solutions. The K-means land-
scapes allow us to address the second of these considerations. For most dataset
properties the resulting topography changes are largely systematic, and the con-
cepts can be applied to novel datasets to predict K-means performance, highlight
possible problems, and select a sampling strategy to improve future applications.
Abbreviations
(A)RI (Adjusted) Rand index
(D)NEB (Doubly-)nudged elastic band
DPS Discrete path sampling
GM Global minimum
GT Graph transformation
(H)EF (Hybrid) eigenvector-following
KML K-means landscape
KT Kinetic trap
KTN Kinetic transition network
L-BFGS Limited-memory BFGS minimisation algorithm
MC Monte Carlo
MECP Minimum energy crossing point
ML Machine learning
NP Non-deterministic polynomial-time
PCA Principal component analysis
PES Potential energy surface
SD Steepest-descent
SSE Sum of squared error
TS(A) Transition state (analogue)
Contents
1 Introduction 1
1.1 Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.2 K-means . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
1.2.1 Locating good clustering solutions . . . . . . . . . . . . . . . . 3
1.2.2 Dataset structure . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.2.3 Cost function topography . . . . . . . . . . . . . . . . . . . . 6
2 Methods 8
2.1 Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
2.1.1 Outliers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
2.1.2 Standardisation . . . . . . . . . . . . . . . . . . . . . . . . . . 9
2.1.3 Overlap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
2.1.4 Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
2.2 K-means landscape . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
2.2.1 Locating minima . . . . . . . . . . . . . . . . . . . . . . . . . 14
2.2.2 Locating transition states . . . . . . . . . . . . . . . . . . . . 15
2.2.3 Discrete path sampling . . . . . . . . . . . . . . . . . . . . . . 21
2.3 Visualising landscapes . . . . . . . . . . . . . . . . . . . . . . . . . . 24
2.3.1 Disconnectivity graphs . . . . . . . . . . . . . . . . . . . . . . 24
2.3.2 Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
2.4 Thermodynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
2.4.1 Heat capacity . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
2.4.2 Shannon entropy . . . . . . . . . . . . . . . . . . . . . . . . . 28
2.5 Rates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
2.5.1 Dijkstra fastest path . . . . . . . . . . . . . . . . . . . . . . . 28
2.5.2 Graph transformation . . . . . . . . . . . . . . . . . . . . . . 29
2.6 Comparing clusterings . . . . . . . . . . . . . . . . . . . . . . . . . . 30
2.6.1 Adjusted Rand index . . . . . . . . . . . . . . . . . . . . . . . 31
CONTENTS
3 Cluster number 33
3.1 Landscape topography . . . . . . . . . . . . . . . . . . . . . . . . . . 33
4 Preprocessing: Outliers 41
4.1 Clustering solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
4.2 Landscape structure . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
4.2.1 Topography . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
4.2.2 Minima distribution . . . . . . . . . . . . . . . . . . . . . . . 48
4.2.3 Kinetic trap relaxation rates . . . . . . . . . . . . . . . . . . . 51
4.3 Transitions between clusterings . . . . . . . . . . . . . . . . . . . . . 55
4.4 Clustering quality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
4.4.1 Diversity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
4.4.2 Accuracy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
4.4.3 Similarity to original dataset . . . . . . . . . . . . . . . . . . . 62
4.4.4 Relation to topography . . . . . . . . . . . . . . . . . . . . . . 63
5 Preprocessing: Standardisation 65
5.1 Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
5.2 Iris . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
5.2.1 Landscape structure . . . . . . . . . . . . . . . . . . . . . . . 67
5.2.2 Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
5.2.3 Clustering quality . . . . . . . . . . . . . . . . . . . . . . . . . 77
5.3 Glass . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
5.3.1 Landscape structure . . . . . . . . . . . . . . . . . . . . . . . 79
5.3.2 Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87
5.3.3 Clustering quality . . . . . . . . . . . . . . . . . . . . . . . . . 89
6 Dataset structure: Overlap 93
6.1 Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
6.2 Clustering behaviour . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
6.3 Landscape structure . . . . . . . . . . . . . . . . . . . . . . . . . . . 95
6.3.1 Kinetic traps . . . . . . . . . . . . . . . . . . . . . . . . . . . 98
6.3.2 Minima distribution . . . . . . . . . . . . . . . . . . . . . . . 104
6.4 Distribution of properties . . . . . . . . . . . . . . . . . . . . . . . . . 107
6.4.1 Rates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112
7 Dataset structure: Cluster distribution 115
7.1 Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
7.2 Iris . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
CONTENTS
7.2.1 Landscape structure . . . . . . . . . . . . . . . . . . . . . . . 115
7.2.2 Rates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
7.2.3 Accuracy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120
7.3 Glass . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
7.3.1 Landscape structure . . . . . . . . . . . . . . . . . . . . . . . 122
7.3.2 Distribution of clustering properties . . . . . . . . . . . . . . . 126
7.3.3 Rates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128
8 Conclusions 132
8.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132
8.2 Cluster number . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134
8.3 Outliers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134
8.4 Standardisation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136
8.5 Overlap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138
8.6 Cluster distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139
8.7 Application to novel datasets . . . . . . . . . . . . . . . . . . . . . . 140
Appendix A Outliers 143
Appendix B Standardisation 155
Appendix C Overlap 160
Appendix D Cluster distribution 171
References 181
Chapter 1
Introduction
1.1 Clustering
Developed heavily in the fields of computer vision and speech processing, machine
learning encompasses the study of computer algorithms capable of improving perfor-
mance based on previous experience. Algorithmic advances, paired with the avail-
ability of large datasets, has led to a rapid growth in machine learning applications
over the last decade. Many of these novel applications have been in the physical and
life sciences, and machine learning now pervades all aspects of physics,1 chemistry2
and biology.3
Machine learning algorithms are usually separated into two classes: supervised
and unsupervised. Supervised methods, which comprise classification and regression
models, learn a relationship between input and output from labelled data. In con-
trast, unsupervised learning algorithms attempt to infer structure and relationships
from unlabelled data.
Clustering is the primary unsupervised learning task. The aim of clustering is
to separate a given dataset into subsets (clusters) whose members display a greater
similarity with points in the same cluster to those in other clusters. Successful
clustering can reveal the underlying structure of a dataset, and provide insight into
the phenomena that govern it.
Many clustering algorithms have been developed, and they can be broadly di-
vided into hierarchical and non-hierarchical methods. Hierarchical algorithms, which
can be agglomerative or divisive, generate a hierarchy of clusterings with varying
numbers of clusters. Agglomerative hierarchical clustering begins with each data
point as a separate cluster and repeatedly merges the nearest two clusters until all
data points belong to a single cluster. Divisive clustering instead begins from a clus-
ter containing all the data points, which it splits iteratively. In contrast, partitional
Introduction
algorithms find a single solution, with a given cluster number, after all clusters are
optimised simultaneously.
A further relevant distinction between clustering algorithms is hard or soft clus-
tering. Hard clustering algorithms allow a data point to be a member of only a
single cluster. In contrast, soft clustering algorithms allow data points to belong to
multiple clusters simultaneously, each with a given probability.
1.2 K-means
K-means,47 a hard partitional clustering algorithm, is one of the simplest and
most popular approaches. It has been used in many diverse fields, and there have
been recent applications in finance,811 signal processing,12 text processing,13 image
processing,14,15 mobile networks,16 wind power forecasting,17 chemical modelling,18
and biology.1924
The K-means algorithm partitions N data points for an Nf -dimensional space,
into a prespecified number of clusters, K. Each cluster is represented by the mean
of the data points assigned to it, each data point is assigned to its nearest cluster,
and the cost function is the sum of the squared distances between each cluster
and its assigned data points. For some given initial cluster positions, the K-means
algorithm obtains a valid clustering solution by minimising the sum-of-squares cost
function until a local minimum is attained. The cost function surface is non-convex
and can support many local minima, which correspond to clusterings of varying
quality. Therefore, to locate the low-valued clustering solutions the algorithm is
repeated from many initial cluster positions.
The popularity of K-means stems from its efficiency and its algorithmic simplic-
ity, which results in straightforward implementation and interpretation of results.
Furthermore, and most importantly, K-means provides results in agreement with
many more complex, and time-consuming, algorithms over a wide range of applica-
tions.25,26 However, the K-means algorithm is not without limitations.
The number of clusters must be prespecified, which necessitates repetition of K-
means at a variety of K for new datasets. There are a variety of internal metrics that
assess clustering quality without reference to a known labelling, such as the Dunn
index,27 silhouette coefficient,28 Davies-Bouldin index29 and the score function.30
These metrics can be calculated at each K to aid selection of the best number of
clusters to represent the data. However, all the metrics have drawbacks and it is
not usually obvious which value of K is most appropriate.
Furthermore, the K-means cost function generates spherical clusters and, con-
Introduction
sequently, K-means struggles to partition clusters with non-spherical distributions.
Alternative distance metrics have been used to allow non-spherical clusters to be
captured, such as the Mahalanobis distance to allow for hyperellipsoidal clusters.31
Variants of K-means with both the Itakura-Saito distance32 and the L1 distance
have also been proposed.
Many other variants of K-means have been proposed that attempt to alleviate
some of its drawbacks. Kernel K-means3436 allows for detection of non-linearly
separable clusters, which cannot be distinguished by standard K-means. Fuzzy
K-means37,38 is a soft K-means algorithm that allows data points to belong to
multiple clusters and performs better on overlapping clusters. K-medoids39 does
not use the mean for cluster centres, but rather the data point within the cluster
that minimises the cost function. This cluster definition makes the algorithm more
robust to outliers. Finally, X-means40 automatically selects the appropriate number
of clusters by optimising the Akaike (or Bayesian) Information Criterion.
1.2.1 Locating good clustering solutions
The number of local minima for the K-means cost function can be vast, and enu-
meration of all minima is impossible for all but trivial cases.39 Location of the global
minimum clustering solution is an NP-hard problem even for two clusters41 or two
dimensions.42,43 Despite the complexity of the clustering problem, K-means should
converge to the global minimum clustering with a high probability when the clus-
ters are well separated.44 Furthermore, even for experimental datasets, which rarely
provide clearly-separable clusters without noise, location of the global minimum
clustering may not always be necessary. There can be many clusterings similar to
the global minimum at slightly higher values of the cost function.45 For small K a
large number of the low-valued minima are similar to the global minimum, allowing
a nearly optimal solution to be attained repeatedly.46 However, it is worth noting
that for larger values of K similar cost function values can correspond to markedly
different cluster partitions.46
Many schemes for initialising cluster centres have been proposed, and their rel-
ative performance is reviewed for many datasets.4751 The results demonstrate that
the best initialisation method depends heavily upon the datasets studied. There-
fore, we describe several of the most popular K-means initialisation methods. For
all these schemes it is possible to retain solutions from all initial cluster centres.
These clusterings can then be combined using ensemble methods to obtain the final
clustering solution, and an estimate of its quality.5254
Several initialisation schemes rely upon random samples of the data, which gives
Introduction
them a low computational cost. Each data point can be randomly assigned to
one of K clusters,55 and the means of each of these random clusters provides the
initial cluster coordinates. Alternatively, a small randomly-selected subset of the
data points can be clustered using K-means and used as initial cluster positions
for the full dataset,56 or K random data points can be selected for initial cluster
positions.57 These two last methods rely on random samples being preferentially
drawn from high-density regions, which are more likely to be near cluster centres.
Modifications to randomly selectingK data points for cluster centres can improve
efficiency, and a popular example is the K-means++ initialisation scheme.58 The
first random data point is selected as before, but the subsequent K  1 data points
are selected with a probability inversely related to their proximity to the current
cluster centres. This heuristic preferentially selects well-separated initial cluster
coordinates, whilst still favouring high-density regions. However, the addition of
distance calculations increases the computational cost. Modifications have been
proposed to reduce this computational cost,59,60 but this is a common result; better
seedings entail a greater computational cost.
Further more expensive initialisation schemes use other algorithms to help gen-
erate initial positions. The result of a hierarchical clustering can be used to seed
K-means.61 This method performs well,48 but the computational cost of hierarchical
clustering prevents application to large datasets. A similar size limitation is seen
for a pairing of K-means and principal component analysis (PCA).62
All the schemes described above generate uncorrelated initial cluster positions. It
is also possible to use information from known solutions to guide future searching, as
is common in global optimisation algorithms. Such schemes are not very popular in
K-means research, where the preference is for many parallel initialisations. Perhaps
the simplest proposed sequential method perturbs the current clustering solution by
changing data point labels, repeats the minimisation, and accepts the new clustering
based on the cost function.63 Additionally, K-means has been paired with a global
optimisation algorithm. Genetic algorithms have been used to follow a population
of K-means solutions and generate more initial cluster positions.6466
1.2.2 Dataset structure
The structure of a dataset, along with the number of clusters, controls both the
quality of clustering solutions and the ease with which low-valued clustering solu-
tions can be attained. Datasets vary widely, and an understanding of the properties
(data type, dimensionality, number of data points, cluster overlap, cluster distribu-
tion) of a given dataset is essential to clustering success. Therefore, datasets are
Introduction
frequently modified prior to clustering, in a step known as preprocessing, to change
the structure and favour more accurate clusterings.67
Preprocessing commonly includes both removal of outliers and standardisation.
Outliers, defined as data points that differ significantly from the rest, can badly
degrade the quality of clustering that can be obtained.68 Therefore, care is taken to
identify, and remove, outliers as the first step of preprocessing. Outlier removal can
also be performed during clustering,6971 but standardisation is much more effective
if outliers have already been removed.
Standardisation applies a function to every data point that attempts to equalise
the importance of each feature. Features can span very different ranges; for example
they could be measured in different units. The features with larger ranges will have
a greater effect on the clustering, and mask the information in other dimensions.
Consequently, many functions have been proposed to equalise feature importance,
and achieve more accurate clustering. The optimal standardisation depends upon
context and data structure,72 but two standardisations, Z-score and MinMax, appear
to be amongst the best performing in a range of scenarios.26,73,74
The structure of a dataset, with or without preprocessing, underpins clustering
success. Consequently, several studies have related dataset properties to clustering
accuracy. The number of data points,55,75,76 and the dimensionality of the dataset,77
do not directly effect the quality of clustering. Increased dimensionality is expected
to degrade the clustering, due to sparseness of space78 or distances becoming uni-
form,79 but the dimensionality only changes the quality of clustering when it affects
the cluster overlap.
The degree of overlap itself has a profound effect on clustering, and is seen as
the best indicator of clustering quality.48 Increasing overlap reduces clustering ac-
curacy of the K-means algorithm, as obtained in various datasets and initialisation
schemes. For significant overlap the cost function will be unable to support a clus-
tering solution that distinguishes between the original clusters, and the accuracy
of the K-means global minimum clustering will be poorer. However, overlap can
aid location of the global minimum clustering.77 The presence of overlap allows bad
initialisations to be overcome by allowing cluster centres to move across the dataset
during minimisation, which is not possible if the clusters are separated by regions
without data points.
The probability distribution of each feature also has a strong effect on clus-
tering accuracy, and the best accuracy is achieved when all features are normally
distributed. However, in experimental datasets it is common for features to corre-
spond to different probability distributions.48,76,80 The other property with a large
Introduction
effect on the quality of K-means clustering is the relative size of the clusters. An
increasingly skewed distribution of datapoints amongst the clusters leads to degrada-
tion in clustering performance for all previously studied datasets.48,77,81 Unbalanced
cluster sizes cause both the cost function to be unsuitable for capturing the clusters,
and almost all initialisation methods to perform poorly, due to the low density of
one cluster relative to the others.
1.2.3 Cost function topography
The topography of the cost function surface, described by the K-means minima
and the intermediate behaviour of the cost function, is defined by both K and the
dataset. The effect of dataset structure on K-means performance is encoded by
the cost function topography and governs the ability of initialisation schemes to
locate low-valued K-means solutions. Furthermore, the quality of those clustering
solutions determines the maximum accuracy that K-means can obtain, and whether
the global minimum clustering is required.
The K-means cost function defines a surface analogous to the potential energy
surfaces routinely studied in physics and chemistry. Exploration of potential energy
surfaces is commonly performed within the energy landscape framework,82 which
provides a suite of algorithms for investigating such surfaces. Recent applications of
this framework have provided valuable insights into a wide range of physical systems,
such as nucleic acids,83,84 proteins,85,86 molecular clusters87,88 and glasses.89 Hence,
the application of the energy landscape exploration algorithms should enable us to
elucidate the K-means cost function topography.
The energy landscape exploration algorithms need to be modified for application
to machine learning landscapes, as has previously been achieved for the cost func-
tions of neural networks.9092 Here, we present a novel methodology for exploring
K-means cost function surfaces, which is detailed in Chapter 2. In the results chap-
ters that follow, we present K-means solution landscapes for Fishers Iris dataset,93
and the glass identification dataset.94 We present landscapes for Fishers Iris dataset
constructed with different numbers of clusters in Chapter 3. We construct K-means
landscapes (KMLs) for datasets containing outliers, and with standardisations ap-
plied, in Chapters 4 and 5, respectively. Removal of outliers and standardisation are
commonly performed during preprocessing of datasets to ensure greater K-means
accuracy. We address the effect of dataset features on cost function topography in
Chapters 6 and 7. Chapter 6 presents solution landscapes for datasets of varying
overlap. Chapter 7 investigates the change in solution landscape with varying cluster
populations. Finally, we provide an overview of how these landscapes can be used
Introduction
to understand the observed success and failure of K-means, and provide insight that
can guide more successful future applications in Chapter 8.
Chapter 2
Methods
2.1 Datasets
Two datasets were used in the studies here: Fishers Iris dataset93 and the glass
identification dataset,94 which can be downloadeda from the UCI machine learning
data repository.95 Both datasets are labelled, and we refer to this known correct
partitioning as the ground truth clustering. These datasets are commonly used to
evaluate clustering algorithms, as they are moderately challenging to cluster accu-
rately, and cannot be correctly partitioned using K-means. Furthermore, both these
datasets are low-dimensional and contain a small number of data points, which per-
mits a clear interpretation of the results for these preliminary studies. Moreover, the
low dimensionality reduces the computation time, allowing more dataset features to
be systematically explored.
Fishers Iris dataset contains 150 data points of four features each. There are
fifty measurements of each of the three Iris species: Virginica, Versicolor and Setosa.
The Setosa data points form a clearly-separable cluster, but measurements of the
Versicolor and Virginica species overlap, and cannot be separated by the K-means
algorithm.
The glass dataset is more complex than Fishers Iris dataset. It contains 214
data points separated into six uneven clusters of populations: 70, 76, 17, 13, 9
and 29. Each data point has nine features, which exhibit uneven feature ranges
that vary by two orders of magnitude. The ground truth clusters have significant
overlap, and around 50% of the data points cannot be correctly assigned within the
aThe data can be downloaded at the links:
 Iris  http://archive.ics.uci.edu/ml/datasets/Iris/
 Glass  https://archive.ics.uci.edu/ml/datasets/glass+identification
Methods
Table 2.1: The position of outliers added to the Iris dataset. The values of the features
are all given in centimetres.
Outlier Sepal length Sepal width Petal length Petal width
1 9.6 6.0 5.0 4.0
2 2.0 0.1 0.1 2.5
3 6.0 7.0 0.1 4.0
4 1.0 4.0 8.0 0.5
Mean 5.84 3.05 3.76 1.20
Range 4.37.9 2.04.4 1.06.9 0.12.5
Table 2.2: The coordinates of the outliers added to the glass dataset. Features are provided
in the same order as the data taken from the UCI machine learning data repository.
Dataset x1 x2 x3 x4 x5 x6 x7 x8 x9
1 1.3 5.0 7.0 6.0 60.0 9.0 2.0 5.0 3.0
2 1.7 23.0 7.0 7.0 90.0 9.0 21.0 5.0 4.0
3 1.8 4.0 8.0 7.0 85.0 9.0 1.0 5.0 4.0
Mean 1.52 13.41 2.68 1.44 72.65 0.50 8.96 0.18 0.06
Range 1.511.53 10.7317.38 0.004.49 0.293.50 69.8175.41 0.006.21 5.4316.19 0.003.15 0.000.51
sum of squared error (SSE) cost function used by K-means. Therefore, the accuracy
achievable by K-means is limited for this dataset.
We modify both datasets to give variants that differ in dataset properties relevant
to K-means clustering accuracy. Datasets are produced that vary in the number of
outliers, standardisation, overlap and cluster distribution. The construction of each
of these datasets is detailed below.
2.1.1 Outliers
Outliers were sequentially added to both datasets, four to the Iris dataset and three
to the glass identification dataset. The addition of each successive outlier produced
a distinct dataset, giving four Iris datasets with an increasing number of outliers.
Outliers were placed distant from both the original data points and any existing
outliers. Their coordinates were selected at random, and are given in Tables 2.1 and
2.2. For the Iris dataset, the coordinates lie outside the range of the original data
in at least half the features. In the glass datasets, the outliers lie outside the range
of the original data in every dimension. The distance between each outlier and the
mean of the overall data is not constrained, but is similar for all outliers.
2.1.2 Standardisation
Prior to standardisation it is common to remove outliers, as their presence is ex-
pected to degrade clustering of the standardised datasets significantly. We apply the
Methods
transformations to both the original Iris and glass datasets, and those to which we
added two outliers as specified above. Many standardisations have been proposed73
and we selected three of the most popular schemes to apply to all datasets. Each
scheme applies a simple function to every data point, X, to rescale the data and
prevent a single feature having a disproportionately large effect on clustering.
The first standardisation, Z1, known as Z-score normalisation, is,
(X X)
. (2.1)
X is the mean of the complete dataset, and s is its standard deviation. This function
transforms the dataset to have mean 0.0 and variance of 1.0 in every dimension.
Information about the position of the cluster centroids is lost.
The second standardisation is,
Max(X)
, (2.2)
which rescales the data relative to the largest value of the dataset in each given
feature, Max(X). For datasets where all the values are positive, such as those used
here, Z2 constrains the data to lie within the range 0  X  1. The same range is
enforced for the final standardisation,
X Min(X)
Max(X)Min(X)
, (2.3)
which is commonly referred to as min-max normalisation. After application of Z3
the data is also bounded by 0.0 and 1.0, but unlike Z2 the lower limit of 0.0 is
guaranteed to be observed.
2.1.3 Overlap
We created four variants of the Iris and glass datasets that differ in cluster overlap.
Two have increased overlap between the underlying clusters, and two have reduced
overlap, relative to the original dataset. We refer to the datasets with increased
overlap as Aggregation 1/2, and those with reduced overlap as Separation 1/2. The
numerical label indicates the extent of overlap, or separation; Separation 2 will have
a lower overlap than Separation 1.
Methods
Aggregation datasets were prepared by translation of every Setosa data point. The
Setosa cluster was displaced along the vector between its mean, xs, and the joint
mean of the Versicolor and Virginica clusters, (xs1 + 0.312x1, xs2  0.137x2, xs3 +
0.861x3, xs4+0.359x4)
T . The vector describes translation of a quarter of the distance
between the means. Displacement by one length of the vector gives the first dataset
(Aggregation 1) and successive application gives the second dataset (Aggregation
To produce the Separation datasets, with reduced overlap, we displaced the
Virginica measurements along the vector between their mean, xv, and the mean of
the Setosa data points. Data points were displaced in the direction away from the
Setosa cluster. Movement along this vector separates the Virginica and Versicolor
clusters, whilst maintaining separation from the Setosa data. The perturbation,
(xv1 + 0.650x1, xv2 + 0.200x2, xv3 + 1.290x3, xv4 + 0.706x4)
T , was applied once and
twice to generate the Separation 1 and Separation 2 datasets, respectively.
Glass
For the glass dataset we generated variants by displacing each cluster along the
vector from its own mean to the grand mean. Clusters were displaced towards,
or away from, the grand mean to generate Aggregation and Separation datasets,
respectively. The Aggregation 1 dataset has each cluster displaced one quarter of
the distance between their mean and the grand mean, Aggregation 2 has each cluster
displaced by half the distance. Displacement of half, and one times the distance, in
the opposite direction produced datasets Separation 1 and 2, respectively.
Quantifying overlap
We quantify the overlap of the datasets using the misclassification probability.96 The
misclassification probability is defined as
m(xi, ), (2.4)
where
m(xi, ) =
1, if d(xi,j) < d(xi,k)0, otherwise. (2.5)
Methods
Table 2.3: Overlap of the Iris and glass identification datasets, and their variants, as
measured by the misclassification probability.
Separation 2 Separation 1 Original Aggregation 1 Aggregation 2
Iris 0.00 0.67 7.34 8.67 11.34
Glass 38.32 46.26 51.87 55.14 59.81
d(xi,j) is the Euclidean distance between data point i and cluster centre j. Cluster
centres lie at the mean of their assigned data points in the ground truth labelling.
Cluster centre j is the cluster to which data point i belongs in the ground truth
labelling, and k can be any of the other clusters. The misclassification probability
gives the percentage of data points that are closer to another cluster than their
ground truth cluster. K-means will be unable to correctly assign all these data
points, due to its distance-based cost function.
The aggregation and separation of datasets shows the expected change in overlap
(Table 2.3). The overlap for the Iris dataset is modest. In the original dataset the
Setosa cluster does not overlap with the remaining two, and around 14% of the
data points in these Virginica and Versicolor clusters overlap. Translation of the
Setosa cluster towards to the remaining data points, as in the aggregated datasets,
is sufficient to cause all three clusters to overlap. Separation of the two overlapping
clusters by an equivalent amount is sufficient to remove all overlap.
The glass identification dataset has much more significant overlap. For the orig-
inal dataset we see over 50% of the data points are closer in space to another cluster
than the one their ground truth label provides. Moreover, translation of all data
points away from the grand mean reduces the overlap in the predicted manner, but
a significant proportion of the data points still overlap in the Separation 2 dataset.
2.1.4 Distribution
For both the Iris and glass datasets we prepared variations that differ in cluster dis-
tribution. Cluster distribution here refers to the relative populations of the clusters,
and not the underlying statistical distribution. For all datasets we retain the total
number of data points.
Where the population of a given cluster is reduced, relative to the original
dataset, data points were selected at random for removal. The selected data points
remain the same in all datasets. In the case of additional data points assigned to a
cluster, relative to the original dataset, these data points were generated by random
sampling from a Gaussian distribution fitted to the cluster. Clusters in the Iris
and glass are not necessarily Gaussian, but we expect the changes in the statistical
Methods
Table 2.4: The number of datapoints in each cluster for the original and modified Iris
datasets. There are eight modified datasets that are separated by horizontal lines into
groups with similar variations.
Dataset Versicolor Virginica Setosa
Original 50 50 50
1 70 40 40
2 90 30 30
3 110 20 20
4 40 40 70
5 30 30 90
6 20 20 110
7 70 30 50
8 90 10 50
distribution as a result of additional points to be modest. Again, the additional
data points remain the same for a given cluster in all datasets.
The Iris dataset variants are created as prescribed above, and the new populations
are given in Table 2.4. The new datasets are broadly separated into three groups
that have similar modifications to the populations.
 Datasets 13 increase the population in one of the two overlapping clusters,
by removing data points evenly from the two remaining clusters.
 Datasets 46 increase the population of the well-separated cluster by evenly
removing data points from all other clusters.
 Datasets 78 increase the population of the overlapping Versicolor cluster, by
removing data points exclusively from the Virginica cluster it overlaps with.
Glass
The modified populations for the glass dataset are shown in Table 2.5. The mod-
ifications to populations are similar to those of the Iris dataset. We separate the
modified datasets into:
 Datasets 12 transform the population distribution to give an even distribution
of data points amongst all clusters.
 Datasets 34 increase the relative population of the most overlapping cluster
by removing data points evenly from all other clusters.
Methods
Table 2.5: Cluster populations for variants of the glass dataset. Datasets with similar
cluster population modifications are separated by horizontal lines.
Dataset N1 N2 N3 N4 N5 N6
Original 70 76 17 13 9 29
1 53 56 26 24 22 33
2 36 36 36 35 35 36
3 31 61 31 30 30 31
4 26 86 26 25 25 26
5 31 31 31 60 30 31
6 26 26 26 85 25 26
 Dataset 56 increase the relative population of the least overlapping cluster
by removing data points evenly from all other clusters.
2.2 K-means landscape
The K-means cost function defines a surface analogous to the potential energy
surfaces (PESs) for molecular systems. Therefore, we apply the energy landscape
framework, which is commonly used to explore PESs, to elucidate the topography
of K-means cost function surfaces. The application of energy landscape exploration
algorithms to the cost function surface is not straightforward. Here, we present
the essential modifications of the algorithms for applications to the K-means cost
function surface.
2.2.1 Locating minima
The K-means algorithm partitions N data points, each with Nf features, into a
prespecified number of clusters, K. The aim of K-means clustering is to minimise
the cost function,
J() =
rikxi  k2. (2.6)
x, an (Nf  N)-dimensional matrix, contains the coordinates of the N fixed data
points, and each xi, the i
th column of x, contains the components of data point
i.  contains the cluster coordinates, an (Nf  K)-dimensional matrix, with k
(the kth column of ) the position of cluster k in the Nf-dimensional space. The
cluster positions are the optimisable parameters. r is an (NK)-dimensional matrix
that indicates the assignment of each data point to the K clusters. rik gives the
Methods
assignment of data point i to cluster k, and each data point is assigned to only the
nearest cluster in Euclidean distance, according to
rik =
1, if k = minj xi  j
0, if otherwise.
(2.7)
Starting from any given cluster positions, the cost function is locally minimised
using Lloyds algorithm,7 which alternately: (i) calculates the optimal data point
assignment, r, for the current cluster positions, , (ii) places each cluster, k, at
the centre of the data points assigned to it. These two steps are repeated until
no change in data point assignment is observed, and the resulting  is a valid K-
means solution. Each K-means solution is guaranteed to be a local, but not global,
minimum on the cost function surface, and many initial cluster coordinates may be
necessary to find a good clustering solution.
A nave initialisation of cluster coordinates was employed to enumerate K-means
minima for the benchmark in question. Uniform random points were drawn in the
intervals {xj,min, xj,max}, where xj,min/max is the minimum/maximum extent in the
jth direction. For each specified number of clusters, K, 106 initial cluster coordinates
were generated, and minima with empty clusters were discarded.
2.2.2 Locating transition states
Cost function surface
The K-means cost function can be written in terms of all possible cluster assign-
ments,
J() = min
ik xi  k
, (2.8)
where S is the set, of size KN , containing all valid r matrices, those in which each
data point is assigned to a single cluster. This naive upper bound, O(KN), to the
number of K-means solutions is significantly higher than the best general bound of
O(NNf K), which is calculated by counting the number of Voronoi partitionings.97
The r(j) that gives the minimum value of the cost function, for given , is found by
assigning each data point to the nearest cluster in Euclidean distance. All r(j) that
correspond to permutations of the K clusters are considered equivalent.
The double summation in Eqn. (2.8), for any fixed r(j), describes a quadratic
and there is, therefore, a single quadratic form associated with each possible cluster
assignment. Many of these cluster assignments do not appear on the cost function
Methods
surface because they are not the optimal cluster assignment at any  (unfeasi-
ble clusterings). Other cluster assignments can be optimal, but not in the region
encompassing the minimum of their corresponding quadratic (non-minimal cluster-
ings). K-means solutions are the minima of those quadratics that correspond to
the optimal cluster assignment in the region that contains their minimum (minimal
clusterings), as shown in Fig. 2.1.
Transition state analogues
A transition state is defined as a point with zero gradient and a single negative
eigenvalue of the Hessian matrix.98 The spectrum of Hessian eigenvalues specifies
that a transition state is a maximum with respect to one direction, and a minimum in
all orthogonal directions, aside from those corresponding to continuous symmetries
with zero Hessian eigenvalues.
The gradient, for fixed r, is
g() = 2
rik(xi  k), (2.9)
and the associated Hessian is,

n1I 0 . . . 0
0 n2I . . . 0
. . .
0 0 . . . nK I
 . (2.10)
I is an (Nf Nf)-dimensional identity matrix, and nj is the number of data points
assigned to cluster j. The gradient and Hessian above are correct for any given
cluster assignment. However, at a change in cluster assignment both the gradient
and Hessian are undefined.
A change in cluster assignment corresponds to the intersection of two quadratics
on the cost function surface, defined by different cluster assignments. In almost all
cases, the change in cluster assignment corresponds to a single data point exchanged
between two clusters. The condition for intersection is that the two clusters are
equidistant from the data point they exchange, and all other data points retain the
same cluster assignment; the cost function is then continuous across all intersections.
Because the gradient of the cost function is undefined at intersections between
quadratics, no intersection seam can contain a transition state. Consequently, the
cost function surface contains no transition states, as the only points on the sur-
Methods
2 4 6 8 10 12 14 2 4 6 8 10 12 14
2 4 6 8 10 12 14 2 4 6 8 10 12 14
2 4 6 8 10 12 14 2 4 6 8 10 12 14
2 4 6 8 10 12 14
Figure 2.1: The K-means cost function surface for two clusters (K = 2), and a one-
dimensional dataset (Nf = 1) containing seven data items (N = 7). Intersections between
different clusterings are denoted by red lines, and each clustering is labelled and visualised
below. The data points are distributed along the one feature, x1, and assignment to the
two clusters is indicated by red or blue dots. The surface contains two minimal clusterings
(r(2) and r(4)), where the minima are shown by white dots. The remaining four clusterings
on the surface are non-minimal. r(7) is an example of an unfeasible clustering that does
not appear on the K-means surface. The surface contains a plane of symmetry at 1 = 2,
and reflected clusterings correspond to permutation of the cluster labels. We only consider
one of the possible permutations.
Methods
face where the gradient vanishes are the minima of minimal clusterings. However,
the minimum of an intersection seam, known as a minimum-energy crossing point
(MECP), can function as a transition state in the present context. An MECP is
minimal in all directions corresponding to motion along the intersection seam, and
can be a maximum in the remaining direction orthogonal to the intersection seam.
We consider an MECP that is a maximum orthogonal to the intersection seam as
a transition state analogue, Fig. 2.2. Transition state analogues, despite not hav-
ing the correct spectrum of Hessian eigenvalues, because the Hessian is undefined,
have the appropriate curvature for a transition state. Non-maximal MECPs do not
have the appropriate curvature, and are not considered further in the analysis of
networks.
A transition state analogue, when it exists between two quadratics, is the maxi-
mum on the minimum energy pathway between the clusterings. Therefore, transition
state analogues give the minimum height of the cost function barrier between two
different clusterings. An intersection seam does not have to contain an MECP, be-
cause it is possible that another intersection seam may appear before the minimum
is attained. In this case the minimum height of the cost function barrier will occur
at the triple intersection, which would correspond to the exchange of two data points
simultaneously, and is therefore unlikely to occur in practice.
Locating transition state analogues
There are numerous methods for locating transition states,99102 but many of them
rely on the function being smooth, and with discontinuous first and second deriva-
tives these approaches cannot be applied to transition state analogues. Therefore,
we employ a penalty-constrained MECP optimisation algorithm, used for approxi-
mately locating MECPs on conical intersections.103 A conical intersection, the inter-
section of two surfaces that represent electronic states, is analogous to an intersection
of quadratics on the K-means cost function surface.
The location of the MECP between any pair of cost function quadratics, defined
by cluster assignments r1 and r2, can be achieved through minimisation of the
objective function,
F+(, r1, r2, , ) =
J(, r1) + J(, r2)
J(, r1) J(, r2)
J(, r1) J(, r2)+ 
 . (2.11)
Methods
1 2 3 4 5
2 4 6 8 10 12 14 2 4 6 8 10 12 14
2 4 6 8 10 12 14 2 4 6 8 10 12 14
2 4 6 8 10 12 14 2 4 6 8 10 12 14
Figure 2.2: A portion of the K-means surface given in Fig. 2.1. r(2) and r(4) are minimal
clusterings, with minima (i) and (vi). For all labelled points, the positions of clusters 1
and 2, and the corresponding cluster assignments, are given below the surface. (iii) is
the MECP, and a transition state analogue, between states r(2) and r(3). (ii) and (iv) are
non-MECP points on the intersection seam that correspond to motion of only one cluster
from minimum (i). (v) is an MECP between r(3) and r(4) that is non-maximal in the
direction orthogonal to the intersection seam.
Methods
J(, rj) is the the cost function, evaluated under cluster assignment rj, at cluster
positions .  is a smoothing parameter to avoid a discontinuity at the intersection
seam.  determines how heavily to penalise the deviation of the current point from
the intersection seam.
For finite  the minimum of F+ will not lie at the true MECP. However, the
intersection seam can be found to arbitrary accuracy by evaluating the difference
between the two quadratics at the minimum of F+, and if not sufficiently small,
reminimising using  =  + , where  is a small positive value. Here, for each
F+, minimisation was performed using the L-BFGS (limited-memory Broyden,
Fletcher,105 Goldfarb,106 Shanno,107) algorithm,108,109 from an initial position close
to the intersection seam.  = 0.02 and  = 30 gave a difference between quadratics
of O(104), which was considered sufficiently small without performing multiple
minimisations. The MECP, obtained by this minimisation of F+, was then evaluated
on J to give the cost function at the transition state analogue between clusterings
r1 and r2.
For each MECP located we calculate the optimal cluster assignment and check
that it is the same as either the r1 or r2 between which it is a maximum. We only
retain MECPs where this condition is true, because otherwise the MECP does not
exist on the cost function surface because another intersection has been crossed, and
a different lowest-valued quadratic is present at this point.
Each transition state has two connected minima, which are found by following
(approximate) steepest-descent paths parallel and antiparallel to the eigenvector
corresponding to the single negative Hessian eigenvalue. The Hessian is undefined
at transition state analogues, so to obtain the required eigenvector an alternative
function is constructed, namely
F(, r1, r2, , ) =
J(, r1) + J(, r2)
J(, r1) J(, r2)
J(, r1) J(, r2)+ 
 . (2.12)
r1, r2,  and  are given the same values as in F+, and the function only differs in
penalising, rather than favouring, the two quadratics being similar in value. There-
fore, F has the desired curvature about the intersection seam, with a negative
curvature along the direction perpendicular to the intersection. At the minimum
of F+, we can evaluate the Hessian on F to find the negative eigenvalue and cor-
responding eigenvector (Fig. 2.3). It is worth noting that F will not distinguish
between transition state analogues and non-maximal MECPs. However, in almost
Methods
all cases the steepest-descent paths from a non-maximal MECP will both end at the
same minimum, as the path from the higher-valued clustering will, within a small
number of steps, recross the intersection seam. Therefore, non-maximal MECPs
provide no information about the connectivity between different minima on the cost
function surface.
The steepest-descent paths from a transition state analogue do not necessar-
ily end at the minima of the two quadratics for which it is an MECP. The two
quadratics between which the transition state analogue exists may correspond to
non-minimal clusterings and, therefore, the steepest-descent paths will pass through
these quadratics before attaining a minimal clustering.
2.2.3 Discrete path sampling
The resulting stationary point database provides the analogue of a kinetic transition
network in molecular science.110113 The database can be represented as a weighted
graph, where all minima are nodes and all pairs of minima connected by a transition
state analogue are joined by an edge weighted by a function of the barrier height.
To connect all minima in the stationary point database we select pairs of minima
using a distance-based criterion.114 This criterion finds the lowest-valued minimum
not currently in the subset containing the global minimum, and selects the nearest
member of this subset in Euclidean distance. The repeated application of this metric
to select minima leads to growth of the subset containing the global minimum until
all minima are connected.
An initial linear path is constructed between the chosen pair of minima, which is
composed of a discrete set of equally-spaced cluster positions, referred to as images.
This initial path is perturbed through random displacements applied to images
separated by a fixed interval. Each path segment between perturbed images remains
linear. The number of images is adapted such that at most one data point changes
cluster assignment between any two adjacent images.
Subsequently, the path is analysed and, at any change in cluster assignment
between adjacent images, the first image in the pair is considered a candidate for a
transition state analogue. For each candidate we construct the relevant F+, using
the differing cluster assignments, and minimise to obtain the MECP. Provided the
number of images is large, the two images differing in cluster assignment will be close
to the intersection seam, so that fewer minimisation steps are required. For every
valid MECP we subsequently evaluate the Hessian on F and find the connected
minima, Fig. 2.4. After evaluating all candidates, the valid transition state analogues
are added to the stationary point database, along with their connectivity.
Methods
Figure 2.3: A two-dimensional (Nf = 2, N = 7) and three-dimensional (Nf = 3, N = 9)
clustering example with two clusters (K=2). The assignment of data points to the two
clusters, given in black, is indicated by red or blue, and the data point that is exchanging
cluster membership is green. The transition state analogues were located by minimising
F+, and the Hessian evaluated on F. In two dimensions, the intersection seam requires
that both clusters lie on a circle about the exchanged data point, and the MECP is
the minimum with respect to radius and positions. In three dimensions the equivalent
condition corresponds to clusters equidistant on a sphere about the exchanged data point.
In both cases a single negative eigenvalue is observed, and the corresponding eigenvector
is aligned perpendicular to the intersection seam. The other Hessian eigenvectors, with
positive eigenvalues, correspond to motion along the intersection seam.
Methods
Figure 2.4: A scheme for locating transition state analogues and their connected minima,
illustrated for one transition state analogue on the cost function surface of Fig. 2.2. The
cost function (top left) contains one transition state analogue, on the intersection seam
between the clusterings r(2) and r(3), one non-maximal MECP, and two minima. F+,
constructed using r(2) and r(3), is minimised to obtain the yellow point (top right). The
equivalent F function is constructed (bottom left) and the Hessian evaluated at the
minimum of F+ to obtain the eigenvector that we use to define the steepest-descent paths,
shown with arrows. The original cost function, J , is evaluated at the minimum of F+ and
the steepest-descent paths calculated to obtain the connected minima (bottom right). The
paths for the forwards and backwards directions of the eigenvector are shown in orange
and blue.
Methods
For all K, after connection of all minima into a single set, we observe that a small
number of transition state analogues are lower in cost function than the higher of
their connected minima. A transition state analogue, as a maximum orthogonal
to the intersection seam, should not be lower in cost function than the minima
found by steepest-descent paths. The cause of the reversal in cost function values
is likely due to inexact location of MECPs. We expect to locate the intersection
seam with an accuracy of O(104), and the largest gap between a transition state
analogue and connected minimum is 1.6 104 for both the original Iris and glass
dataset. Moreover, this gap remains similar for the landscapes with larger cost
function ranges, such as those with mutliple outliers. Therefore, we add 2  104
to the cost function of each transition state analogue to leave the barriers almost
unchanged, whilst removing the cases where the connected minimum is higher than
the transition state analogue, which makes further analysis more convenient.
2.3 Visualising landscapes
The cost function surface is impossible to view directly for all but trivial clustering
problems. Therefore, we use two different two-dimensional mappings of cost function
surfaces to understand their topography; disconnectivity graphs and network graphs.
2.3.1 Disconnectivity graphs
To combat the high dimensionality of potential energy and cost function surfaces
we employ disconnectivity graphs,115,116 examples of which are shown in Fig. 2.5.
Disconnectivity graphs can be constructed for surfaces of arbitrary dimensions and
illustrate the minima and the barriers separating them. The barrier between any
two minima is given by the value of the cost function at the highest transition state
analogue on a path between them, which many contain one, or more, transition
state analogues.
Palm tree Weeping willow Banyan tree
Figure 2.5: Correspondence between the surface and the resulting disconnectivity graph.
Three different classes of energy landscapes are shown, (left) Palm tree  a steep funnel
with small barriers, (centre) Weeping willow  a gentle funnel with large barriers, (right)
Banyan tree  a rough landscape. Adapted from Ref. 116.
Methods
Disconnectivity graphs include all clustering solutions with vertical lines originat-
ing at the corresponding cost function value. For any given cost function threshold,
the minima can be separated into groups that are connected by barriers below the
threshold. Starting from a cost function value above the highest-valued minimum
we reduce the threshold by regular intervals and separate the minima into disjoint
sets, known as superbasins. The vertical lines corresponding to minima join when
the cost function value is greater than the barrier between them. The horizontal axis
is chosen to arrange minima so as to provide a clear representation the topography.
The application of disconnectivity graphs to potential energy surfaces in molecu-
lar science has allowed various surface topographies to be classified, notably: single-
funnel,117 multifunnelled118120 and glassy.121 This classification of landscapes is
immediately suggestive of observable properties, such as broken ergodicity and mul-
tiple relaxation timescales.122,123 Single-funnel surfaces are characterised by efficient
relaxation to a well-defined global minimum, and correspond to magic number
clusters, and biomolecules that have evolved to perform a single function. Multi-
funnelled landscapes arise when there are two, or more, funnels that are similar
in cost function and separated by significant barriers. Multifunnelled surfaces are
more challenging for global optimisation, and locating the global minimum can be
significantly hindered by alternative funnels acting as kinetic traps. In glassy land-
scapes there is no clear global minimum if the crystal is excluded, as there are a
large number of almost equivalent amorphous minima, corresponding to funnels that
are separated by large barriers.124,125 For these surfaces global optimisation is very
challenging and minima from more than one of these funnels would be needed to
fully describe the optimal clustering solutions.
2.3.2 Networks
The stationary point database defines a weighted graph, known as a kinetic tran-
sition network in molecular science.110,111,113 Each minimum is a vertex, and each
transition state is an edge between the two minima connected to it by steepest-
descent paths. The edges are weighted according to the difference in cost function
value relative to the corresponding transition state.
Networks provide a complementary description of the energy landscape to the
topographical representation of disconnectivity graphs. There are various properties
that can be extracted from networks to understand their organisation, and here we
calculate the average degree, density and transitivity.
The degree of a node is its number of connected edges, and the average degree
is calculated over all vertices in a network. The density of a network is the ratio
Methods
of the observed number of edges to the maximum possible number of edges. For
undirected graphs, such as those considered in this work, the density is calculated
|V |(|V |  1)
, (2.13)
where |E| is the total number of edges, and |V | is the total number of vertices. The
transitivity,126 defined as
CT = 3
Ntriangles
Ntriads
, (2.14)
is a measure of the triadic closure of a network. Ntriangles and Ntriads are the number
of triangles and triads in the network, respectively. A triad exists if nodes r and s,
and s and t are connected by edges. A triad is also a triangle if nodes r and t are
also connected by an edge.
In this work, networks were visualised using the Gephi software package.127 All
networks were depicted using a force-directed layout, as applied using ForceAt-
las2.128 For large graphs, such as those constructed from the glass datasets, we also
include gravity, which attracts nodes to the centre of the network space, to pre-
vent excess expansion. The weight of each edge is specified by the average barrier
between each transition state and its two connected minima. Minimisation from ran-
dom initial vertex positions results in one possible stable graph representation, and
we repeated the process from several different initialisations. The most appropriate
representation was then selected.
2.4 Thermodynamics
2.4.1 Heat capacity
There are many tools used to characterise the observable properties of physical sys-
tems, and relate them to the potential energy surface. One such thermodynamic
quantity is the heat capacity, as peaks in its temperature profile can reveal phase
transitions. These heat capacity features arise from the complex interplay of min-
ima energies, their corresponding vibrational density of states, and their degeneracy,
across a given temperature range. It is possible to relate phase transitions to the
specific minima that drive them, by expressing the heat capacity in terms of chang-
ing minima populations.129 In physical systems, low-temperature peaks in the heat
capacity profile tend to correspond to solid-solid phase transitions,118 which arise
due to two different structural morphologies of comparable energy. Larger high-
temperature peaks correspond to melting transitions, which are largely driven by
Methods
the increased landscape entropy of a large number of amorphous minima at high
temperature. In this section, we show how heat capacities can be calculated from a
database of stationary points, and applied to K-means landscapes.
In this work, we calculated heat capacities using the harmonic superposition
approach.130132 The total partition function, Z, in the superposition method,130,131
is calculated as a sum over the partition functions for the basin of attraction of
each minimum , Z(T ). The equilibrium occupation probability of minimum 
is then peq (T ) = Z(T )/Z(T ). We set the physical masses to unity, and apply a
normal mode analysis to each minimum to produce a harmonic approximation to
the density of states. The equilibrium occupation probability, assuming a harmonic
vibrational density of states,132 is defined as
peq (T ) =
eJ/kBT/
J/kBT/
. (2.15)
kB is the Boltzmann constant, J is the cost function value of minimum , and  is
the number of non-zero eigenvalues of the Hessian matrix, in this case  = K Nf .
 is the geometric normal mode vibrational frequency.
The Hessian matrix has a simple analytic form for K-means (Eqn. 2.10), with
eigenvalues given by the cluster populations. Therefore, the largest geometric nor-
mal mode frequencies will arise from the most even cluster populations. The Hes-
sian eigenvalues correspond to the principal curvatures of each K-means minimum,
which define the local density of states and configuration volume. Smaller Hessian
eigenvalues are associated with lower curvatures and wider basins of attraction in
clustering space. Larger basins of attraction for minimal clusterings will likely corre-
spond to more pathways leading there. A further consideration for K-means is that
the basins of attractions for all minima can contain not only the minimal clustering,
but multiple non-minimal clusterings. The minimal clustering itself is harmonic,
but the harmonicity of the basin of attraction is expected to be significantly vio-
lated if there are many non-minimal clusterings of different curvature. Corrections
for anharmonicity can be applied,133 but here we continue with the harmonic ap-
proximation, under the assumption the non-minimal clusterings that intersect with
minimal clusterings will be similar in cluster populations.
Methods
2.4.2 Shannon entropy
The Shannon entropy,134 s(T ), is defined as
s(T ) = 
peq (T )lnp
 (T ), (2.16)
where T is the temperature, and peq is the population of minimum  at equilibrium,
calculated using Eqn. 2.15. It is a thermodynamic metric that does not require
connectivity information to be known, and it can be used as a simple measure of
the frustration in an energy, or K-means, landscape.135
Frustration describes the difficulty in attaining the global minimum of a system,
and largely correlates with the challenge to global optimisation algorithms. Self-
organising systems, such as folded proteins, have little frustration and efficiently
reach the global minimum. These systems typically have landscapes that are single-
funnelled, with a unique global minimum, and no competing morphologies with
high barriers between them.136,137 In contrast, highly frustrated systems relax to
the global minimum much more slowly, and are associated with glassy landscapes.
Glassy landscapes have significant barriers between numerous low-lying minima.
The Shannon entropy largely correlates with frustration,135 and allows us to probe
intermediate behaviour between these two limits. However, it does not include
kinetic information, so we pair this analysis with rate calculations.
2.5 Rates
2.5.1 Dijkstra fastest path
Kinetic properties of systems can be extracted from the database of minima and
transition states, allowing us to calculate transition rates between any groups of
minima. We select reactant (A) and product (B) sets, which can both contain
multiple minima, and designate any minimum not within these sets as intervening,
denoted as I. Assuming Markovian transitions between adjacent minima, and that
the I minima are in steady-state, the rate constant for the transition A  B can
be expressed as
kSSBA =
kbi1ki1i2 . . . kinap
k1i1
k2i2   
knin
. (2.17)
peqa is the equilibrium occupation probability of minimum a, as calculated in Sec. 2.4.1,
and p
A the equilibrium occupation probability of the A set. Each rate constant, kij,
Methods
for transition between minima i and j that are directly connected by a transition
state, is obtained by transition state theory.138,139 Such transitions between minima
directly connected by a transition state are referred to as elementary transitions,
and kij is the sum of rate constants for all elementary transitions between the two
minima. j can be any of the minima directly connected to intervening minimum
ij. The overall sum is calculated over all paths that begin and end at the boundaries
of the A and B sets.
A useful restatement of Eqn. 2.17 is
kSSBA =
Pbi1Pi1i2 . . . Pin1inPinap
CBa p
, (2.18)
where Pji = kjii is the branching probability for transition from minimum i to
minimum j, among the possible elementary transitions. i = 1/
 ki is the mean
waiting time for an elementary transition out of minimum i. The committor prob-
ability, CBa , is the likelihood that a random walk from minimum a will encounter
a member of B, before returning to a minimum from the A set. This sum can be
evaluated using a weighted adjacency matrix mutliplication method,140 or analytic
results from the theory of a one-dimensional random walk.141,142
The equation above relies upon the steady-state assumption for the intermediate
minima. The formulation enables us to apply Dijkstras shortest path algorithm143
to find the pathways with the largest contribution to the overall rate constant.
Edge weights in the kinetic transition network are chosen to reflect the branching
probabilities in the Dijkstra analysis. Visualising the stationary points on the fastest
pathway can elucidate the geometric changes necessary for transition between two
configurations. Moreover, the energy, or cost function, profiles can reveal the rate-
determining step, other significant barriers, and the path length. For the K-means
landscapes, it is very valuable to visualise pathways to understand the transitions
across clustering space in this novel application. It is straightforward to visualise the
pathway profiles, but visualising the stationary points is more challenging because
the clustering solutions exist in a feature space with more than three dimensions
for both the glass and Iris datasets. We visualise the Iris, but not glass, clustering
solutions by using only three of the four features.
2.5.2 Graph transformation
The graph transformation (GT) algorithm144146 is an efficient method for calcu-
lating phenomenological rate constants without the restriction of the steady-state
approximation for intermediate minima, or local equilibrium within the A and B
Methods
regions. However, it still requires the assumption of Markovian dynamics. All rates
quoted in this work are calculated using this approach.
The algorithm progressively removes minima from the kinetic transition network,
whilst leaving the relevant average properties unchanged. For each reactant min-
imum, a, we remove minima belonging to the intervening set, and once removed,
all other reactant minima. Before removing each minimum the branching probabil-
ities and mean waiting times are renormalised to retain the mean first-passage time
between the A and B states in the original and transformed networks.
The rate constants for elementary transitions and equilibrium occupation prob-
abilities take the same form as those in Sec. 2.5.1. The required rate constants, in
the GT formalism, can be written as
kGTBA =
P ba and k
P ab. (2.19)
 a is the renormalised waiting time for a transition from minimum a to any member
of the B set, and P ba is the transformed branching probability for the path from a
to b, among the complete set of paths from a to all minima in the B set.
For both steady state (Sec. 2.5.1) and GT rates we set the vibrational density
of states to unity for the K-means landscapes. The vibrational frequencies of the
transition states are calculated on the constructed F surface, and not on the cost
function surface. Hence, the resulting vibrational density of states of minima and
transition states are not calculated on the same surfaces, and are not expected to be
directly comparable. Therefore, we set all the vibrational densities of states to unity
to hence avoid any effects arising from a mismatch between minima and transition
state values.
The temperature is a fictituous parameter for the K-means landscapes, but this
choice still strongly affects the observed rates and pathways. Larger temperatures
give faster effective rates, and larger barriers are less heavily penalised. We select a
fictituous temperature for each landscape to produce relatively low rates (approx-
imately in the range between O(106) and O(103)). This choice ensures that we
favour small barriers, and see more distinction between pathways and rates.
2.6 Comparing clusterings
There are a variety of metrics for evaluating the clusterings generated by K-means.
Without known data point assignments, as is common in exploratory data analysis,
we have to use internal metrics to assess clustering quality. Internal metrics use
Methods
properties of the resulting clusters, such as their compactness and separation. The
central internal metric to K-means is the cost function, which is the sum of squared
errors. We assume clusterings low in cost function are the most accurate based
on this metric. Other popular internal metrics are the Dunn index,27 the stability
metric147 and the silhouette coefficient.28
External metrics measure the similarity of a clustering to a given reference. A
common reference is the ground truth labels, in which case external metrics measure
the clustering accuracy. However, there are other possible choices, and we also use
the global minimum clustering of a dataset in this work. The similarity of each
clustering to the appropriate global minimum allows us to estimate the diversity
of clustering solutions. Popular external metrics based upon cluster matchings are
the Jaccard,148 Rand,149 adjusted Rand,150 Fowlkes-Mallows,151 and van Dongen
indices.152 Alternative popular metrics based upon information theory are mutual
information134 and variation of information,153 and another important approach is
the use of meta-clusters.154 The external metric we use throughout this work is the
adjusted Rand index.
2.6.1 Adjusted Rand index
The Rand index,149 which compares two partitions, is defined as
TP + TN
TP + TN + FP + FN
, (2.20)
where TP, TN, FP, FN are the number of true positives, true negatives, false posi-
tives and false negatives, respectively. We calculate the total number of each of these
cases when evaluating all pairs of data points within a dataset. A true positive oc-
curs when the pair of data points are placed in the same cluster in both partitions,
and a true negative when they are placed in different clusters in both partitions.
False positives and negatives arise when one clustering assigns the two data points
to the same cluster, but not the other. A false negative is distinguished by the two
data points being assigned to the same cluster in the reference clustering, and for a
false positive the data points are in different clusters in the reference.
The Rand index takes values between 0 and 1. A value of 0 indicates no similarity
between the clusterings, and 1 indicates identical clusterings. The Rand index is
often close to 1, even with significant differences between the clusterings, because
there are often a large number of similar pairwise labellings that arise at random.
The Rand index does not account for the similarity expected for random datasets,
and the value of the Rand index, calculated for two random clusterings, is greater
Methods
than zero.
The adjusted Rand index150 (ARI) accounts for cluster labels being the same
by chance. It provides a new baseline, such that expected similarity of all pairwise
comparisons within a random model is approximately 0. The expression becomes
ARI =
RI  E[RI]
max(RI) E[RI]
, (2.21)
where E[RI] is the expected similarity within a random ensemble of clusterings,
and max(RI) is the maximum similarity observed within the ensemble. In this
work, we use the ARI as implemented within the scikit-learn package.155 The ARI
can take values between 1 and +1. A value of 1 still indicates perfect agreement
between clusterings, and 0 indicates no agreement, which now reflects the expected
similarity within clusterings specified by the random model. Negative values indicate
less similarity than that expected at random.
Chapter 3
Cluster number
As noted previously, locating the global minimum clustering solution for K-means is
challenging, and success depends upon the topography of the cost function surface.
In turn, the cost function surface depends upon both the number of clusters and
the dataset. Here, and in the following chapters, we present K-means landscapes
that explore various features of dataset structure and their influence on cost function
topography. Variations of the same two datasets are used in all the following studies:
Fishers Iris dataset and the glass identification dataset.
In this chapter, we analyse K-means landscapes constructed for the Iris dataset
with a varying number of clusters. In Chapters 4 and 5, we build KMLs for datasets
that contain outliers and have standardisations applied, respectively. The removal
of outliers and application of a standardisation are both common procedures during
preprocessing of datasets prior to K-means clustering. In Chapters 6 and 7 we ad-
dress two of the most important features of dataset structure for K-means accuracy.
Chapter 6 presents a systematic study on the effect of overlap on cost function to-
pography, and Chapter 7 examines the effect of cluster populations on the K-means
cost function surface.
3.1 Landscape topography
The Iris dataset can be viewed in terms of two underlying clusters; the Setosa cluster
and the cluster composed jointly of Virginica and Versicolor measurements. These
two clusters are well-separated in the feature space. K-means, at K  3, is forced
to distribute its clusters between the two underlying clusters of the Iris dataset in
varying numbers. These different partitionings of the K clusters give rise to distinct
groups of clustering solutions. Within a given partitioning the minima have only
small variations in data point assignment relative to those in different partitionings.
Cluster number
The K-means landscapes are visualised using disconnectivity graphs in Fig. 3.1,
revealing the presence of a well-defined global minimum for all K. The landscapes
contain no minima with similar values of the cost function to the global minimum
that are separated by large barriers. In the terminology of energy landscapes this
organisation is considered unfrustrated.135,136,156 All global minima are accompanied
by a set of low-lying solutions, similar in structure and separated by small barriers,
which can be viewed as equivalent clusterings. Any member of this set provides
a suitable substitute for the global minimum clustering, as they retain the same
partitioning and only deviate in cluster boundaries. In all cases the global minimum
clustering, and its associated set of minima, corresponds to the optimal partitioning
of K-means clusters between the underlying clusters of the dataset. However, the
gap between partitionings decreases with increasing K, as fewer data points are
assigned to each cluster, which leads to alternative partitionings becoming more
competitive with the global minimum.
The K-means landscapes are all quite funnelled towards the global minimum,
with some interesting structure. Funnelled surfaces are associated with self-organising
systems, which exhibit efficient relaxation:157159 all local minima can reach the
global minimum via small barriers. The funnelled topography that leads to effi-
cient structure-seeking systems in nature also leads to easier global optimisation.
Therefore, K-means methods that use optimisation algorithms to perturb and up-
date local minima are expected to perform well on cost function surfaces such as
these.160163
Subfunnels, which contain minima with alternative morphologies to the global
minimum, exhibit the same local topography that characterises efficient optimisa-
tion. They also have small downhill barriers from all minima within the subfunnel
to the local funnel bottom. Once within a subfunnel, optimisation to its lowest-
valued representative is efficient, but transitions to the global minimum funnel are
much less probable because of the larger barrier involved. Consequently, subfun-
nels constitute kinetic traps, because they hinder location of the global minimum.
Topological persistence can be used to simplify landscapes containing kinetic traps,
without loss of precision, by identifying the most stable minima,164 which will likely
correspond to the minima of subfunnels and the global minimum.
In the K-means landscapes the subfunnels correspond to different partitionings
from the global minimum. The minima within a subfunnel vary locally in data point
assignment, while retaining the same partitioning. More partitionings are possible
for larger K, leading to an increase in the number of kinetic traps. The number
of minima associated with each partitioning also increases with K, due to more
Cluster number
(a) K = 3
(b) K = 4
(c) K = 5
(d) K = 6
(e) K = 7
(f) K = 8
Figure 3.1: Disconnectivity graphs illustrating the changing topography of the K-means
cost function surface with cluster number, K, for Fishers Iris dataset. The colouring of
each minimum indicates its partitioning, given as the number of K-means clusters within
the smaller of the two underlying clusters of the dataset, as indicated in the scale bar.
Cluster number
(a) K = 3 (b) K = 4 (c) K = 5
(d) K = 6 (e) K = 7 (f) K = 8
Figure 3.2: Histograms for each K, showing the number of minima at values of the cost
function between the highest and lowest observed minima.
minimal clusterings, and the relative number of minima in each subfunnel can vary
significantly. For example, at K = 6 the number of minima for alternative parti-
tionings of the lowest subfunnel vastly outnumbers the number of minima associated
with the global minimum partitioning. In general, there is an increase in the number
of minima in lower subfunnels, because they correspond to a more even distribution
of data points between clusters, which allows them to support more minima, as seen
in Fig. 3.2. Hence, these subfunnels exhibit greater landscape entropy,165167 which
is determined by the number of minima.
All the cost function landscapes for the Iris dataset could be roughly classified as
single-funnel landscapes. However, the growing number of kinetic traps, and their
competitiveness with the global minimum, leads to more multifunnelled character
at larger K. Therefore, as the number of clusters increases the cost function land-
scape changes, such that global optimisation becomes more challenging, and the cost
functions of alternative partitionings approach the value for the global minimum.
The ability of subfunnels to act as kinetic traps can be described in detail using
kinetic analysis,143145 which provides transition rates or mean first passage times
between minima in physical systems. Here, we apply that same framework to the
networks of the cost function landscapes. The density of states, which appears in the
partition function for each minimum and the transition state theory rate constant
formulation,168,169 was set to unity in the present calculations. Hence we do not
attempt to include the analogue of a vibrational density of states, but simply use
Cluster number
Table 3.1: The average path length, measured in terms of the number of transition state
analogues. The average barrier height is tabulated for all transition state analogues, and
the largest of these barriers, for pathways downhill between the lowest minima in different
partitionings. Averages are only calculated for pathways between partitionings that differ
by exchange of one cluster between the two underlying clusters of the dataset.
K Path length Average barrier Largest barrier
3 5 0.77 2.53
4 2.5 1.05 2.62
5 7 1.03 4.19
6 12 0.74 3.38
7 12.5 0.61 3.45
8 19.8 0.30 3.02
the appropriate Boltzmann factor to assign relative occupation probabilities for the
local minima.
Overall rates between selected minima in the database, accounting for all possi-
ble paths between them, were calculated using the graph transformation method.146
The transition rates between minima in different subfunnels provide a quantitative
measure of the difficulty of escaping kinetic traps using optimisation algorithms. As
for physical systems, we have to specify a temperature at which the rates are calcu-
lated. For the cost function surfaces, as the temperature has no physical meaning,
we calculate rates at an arbitrary value (T = 3) in reduced units corresponding to
the cost function. For all K, we calculated rates for the paths between the lowest-
valued minima in each partitioning, which are the global minimum and the lowest
minima in all subfunnels.
In almost all cases, the rate for moving to lower-valued partitionings is higher
than the reverse direction, as shown in Fig. 3.3. These rates provide insight into
the efficient global optimisation on these funnelled landscapes, as they illustrate the
preference for escaping kinetic traps and reaching the global minimum compared to
the reverse direction. Additionally, we observe that the rates for reaching the global
minimum, from all other subfunnels, decrease with increasing K, and the reverse
rates become more competitive. Therefore, as the number of kinetic traps increases
so does the difficulty in escaping from them, illustrating the likely performance of
global optimisation algorithms. The lower rates between partitionings at higher K
is due to the increased number of steps, rather than the barriers that have to be
overcome on the pathways, as shown in Table 3.1. The barriers remain small for
all the landscapes, but the number of steps increases due to the greater number of
minimal clusterings.
We observe that, as expected, rates for the conversion between partitionings are
Cluster number
(a) K = 3 (b) K = 4 (c) K = 5
(d) K = 6 (e) K = 7 (f) K = 8
Figure 3.3: Rates calculated between the lowest minimum in each different partitioning, at
all K. Every node represents a minimum and is labelled by its partitioning, the number of
clusters in the smaller of the two underlying clusters of the dataset, and its cost function
value. Rates are given in both directions between all different minima.
Cluster number
(a) K = 3 (b) K = 4 (c) K = 5
(d) K = 6 (e) K = 7 (f) K = 8
Figure 3.4: Histograms showing the distribution of barrier heights for each cost function
landscape. The barrier height is calculated as the difference in cost function between a
transition state analogue and the higher, or lower, of its two connected minima. These
barrier heights were calculated for every transition state analogue in the corresponding
kinetic transition network.
significantly faster for adjacent partitionings, namely those that correspond to a
difference of one in the number of clusters assigned to the smaller of the under-
lying clusters of the dataset. This rate reduction is because the fastest pathways
between larger changes in partitioning pass through intermediate partitionings, and
are not connected directly. Consequently, the path length increases to incorporate
the additional change in partitioning, and the additional barriers reduce the rate.
At all K, the transition state analogues are very similar in cost function to the
higher of their two connected minima, Fig. 3.4. Hence, there are small barriers for
each step on pathways between distant minima, which contribute to the funnelling
properties of the landscape. The distribution of barrier heights shows that the most
probable height, at all K, is small, and transition state analogues with larger bar-
riers become increasingly unlikely. For some surfaces there are barrier heights at
larger values (around twenty). These barriers will occur when the transition state
analogues are MECPs between two cluster assignments in a higher-valued partition-
ing, but both steepest-descent paths end at minima of a lower-valued partitioning.
The distributions indicate that most transition state analogues lie within the same
partitioning as one of the connected minima.
The small barriers indicate that the vast majority of transition state analogues
Cluster number
Table 3.2: Analysis of transition state analogues, and their relation to their connected
minima. r and D are the average number of data points that vary in assignment, and
the average difference in Euclidean distance, between the transition state analogue and its
higher or lower connected minimum, respectively. For the higher connected minima, only
transition state analogues with barrier heights in the range 0.00.1 were considered, and
the proportion of the total transition state analogues these contribute is given by P . For
the lower connected minima we give values for the three transition state analogues with
the largest barriers.
Higher Lower
r D P r D
3 0.89 0.012 0.9 68.67 3.05
4 0.98 0.013 0.96 99.33 3.35
5 0.96 0.016 0.86 54.00 2.35
6 0.96 0.017 0.92 63.33 2.69
7 0.96 0.016 0.95 58.00 2.53
8 0.96 0.015 0.97 63.67 1.66
lie at, or close to, an intersection directly involving the higher of the two connected
minima, Table 3.2. Therefore, transition state analogues often correspond to the
exchange of a single data point from the higher connected minimum. Furthermore,
as a result of the high density of data points the distance between the transition
state analogue and the higher connected minimum is small.
The transition state analogues closely follow Hammonds postulate, that the
transition state resembles the reactant or product most similar in energy,170 which
can be analysed using catastrophe theory.82,171,172 When the distance between mini-
mum and transition state analogue is very small, which is expected to occur at high
data point density, we expect the corresponding stationary points to merge at a non-
Morse point corresponding to a fold catastrophe.171 However, catastrophe theory is
not directly applicable for the K-means cost function surface due to discontinuities
preventing the calculation of transition state eigenvalues.
The steepest-descent path to the lower minimum can pass through one, or many,
other clusterings. In most cases the barrier from the lower connected minimum is
only slightly larger than for the corresponding higher minimum, as reflected in the
distributions of barrier heights, which are shifted to slightly higher values. How-
ever, for transition state analogues that connect distant minima, the pathway to
the lower-valued minimum must pass through multiple non-minimal clusterings and
move through a large distance in configuration space. The difference between the
transition state analogue and its lower-valued minimum in these cases can be large,
as shown in Table 3.2.
Chapter 4
Preprocessing: Outliers
After an initial exploration of the solution landscapes arising from Fishers Iris
dataset, we use an additional dataset for the following studies, the glass identification
dataset. We use both the glass and Iris datasets to study the effect of two common
stages of preprocessing, removal of outliers and standardisation, on the cost function
topographies.
Outliers can change both the quality and structure of the K-means clustering
solutions, and their interrelation in the solution landscape. As we will show, outliers
have a profound effect on the cost function topography, and consequently, the ease
of obtaining low-valued clustering solutions. Furthermore, the change in solution
landscapes induced by outliers strongly determines the quality and diversity of low-
valued clusterings. Results are presented for Fishers Iris dataset, with both K = 3
and K = 6, and the glass identification dataset prepared with an increasing number
of outliers, as described in Sec. 2.1.1.
4.1 Clustering solutions
Outliers modify K-means clustering to give two alternative types of solutions:
1. The outlier belongs to a cluster containing no data points from the original
dataset.
The outlier belongs to its own cluster that contributes nothing to the cost func-
tion, but there remain only K  O clusters to represent the original dataset,
where O is the number of single outlier clusters. Therefore, K-means per-
formance is degraded due to poorer partitioning of the original data. This
degradation will be greater when the original dataset is composed of K well-
separated clusters, and reduced if the dataset can be equally well represented
Preprocessing: Outliers
with fewer than K clusters, or the clusters overlap significantly.
For datasets with more than one outlier it is also possible for multiple outliers
to belong to a single cluster, still containing no original data points. The effect
will be the same as for one outlier, but there remain more clusters to model
the original data in accordance with the number of outliers grouped together.
2. The outlier belongs to a cluster containing some original data points.
If an outlier does not belong to its own cluster then it must be accommodated
into a cluster containing original data points. These data points do not have
to be a possible cluster of the original dataset. The outlier will skew the
cluster centre away from the mean of the original data points, and due to
its remoteness from the other data points, the mean may move a significant
distance. The displacement of the cluster centre from the mean of a large
number of data points, and the remaining large distance between the cluster
centre and the outlier, produces a large increase in the cost function.
The distinction between the two outlier solutions can be used to understand the
structure of different K-means solutions. The cases above apply to a single outlier
and for datasets with multiple outliers, and each of them can correspond to one
of the two cases, resulting in many possible structures. Additionally, with more
than one outlier it is possible to have single clusters, with or without original data
points, containing multiple outliers. We have provided a classification of the possible
structures in Fig. 4.1 to provide insight into the distribution of solutions amongst
the solution landscape.
The structure type, in our specified scheme, is independent of the outlier per-
forming the function. For example, we do not consider which outlier corresponds to
its own cluster, giving four options for structure type 1 in a dataset containing four
outliers. Each different type 1 solution will have different values of the cost function
depending upon where the outlier lies in relation to the original data. However, the
scheme remains valuable for understanding the relevant structure of the clusterings.
For a fixed structure type there can be a variety of clustering solutions with small
changes in data point assignment. For example, type 1 clusterings can correspond
to different partitions of the original dataset for K1 clusters, and type 2 solutions
can vary in the position of the cluster border within the original data. Each outlier
behaves in a similar fashion for datasets of more outliers.
With this overview of the possible clustering solutions available to K-means
for datasets containing outliers we now consider the KMLs for the Iris and glass
datasets. We present the landscapes in this chapter using disconnectivity graphs,
Preprocessing: Outliers
Figure 4.1: The structure type scheme used to distinguish clusterings containing outliers.
Each cross represents an outlier in the K-means landscape, irrespective of position in
space. Circles surrounding one or more outliers indicate membership of a distinct cluster
with no original data points. All other outliers are incorporated into clusters with original
data points. The scheme is shown for four outliers, but the ordering remains the same for
datasets with fewer outliers.
which reveal the topography of the cost function surfaces. We consider the implica-
tions of these topographies for global optimisation, before discussing the distribution
of accurate minima about the landscapes, which is an equally important facet of the
landscapes.
4.2 Landscape structure
4.2.1 Topography
There are several general features of the landscapes that are observed for all datasets.
The cost function range of the minima clearly increases with an increasing number
of outliers. The outliers increase the cost function value of each global minimum
for the reasons described above, and this increase can be significant, as shown in
Table 4.1. Additionally, the outliers allow increasingly poor solutions, as measured
by the cost function. With more outliers, worse partitionings of the data now have
an increased number of remote data points to contribute to the cost function. The
increase in cost function of the worst solution is larger than that for the best, giving
an increase in the range of cost function values for the minimum.
Preprocessing: Outliers
Table 4.1: The increase in cost function for the global minimum clustering on addition
of an increasing number of outliers. The cost function is measured relative to the global
minimum of the original dataset.
Dataset Outlier 1 Outlier 2 Outlier 3 Outlier 4
Iris (K = 3) 19.820 46.286 75.685 114.341
Iris (K = 6) 7.605 18.387 36.763 58.995
Glass 64.198 152.978 252.971 -
Despite the large variation of cost function in the solution landscapes there re-
mains little change in the barrier heights to the global minimum. In all landscapes
these barriers remain very small for the vast majority of minima, even with a greatly
increased cost function range. However, there remain regions of solution space sep-
arated from the global minimum by significant barriers in all KMLs. The number
of such kinetic traps is comparable in all landscapes of increasing outlier number,
despite the large increase in the number of different structure types available to
K-means clustering. Furthermore, as the number of outliers increases there is a
smaller proportion of minima associated with these kinetic traps. Therefore, with
the small barriers retained for many minima, and fewer minima in kinetic traps, the
landscapes appear increasingly funnelled for a greater number of outliers.
As seen in Fig. 4.2, for the Iris landscapes with K = 3, the high-valued kinetic
trap present in the original dataset is removed by the addition of two outliers.
Alternative small and low-valued traps begin to appear, but the landscape for the
dataset with four outliers has small barriers separating all minima from the global
minimum.
We see that for any number of outliers the global minimum best accommodates
the outliers by appending them to existing clusters (Fig. 4.3). For the Iris dataset,
with three clusters, the outliers can be accommodated relatively easily into the
existing clusters, with little change in the cluster boundaries from those in the
original dataset. Many of the low-valued solutions correspond to variations within
structure type 0, but for landscapes of two outliers and above we see low-valued
solutions that correspond to allowing one cluster to only contain one, or two, outliers.
The Iris dataset can be well represented with two clusters due to the overlap of
Versicolor and Virginica data points, so solutions with one outlier cluster perform
well. Structures with a single cluster representing the original data perform poorly in
all datasets where this arrangement is possible, but all of them retain small barriers
to the global minimum.
For the Iris dataset with an increased number of clusters (K = 6) there are
more significant kinetic traps in the original landscape, as shown in Fig. 4.4. The
Preprocessing: Outliers
1 (a) Original
(b) Outlier 1
(c) Outlier 2
(d) Outlier 3
(e) Outlier 4
Figure 4.2: K-means landscapes for Fishers Iris dataset with K = 3 and an increasing
number of outliers. The scale bar represents the same cost function range in all plots.
Minima are coloured according to the scheme, given in Fig. 4.1, for distinguishing different
types of K-means solutions.
Preprocessing: Outliers
(a) Original (b) Outlier 1 (c) Outlier 2
(d) Outlier 3 (e) Outlier 4
Figure 4.3: Global minima for the Iris landscapes with K = 3. Three of the four fea-
tures are presented to allow visualisation. Petal width is excluded as it contains little
information.
majority of these kinetic traps can be understood using the different partitions of
the underlying Iris data. Disconnectivity graphs coloured by partitioning are given
for Iris datasets with both cluster numbers in Figs. A.1 and A.2.
For the original landscapes kinetic traps can be rationalised as minima that
differ in partition from the global minimum, as shown in Chapter 3. Partitions
remain useful to understand kinetic traps for both cluster numbers, but as the
number of outliers increases the structure type gains more control over cost function
topography. Partitions become less clear, as many clusters now incorporate outliers,
and a growing number of kinetic traps can be rationalised by a difference in structure
type, and not partition.
The number of kinetic traps, and the overall barrier separating them from the
global minimum, remains comparable at all outlier landscapes, without the reduction
seen for three clusters. However, the characteristic landscape funnelling is still
observed as the population of kinetic traps is reduced, whilst the other features
remain similar.
The additional clusters allow many more structure types to be viable, and the
global minimum changes with the number of outliers, as displayed in Fig. 4.5. The
three additional clusters, above the ground truth K = 3, also favour more distinct
outlier clusters. We see an increase in the number of clusters containing only outliers,
Preprocessing: Outliers
(a) Original
(b) Outlier 1
(c) Outlier 2
(d) Outlier 3
(e) Outlier 4
Figure 4.4: Disconnectivity graphs showing the K-means solution landscapes for variants
of Fishers Iris dataset with six clusters. The scale bar represents the same cost function
range in all disconnectivity graphs, and minima are coloured according to the scheme given
for distinguishing different types of K-means solutions.
Preprocessing: Outliers
(a) Original (b) Outlier 1 (c) Outlier 2
(d) Outlier 3 (e) Outlier 4
Figure 4.5: Global minima for the Iris landscapes with K = 6, and a growing number of
outliers. Minima are presented in three of the four features to allow visualisation. Petal
width is excluded as it contains little information.
and at four outliers we regain the original dataset represented by three clusters.
Similar behaviour to the Iris landscapes is seen in the glass landscapes, Fig. 4.6.
The significant overlap in the original dataset reduces the structure in the KML,
leaving a small number of kinetic traps. Like the Iris landscapes the kinetic traps
remain similar in number, comparable in depth and have fewer minima associated
with them. Therefore, we again see an example of the funnelling of K-means land-
scapes with the addition of outliers.
Moreover, we see many more low-valued minima corresponding to structure types
with isolated outlier clusters. This trend reflects the overlapping nature of the glass
dataset and the ability to represent the original dataset well using a reduced number
of clusters.
4.2.2 Minima distribution
Disconnectivity graphs allow us to visualise the topography of a cost function surface
and identify kinetic traps. Such a topographical view is important for understanding
many surface properties, and allows us to estimate the ease of global optimisation.
The funnelling of the landscapes indicates that global optimisation should become
less challenging as the number of outliers increases.
We quantify the resultant funnelling of the landscapes using the Shannon (or
Preprocessing: Outliers
(a) Original
(b) Outlier 1
(c) Outlier 2
(d) Outlier 3
Figure 4.6: K-means solution landscapes for the glass identification dataset, and variations
with additional outliers. Minima are coloured according to the structure types specified
in Fig. 4.1. The scale bar gives the same cost function range in all plots.
Preprocessing: Outliers
(a) Iris (K = 3) (b) Iris (K = 6)
(c) Glass
Figure 4.7: The Shannon entropy, used as a measure of landscape frustration, calculated
for each solution landscape from the Iris and glass datasets. The entropy quantifies
the complexity of the cost function surface at increasing values of the cost function, here
represented by a fictituous temperature, where kBT has the same units as the cost function.
landscape) entropy.134 The Shannon entropy is a thermodynamic metric reflecting
the number of minima available at a given value of the cost function. It has been
used to estimate the frustration in energy landscapes,135 which correlates with the
difficulty of global optimisation. Small values of the landscape entropy indicate a
smooth change in population of minima, and strong funnelling of the landscape.
Landscape entropy profiles are shown for all KMLs in Fig. 4.7.
We observe that the stronger funnelling seen in the disconnectivity graphs cor-
relates with landscape entropy for the Iris (K = 6) and glass datasets. For both
these sets of landscapes the entropy is reduced with addition of each outlier. The
Shannon entropy provides a clear demonstration that the addition of outliers drives
funnelling of the landscapes.
The trend for the Iris (K = 3) profiles is not as clear, but the original landscape
still remains highly frustrated. All the landscapes support a relatively small number
Preprocessing: Outliers
Table 4.2: Escape rates from kinetic traps on the solution landscape for the Iris dataset
with K = 6, and variations with additional outliers. The rates are calculated from the
set of minima at the bottom of the kinetic trap to the global minimum, at an effective
temperature of T = 1.0. The selected kinetic traps are given in Fig. A.3.
Kinetic trap Original Outlier 1 Outlier 2 Outlier 3 Outlier 4
1 3.65 104 2.25 105 2.64 104 3.23 104 7.25 107
2 1.74 105 1.02 103 6.55 108 3.26 106 1.12 106
3 - 2.66 105 2.20 105 2.26 109 8.57 107
of minima, and the large weighting placed on each minimum makes the frustration
a less reliable metric for these landscapes.
4.2.3 Kinetic trap relaxation rates
The Shannon entropy is a useful metric for landscape funnelling, but it incorporates
no kinetic information. With the transition state analogues in KMLs we can further
probe funnelling through rates and pathways between minima. Escaping kinetic
traps is essential for global optimisation, so we calculate the transition rates between
kinetic traps and the global minimum in the Iris (K = 6) and glass identification
landscapes. The Iris dataset with three clusters does not have significant kinetic
traps for any outliers so it is omitted from this analysis.
For the original Iris dataset we select the two large kinetic traps, and for the
outlier landscapes we select three kinetic traps. It is somewhat subjective as to what
qualifies as a kinetic trap, and selection here is based on barrier height. We choose
traps with the largest barriers. The selected kinetic traps, and their labels, are given
in Fig. A.3.
The rates, given in Table 4.2, generally decrease as the number of outliers in-
creases. The escape rates from different kinetic traps, within the same landscape,
exhibit large fluctuations as a result of the specific features, such as barrier heights
and populations. However, there remains an overall reduction in escape rates and,
consequently, escaping kinetic traps becomes more difficult as the number of outliers
increases.
We perform a similar analysis for the glass datasets with two chosen kinetic
traps shown in Fig. A.4. We select the two traps that most closely resemble those
of the original dataset in all outlier landscapes. The glass landscapes rates show a
similar trend to the Iris dataset with the addition of outliers, Table 4.3. For these
landscapes we observe a much smaller increase in the rates, with values remaining
of the same order of magnitude. Furthermore, we note that the landscape with one
outlier has much larger escape rates relative to all the other landscapes.
Preprocessing: Outliers
Table 4.3: Rates of transition from minima at the bottom of kinetic traps to the global
minimum for various K-means landscapes of the glass dataset. The rates are calculated
for selected kinetic traps, which are shown in Fig. A.4, at a fictituous temperature of
T = 3.0.
Kinetic trap Original Outlier 1 Outlier 2 Outlier 3
1 1.77 105 7.83 104 2.49 106 1.30 106
2 8.25 106 4.86 104 3.55 106 4.84 1010
For the glass and Iris landscapes we see similar behaviour. The kinetic analysis
illustrates the increasing difficulty associated with escaping kinetic traps in land-
scapes containing more outliers. These results differ from the topographical and
thermodynamic measures and give a more complete picture of landscape funnelling.
We can probe the rate changes by visualising the cost function profile for sta-
tionary points on pathways. The pathway profiles for the Iris landscape are given
in Fig. 4.8. We see large barriers associated with changes in partition and structure
type in the pathways on all landscapes. The intermediate regions of the pathways
tend to be relatively flat, indicating rearrangement within a partition or structure
type to allow for the larger structural transitions. The number of structure type
changes increases with the number of outliers, but the number of partition changes
remains similar. Therefore, the rate changes can be largely explained by the in-
creased number of large barriers associated with structure type changes.
We visualise the stationary points on the pathways for both kinetic trap 2 of
original dataset (Fig. A.5), and kinetic trap 1 of the dataset with three outliers
(Fig. A.6). Both these pathways show the expected small changes in cluster labels
within a partition or structure type, punctuated by large structural transitions as-
sociated with large barriers. The pathway in the original dataset only has a single
partition change, so it contains a much greater number of similar intermediate min-
ima. In contrast, the pathway in the dataset with three outliers shows changes in
structure type that require much less rearrangement.
The glass pathways, shown in Fig. 4.9, are markedly different from the Iris path-
ways: they are much shorter, and have fewer large barriers. These characteristics
are likely due to the selected kinetic traps having the same structure type as the
global minimum for all but the Outlier 3 landscape. We again see structure type
changes associated with the largest barriers on pathways, and these structure type
barriers explain the large decrease in escape rates on the Outlier 3 landscape. The
pathways on other landscapes still contain large barriers, and these are likely due
to changes in partition, which we cannot simply monitor as for the Iris landscapes.
Preprocessing: Outliers
(a) Original (b) Outlier 1 (c) Outlier 2
(d) Outlier 3 (e) Outlier 4
Figure 4.8: The fastest paths between minima corresponding to the kinetic traps given
in Fig. A.3 and the global minimum for the Iris dataset with K = 6, and variants with
additional outliers. Paths are represented by minima, at integer values of the x-axis, and
the transition states between them. These minima and transition states are connected by
straight lines that do not reflect the curvature of the cost function surface. Changes in
structure type are denoted by triangles, and changes in partition by crosses, placed on the
transition states that connect the two differing minima.
Preprocessing: Outliers
(a) Original (b) Outlier 1
(c) Outlier 2 (d) Outlier 3
Figure 4.9: Fastest paths from selected kinetic traps to the global minimum, for the K-
means landscapes of the glass dataset and its variations. Paths are represented by minima
and transition states, connected by straight lines. The minima lie at integer values on
the x-axis and transition states give the cost function barrier between them. Changes in
structure type are denoted by triangles placed on the transition state between the two
differing minima.
Preprocessing: Outliers
Table 4.4: Rates for transitions between the set of minima of a given structure type to the
global minimum, calculated at a fictituous temperature of T = 1.0. Rates are calculated
for the Iris dataset with K = 6, and those datasets with additional outliers. The dot
indicates the structure type of the global minimum.
Type Outlier 1 Outlier 2 Outlier 3 Outlier 4
0 3.11 103 8.82 104 2.82 105 1.68 107
1  1.08 102 2.78 103 1.94 107
2   8.81 107
3 8.68 104 5.90 107
4 5.11 104 5.34 103
5 2.85 103 1.10 105
6 2.12 107
7 1.11 103
9 1.70 106
4.3 Transitions between clusterings
For the majority of minima not within kinetic traps, the small barriers that need to
be overcome to reach the global minimum allow different structure types, which can
vary widely, to lie on a funnelled landscape. However, these small maximum barrier
heights do not imply that the pathways between two different structures and the
global minimum are similar. In fact the rates between different structures embedded
in the funnel can vary significantly, even for minima of similar cost function, as shown
in Table 4.4.
We see that the average rate to move between different structures decreases as
the number of outliers increases. The rates remain comparable for structure types
that are similar to that of the global minimum in all cases, but the transition rates
from more distant structure types become progressively slower. The rearrangement
of multiple outliers involves more transition states and intermediate minima, and
consequently, the ease of transition is reduced. Furthermore, similar structure types
retain a smaller proportion of the clustering solutions as more structure types become
available.
As for kinetic traps, we can rationalise the rate changes using the pathway profiles
in Fig. 4.10. These profiles show that the maximum barrier height is small because in
all cases the highest minimum has a connected transition state with a small barrier,
and a second connected minimum much lower in cost function. The steep downhill
path ensures that the maximum barrier only depends upon the first transition state,
and in almost all cases the uphill barrier is small in the fastest paths. Therefore,
the overall barriers between each minimum and the global minimum remain small
Preprocessing: Outliers
(a) Outlier 1 (b) Outlier 2
(c) Outlier 3 (d) Outlier 4
Figure 4.10: The fastest paths from the set with a given structure type to the global
minimum for the Iris dataset with K = 6, and its variations with outliers. All stationary
points on the pathway are shown, and changes in structure type and partition are denoted
by triangles and crosses on transition states, respectively.
in the disconnectivity graphs despite the kinetic variety.
Since the highest transition state is similar in structure to the highest minimum
the rates are largely determined by path length and intermediate barriers. For the
datasets with one or two outliers there are no significant intermediate barriers and
very short pathways. For three and four outliers there are an increasing number of
quite significant intermediate energy barriers on the pathways, many of which are
associated with structure type changes. Furthermore, many of the pathways pass
through a much greater number of minima. Therefore, the change in rates arises
from both longer paths and the growth of intermediate barriers associated with
additional alternative structure types available in the landscapes. Some transitions
with large rates are still present in the landscapes with more outliers, and these are
the shorter pathways that require fewer structure type changes.
We visualise the minima and transition states on the fastest pathways between
the global minimum and structure types 0 (Fig. 4.11) and 7 (Fig. 4.12) for the Iris
dataset with four outliers. The two pathways differ significantly in the transition
rate to structure type 8. The pathway from structure type 7 to 8 retains a rate
Preprocessing: Outliers
(a) Minimum 1 (b) TS 1 (c) Minimum 2 (d) TS 2
(e) Minimum 3 (f) TS 3 (g) Minimum 4 (h) TS 4
(i) Minimum 5
Figure 4.11: Stationary points on the fastest path between all minima of structure type
0 and the global minimum of structure type 8 for the Iris dataset with K = 6 and four
outliers. Each stationary point is visualised using three of the four features.
similar to that in the landscape with one outlier, because of the similarity between
the start and end minima. The similar structure types mean that less rearrangement
is needed, and the transition can be achieved with fewer intermediate minima. For
the transition from structure type 0 to 8 we need three large structural changes to
progress to the global minimum and, consequently, the rate is much lower.
The reduction in rate with more outliers is much more pronounced for the glass
dataset. The reduction is three orders of magnitude for each additional outlier, as
shown in Table 4.5. Furthermore, the rates remain remarkably stable between all
structure types, without the large preference for transitions between structures of
similar type seen in the Iris dataset. Again, the pathways reveal that the addition
of outliers makes transitions between different structure types and the global min-
imum more challenging, due to the increase in the path length, and the growth of
intermediate barriers, as shown in Fig. 4.13.
Overall, for both landscapes, there is a reduction in rates between minima within
the large funnel and the global minimum as the number of outliers increases. The
rates remain comparable for similar structure types in landscapes for all K, but
these minima constitute a smaller proportion of the minima in the funnel. Therefore,
Preprocessing: Outliers
(a) Minimum 1 (b) TS 1 (c) Minimum 2
(d) TS 2 (e) Minimum 3
Figure 4.12: Minima and transition states on the fastest path between the set of minima
of structure type 7 and the global minimum of structure type 8 for the Iris dataset with
six clusters and four outliers.
Table 4.5: The transition rates between the set of minima with a given structure type to
the global minimum, for the glass dataset and variations with additional outliers. Rates
are calculated at a fictituous temperature T = 3.0. The dot indicates the structure type
of the global minimum.
Type Outlier 1 Outlier 2 Outlier 3
0 2.54 104 8.13 107 2.17 1010
1  5.45 107 2.45 1010
2  4.84 1010
3 2.92 1011
5 9.41 1011
(a) Outlier 1 (b) Outlier 2 (c) Outlier 3
Figure 4.13: The fastest of all possible paths from a given structure type to the global
minimum for landscapes of the glass dataset, and additional datasets containing outliers.
The paths are shown as sequences of minima and transition states connected by straight
lines. The minima lie at integer values on the x-axis.
Preprocessing: Outliers
these results compound the reduced escape rates for kinetic traps, showing that an
increased number of outliers hinders traversal of the landscape to attain the global
minimum.
The topographical view, and landscape entropy, highlight that the organistion
of the minima produces more funnelled cost function surfaces as the number of out-
liers increases. However, the transition state information shows that this favourable
organisation of minima is also accompanied by increased difficulty in moving be-
tween solutions. Therefore, overall we expect global optimisation to become more
challenging as the number of outliers increases. Furthermore, with more different
structure types, and the large difference in cluster centre placement, K-means clus-
ter initialisation may find it increasingly challenging to locate the structure types
similar to the global minimum.
4.4 Clustering quality
There are additional metrics to quantify the distribution of minima on the solution
landscapes. In the previous section we considered the structural features, structure
type and partition. Here, we use comparative metrics based upon cluster labels that
allow us to evaluate the clustering accuracy and diversity. For all the metrics based
upon cluster labels we consider the similarity of clusterings, which we quantify using
the adjusted Rand index (ARI).
4.4.1 Diversity
One important factor to consider is not just the properties of the global minimum,
which may not be attained by K-means, but the similarity of structures to the global
minimum. If there exists an ensemble of structures similar to the global minimum
then finding the true global minimum is not always required. The diversity of
clustering solutions increases significantly with the number of outliers (Fig. 4.14).
The increase in the number of structure types with outlier number, and the
expected difference in cluster labels, manifests itself in the diversity. Around the
global minimum we see little change in the distribution, and there remains a small
number of minima that are similar to the global minimum clustering. The number of
such minima is reduced as the number of outlier increases, but they remain similar
to the global minimum in all landscapes.
At higher values of the cost function we see much greater diversity in the land-
scapes with more outliers. The minimum similarity broadly decreases with an in-
creased number of outliers, and we observe much wider diversity ranges at all values
Preprocessing: Outliers
(a) Iris (K = 3) (b) Iris (K = 6)
(c) Glass
Figure 4.14: The distribution of clustering solution diversity for different datasets. The
similarity between any two clusterings was quantified using the adjusted Rand index (ARI),
and the diversity was measured as the difference between each clustering and the global
minimum of its landscape.
Preprocessing: Outliers
(a) Iris (K = 3) (b) Glass
Figure 4.15: The distribution of clustering solutions across the cost function and accuracy
space for all K-means lanscapes. The accuracy is given by the ARI between each clustering
and the ground truth labels.
of the cost function. Therefore, for clustering solutions low in cost function, but not
in the global minimum ensemble, there can be more variety in the cluster labels.
We find several low-valued minima that are all structurally distinct from the global
minimum clustering, which poses a big problem to clustering datasets with outliers.
4.4.2 Accuracy
Another important feature of the landscape is the accuracy of clustering solutions,
and their distribution in the landscape, as shown in Fig. 4.15. The accuracy measures
the similarity between the ground truth cluster labels and the cluster labels of a
given minimum. The cost function favours solutions with low sum of squared error,
which does not necessarily correlate with high accuracy. For the Iris dataset with
K = 3 the cost function correlates strongly with clustering accuracy, due to the
small amount of overlap. The highest achievable accuracy is good, but the K-means
algorithm is unable to separate the Versicolor and Virginica clusters, limiting the
cluster similarity to 0.730, as shown in Table 4.6. There is very little reduction in the
best accuracy on addition of increasing outliers, because the structure of the global
minimum retains the same structure type, which incorporates the outliers effectively
into existing clusters. However, the worst accuracy is significantly reduced.
The accuracy distribution of the glass identification dataset shows different be-
haviour. This dataset contains significant overlap and, consequently, the K-means
cost function is not sufficient to separate the data into its constituent ground truth
clusters, as shown in Table 4.7. The large overlap makes the K-means cost function
less appropriate for modelling the data, and the most accurate clustering is not the
Preprocessing: Outliers
Table 4.6: The highest K-means accuracy for the Iris dataset with three clusters, and
variations with outliers. The cost function difference between each clustering and the
global minimum is given, along with its structure type. The accuracy is then given for the
global minimum (GM) clustering, with its structure type.
Best accuracy J Type Accuracy (GM) Type (GM)
Original 0.730 0.000 - 0.730 -
Outlier 1 0.730 0.054 0 0.717 0
Outlier 2 0.730 0.054 0 0.717 0
Outlier 3 0.730 0.054 0 0.717 0
Outlier 4 0.704 0.000 0 0.704 0
Table 4.7: The accuracy of clustering solutions in the glass landscapes. The highest
achievable accuracy is given, along with the cost function gap between this clustering and
the global minimum, and its structure type. The accuracy of the global minimum (GM)
is also given, along with its structure type.
Best accuracy J Type Accuracy (GM) Type (GM)
Original 0.313 126.80 - 0.255 -
Outlier 1 0.303 24.04 / 180.66 1 / 0 0.252 1
Outlier 2 0.303 595.26 1 0.252 2
Outlier 3 0.299 811.15 3 0.226 4
global minimum in any landscape. Both the highest attainable accuracy and the
accuracy of the global minimum decrease. The most accurate clustering moves to a
higher cost function value, relative to the global minimum, as the number of outliers
increases.
The accuracy still broadly decreases with increasing cost function, for the orig-
inal glass dataset, but the relationship is not as clear as for the Iris dataset. This
relationship between the cost function height and accuracy gets weaker with a grow-
ing number of outliers, and at three outliers the accuracy has no relation to the cost
function.
4.4.3 Similarity to original dataset
We also calculate the difference between each clustering and the global minimum of
the original dataset. Using this metric, we are able to follow the movement of the
best unperturbed solution under the effect of outliers. The distributions are shown
in Fig. 4.16. For K = 3 we see that the global minimum of all outlier landscapes
retains a very similar structure to that of the original dataset, which explains why
the accuracy remains high in the outlier datasets. In the same dataset, but with
K = 6, the minima that resemble the best solution of the original dataset no longer
lie at the bottom of the cost function landscape. Such a clustering is clearly retained
Preprocessing: Outliers
(a) Iris (K = 3) (b) Iris (K = 6)
(c) Glass
Figure 4.16: The similarity of all clustering solutions to the global minimum of their
corresponding original dataset. The similarity is quantified using the adjusted Rand index
(ARI).
in each landscape, but it moves to higher values of the cost function relative to the
global minimum as the number of outliers increases.
For the glass dataset the global minimum clustering of the original dataset is
again retained in the outlier landscapes. However, as in the Iris dataset with K = 6,
this solution migrates further up the cost function landscape. Here, due to the sig-
nificant overlap, there are multiple sets of minima that resemble the global minimum
of the original dataset.
4.4.4 Relation to topography
The overlay of all the properties listed above in the disconnectivity graphs can be
seen in Figs. A.7, A.8, A.9 and A.12, which reveal how the organisation of the
landscape correlates with the properties of the minima. For the Iris landscapes
with K = 3, the accuracy, similarity and diversity all correlate strongly with the
Preprocessing: Outliers
cost function value. Minima with a similar cost function display similar properties.
For the Iris dataset with six clusters the minima in kinetic traps can show signif-
icantly different diversity relative to other minima of similar cost function values in
the main funnel. However, there are many further sharp changes in diversity that do
not correlate with landscape organisation, arising from the availability of different
structure types within the main funnel.
In both the landscapes with one and two outliers, the minima most similar to the
unperturbed global minimum lie within kinetic traps on the landscape. Furthermore,
for three and four outliers it remains the case that the kinetic traps contain many
of the minima with the highest similarity.
In the glass outlier landscapes, those minima with high similarity to the global
minimum of the original dataset are also seen in kinetic traps. Such high similarity
is not seen for minima of equivalent cost function in the main funnel. However, in
the glass datasets, because there are more separate regions that resemble the global
minimum of the original dataset, many minima inside the main funnel also have high
similarity. These landscapes highlight the importance of kinetic traps for datasets
with outliers.
The diversity of the glass datasets behaves similarly to the Iris dataset with
K = 6. Minima in kinetic traps can show significantly different similarity to the
landscape global minimum compared to the minima of equivalent cost function in
the main funnel. However, due to the poor clustering quality of all minima the
accuracy is not significantly different in kinetic traps. Many of the large changes
arise through the availability of new minima within the main funnel.
Chapter 5
Preprocessing: Standardisation
Standardisation is a common preprocessing step, aiming to equalise the importance
of all dataset features. Such rescaling of the feature space is expected to have a
large effect on the solution landscape, as it significantly changes dataset structure.
Standardisation is used to improve K-means accuracy, and here we also consider its
effect on cost function topography.
5.1 Datasets
We apply standardisations to the Iris and glass datasets both with, and without,
outliers. Outlier datasets with two outliers were chosen, as prepared in Sec. 2.1.1.
Outliers are usually removed prior to standardisation as they can significantly reduce
its effectiveness. Here, they are retained to analyse the effect on the cost function
topography.
We apply three types of standardisation to each dataset, as described in Sec. 2.1.2.
We provide a short description of the feature ranges before and after standardisa-
tions of the original Iris and glass datasets in Tables 5.1 and 5.2, respectively. The
feature ranges are more homogeneous in the original Iris dataset: they all lie within
the same order of magnitude. The feature ranges of the glass dataset vary by three
orders of magnitude (O(102)O(101)) and, consequently, we expect that standard-
isation will have a greater effect on the glass dataset.
One of the most relevant dataset properties that standardisation changes is the
overlap. Overlap strongly determines K-means clustering accuracy, and standardi-
sations would ideally reweight the features to separate ground truth clusters more
clearly. The overlap, quantified using the misclassification probability, is given for
the Iris and glass datasets in Tables 5.3 and 5.4, respectively. It is worth noting that
overlap is not the only factor determining K-means accuracy, so there is no simple
Preprocessing: Standardisation
Table 5.1: Feature ranges for the original Iris dataset, and its standardisations. All values
are given in cm.
Sepal length Sepal width Petal length Petal width
Original 3.60 2.40 5.90 2.40
Z1 4.36 5.55 3.36 3.15
Z2 0.46 0.55 0.86 0.96
Z3 1.00 1.00 1.00 1.00
Table 5.2: Feature ranges for the original glass dataset, and those generated by applying
standardisations. The features are listed in the same order as the data taken from the
UCI machine learning data repository.
x1 x2 x3 x4 x5 x6 x7 x8 x9
Original 0.02 6.65 4.49 3.21 5.70 6.21 10.76 3.15 0.51
Z1 7.52 8.16 3.12 6.44 7.25 9.54 7.58 6.35 5.25
Z2 0.02 0.38 1.00 0.92 0.07 1.00 0.66 1.00 1.00
Z3 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00
relationship between overlap and accuracy for the standardised datasets. However,
it is a useful guide to the ability of standardisations to separate overlapping clusters,
as found for both of the datasets here.
The datasets show fluctuations of similar magnitude in the overlap as a result of
standardisation. The standardisations can change the number of overlapping data
points by around 7%, with some schemes reducing and some schemes increasing the
overlap relative to the original dataset. The relative performance of the standardi-
sations in reducing overlap is significantly different in the two datasets. Therefore,
we again see that the optimal standardisation depends heavily upon the dataset.
Table 5.3: Dataset overlap, as calculated using the misclassification probability, for various
Iris datasets. The overlap is given for both the original Iris dataset, the Iris dataset with
two additional outliers, and their standardisations.
Original Z1 Z2 Z3
Original 7.3 14.7 4.0 6.7
Outlier 7.3 11.3 3.3 4.0
Preprocessing: Standardisation
Table 5.4: The percentage of data points that overlap, as determined by the misclassifi-
cation probability, for the various glass datasets.
Original Z1 Z2 Z3
Original 51.9 45.3 51.9 49.5
Outlier 51.9 48.6 50.0 50.5
(a) Original
(b) Z1
(c) Z2
(d) Z3
Figure 5.1: K-means landscapes for the original Iris dataset, and those generated by
applying standardisations. Minima in the disconnectivity graphs are coloured according
to the clustering accuracy, as measured by the ARI between each minimum and the ground
truth labels. The colour range is the same in all plots.
5.2 Iris
5.2.1 Landscape structure
Original
The other properties studied in this work, namely outliers, overlap and cluster pop-
ulation, involve sequential dataset changes. Here, the datasets do not change in a
systematic way, and we compare all standardised datasets to the unstandardised
example. The Iris landscapes, both with and without outliers, show little change
in organisation with standardisation. The overall topography seems to be relatively
well retained for all standardisations with three clusters.
For Fishers Iris dataset with no outliers the landscapes remain funnelled. The
landscape of the original dataset contains a small kinetic trap, which is reduced in
depth in the standardised landscapes, as shown in Fig. 5.1. The kinetic trap only
really remains for the Z2-standardised dataset, but we visualise, and calculate the
kinetic properties of, high-valued minima in the Z1 landscape too for comparison.
The distribution of minima remains similar as well. The global minimum, and
its associated clusterings, are separated from alternative clusterings by a large cost
function gap. Furthermore, both the global minima, and the high-valued minima,
Preprocessing: Standardisation
(a) Original (b) Z1 (c) Z2 (d) Z3
Figure 5.2: The global minimum clustering for the original Iris dataset, and its stan-
dardised variants. To allow visualisation the clusterings are displayed in three of the four
features, with petal width excluded. The crosses represent the cluster centres, and the
data points associated with each cluster are distinguished by colour.
(a) Original (b) Z1 (c) Z2
Figure 5.3: The kinetic trap minima, as specified in Fig. B.1, on the K-means landscapes
for the original Iris dataset, and its standardisations. Minima are visualised in three
features, excluding petal width.
are structurally similar in all landscapes. These clusterings retain the same par-
titionings, with small changes in cluster boundaries due to the different feature
distributions. The global minima are shown in Fig. 5.2.
Selected kinetic trap minima, as highlighted for the disconnectivity graphs in
Fig. B.1, are visualised in Fig. 5.3. These minima all have an alternative parti-
tioning of the cluster centres amongst the underlying clusters, relative to the global
minimum. Given the structural similarity of low- and high-valued minima the re-
duction in scale of the clustering problem, due to feature rescaling, seems to be the
main reason for the reduction in kinetic trap depth.
We calculate rates for these transitions, which show that the kinetic trap depth
largely explains the rates from kinetic trap to global minimum, as given in Ta-
ble 5.5. The cost function gap between the endpoint minima differ widely, but this
difference exerts little effect on the rates. The pathways all have small barriers
at each step, and the small difference in maximum barrier height, relative to the
higher endpoint minimum, between landscapes is sufficient to modify the rates, as
shown in Fig. B.2. Therefore, escape from kinetic traps is made easier by dataset
standardisation, thereby improving the optimisation properties of the cost function
surface.
Preprocessing: Standardisation
Table 5.5: Escape rates from the lowest minima of each kinetic trap, as highlighted in
Fig. B.1, to the global minimum for the Iris landscapes without outliers. Rates are
calculated at the fictituous temperature T = 1.0.
Standardisation Rate
Original 3.33 104
Z1 9.33 103
Z2 5.55 104
(a) Original (b) Z1 (c) Z2
(d) Z3
Figure 5.4: The distribution of barrier heights for all transition states on a given landscape.
The higher barrier is the cost function gap between the transition state and its higher
connected minimum. The lower barrier is measured relative to the lower of the two
connected minima.
We see small barriers in all the fastest pathways for escaping kinetic traps, but it
is possible to gain more insight by analysing all the transition states. We calculate
both barriers for each transition state, which are the cost function gaps between the
higher and lower of the two connected minima. The barrier distributions are plotted
for all landscapes in Fig. 5.4.
We see that it is unlikely the kinetic trap pathways are unique in containing
only small uphill barriers as both higher and lower barriers are sharply peaked at
small values in all landscapes, independent of the cost function range. The large
density of higher barriers at small values indicates that each transition state is
usually similar in cost function to its highest connected minimum.170 Therefore,
the application of standardisations does not alter the relative position of transition
states, which usually lie at, or near to, the intersection seam involving the higher
Preprocessing: Standardisation
(a) Original
(b) Z1
(c) Z2
(d) Z3
Figure 5.5: K-means landscapes for the Iris dataset, and its standardised variations, with
two outliers. The minima are coloured according to the ARI between their cluster labels
and the ground truth labels. The colour range is the same in all plots.
connected minimum.
The low-valued peak in lower barrier heights lead to a similar conclusion for the
lower connected minimum, indicating that in many cases the transition states lie at
the intersection seam between two similar minimal clusterings. However, there are
several lower barriers that are much larger. These transition states allow us to move
between different partitionings, and cost function ranges, via a long steepest-descent
path that does not attain a minimal clustering until a lower partitioning has been
attained. There is an increased proportion of large higher and lower barriers in the
Z3 landscape, but in general the standardisations make little difference to the barrier
heights and transition state positioning relative to minima.
Outliers
Many of these trends are repeated for the datasets with two outliers. The disconnec-
tivity graphs show that the topographies of the cost function surfaces are strongly
retained in Fig. 5.5. The landscape for the unstandardised dataset contains three
distinct bands of minima in the cost function, which can be seen in all standardised
landscapes. As for the original Iris dataset, there is a kinetic trap, although it is
much lower in cost function for the outlier landscapes. In fact, for these landscapes
there are two kinetic traps that clearly remain in all the standardised landscapes,
with more associated minima in the Z1 and Z2 landscapes.
The global minima, shown in Fig. 5.6, again have a similar structure across all
outlier landscapes. The partitioning is conserved and the outliers are assigned to the
same clusters in all but the Z1 landscape, where one outlier is assigned differently.
In this case, the resulting change in the cluster boundaries is very small.
The kinetic traps are also similar in structure, as visualised in Fig. 5.7. All kinetic
Preprocessing: Standardisation
(a) Original (b) Z1 (c) Z2 (d) Z3
Figure 5.6: The global minimum clusterings for the standardised and unstandardised Iris
datasets with two outliers. Original refers to the unstandardised dataset containing two
outliers.
(a) Original (1) (b) Original (2) (c) Z1 (1) (d) Z1 (2)
(e) Z2 (1) (f) Z2 (2) (g) Z3 (1) (h) Z3 (2)
Figure 5.7: The lowest minimum from the two kinetic traps on each K-means landscape,
for the Iris datasets with two outliers. The kinetic trap labelling corresponds with the
highlighted minima in Fig. B.1. Original refers to the unstandardised dataset containing
two outliers.
traps correspond to an outlier being placed in its own cluster, and the accommo-
dation of the other outlier into one of the two K-means clusters used to represent
the original data. The two kinetic traps correspond to each different outlier being
assigned to its own cluster. We highlight the chosen kinetic trap minima for the
disconnectivity graphs in Fig. B.1.
The escape rates for these kinetic traps are given in Table 5.6; the rates are faster
in the two standardisations with the smaller cost function ranges, Z2 and Z3. The
reduction in cost function range increases the rate in both these cases as it reduces
the absolute barrier heights of the pathways (Fig. 5.8). The barrier heights, relative
to the cost function gap between the endpoint minima, are similar in all landscapes,
but the absolute barrier heights decrease. Therefore, we do not expect the increase
in rates to necessarily correlate with increased global optimisation properties, as the
Preprocessing: Standardisation
Table 5.6: Escape rates from the lowest minima of each kinetic trap to the global minimum
for the Iris landscapes with two outliers. The selected minima for the two kinetic traps
are shown in Fig. B.1. Rates are calculated at the fictituous temperature T = 1.0.
Standardisation KT1 KT2
Outlier 2 3.58 108 5.28 102
Z1 1.82 106 1.95 104
Z2 4.12 103 4.38 103
Z3 4.36 102 3.28 103
(a) (b)
Figure 5.8: Pathway profiles for the fastest paths between kinetic trap minima and the
global minimum for the Iris dataset with two outliers, and its standardisations. Due to the
different cost function scales we present the pathways for the original and Z1-standardised
landscapes in (a), and the remaining standardisations in (b).
only relevant change is the cost function scale of the problem. On average, the rates
remain comparable for the unstandardised and Z1-standardised datasets of similar
cost function range.
The barrier distributions show analogous behaviour to the Iris landscapes with-
out outliers. Standardisation makes little difference to the barrier height distribu-
tions, which remain highly localised at the lowest region of the barrier height range,
as shown in Fig. 5.9. Again there are a small number of barriers to lower con-
nected minima that are much larger, which remain important for transition between
partitionings. However, we still observe that the majority of transition states lie
on, or close to, the intersection seam between two similar and minimal clusterings,
irrespective of the standardisation.
5.2.2 Networks
One valuable feature of building stationary point databases that contain transition
states, is the ability to represent the clustering solutions as a network. Graph al-
gorithms, such as those used for the rate calculations, can then be exploited to
Preprocessing: Standardisation
(a) Original (b) Z1 (c) Z2
(d) Z3
Figure 5.9: Barrier distributions for the standardised and unstandardised Iris datasets
containing two outliers. The higher and lower barriers are given by the cost function gap
between the transition state and its higher-valued and lower-valued connected minima,
respectively.
provide insight into the solution space based entirely on connectivity and not phys-
ical dimensions. Several methods for understanding network shape are presented
in Table 5.7. Other schemes are available; here we have selected some of the most
popular approaches.
In such small networks, and especially those that have not been sampled exhaus-
tively, we cannot be sure of having located all connections between known minima.
However, we expect the large number of attempted connections in different regions
of space to enable us to differentiate overall network features for the Iris landscapes.
The average degree is one of the simplest measures to understand network struc-
Table 5.7: Network properties of the K-means landscapes calculated from two Iris datasets
and their standardisations.
Standardisation Average degree Density Triadic closure
Original 1.80 0.20 0.00
Z1 2.11 0.12 0.00
Z2 2.00 0.1 0.11
Z3 2.40 0.6 0.55
Outlier 2 2.00 0.25 0.00
Z1 2.19 0.07 0.11
Z2 2.33 0.21 0.13
Z3 2.13 0.15 0.09
Preprocessing: Standardisation
ture. Here, we find low average degrees for all landscapes, and we infer that the
networks are highly linear. Each transition state defines a single edge, and for a
linear sequence of minima forming a single connected component we would have an
average degree of just under 2. Therefore, we see that in these landscapes all minima
are largely connected in sequence, with no alternative paths between them. There
are several other network structures commensurate with such an average degree, but
we visualise the graphs to validate our interpretation.
The density is relatively low in all cases, and largely reflects the number of
minima. For a linear graph the density decreases as the number of minima increases.
Therefore, even with the small decrease in linearity for standardised forms of both
datasets, the higher density in the unstandardised datasets is largely due to a smaller
number of minima. The triadic closure provides further evidence of linearity, as there
are very few triangles present in all the networks. However, there are more in the
standardised datasets, which agrees with the average degree exhibiting decreasing
linearity.
For these small networks it is possible to produce two-dimensional visualisations.
We relate them to the corresponding disconnectivity graphs in Figs. 5.10 and 5.11,
for the Iris landscapes with and without outliers, respectively. We highlight equiv-
alent minima in all cases to see the interplay of connectivity and topography.
In these plots we can directly observe that the networks are highly linear. For the
Iris landscapes without outliers the high-valued minima are highly distinct in con-
nectivity. The disconnectivity graphs show little contrast between the high-valued
minima, but the network graphs highlight that in all cases, there is a distinction
between these minima based on connectivity. In the linear arrangement of minima
some high-valued clusterings lie on opposite sides of the global minimum. These
plots show evidence for retained network structure, in addition to topography, in all
the standardised landscapes.
The Iris landscapes with outliers are also largely linear, but to a lesser ex-
tent than the landscapes without outliers, as reflected in the higher average de-
gree. We see that the global minimum becomes a hub in all these landscapes, as
seen for Lennard-Jones clusters.173,174 There are various linear branches that ex-
tend from the global minimum, and the disconnectivity graphs show that different
branches correspond to the two kinetic traps. Therefore, the combination of these
two methodologies provides great insight into the K-means landscapes.
Preprocessing: Standardisation
(a) Original (b) Z1
(c) Z2 (d) Z3
Figure 5.10: Plots showing the graph representation of the K-means landscapes for the
Iris datasets without outliers, alongside the corresponding disconnectivity graphs. The
nodes in the graph representation are coloured by cost function, using a linear scale from
white (global minimum) to dark green (highest-valued minimum). Equivalent minima in
both visualisations are connected by red lines.
Preprocessing: Standardisation
(a) Outlier 2 (b) Z1
(c) Z2 (d) Z3
Figure 5.11: Network graphs alongside the corresponding disconnectivity graphs for the
Iris dataset with two outliers, and its standardised forms. The networks are visualised
with black edges, and nodes coloured between white and dark green based on the cost
function value, with white denoting the global minimum. The equivalent minima in both
visualisations are connected by red lines.
Preprocessing: Standardisation
(a) Accuracy (b) Diversity
Figure 5.12: The distribution of the clustering solutions across the cost function and sim-
ilarity space. The similarity between any two clusterings is quantified using the adjusted
Rand index (ARI). Accuracy measures the difference between the ground truth and every
clustering. Diversity measures the difference between the global minimum clustering of
each landscape and the other solutions in that landscape. The distributions are calculated
for the Iris landscapes with datasets containing no outliers.
5.2.3 Clustering quality
We now consider the distribution of minima properties in the cost function space,
specifically the accuracy and diversity. The accuracy of the clustering solutions
follows a simple relationship for the original dataset, and its standardisations, as il-
lustrated in Fig. 5.12. The accuracy of the cost function solutions correlates strongly
with the height of the solutions in the cost function space. The Z1 landscape has
several low-valued minima, of which the global minimum is not the most accurate
solution. However, the most accurate solution is still located in the lowest band of
minima.
The diversity is also shown, and due to the high accuracy of the global minimum,
the diversity relationships correlate strongly with the accuracy. Again the similarity
to the global minimum decreases as the cost function difference increases. The
diversity is shown in the disconnectivity graphs in Fig. B.3.
The distributions for the outlier landscapes behave in a similar manner. As with
the original dataset the accuracy of minima largely decreases with increasing cost
function, as illustrated in Fig. 5.13. The higher bands correspond to sets of less
accurate solutions. However, the Z1-standardised landscape has some minima of
similar accuracy to the global minimum at significantly higher cost function, which
are not present in the other landscapes.
Again the diversity and accuracy are closely correlated due to the high accuracy
of the global minimum. The diversity differences can be more pronounced due to
Preprocessing: Standardisation
(a) Accuracy (b) Diversity
Figure 5.13: The distribution of clustering properties across the cost function range for
the Iris datasets containing two outliers. The similarity between any two clusterings is
quantified using the adjusted Rand index (ARI). Accuracy gives the ARI between minima
and ground truth cluster labels. Diversity uses the global minimum of the corresponding
landscape as the reference.
the effect of outliers, and those at high cost function have no similarity to the global
minimum clustering.
We present the accuracy of K-means for all landscapes in Table 5.8. We see vari-
able performance of the standardisation methods for both datasets with and without
outliers. The Z2 standardisation performs significantly better in both cases than the
other two standardisations. Z3 is comparable in performance to the original dataset
over both datasets, but the Z1 standardisation degrades clustering performance.
The global minimum is the most accurate clustering for only the Z3 landscape
in the Iris datasets containing outliers. However, the cost function difference to the
global minimum is very small in all cases. The difference in accuracy is pronounced
for such a small change in the Z2 landscape though. For the original dataset it is
only the Z1 landscape where the global minimum is not the most accurate clustering,
and again the most accurate clustering remains similar in cost function to the global
minimum.
Interestingly, we see little correlation between the landscape structure and the
properties of the K-means solutions. The minima in kinetic traps show little differ-
ence in behaviour from those of similar cost function in the main funnel. Therefore,
the quality of solutions depends solely upon the value of the cost function.
The landscape structure is strongly retained in all standardisations, with both the
global minimum and kinetic traps keeping the same clustering structure. We see that
standardisation generally speeds the escape from kinetic traps, even if the accuracy
is not improved from the original dataset. Therefore, all standardisations offer
Preprocessing: Standardisation
Table 5.8: The highest achieved accuracy of K-means clustering, measured as the adjusted
Rand index between the ground truth labelling and each clustering, for all standardised
and unstandardised Iris datasets. Where the global minimum (GM) is not the most
accurate solution we also provide its accuracy, and the relative height of the most accurate
solution in the normalised cost function range.
Standardisation Best accuracy J Accuracy (GM)
Original 0.730 0.000 -
Z1 0.645 0.018 0.620
Z2 0.886 0.000 -
Z3 0.716 0.000 -
Outlier 2 0.730 0.000 0.717
Z1 0.632 0.001 0.620
Z2 0.835 0.001 0.759
Z3 0.759 0.000 -
improved chances of locating the global minimum, but successful standardisation
schemes also improve the accuracy of the global minimum.
5.3 Glass
5.3.1 Landscape structure
The glass landscapes change more significantly on standardisation. The greater
effect of standardisation is expected due to much more uneven feature ranges. The
landscapes, which show little structure due to the significant overlap, still display
significant changes in behaviour.
Original
The landscapes for the original dataset, shown in Fig. 5.14, all retain small barriers
to the global minimum. Therefore, standardisation does not change the single-funnel
nature of the K-means landscapes. However, there are significant changes in both
the character of kinetic traps, and the distribution of minima.
In the landscapes of the original dataset, there are only two low-lying kinetic
traps. The lowest kinetic trap in all standardised landscapes resembles that in the
unstandardised landscapes, and we denote these similar kinetic traps as 1, with their
position in the landscapes shown in Fig. B.4. However, it is much more difficult to
assign equivalent kinetic traps at higher values of the cost function. Due to the Z2
and Z3 standardisations these high-valued kinetic traps have a significantly different
character in these landscapes. They have more deep kinetic traps, each of which
Preprocessing: Standardisation
(a) Original
0.35 (b) Z1
0.35 (c) Z2
0.35 (d) Z3
Figure 5.14: Disconnectivity graphs corresponding to the K-means landscapes for the glass
identification dataset and its standardisastions. Each minimum is coloured according to
the difference in cluster labels compared to the ground truth labels, as measured by the
ARI. The colourbar indicates the range, which is the same in all cases.
Preprocessing: Standardisation
Table 5.9: Escape rates from the lowest minima of each kinetic trap to the global minimum
for the glass landscapes without outliers. The kinetic traps are labelled according to
Fig. B.4. Rates are calculated at the fictituous temperature T = 3.0.
Standardisation KT1 KT2
Original 3.65 104 1.74 105
Z1 2.40 105 7.82 1010
Z2 2.07 104 1.29 104
Z3 2.30 104 1.30 104
have much reduced populations relative to the original dataset. These kinetic traps
do not have a localised funnel structure as they only contain the lowest minima, and
not the solutions lying at higher values within the kinetic trap.
The escape rates from the similar low-valued kinetic traps are presented in Ta-
ble 5.9, along with rates from an additional kinetic trap. All selected kinetic traps
for this analysis are given in Fig. B.4. The rates are similar for the unstandardised
and both the Z2- and Z3-standardised datasets. Given the different character of
kinetic trap 2 in these two standardised landscapes it is interesting to note that
the rates remain comparable, only increasing slightly relative to the unstandardised
dataset. The Z1 standardisation decreases the escape rates from both kinetic traps.
The associated pathway profiles for the fastest paths are given in Fig. 5.15. For
kinetic trap 1, which appears in all the landscapes, the pathways are all of similar
length. Furthermore, the maximum barrier height, relative to the cost function gap
between the two endpoint minima, remains comparable in all the landscapes. In ab-
solute terms the barrier is larger for the Z1 and unstandardised landscapes so, with
all rates calculated at the same temperature, the rates are expected to be slower.
However, despite the much larger barrier heights the rates for standardised Z2 and
Z3 remain comparable. The best pathway is a little shorter in the unstandardised
landscapes, but the overall rates are calculated as a sum over all pathways and, con-
sequently, the similar rates can more likely be explained by more distinct pathways
being feasible in the unstandardised landscape.
The pathways from kinetic trap 2 to the global minimum show much more vari-
ation in properties between the standardised and original dataset. These kinetic
traps are not similar, so we expect to see more variation in properties, but the
behaviour of all standardised datasets remains similar. The paths are significantly
longer for the standardised datasets, and contain more intermediate barriers. Hence,
the character of the kinetic traps in Z2 and Z3 appears to make little difference to
the pathways. The escape rates are faster for these two standardised datasets than
the original dataset, but we see from the pathways that this difference is largely due
Preprocessing: Standardisation
(a) Original (b) Z1 (c) Z2
(d) Z3
Figure 5.15: Pathway profiles for the fastest paths between the minima contained in
kinetic traps and the global minimum. Pathways are calculated for selected kinetic traps,
as shown in Fig. B.4, on the glass landscapes constructed from datasets without outliers.
to the reduced cost function scale, as transitions between regions appear to require
more intermediate minima and more significant intermediate barriers.
In addition to kinetic traps, the other significant effect of standardisation is
to transform the distribution of minima in the cost function range, as shown in
Fig. 5.16. The cost function is represented as a fraction of the complete range to
give a clear view of the distribution, independent of the range. The distribution for
the original dataset has a unique global minimum, with few minima similar in cost
function. Such a unique global minimum is observed for the Z3 standardisation, but
the other two standardisations exhibit a much larger ensemble of minima related
to the global minimum. Therefore, from a probabilistic perspective, Z1 and Z2
standardisations are likely to improve the efficiency with which initialisations locate
the global minimum ensemble.
Furthermore, the density peaks that arise at intermediate cost function values,
due to the availability of many distinct partitionings, occur at similar cost function
values in unstandardised and Z2 landscapes. However, both Z1 and Z3 standardis-
ations shift the density peak(s) to larger cost function values, despite Z1 having a
higher density about the global minimum.
In addition to the minima distribution in terms of the cost function, it is worth
considering the barrier distribution in each landscape. The barriers between minima
Preprocessing: Standardisation
Figure 5.16: The distribution of minima about the cost function range, for the original
glass dataset and its standardisations. The cost function range is rescaled in all plots to
lie between 0 and 1.
determine transition rates between clustering solutions. We calculate the barrier
distribution for all transition states, giving the higher and lower barriers separately.
Due to the relatively smooth distribution of minima there are barriers spread across
the full cost function range, as illustrated in Fig. 5.17.
The barriers show similar behaviour to the Iris datasets. The higher and lower
barriers remain strongly peaked at very low values, indicating that transition states
in all the landscapes remain at, or near to, the intersection seam between two min-
imal clusterings of similar cluster labels.
We find minimal changes to the distributions on standardisation. However, we
observe that for the standardised landscapes there are a greater proportion of large
barrier heights, relative to the cost function range. Consequently, standardisation
may transform the solution space such that more steepest-descent paths from tran-
sition states travel further without encountering a minimal clustering. Therefore,
more transition states are structurally distinct from both connected clusterings,
which may result from non-minimal clusterings contributing to more of the cost
function surface.
Outliers
The landscapes for the outlier datasets exhibit the greatest change in topography
as a result of standardisation, as shown in Fig. 5.18. The original glass dataset
Preprocessing: Standardisation
(a) Original (b) Z1 (c) Z2
(d) Z3
Figure 5.17: Barrier distributions for the glass dataset, and for standardisations, without
outliers. The barriers are calculated for all transition states, with the higher and lower
barriers corresponding to the cost function gap between each transition state and the
higher and lower connected minima, respectively.
containing two outliers gives rise to a strongly funnelled landscape, as noted in
Chapter 4. The Z1 landscape retains much of the topography of the original dataset,
and its funnelled structure. The Z3 landscape also remains funnelled, but with the
addition of significantly more kinetic traps at higher values of the cost function.
These high-valued kinetic traps exhibit many similar features to those in the Z2
and Z3 glass datasets without outliers: they contain a very small number of minima
lying at the bottom of each trap.
The Z2 landscape can be viewed as double-funnelled. Double-funnel landscapes
typically arise in physical systems due to the presence of two competing morpholo-
gies of similar energy. The competing structural morphologies are separated by a
large barrier, but the region around each stucture is locally funnelled. Interest-
ingly, in the Z2 landscape, the clusterings in both funnels show high similarity. The
disconnectivity graph coloured according to the similarity of minima to the global
minimum is given in Fig. B.6. There is little evidence of the distinct morphologies
that characterise double-funnel landscapes for physical systems. The appearance of
such a landscape topography is unique amongst the many different datasets studied
here and proves that such landscapes can exist for K-means.
The effect of the landscape structure on the kinetic trap escape rates is shown in
Table 5.10. The selected kinetic traps are highlighted in Fig. B.4. We observe similar
Preprocessing: Standardisation
(a) Original
0.35 (b) Z1
0.35 (c) Z2
1 (d) Z3
Figure 5.18: Disconnectivity graphs illustrating K-means landscapes for the glass iden-
tification datasets containing two outliers. Each minimum is coloured according to its
clustering accuracy, as measured by the ARI relative to the ground truth labels. The
colour bar range is the same in all plots.
Preprocessing: Standardisation
Table 5.10: Transition rates from the lowest minima of selected kinetic traps to the global
minimum for the glass landscape containing two outliers, and its standardisations. Rates
are calculated at a fictituous temperature T = 3.0, and the labelled kinetic traps are given
in Fig. B.4.
Standardisation KT1 KT2 KT3
Outlier 2 2.64 104 6.55 108 2.20 105
Z1 4.73 105
Z2 2.81 104 4.70 104 1.55 104
Z3 1.19 103 1.36 103 3.99 104
(a) Outlier 2 (b) Z1 (c) Z2
(d) Z3
Figure 5.19: Pathway profiles for the fastest paths from selected kinetic traps to the global
minimum for the glass dataset containing two outliers, and its standardisations.
results to the datasets without outliers, and comparable rates. Z1 standardisation
makes escape from kinetic traps more challenging, whilst Z2 and Z3 makes it easier,
as estimated by the transition rates.
The pathway profiles, visualised in Fig. 5.19, show that the increased rates in
the Z2-standardised landscape are likely due to the smaller cost function scale. Rel-
ative to the cost function gap these pathways contain more significant intermediate
barriers than the unstandardised landscape in pathways of comparable length. In
comparison, the Z3 pathways are much smoother, with very few significant barri-
ers. Therefore, the increased Z3 rates arise from smaller barriers, in addition to the
reduced cost function range.
We can again calculate the distribution of minima over the cost function range
(Fig. 5.20). The outlier 2 landscape has many distinct density peaks due to the large
Preprocessing: Standardisation
Figure 5.20: Distributions of minima over the cost function range for the standardised and
unstandardised glass datasets containing two outliers. The cost function range is rescaled
in all plots to lie between 0 and 1.
number of different types of clustering structure that become available at different
cost function ranges. The multiple peaks largely disappear on standardisation. Z2
has a significantly smoother profile, but with reduced density about the low region
of clustering space. Z3 exhibits fewer minima low in cost function and, along with
Z1, additional density in the highest region of the cost function range. Z1 is the only
standardisation to retain a significant number of minima near the global minimum.
Therefore, the distribution changes induced by standardisation are not necessarily
favourable for global optimisation.
The barrier height distributions exhibit similar properties to the landscapes with-
out outliers. Standardisations retain more density at larger barrier heights, as shown
in Fig. 5.21. However, it is the Z2 and Z3 landscapes that retain more large barriers
here than Z1, as seen previously. Therefore, it seems a general trend that standardi-
sations increase the proportion of transition states that are far from their connected
minima.
5.3.2 Networks
As for the Iris landscapes we calculate network properties from the stationary point
databases. The average degree, density and triadic closure are all presented in
Table 5.11. The average degree is greater than for the Iris landscapes, so we ex-
pect these networks to be less linear in organsation, and to have more alternative
Preprocessing: Standardisation
(a) Outlier 2 (b) Z1 (c) Z2
(d) Z3
Figure 5.21: The distribution of barrier heights, as calculated from every transition state
on the K-means landscapes for the glass datasets containing two outliers.
pathways across solution space. However, the overall connectivity remains low, as
reflected by the density.
The landscapes constructed from the original glass dataset are too large for
useful network visualisation. However, the landscapes constructed from the glass
dataset containing two outliers remain in the range for feasible visualisation. These
networks are presented in Fig. B.5. We can clearly observe the reduced linearity of
the networks, but they still retain large linear regions. Such linear regions correspond
to a valley in solution space where escape can only progress through a sequence
of minima along the valley bottom. However, the networks also contain densely-
connected hubs of minima from which the linear regions protrude. These hubs drive
Table 5.11: Network properties calculated from K-means landscapes. The chosen KMLs
are calculated from two unstandardised glass datasets, and their standardisations.
Standardisation Average degree Density Triadic closure
Original 3.58 1.9 104 0.032
Z1 3.14 2.6 105 0.069
Z2 3.72 8.2 105 0.063
Z3 3.33 2.7 105 0.055
Outlier 2 3.86 6.7 104 0.086
Z1 3.07 1.9 104 0.080
Z2 2.97 7.6 104 0.090
Z3 3.09 6.3 104 0.092
Preprocessing: Standardisation
Table 5.12: The best achievable accuracy of the K-means algorithm for the various stan-
dardised and unstandardised glass datasets. The most accurate clustering is not the global
minimum in any case, so we also present its position in the normalised cost function range,
J, and the global minimum (GM) accuracy. Accuracies are measured as the ARI be-
tween ground truth labels and those of a given minimum.
Standardisation Best accuracy J Accuracy (GM)
Original 0.313 0.360 0.255
Z1 0.314 0.310 0.170
Z2 0.303 0.712 0.182
Z3 0.314 0.587 0.164
Outlier 2 0.303 0.547 0.252
Z1 0.335 0.117 0.238
Z2 0.318 0.513 0.244
Z3 0.306 0.446 0.244
the increase in average degree.
There are fewer clear trends in these networks. The average degree somewhat
decreases in the standardised landscapes, in contrast to the behaviour of the Iris
datasets. There is a significant decrease for the outlier landscapes, and the reduced
connectivity across the whole landscape can be seen clearly in the network visuali-
sations. The triadic closure is relatively independent of the average degree, and we
observe that the short-range connectivity it implies is greater in the standardised
datasets. The decrease in average degree and increased triadic closure is likely due
to more linear regions in the networks of standardised datasets, but also to more
dense hubs of similar clusterings.
5.3.3 Clustering quality
The clustering quality for the glass landscapes is much lower than for the Iris dataset
due to the significant overlap. The standardised and unstandardised datasets have
very similar best accuracies for the datasets without outliers, as shown in Table 5.12.
However, the global minimum is not the most accurate clustering in any landscapes,
and all the standardisations have significantly reduced accuracy for the global mini-
mum. Consequently, despite the comparable maximum accuracy of the standardised
and unstandardised datasets, the global minimum will be less accurate in the stan-
dardised datasets.
The cost function position of the most accurate clustering is significantly in-
creased for both Z2 and Z3. Without ground truth cluster labels as a reference
it cannot be known that the most accurate solutions occur in the top half of the
cost function range, and that the global minimum gives poor clustering solutions.
Preprocessing: Standardisation
(a) Accuracy (b) Diversity
Figure 5.22: The distribution of clustering solution properties across the cost function
range for the glass datasets without outliers. The similarity between any two clusterings
is quantified using the adjusted Rand index (ARI). Accuracy measures the difference
between the ground truth and every clustering. Diversity measures the difference between
the global minimum clustering of each landscape and the other solutions in that landscape.
The cost function gap between the global minimum and most accurate minimum
is also pronounced for the original dataset, but becomes more significant for these
standardised datasets.
Outliers are expected to significantly degrade the performance of standardisa-
tion. However, they provide a small accuracy improvement in comparison to the
unstandardised dataset here. We again see a decrease in the accuracy of the global
minimum, but the reduction is much smaller for the outlier datasets. The most ac-
curate solution is much higher in the cost function landscape for the unstandardised
outlier landscape, than for the unstandardised original landscape, and in this case
standardisations reduce this value, especially for Z1.
As expected, due to the significant overlap, the distribution of accuracy and di-
versity across the cost function surface is more complex than for the Iris dataset.
The distributions of accuracy (ARI relative to the ground truth) and diversity (ARI
relative to the global minimum) are shown in Figs. 5.22 and 5.23, respectively. The
overlay of accuracy on the cost function topography is shown in earlier disconnec-
tivity graphs (Figs. 5.14 and 5.18); the diversity is shown in Fig. B.6.
For the unstandardised dataset without outliers, the accuracy generally decreases
with increasing cost function. The peak in accuracy is not at the global minimum,
but higher in cost function, and the accuracy decreases with increasing cost function
above this solution. Below the most accurate clustering there is little correlation
between increasing cost function and accuracy.
The accuracy distributions are much broader for the standardised datasets. The
Preprocessing: Standardisation
(a) Accuracy (b) Diversity
Figure 5.23: The accuracy and diversity of clustering solutions across the cost function
range for the glass dataset containing two outliers, and its standardisations. Accuracy and
diversity are both measured by the ARI between each minimum and a reference clustering.
The reference clustering for accuracy and diversity corresponds to the ground truth cluster
labels and the global minimum of the corresponding landscape, respectively.
possible accuracy range, at a given cost function, is much wider in the standardised
datasets across much of the cost function range. The increased accuracy ranges
reflect the overlap in cost function of many alternate partitionings of the dataset,
and greater structural diversity within these partitionings, as evidenced by the di-
versity distributions. Therefore, it is harder to select an accurate solution based on
cost function, or with structural similarity to the global minimum clustering. Fur-
thermore, there is little correlation between accuracy and cost function across the
whole cost function range for the standardised landscapes, with the most accurate
solutions at high cost function in the Z2 and Z3 landscapes.
Even the unstandardised glass dataset, with two additional outliers, has a re-
markably structureless accuracy distribution. There is almost no correlation between
accuracy and cost function, as seen in Fig. 5.23. The standardised datasets again
show no correlation between accuracy and cost function, but in this case it is more
expected, due to the presence of outliers.
The diversity distributions show wide diversity ranges, at given cost function
values, in each of the outlier landscapes. The diversity does not correlate with cost
function, and the clusterings are highly diverse even at low cost function values.
Furthermore, many minima similar to the global minimum are seen at high cost
function values. We present aggregate properties of the diversity distributions in
Table 5.13, where we quantify both the percentage of minima most, and least, similar
to the global minimum.
For the landscapes without outliers we observe that standardisations produce
Preprocessing: Standardisation
Table 5.13: The percentage of minima that have the lowest, and highest, similarity to
the global minimum in the glass landscapes. The minima with the lowest similarity are
selected as those with ARI < 0.5. The minima with highest similarity are selected as those
with ARI > 0.9.
Standardisation Least similar Most similar
Original 0.8 19.4
Z1 19.0 4.7
Z2 18.8 1.5
Z3 5.3 8.4
Outlier 2 13.4 16.4
Z1 9.3 22.3
Z2 20.4 25.8
Z3 7.6 18.1
an increase in the number of distinct clusterings, and a decrease in the relative
population of similar structures. In contrast, all the standardisations give a larger
percentage of minima similar to the global minimum in the outlier landscapes. Fur-
thermore, Z1 and Z3 standardisations reduce the amount of dissimilar clusterings
from the global minimum. Therefore, standardisations applied to the datasets with-
out outliers make location of the global minimum partitioning less likely using ini-
tialisation schemes. Furthermore, we observe in all K-means landscapes that the
rates correlate largely with clustering similarity, so global optimisation algorithms
are less likely to locate the global minimum from random starting points. The
opposite is true of the standardisations applied to the datasets containing outliers.
The cost function topography appears to have limited effect on the accuracy
of minima. The accuracy changes abruptly with cost function, due to different
partitionings becoming available, but with no corresponding change in barriers to
the global minimum. The Z2 and Z3 landscapes have significantly more kinetic
traps, but again there is little correlation between topography and accuracy. The
poor accuracy at all regions of the landscape likely contributes to this observation,
as the diversity shows a stronger correlation with cost function barriers to the global
minimum in all landscapes. Many minima in kinetic traps have significantly different
similarity to the global minimum, compared to minima of equivalent cost function
in the main funnel.
Chapter 6
Dataset structure: Overlap
In this, and the following, chapter we address two of the most relevant dataset
properties for predicting K-means accuracy: overlap and cluster distribution. It
is widely accepted that increased overlap, and a more uneven distribution of data
points, degrades accuracy. We explore these trends with reference to the cost func-
tion topographies, and highlight the deeper understanding that this perpsective
offers.
6.1 Datasets
To probe the relationship between cost function topography and overlap we map
K-means landscapes for datasets of varying overlap. The chosen examples are varia-
tions on the Iris and glass identification datasets, prepared as described in Sec. 2.1.3.
Landscapes are constructed for the Iris dataset with both three and six clusters.
The datasets with increased overlap are referred to as aggregation (or Agg) and
with decreased overlap as separation (or Sep). The numerical label gives the extent
of aggregation or separation, for example the aggregation 2 dataset has a greater
overlap than aggregation 1.
6.2 Clustering behaviour
As in Chapter 3 a partitioning refers to a particular arrangement of cluster centres
relative to the underlying clusters of the dataset. As the overlap increases between a
pair of clusters the cost function difference between assigning them to one or two K-
means clusters decreases, as illustrated in Fig. 6.1. Therefore, with increased overlap,
it may be favourable to represent overlapping ground truth clusters with fewer K-
means clusters, and use the spare K-means clusters to split another underlying
Dataset structure: Overlap
(a) (b)
(c) (d)
Figure 6.1: Illustration of the effect of overlap on cluster partitions. (a) and (b) show
alternative partitions of the same dataset; the three clusters are denoted by red crosses.
(c) and (d) show alternative partitions of a related dataset in which the overlap of the
two lower clusters increased. The overlap of these two lower clusters allows them to be
better represented using only a single cluster. Therefore, the greater overlap in the second
dataset means that (c) and (d) will be more similar in cost function than clusterings (a)
and (b).
cluster. The same concept applies to more than two clusters overlapping.
The same partitionings that are available to the well-separated dataset remain
available as the overlap increases, but the cost function ranking of these partitions
may change. Moreover, the cost function difference between them will decrease. The
data point distribution becomes more uniform, and in the limit of complete overlap,
the data resembles a single cluster, and can be well represented as such. In such a
dataset, structure is removed and partitions become unclear and equivalent in cost
function.
Dataset structure: Overlap
(a) Separation 2
(b) Original
(c) Aggregation 1
(d) Aggregation 2
Figure 6.2: Disconnectivity graphs for K-means landscapes for variations of the Iris
dataset with different overlap, constructed using three clusters. The scale bar represents
the same cost function range in all disconnectivity graphs. Minima are coloured according
to the ARI difference between their cluster labels and those of the global minimum of their
landscape. The colour range remains the same in all plots.
6.3 Landscape structure
The topographies of the clustering solution landscapes, at varying degrees of over-
lap, are visualised using disconnectivity graphs. The landscapes exhibit the expected
reduction in cost function range for datasets with a greater degree of overlap. Fur-
themore, the distribution of minima over the cost function range becomes smoother
due to the reduced cost function difference between partitionings. Consequently, we
observe a decreasing cost function gap between the global minimum and alternative
structures, and an increased number of minima close to the global minimum.
The solution landscapes also become increasingly funnelled as the overlap in-
creases. The number of kinetic traps decreases because the regions of low data
point density between underlying clusters decrease in size. Therefore, the reduced
distance scale ensures that paths between different partitionings, which give rise to
kinetic traps, do not experience such large increases in the K-means cost function.
Kinetic traps decrease in depth, causing some to entirely fall within the central
funnel. Fewer of these changes are observed in the glass datasets, as the overlap is
profound even for the most separated dataset.
The Iris landscapes with K = 3 illustrate the change in kinetic traps very clearly,
as shown in Fig. 6.2. These simple landscapes have a single kinetic trap at high
values of the cost function for the original dataset. The depth of this kinetic trap
decreases as the overlap increases, and the trap is removed by the first aggregated
dataset.
For all these Iris landscapes the global minimum retains the same partitioning
as the original dataset, as shown in Fig. 6.3. The separation of the clusters allows a
Dataset structure: Overlap
(a) Sep 2 (b) Original (c) Agg 1
(d) Agg 2
Figure 6.3: Global minima for the Iris landscapes of different overlap, with K = 3. Minima
are visualised using three of the four features. Petal width is excluded as it contains the
least information.
clearer data point assignment, and the overlap causes movement of the boundaries
between underlying clusters. However, the partitional structure remain unchanged
in each case.
Landscapes for the Iris dataset, but with additional clusters (K = 6), exhibit
similar trends. Again, the KMLs, shown in Fig. 6.4, have a reduction in the number
and size of kinetic traps. The significant number of kinetic traps in the separated
landscapes are not present in the aggregation 2 landscape. Therefore, additional
clusters do not change the underlying trend for fewer kinetic traps with increasing
overlap.
The character of the kinetic traps is similar in all the Iris landscapes as well.
These traps arise due to sets of minima differing in partition from the global min-
imum, as can be seen in the disconnectivity graphs coloured by partitioning in
Figs. C.1 and C.2. The partition of the global minimum itself varies in the landscapes
with six clusters. The global minimum of each clustering is shown in Fig. 6.5, and we
see a change in partitioning between the separation 1 and original landscapes. The
overlap in the Virginica and Versicolor clusters is sufficient at the original dataset
to allow this region to be represented equally well using four clusters.
Increasing overlap removes landscape structure in the Iris datasets, so it is not
unexpected that the landscapes for the glass dataset, with significant overlap, have
Dataset structure: Overlap
(a) Separation 2 4.86 (b) Separation 1 5.07 (c) Original
5.05 (d) Aggregation 1 5.05 (e) Aggregation 2
Figure 6.4: Disconnectivity graphs for K-means landscsapes for the Iris with six clusters.
The colouring reflects the ARI between the cluster labels of each minimum and those of
the global minimum. The colour range extends from 0 to 1 in all plots, and the scale bar
represents the same range.
Dataset structure: Overlap
(a) Sep 2 (b) Sep 1 (c) Original
(d) Agg 1 (e) Agg 2
Figure 6.5: Global minima of the Iris landscapes with K = 6. Minima are shown in only
three features, with the petal width excluded.
few features. However, the same trends are observed. There are still small kinetic
traps at low values of the cost function, as seen in Fig. 6.6, and one of these two
kinetic traps decreases in size and is no longer present in the aggregated datasets.
Additionally, the gap between the global minimum and the alternative minima de-
creases as the overlap increases.
6.3.1 Kinetic traps
The reduction in the kinetic trap depth is clear in the disconnectivity graphs. How-
ever, these graphs only reflect the minimum barrier between minima and the global
minimum. The transition rate also depends on the path length and intermediate
barriers, as addressed in previous chapters.
The escape rates from the single kinetic trap to the global minimum in the Iris
landscapes with three clusters are given in Table 6.1. There is a large increase in the
rate as the overlap increases. The reduction in maximum barrier height correlates
strongly with increased escape rate for these landscapes with a small number of
minima.
The equivalent rates for the Iris dataset with six clusters are given in Table 6.2.
We consider two kinetic traps in each landscape, as shown in Fig. C.3. We attempt
to keep the kinetic traps as similar as possible between landscapes.
The escape rates from the kinetic traps also increase with greater overlap for the
Dataset structure: Overlap
(a) Separation 2 5.07 (b) Separation 1 5.05 (c) Original
5.05 (d) Aggregation 1 5.07 (e) Aggregation 2
Figure 6.6: KMLs for the glass datasets with K = 6 depicted using disconnectivity graphs.
The minima are coloured according to their similarity to the global minimum clustering,
which is quantified using the ARI difference. The colour range is constrained to be the
same in all plots, and the scale bar represents the same range too.
Table 6.1: Transition rates between the lowest minima in the kinetic trap and the global
minimum for selected Iris K = 3 datasets. The rates are all calculated at the same
fictituous temperature, T = 1.0.
Dataset Rate
Separation 2 2.03 105
Original 2.53 103
Aggregation 1 5.12 102
Dataset structure: Overlap
Table 6.2: Transition rates between kinetic traps and the global minimum, for the Iris
landscape with K = 6. The rates are calculated at a fictituous temperature of T = 3.0.
Similar kinetic traps are selected for all the landscapes, and we show the chosen kinetic
traps in Fig. C.3.
Dataset KT 1 KT 2
Separation 2 3.70 105
Separation 1 1.74 104 3.14 104
Original 5.05 104 1.17 104
Aggregation 1 5.56 104 1.44 104
Aggregation 2 5.60 104 8.28 104
Iris dataset with an increased number of clusters. Escape from kinetic trap 2 of
the separation 1 landscape is the only transition that does not exactly follow this
trend. This transition differs in partition from the remaining three, due to a change
in global minimum partition. Therefore, the character of the kinetic trap will differ,
and the rate is not expected to closely follow the same trend.
The cost function profiles of these pathways, displayed in Fig. 6.7, clearly show
that in all landscapes the largest barriers tend to be associated with reaching the
maximum transition state analogue, which is significantly higher in cost function
than the kinetic trap minimum. Therefore, the decreasing kinetic trap depth seen
in the disconnectivity graphs provides the greatest driving force for the faster rates.
This reduction in height is accompanied by shorter pathways, and a small reduction
in the number of intermediate barriers after overcoming the highest transition state,
which tends to be early in most pathways.
The stationary points on the pathways are visualised for comparison of the tran-
sition between kinetic trap 2 and the global minimum in the separation 1 (Fig. C.4),
original (Fig. C.5), aggregation 1 (Fig. C.6) and aggregation 2 (Fig. C.7) landscapes.
For visualisation, one of the features of the Iris dataset is omitted, namely the petal
width as it contains the least information. However, it is worth noting that informa-
tion is still contained in the final dimension and the visualised pathways may reflect
that.
For the separation 1 dataset there are many small changes needed to bring the
cluster centres into a position where the partition can change through transfer of
the single nearest cluster. The separated datasets have a much larger range of petal
width and, consequently, the pathways are not so clearly represented using the
remaining three features. The pathways look very similar for all the other datasets,
albeit reversed due to the different global minimum partition. Again, there are
small changes in cluster boundaries that bring the clustering into a position where
exchange of partitions is possible. It is clear why the overlap aids this transition,
Dataset structure: Overlap
(a) Separation 2 (b) Separation 1 (c) Original
(d) Aggregation 1 (e) Aggregation 2
Figure 6.7: Pathway profiles for the fastest paths between kinetic traps and global minima
in the Iris (K = 6) landscapes of varying overlap. The fastest paths are calculated for the
same transitions as in Table 6.2. Crosses placed on transition states indicate a different
partition for its two connected minima.
namely the increased density of points, and smaller distance, in the region between
underlying clusters allows changes in partition to proceed with smaller cost function
barriers. The reduction in barriers for the partition change reduces the kinetic trap
depth, and increases the transition rate.
The faster escape rates, and the reduced number of kinetic traps, both suggest
that more efficient global optimisation should be possible as the overlap of a dataset
increases. Another important landscape feature for global optimisation are the con-
nections between distinct regions of clustering space. As seen in Chapter 4, there
can be great kinetic diversity, even for transitions within the main funnel. There-
fore, we perform a similar analysis and calculate the rates from all minima of each
partition to the global minimum. The rates reveal the difficulty in traversing regions
of space that, although they have a small maximum barrier, can vary in path length
and intermediate barrier heights.
The rates are presented in Table 6.3. We see the same trend as for transitions
from kinetic traps. Not only are the traps easier to escape, but it is possible to
move between different regions of the landscape more easily when the overlap is
greater. The separation 2 landscape is excluded from this analysis, as the clustering
solutions in the connected set all have the same partitioning. Solutions of other
partitionings exist, but they are difficult to connect via transition states in this
Dataset structure: Overlap
Table 6.3: Transition rates between all minima of a given partition and the global min-
imum. Rates are calculated on the KMLs of Iris datasets with varying overlap and six
clusters. Dots indicate the global minimum partitioning for a given landscape. The rates
were all calculated at a fictituous temperature of T = 3.0.
Dataset 1 2 3 4
Separation 1 . 8.35 106 1.68 105 3.81 106
Original 1.22 104 . 3.86 105 3.69 105
Aggregation 1 4.22 105 . 2.31 104 1.26 104
Aggregation 2 4.67 104 . 8.30 103 2.57 104
framework, due to the large separation between clusters. In future work, we aim
to adapt the methodology to locate transition states between different partitionings
more easily for datasets with highly separated clusters.
Pathway profiles, Fig. 6.8, show that transitions to the global minimum from
adjacent partitionings shorten as the overlap increases. These shorter paths also
contain fewer significant intermediate barriers. Pathway contraction and smaller
barriers were also observed, to a lesser extent, in the kinetic trap pathways. There-
fore, these features may allow easier exploration of the cost function space. However,
it must be noted that path length and intermediate barriers play a much larger role
in the rate increase for these pathways than those of kinetic traps. In kinetic trap
pathways the reduction in the relative height of the highest transition state is the
principal reason for faster rates, and in the partition pathways the maximum barrier
is very small relative to the endpoint minimum for all overlap conditions.
The same trend of increasing rate of escape from the kinetic traps is seen for the
glass datasets in Table 6.4. Kinetic traps were here selected as those that had the
most similar topography in all landscapes. The chosen kinetic traps are highlighted
in Fig. C.8. As with the Iris landscapes we see an increase in rate as the overlap
increases, even for these datasets that already have significant overlap. There is
a small increase in rates within the separation 1 landscape, which illustrates that
specific effects can still outweigh the general trend, but the relationship remains
remarkably conserved across all the landscapes.
The pathway profiles are shown in Fig. 6.9. As with the other datasets, the
height of the maximum transition state is reduced, relative to the higher endpoint
minimum, with increased overlap. The reduction in the maximum cost function
height clearly provides the largest driving force here, as trends in pathway length
and intermediate barriers are not clear for these landscapes.
The changes in the topographies, pathways and rates for the three sets of land-
scapes studied here suggest that cost function surfaces become easier to sample as
dataset overlap increases. The landscapes appear more funnelled, and escaping ki-
Dataset structure: Overlap
(a) Sep 1 (b) Original (c) Agg 1
(d) Agg 2
Figure 6.8: Pathway profiles for the fastest path between all minima of a given partition
and the global minimum in the Iris (K = 6) landscapes. Partition changes are indicated
by crosses on the transition states between differing minima.
Table 6.4: Escape rates from kinetic traps on the glass dataset landscapes. The rates are
calculated as a sum over all pathways at the fictituous temperature T = 3.0. Kinetic traps
are selected to be the same in all landscapes, and they are shown in Fig. C.8.
Dataset KT 1 KT 2
Separation 2 2.00 105 3.14 106
Separation 1 1.59 104 3.30 104
Original 1.77 105 8.25 106
Aggregation 1 1.26 105
Aggregation 2 8.62 105
Dataset structure: Overlap
(a) Separation 2 (b) Separation 1 (c) Original
(d) Aggregation 1 (e) Aggregation 2
Figure 6.9: Pathway profiles for fastest paths between kinetic traps and the global minima
in the glass landscapes with different overlap.
netic traps to reach the global minimum becomes easier. Furthermore, we see for
the Iris dataset with K = 6 that we can also traverse the solution space between
different regions of the landscape more easily. Therefore, we expect that suitable
global optimisation algorithms will locate the best K-means solutions more easily
as the overlap increases.
The changing properties of the landscapes arise from systematic alterations to
the pathway length and barrier heights, while the number of partitions remains the
same. Therefore, initialisation schemes will have to sample the same number of
distinct partitions, and initialisation does not necessarily become more successful as
the overlap increases. However, the increased density of minima close to the global
minimum, and the reduced distances, allow better solutions to be reached in terms
of the cost function, even if these are not structurally similar to the global minimum.
6.3.2 Minima distribution
We briefly discussed the change in distribution of minima over the cost function
range in the previous section, with reference to disconnectivity graphs. It can be
difficult to visualise the number of minima in a given cost function range, so in
this section we evaluate the distribution of minima using the thermodynamic heat
capacity, as described in Sec. 2.4.1.
The variation in the vibrational frequencies over all minima of the Iris and glass
Dataset structure: Overlap
Figure 6.10: Heat capacity profiles for the Iris solution landscapes with K = 3. The
fictituous temperature, which is related to the cost function, is chosen for a range that
includes the important heat capacity features of all landscapes.
K-means landscapes are given in Table C.1. The range for the Iris dataset with three
clusters is very small. All minima contribute almost equally to the heat capacity,
which strongly reflects the density of minima.
The range of vibrational frequencies increases for both the Iris and glass datasets
with six clusters. The changes in cluster populations now exert a larger effect on the
vibrational frequencies, but the ranges still remain relatively small. The distribution
of minima is likely to remain the driving force for many heat capacity features, and
it is unlikely that a single minimum will drive a melting transition in contrast to
other machine learning landscapes.129
The CV curves calculated from the Iris landscapes with three clusters are shown
in Fig. 6.10. The profiles all contain a single broad peak, which arises from a large
increase in the number of minima at higher values of the cost function. The peak
moves to lower temperature as the overlap increases because the cost function gap
between the global minimum and the alternative high-valued partitionings decreases.
These heat capacity peaks can be considered the analogue of a melting transition in
physical systems, with a large number of higher lying minima providing an entropic
gain.
The distributions for the Iris dataset with six clusters are shown in Fig. 6.11.
They contain many more minima, and greater structure, so the heat capacity curves
contain more features than for K = 3. The first sharp peaks occur at low temper-
ature, with a smaller secondary peak at higher temperature. For the separation 2
landscape there is a further large peak at significantly higher temperature. This large
peak appears similar to the melting peaks of the Iris landscapes with three clusters;
a large number of minima become available at high values of the cost function, with
Dataset structure: Overlap
(a) (b)
Figure 6.11: Heat capacity profiles for the Iris solution landscapes with six clusters. (a)
shows the full temperature range of interest, and (b) shows a restricted temperature range
to make the low-temperature peaks more easily comparable.
no minima at intermediate values. The other landscapes have a wider distribution
of minima over the cost function surface and, consequently, do not contain a large
melting peak.
The distribution of minima in cost function reveal that the low-temperature heat
capacity peaks are also largely driven by landscape entropy, as seen in the minima
distribution given in Fig. C.9. However, it is worth noting that not all the heat ca-
pacity peaks can be wholly described in these terms. These low-temperature peaks
arise mainly from a sharp increase in the number of available minima, which largely
correspond to a different partitioning to the global minimum. These transitions do
not resemble the melting transitions in physical systems, where population is trans-
ferred to many unstructured minima. Instead, there is more localised transfer of
population from the global minimum ensemble to another clearly-defined morphol-
ogy, which more closely resembles the low-temperature solid-solid phase transitions
seen for molecular clusters. Such transitions are signatures of multifunnelled land-
scapes.
For the glass dataset we see similar peaks to the Iris landscapes with K =
6, Fig. 6.12. These peaks can again be largely explained using the distribution
of minima in terms of the cost function, Fig. C.10. We see large local peaks in
the distribution of minima that provide the entropic drive for the different melting
transitions. However, in these cases we cannot account for the morphology changes
as clearly as for the Iris dataset as we do not have a simple partition labelling
scheme. We could speculate that the low-temperature peak is associated with a solid-
solid phase transition, where the available minima correspond to clearly-localised
Dataset structure: Overlap
(a) (b)
Figure 6.12: Heat capacity profiles calculated from the database of minima in the glass
landscapes. Plot (a) displays profiles over a wide temperature range, (b) employs a reduced
temperature range to emphasise the behaviour in the low-temperature peaks.
morphologies. The higher temperature peaks would then be associated with melting
peaks, driven by a sharp increase in minima from many different partitionings.
6.4 Distribution of properties
One facet of K-means clustering is the surface topography, which determines the
difficulty in obtaining the global minimum clustering. However, equally important
is the distribution of cluster properties over the landscapes. We consider two prop-
erties here, both of which measure the similarity between clusterings and a chosen
reference. The difference in cluster labels is quantified using the ARI in all cases.
The accuracy of clustering solutions is measured with reference to the ground truth
clustering. Diversity is measured by the ARI between the cluster labels of a given
minimum and the global minimum of the landscape. Both these properties play a
key role in determining to the quality of K-means clustering that can be attained.
The accuracy of low-valued K-means solutions is expected to be significantly
reduced as overlap increases, due to the inability to separate the data points within
the K-means cost function. Moreover, the correlation between the depth of the
minimum and its accuracy will be degraded. As described above, it becomes more
favourable to represent overlapping underlying clusters with a reduced number of K-
means clusters. Such solutions can then be low in cost function, but they will display
significantly lower accuracy, as the underlying structure is not well represented.
For the Iris dataset with three clusters the depth in the landscape does correlate
with accuracy in all cases. However, the expected behaviour is seen for the more
Dataset structure: Overlap
(a) Accuracy (b) Diversity
Figure 6.13: Minima properties over the cost function range for the Iris landscapes with
K = 3. Accuracy is measured as the ARI between each clustering and the ground truth
labels. Diversity instead measures the difference between each clustering and the global
minimum of the landscape.
Table 6.5: Accuracy of the global minimum clustering solution for Iris datasets with
varying overlap, calculated using three clusters.
Dataset Best accuracy
Separation 2 1.000
Original 0.730
Aggregation 1 0.681
Aggregation 2 0.582
complex glass datasets, and the Iris datasets with K = 6. The correlation between
accuracy and cost function is not so clear in these cases, and becomes less structured
as the overlap increases. The overlay of diversity properties onto the cost function
topographies was given in the disconnectivity graphs of Sec. 6.2. The overlay of
accuracies is shown for the Iris dataset in Fig. C.11, and for the glass dataset in
Fig. C.12.
The relation is clear for the Iris dataset withK = 3, as seen in Fig. 6.13. The cost
function is appropriate to describe the clusters for all overlaps and, consequently,
the accuracy decreases as the cost function increases. As the overlap increases the
accuracy difference between the global minimum and alternative solutions decreases,
but the solution diversity remains the same. The minima at higher values of the
cost function remain structurally distinct from the global minimum.
As expected, the global minimum, which is the most accurate solution in all
cases, decreases in accuracy as the overlap increases. In Table 6.5 we see a large
drop in accuracy as the dataset overlap increases. When the datasets are fully
separated we see perfect recall of the ground truth cluster labels.
Dataset structure: Overlap
Figure 6.14: Properties of the K-means minima over the cost function space for the Iris
datasets with K = 6. The diversity is given for each landscape, which is quantified as
the ARI difference between the labels of each clustering and the global minimum of the
corresponding landscape. J is the cost function.
For the Iris datasets withK = 6 we only calculate the diversity, as we do not have
the ground truth number of clusters. We observe a broadly decreasing trend in the
similarity to the global minimum as the cost function increases for the two separated
datasets, as shown in Fig. 6.14. However, even for these two datasets with the
strongest correlation, this is not a simple linear relationship. The diversity is strongly
correlated with the partitioning of minima, relative to the global minimum, and these
distributions exhibit localised regions in the ARI-cost function space corresponding
to minima with different partitionings. Due to the well-separated clusters in these
datasets, the regions corresponding to different partitionings generally increase in
cost function as they differ more from the global minimum.
In all landscapes there can be large diversity ranges, at similar values of the cost
function, due to kinetic traps. The minima in kinetic traps can have markedly dif-
ferent properties from those minima of equivalent cost function in the main funnel.
However, as the overlap increases we see a sharp increase in the diversity ranges pos-
sible at a given cost function value, which cannot be accounted for by kinetic traps,
as these decrease in number and size. The clearly-defined regions corresponding to
different partitions are lost because the cost function difference between partition-
ings decreases, until they overlap significantly. Many partitionings are available at
each cost function value in the funnel, so although the diversity range is similar at
all overlap values, the diversity range available at a given cost function increases
Dataset structure: Overlap
Table 6.6: Clustering accuracy for K-means minima of the glass dataset. The best accu-
racy gives the highest ARI between the ground truth labels and any located clustering.
The accuracy of the global minimum clustering is also given, and the cost function differ-
ence between the global minimum and most accurate solution.
Dataset Best accuracy J Global minimum accuracy
Separation 2 0.494 102.6 0.336
Separation 1 0.332 100.2 0.311
Original 0.313 126.8 0.255
Aggregation 1 0.298 48.2 0.258
Aggregation 2 0.293 41.9 0.236
with overlap. However, even for landscapes with large overlap the global minimum
clustering is unique. There are no clusterings at higher values of the cost function
that are similar, and low values of the cost function must be reached to ensure the
best clustering structure is observed.
The relationships are even more complex for the glass lanscapes due to the
significant overlap even for the separated datasets. The best accuracy that can be
attained by K-means decreases for the glass dataset with increased overlap. Despite
the large overlap for even the separated datasets the trend of reduced clustering
quality can still be discerned. However, the relationship between cost function and
accuracy within each landscape is not clear. In all cases the global minimum is
significantly less accurate than the most accurate clustering solution, and the cost
function gap to the most accurate solution can be large, as seen in Table 6.6. The
global minimum accuracy also decreases as the overlap increases.
Although the clustering quality decreases with increasing overlap, the accuracy
and cost function become more strongly correlated, as shown in Fig. 6.15. As over-
lap increases there is a stronger relationship between high cost function and poor
accuracy, and the peak in accuracy moves closer to the global minimum cost func-
tion. Therefore, the cost function becomes a more useful surrogate for the clustering
accuracy. However, the accuracy of all solutions remains lower.
There are large fluctuations in the diversity of the low-valued ensemble. The
distributions of diversity and cost function exhibit a loss of structure, similar to
the Iris dataset. In the separated dataset there is no such clear structure due to
the significant overlap, but again we see an increase in the diversity band width at
each value of the cost function. These bands indicate the loss of clear partitionings,
and their separation in cost function. For both accuracy and diversity there is little
observable correlation between landscape topography and minima properties. The
significant dataset overlap causes little structure in all landscapes.
It is possible to calculate aggregate values from the diversity distributions, as
Dataset structure: Overlap
(a) Accuracy (b) Diversity
Figure 6.15: Distributions of minima over the clustering space for the glass datasets, as
measured by the ARI to both the ground truth labels (accuracy) and the global minimum
of each landscape (diversity). J is the cost function value.
Table 6.7: Diversity of the ensemble of clustering solutions for the glass dataset. Similarity
of each minimum is measured as the ARI relative to the global minimum clustering of the
corresponding landscape. The lowest similarity is given for the minima within 10% of the
cost function range above the global minimum. The high similarity percentage gives the
percentage of minima above the 10% cost function threshold that have an ARI > 0.9.
Dataset Lowest similarity High similarity percentage
Separation 2 0.598 21.3
Separation 1 0.623 37.0
Original 0.738 20.3
Aggregation 1 0.874 15.9
Aggregation 2 0.500 13.4
shown in Table 6.7. For all glass landscapes we see many minima with little similarity
to the global minimum become available at low values of the cost function. The
lowest similarity to the global minimum for minima within the lowest 10% of the
cost function range can reach values of around 0.5, which are significantly different
clusterings. In all cases the similarity drops to 0.5 within a relatively small cost
function range, but the lowest within the 10% range shows how dissimilar solutions
become available. As the overlap increases it appears that the minima in the lowest
10% retain a higher similarity to the global minimum, but the similarity drops
sharply in the aggregation 2 landscape, reversing this trend.
There are a large proportion of clustering solutions at higher cost function,
namely those above the 10% cost function cutoff used previously, which have a high
similarity to the global minimum (ARI > 0.9). Therefore, in all of these significantly
overlapping datasets, the K-means solutions that resemble the global minimum can
extend to high energy, ensuring that the global minimum partition may be reached
Dataset structure: Overlap
Table 6.8: Rates for transitions between the most accurate clustering in each glass dataset
and the global minimum. Rates are calculated at a fictituous temperature of T = 3.0.
Dataset Rate
Separation 2 2.85 104
Separation 1 5.22 104
Original 1.39 102
Aggregation 1 1.50 103
Aggregation 2 1.16 104
even without attaining low values of the cost function. The proportion of high-
valued minima similar to the global minimum generally decreases as the overlap
increases. Therefore, there is a smaller chance of reaching an accurate solution at
high cost function, as dataset overlap increases.
6.4.1 Rates
We perform a similar rate analysis as performed for the Iris dataset in finding path-
ways between partitionings for the glass datasets. However, without such a simple
interpretation of partitionings we calculate rates with reference to both accuracy
and diversity. We first consider pathways to the global minimum from the most ac-
curate solution in each landscape, and those similar in accuracy and cost function.
The rates for different datasets are given in Table 6.8.
The rates do not change in a systematic way with overlap, but they do closely
match the diversity values of Table 6.7. The pathway profiles, shown in Fig. 6.16,
contain small barriers for all landscapes. Therefore, the difference in cost function
between the global minimum and most accurate clustering has little effect on the
rates. Instead, the rate changes arise largely due to the path lengths for the specific
transitions. Longer pathways require more intermediate minima because the cluster
labels differ by a greater amount.
We also calculate rates to the global minimum from those minima that are most
similar, above the lowest 10% of the cost function range, and least similar, below 10%
of the cost function range. These rates are given in Table 6.9 and the pathways in
Fig. 6.17. The rates to the least similar minimum vary over four orders of magnitude
with no observable trend. The diversity in rates arises from a pathway diversity,
with increased overlap providing no systematic changes in barrier heights and path
length.
The pathways for high-valued but similar clusterings show much greater homo-
geneity in terms of rates and pathways. The pathways are very short at all overlap
values. Moreover, the uphill barriers, for progression to the global minimum, are
Dataset structure: Overlap
Figure 6.16: Profiles for the fastest pathways from the most accurate clustering to the
global minimum within each glass landscape.
Table 6.9: Rates for transitions between minima in the landscapes for the glass datasets.
Rates are calculated to the global minimum from the least similar clusterings within the
lowest 10% of the cost function range, and the most similar clusterings above the lowest
10% of the cost function range. Rates are calculated at a fictituous temperature of T = 3.0.
Dataset Least similar rate Most similar rate
Separation 2 1.09 101 2.33 102
Separation 1 2.95 104 5.20 102
Original 1.06 102 2.21 103
Aggregation 1 1.56 105 3.05 103
Aggregation 2 1.04 104 1.85 102
(a) Least similar (b) Most similar
Figure 6.17: Cost function profiles for the fastest pathways between the global minimum
and the most diverse and most similar clustering solutions. The least similar minima are
those with the lowest ARI relative to the global minimum of the landscape, that lie within
the lowest 10% of the cost function range. The most similar minima are those above
the lowest 10% of the cost function range that have the highest similarity to the global
minimum clustering.
Dataset structure: Overlap
small in all cases, ensuring that the height of the cost function does not signficantly
change the rate. The large cost function range is spanned in one or two transition
states by long steepest-descent pathways to the lower of the two connected minima.
It is interesting to observe that clusterings different in cost function are not neces-
sarily far apart in terms of sampling space. Diversity is the strongest determinant
of fast transitions in the overlapping glass datasets.
In conclusion, the distribution of accuracy and diversity change significantly
with overlap. For the simple Iris dataset, with the ground truth number of clusters
(K = 3) and limited overlap, the accuracy still correlates well with the cost function
in all landscapes. We observe a smaller accuracy difference between the global
minimum and alternative structures as the overlap increases. However, for the more
overlapping glass datasets the most accurate K-means solutions are at high values of
the cost function, meaning that the global minimum is not so relevant for accurate
clustering. In both these sets of landscapes we observe the expected decrease in
accuracy with increasing overlap.
We observe similar diversity changes for both the Iris (K = 6) and glass datasets
with overlap. For datasets with little overlap the diversity can be clearly understood
in terms of partitionings. These different partitionings can give rise to kinetic traps
in datasets with small overlap, so the diversity distribution depends strongly upon
the landscape topography. However, as the overlap increases the different partition-
ings all fall within a single funnel, and diversity becomes independent of landscape
structure. The increased dataset overlap causes the cost function ranges of many
partitionings to overlap, which gives larger diversity ranges at small windows of the
cost function. Therefore, for overlapping datasets it becomes clear that multiple
minima need to be evaluated: for the same cost function values the clusterings can
vary widely in structure relative to the global minimum.
Chapter 7
Dataset structure: Cluster
distribution
The distribution of data points amongst the underlying clusters has, along with
overlap, proved to be one of the most relevant dataset properties in predicting K-
means clustering accuracy.48,77 Uneven cluster distributions tend to limit K-means
accuracy. Small clusters are given less weighting in the cost function and, therefore,
it is possible for K-means to place a cluster centre far from the true mean. Here, we
systematically address the effect of changing cluster distribution on both accuracy
and cost function topography.
7.1 Datasets
In this chapter we construct landscapes for both the Iris and glass datasets with
only the ground truth number of clusters. The correct number of clusters allows
us to evaluate the clustering accuracy in all cases. We prepare variants of these
original datasets by modifying the cluster populations, whilst retaining the same
total number of data points, as described in Sec. 2.1.4. We consider eight alternative
datasets for both the Iris, and six for the glass datasets.
7.2 Iris
7.2.1 Landscape structure
We visualise all the K-means landscapes using disconnectivity graphs in this chap-
ter. The cluster distribution can significantly modify the cost function surfaces, and
we see systematic changes in the topographies. The increase in population of the
Dataset structure: Cluster distribution
(a) Original
(b) 1
(c) 2
(d) 3
Figure 7.1: Disconnectivity graphs for the Iris K-means landscapes. The landscapes are
constructed from the original dataset containing evenly distributed clusters, and datasets
13, which have an increased proportion of minima in one overlapping cluster produced by
removing density equally from the other clusters. Minima are coloured according to the
accuracy, as measured using the ARI between ground truth labels and the cluster labels
of each minimum. The colour bar and scale bar represent the same ranges in all plots.
overlapping Versicolor cluster, by evenly reducing the population of the other clus-
ters, leads to the removal of the kinetic trap from the original solution landscape,
as seen in Fig. 7.1. After the first change in population the kinetic trap is no longer
present, and successive further changes lead to a decrease of the cost function gap
between the global minimum and these alternative clusterings.
Alternatively, by increasing the population of the non-overlapping cluster, we
deepen the kinetic trap, as shown in Fig. 7.2. As we add an increasing number of
points to the non-overlapping Setosa cluster, the cost function gap between these
alternative kinetic trap minima and the global minimum decreases. Therefore, these
alternative clusterings become more competitive with the global minimum. However,
they simultaneously become separated from the global minimum by larger barriers.
The effect is similar for the datasets with increased population in one overlapping
cluster, with population reduction in the other overlapping cluster. Again, the
kinetic trap increases in depth, and the energy relative to the global minimum
decreases, as seen in Fig. 7.3
In all datasets the same partitioning remains the global minimum. The minima
are displayed in Fig. 7.4. The cluster boundaries shift as a result of changing cluster
populations, but the overall partitioning remains the same. Furthermore, the kinetic
traps all correspond to an alternative partitioning of the underlying clusters, as
shown in Fig. D.1. Therefore, these changes in cluster populations do not alter
the relative cost function ranking of the partitionings from the original landscape.
However, we do see significant changes in cost function differences between these
partitionings, and their associated barriers.
Dataset structure: Cluster distribution
(a) 4 5.07 (b) 5 5.05 (c) 6
Figure 7.2: KMLs constructed from the 46 Iris datasets, with cluster populations given
in Table 2.4. From dataset 4 to 6 we add more population to the non-overlapping cluster.
Minima in the disconnectivity graphs are coloured according to the accuracy, and the same
colour scale is retained in all plots. The scale bar represents the same cost function range
in all cases.
(a) 7 5.07 (b) 8
Figure 7.3: Disconnectivity graphs showing the KMLs for Iris datasets 7 and 8. These
datasets have increased population in one of the two overlapping clusters, compensated for
by decrease in the other cluster. Minima are coloured according to the clustering accuracy,
and the colour box and scale bar represent the same range in both plots.
Dataset structure: Cluster distribution
(a) Original (b) 1 (c) 2 (d) 3
(e) 4 (f) 5 (g) 6
(h) 7 (i) 8
Figure 7.4: The global minimum K-means clustering for each Iris dataset with different
cluster populations. The minima are all visualised in the same three features, excluding
petal width.
Dataset structure: Cluster distribution
Table 7.1: Transition rates from the lowest minima of the kinetic trap to the global
minimum in the Iris landscapes constructed from datasets of varied cluster population.
Rates are calculated at the fictituous temperature T = 1.0.
Dataset Rate
Original 4.6 104
4 2.3 103
5 4.6 104
6 8.1 105
7 2.6 104
8 4.1 104
7.2.2 Rates
We quantify the ease of escaping the single kinetic trap by calculating rates for
transitions to the global minimum (Table 7.1). The rates appear to be largely
dependent upon the barrier separating the kinetic trap from the global minimum.
From datasets 4 6 we see a decrease in rate and increase in barrier height, and the
same trend applies for datasets 7  8. The pathway profiles, as shown in Fig. 7.5,
show that all pathways are very short and contain no significant intermediate barri-
ers. Therefore, the transition rate depends largely upon the barrier for arranging the
cluster centres to change to the global minimum partitioning, which is associated
with a significant downhill barrier in all cases.
Visualisations of the stationary points on each pathway are given in Figs. D.2,
D.3, D.4 and D.5, for the original, 4, 5 and 6 datasets, respectively. The pathways
appear very similar in structure. The cluster spacing and intermediate data point
density remains similar in all datasets, so the size of the uphill barrier is largely
associated with the number of data points in the Setosa cluster. A larger proportion
of data points within the Setosa cluster produces a greater increase in cost function
for displacing the kinetic trap clusterings from their mean. There are obviously
other considerations, such as the number of distinct pathways, and the structure of
the Versicolor/Virginica cluster, but the largest changes seem to arise from Setosa
population.
The barrier height distributions, as presented in Fig. D.6, provide evidence that
all pathways to the global minimum, not just those from kinetic traps, have small
barriers in all the landscapes. The barriers are calculated for individual transitions
from minimum to minimum, via a single transition state. The barriers between tran-
sition states and higher connected minima are very strongly peaked at small barrier
heights in all the landscapes. Therefore, barriers on pathways to the global minimum
all have small individual barriers associated with each downhill transition between
Dataset structure: Cluster distribution
Figure 7.5: Cost function profiles for the stationary points on the fastest from kinetic trap
to global minimum for the Iris datasets of varying cluster overlap.
minima. The lower barriers are also strongly peaked about zero, indicating many
transitions can arise between similar clusterings with little cost function change.
However, the barriers to lower connected minima also have a significant proportion
of large values in all landscapes, which correspond to paths between partitionings,
as seen in the kinetic trap escape pathways. There is little clear distinction between
the distributions for different landscapes, indicating that the cluster populations
do not significantly change the transition state character. The distance between
transition states and connected minima, Fig. D.8, closely mirrors the barrier height
distribution, due to the simple relationship between distance and barrier height for
the K-means cost function. The cluster label difference between connected minima
and transition states follows a similar trend, Fig. D.7, but with more variation in the
cluster label difference for similar barrier heights, due to the many different paths
available for attaining a minimal clustering.
7.2.3 Accuracy
Equally important to the cost function topography is the quality of the solution
attained at the global minimum, and elsewhere in the landscape. The highest at-
tainable accuracy in each clustering landscape is given in Table 7.2. We see an
increase in the accuracy of the optimal clustering in almost all cases. The global
minima all retained the same partitioning, so the differing accuracy must arise from
changes in the cluster boundaries within the overlapping Virginica/Versicolor clus-
ters.
Dataset structure: Cluster distribution
Table 7.2: The maximum achievable K-means accuracy for the Iris datasets of different
cluster population. Accuracy is quantified as the highest ARI between a clustering and
the ground truth labels. In the case where the most accurate clustering is not the global
minimum, we report the cost function gap and global minimum accuracy.
Dataset Best accuracy J Accuracy (GM)
Original 0.730
1 0.714
2 0.782
3 0.782 3.378 0.422
4 0.802
5 0.933
6 0.968
7 0.811
8 0.883 2.278 0.641
The accuracy differences can largely be explained through the relative popu-
lations of the overlapping and non-overlapping clusters. The increase in accuracy
from datasets 4 to 6 is due to an increasing proportion of data points within the
well-separated Setosa cluster. The changes for the remaining datasets have to be
explained in terms of the overlapping Virginica/Versicolor clusters. The uneven
distribution of overlapping clusters does give a small decrease in overlap, as seen in
Table D.1, by removing some overlapping data points when reducing cluster popu-
lation. Therefore, since the cluster positions are not heavily skewed by the cluster
populations, the accuracy increases from datasets 1 to 3, and 7 to 8.
The distribution of accuracy and diversity over the whole cost function range is
plotted in Fig. 7.6. We see an increasingly weak correlation between cost function
and accuracy as the distribution becomes more uneven (1 3, 4 6, 7 8). For
both datasets 3 and 8 the global minimum is no longer the most accurate clustering;
this solution remains within the lowest band of the cost function, but the accuracy
difference can be pronounced.
We see a much larger change in the accuracy of the higher band of minima
corresponding to the kinetic trap in the landscapes with a deep kinetic trap. Minima
in a deep trap exhibit a wide range of accuracy and diversity values over a small
interval of the cost function. Diversity values overlaid on the disconnectivity graphs
are shown in Figs. D.9, D.10 and D.11.
Dataset structure: Cluster distribution
(a) Accuracy (b) Diversity
Figure 7.6: The distribution of accuracy and diversity over the cost function range for the
Iris datasets of varying cluster populations. Both the accuracy and diversity are quantified
using the ARI between cluster labels. Accuracy measures the difference between each
minimum and the ground truth labels. Diversity measures the difference between each
minimum and the global minimum clustering for the given landscape.
7.3 Glass
7.3.1 Landscape structure
The KMLs for the glass datasets with varying cluster distributions are visualised
using disconnectivity graphs. The changes in cost function topography amongst
these landscapes are more subtle than those for the Iris dataset. Kinetic traps
are largely removed for all but the original dataset, leading to strongly funnelled
landscapes for all cluster distributions.
A more even distribution of data points removes the kinetic traps from the
landscapes, as shown in Fig. 7.7. The original dataset contains two low-valued
kinetic traps, and a unique global minimum that has a clear cost function gap
to alternative clusterings. The kinetic traps decrease in depth with a more even
distribution of data points amongst clusters, and are not present in the evenly-
distributed dataset. Furthermore, the distribution of minima becomes flatter in the
low regions of the cost function, and the separation of the global minimum from
alternative clusterings is less distinct. The removal of kinetic traps should lead
to a more funnelled landscape. However, in landscapes 1 and 2 we observe much
larger barriers from each minimum within the funnel to the global minimum. This
increased frustration is likely largely artificial due to incomplete sampling of the
solution space.
The remaining changes are made relative to the evenly-distributed dataset, which
contains little landscape structure, and no additional kinetic traps appear. However,
Dataset structure: Cluster distribution
(a) Original 5.07 (b) 1 5.07 (c) 2
Figure 7.7: Disconnectivity graphs displaying the K-means landscapes of glass datasets
with different cluster populations. The original dataset is shown, in addition to two
datasets with a more even distribution of data points amongst clusters. Each minimum is
coloured according to the ARI difference from the ground truth cluster labels. The same
range is represented by the colouring and scale bar in all plots.
the redistribution of data points strongly affects the distribution of minima within
the funnel. Increasing the population within the most overlapping clusters, as shown
in Fig. 7.8, restores the separation between the global minimum and alternative
structures that were observed in the original dataset. Addition of data points to
the least overlapping clusters also produces a much more unique global minimum
compared to the original dataset, and landscapes with a much smoother distribution
of minima across the whole cost function range, as shown in Fig. 7.9.
We explicitly calculate the distribution of minima over the cost function in
Fig. 7.10 to provide extra detail that is not immediately obvious from the disconnec-
tivity graphs. We see that a more even distribution of data points amongst clusters
leads to a more even distribution of minima over the cost function. Furthermore,
the changes produce a larger proportion of minima at the lowest values of the cost
function, which was visible in the disconnectivity graphs.
Both datasets 3 and 4, in which we add data points to overlapping clusters, are
similar to those of the most even data point distribution in dataset 2. There is again
a high density of minima around the global minimum, but a much reduced density
at high values of the cost function.
The distributions for the remaining datasets, in which we have increased the
number of datapoints in the least overlapping clusters, are strongly peaked at lower
values of the cost function. There are fewer minima around the global minimum,
as can be seen in the clear separation in the disconnectivity graphs (Fig. 7.9), but
Dataset structure: Cluster distribution
(a) 3 5.07 (b) 4
Figure 7.8: Disconnectivity graphs for glass datasets with different cluster populations.
These datasets have an increased population of data points in the clusters of highest
overlap. Minima are coloured according to the ARI between their cluster labels and the
ground truth labels. The colour range and scale bar represent the same range in all plots.
(a) 5 5.07 (b) 6
Figure 7.9: K-means landscapes for glass datasets with an increased proportion of data
points in the least overlapping clusters. Minima in the disconnectivity graphs are coloured
according to the similarity to the ground truth cluster labels. The scale bar and colour
range is the same in all plots.
Dataset structure: Cluster distribution
Figure 7.10: The distribution of minima over the cost function range for the glass datasets
with varying cluster populations. The cost function range is normalised to lie between 0
and 1.
there remains a large density of minima in the lower region of the cost function
space. Therefore, random initialisation is more likely to yield a minimum that is
considered better in terms of the cost function. However, the density of minima does
not determine the transition rate to the global minimum, which requires a kinetic
analysis.
The aggregate properties of transition states are shown in Figs. 7.11, D.13
and D.12, which present the difference in cost function, distance, and cluster labels
between transition states and their connected minima, respectively. We see that
the barrier distributions strongly depend upon the distributions of minima. The
secondary density peaks at larger barrier heights reflect the equivalent peaks in
the distribution of minima, with gaps also at the relevant parts of the barrier height
distribution. The population of these secondary higher-valued peaks remains similar
in landscapes of all cluster distributions, and largely reflects the density of minima.
The cluster difference and distance distributions do not reflect the distribution
of minima so clearly. These distributions smoothly decrease in probability as the
relevant value increases, indicating there is simply a decreasing chance of steepest-
descent paths moving further across the cost function surface without connecting a
minimal clustering. The Euclidean distance between clusterings is a good estimate
of the cluster label difference for these datasets, so we observe a similar behaviour
for this property. The distance and cluster labels do not correlate strongly with
the cost function barriers for these landscapes, but all these distributions are still
typically strongly peaked at very small values for both higher and lower minima.
The changes in cost function topography as a result of changing cluster distribu-
tion appear moderate for the glass datasets. There is little change in the landscape
features, and we observe no significant change in barrier heights that are not ex-
plained by the minima distribution. The minima distributions themselves constitute
Dataset structure: Cluster distribution
(a) Original (b) 1 (c) 2 (d) 3
(e) 4 (f) 5 (g) 6
Figure 7.11: The distribution of transition state barriers for the glass datasets with varying
cluster populations. The higher and lower barriers denote the cost function gap between
the higher and lower connected minima and the transition state.
the most significant differences between the landscapes.
7.3.2 Distribution of clustering properties
In this section we consider the distribution of accuracy and diversity in terms of
the cost function. First we consider the most accurate clustering in each landscape,
and the best accuracies achieved by K-means are given in Table 7.3. In each case
the most accurate solution is not the global minimum, and the difference in cost
function and accuracy compared to the global minimum can be large. We again see
improved best accuracy in all the datasets relative to the original, but in this case
the original dataset has the most uneven cluster populations. Furthermore, with
six significantly-overlapping clusters it is more difficult to interpret the accuracy
differences.
We present the distributions of accuracy and diversity over the whole cost func-
tion range in Fig. 7.12. The distributions all display similar properties, despite the
changes in cluster populations. The accuracy range, for a given cost function value,
is comparable, and large, in all landscapes. Furthermore, the wide range of accuracy
values is achieved throughout the cost function range. Moreover, the cost function
is not a useful guide to accuracy for any of the datasets, as there is no correlation
between them.
The diversity, defined as the similarity of each minimum to the global minimum
in the landscape, shows similar behaviour to the accuracy. In each case there are
wide diversity ranges throughout the range of the cost function, from those that
Dataset structure: Cluster distribution
Table 7.3: The accuracy of the best K-means clustering for glass datasets with varying
cluster populations. The global minimum is not the most accurate clustering for any
landscape, so we present its accuracy, and the cost function gap to the most accurate
clustering.
Dataset Best accuracy J Accuracy (GM)
Original 0.313 126.80 0.255
1 0.332 60.54 0.231
2 0.345 31.93 0.228
3 0.338 135.53 0.255
4 0.339 51.57 0.282
5 0.389 244.59 0.276
6 0.325 62.38 0.228
(a) Accuracy (b) Diversity
Figure 7.12: The distribution of accuracy and diversity of the clustering solutions within
the KMLs constructed from glass landscapes of different cluster populations. The accuracy
and diversity are measured by the ARI between each clustering and the ground truth labels,
and global minimum clustering, respectively.
Dataset structure: Cluster distribution
Table 7.4: Transition rates from the most accurate clusterings to the global minimum,
for glass datasets with varying cluster population. There are multiple rates where there
are multiple distinct peaks in the accuracy-cost function distribution. The difference in
accuracy and cost function between these minima and the global minima are given for
each pathway. Rates are calculated at the fictituous temperature T = 3.0.
Dataset A J Rate A J Rate A J Rate
Original 0.06 126.8 2.2 103
1 0.10 61.6 5.3 106
2 0.11 33.7 6.0 105
3 0.07 36.7 2.7 104 0.08 135.5 1.5 104
4 0.05 1.2 1.2 103 0.06 51.6 3.4 104 0.05 99.1 1.1 103
5 0.09 42.5 1.3 104 0.11 244.6 1.1 104
6 0.09 62.7 7.9 104
highly resemble the global minimum to those with little correlation. The diversity
variation exhibits a similar range in all landscapes and persists over most of the
cost function range. The distributions for the landscapes with increased density
in non-overlapping clusters exhibit a small distinction; they have few minima that
resemble the global minimum at high values of the cost function. Therefore, the
global minimum has a unique structure in these landscapes that cannot be replicated
at higher values of the cost function. Diversity-coloured disconnectivity graphs are
given in Figs. D.14, D.15 and D.16.
7.3.3 Rates
The distribution of minima in terms of the cost function is one of the primary
landscape features altered by cluster populations. Despite these changes the minima
still retain similar properties across the cost function range, and here we use kinetic
analysis to probe whether the different minima distributions affect pathways between
clusterings of varied accuracies and diversities.
The most important minima in each landscape are those of highest accuracy,
and we calculate rates from these to the global minimum in Table 7.4. The rates
show that the cost function difference for these pathways has little effect on the
resultant rates. The accuracy difference, however, is largely correlated with the
rate, especially for multiple pathways within the same landscape.
The profiles given in Fig. 7.13 illustrate that all transitions involve a very small
number of significant intermediate barriers within long pathways. Therefore, the cost
function gap does not make a large contribution to the rate, as the uphill barriers
remain small at each step, with several downhill barriers spanning large cost function
ranges, producing pathways with similar lengths. The highest transition state in
Dataset structure: Cluster distribution
Figure 7.13: Cost function profiles for the fastest paths from the most accurate clusterings
to the global minimum, for the glass landscapes of varied cluster populations. The paths
correspond to those in Table 7.4. Where multiple paths exist for the same dataset we
distinguish them by cost function.
these pathways must be similar in cost function to the most accurate minimum,
as they lie within the global minimum funnel, but there is no guarantee of small
barriers along the whole pathway. When small barriers occur they are likely due to
the cluster label similarity in all cases.
Therefore, with no clear trend in pathway structures it is expected that the rate
changes between landscapes largely arise through fluctuations due to the different
clustering structures for the transitions. The kinetic behaviour within funnels is
similar for all datasets of differing cluster populations. The barriers are small for
pathways from the most accurate clustering to the global minimum, but the path-
ways require many steps. Consequently, such pathways may be difficult to locate,
especially with many changes that provide equivalent cost function values.
Alternatively we can consider transitions between disparate regions of clustering
space. We select the minimum with the lowest similarity to the global minimum,
within the lowest 10% of the cost function range. We expect this minimum to give
a good indication of the difficulty of moving between different regions of the cost
function landscape. The rates for such transitions are given in Table 7.5, and they
appear to be comparable for all landscapes, with little correlation to the observed
difference in similarity. Therefore, changes in landscape structure may contibute to
the transition rates.
Dataset structure: Cluster distribution
Table 7.5: The lowest similarity between any minimum within the lowest 10% of the
cost function range and the global minimum, for the glass datasets of differing cluster
population. The transition rates to the global minimum are also provided for a fictituous
temperature T = 3.0.
Dataset Lowest similarity Rate
Original 0.738 1.1 102
1 0.620 9.9 105
2 0.563 2.1 105
3 0.608 9.8 105
4 0.455 6.9 104
5 0.533 1.9 104
6 0.639 7.3 104
However, the profiles for these pathways, given in Fig. 7.14, exhibit no clear
trends between landscapes, so the rate differences can be expected to arise naturally
due to the different nature of the transitions. For clusterings of such low similarity
there are few large barriers on the pathways. However, there are significantly more
barriers relative to the pathways from the most accurate clusterings. The increased
number of barriers is compensated by shorter pathways, enabling the rates to remain
similar for pathways between both diverse and accurate clusterings. Hence, transi-
tions within the funnelled clustering space are similarly challenging for all datasets
at differing cluster populations.
Dataset structure: Cluster distribution
Figure 7.14: Visualisation of the fastest paths from the most diverse minima within the
lowest 10% of the cost function range to the global minimum. All pathways show the cost
function values of each stationary point, connected by linear regions.
Chapter 8
Conclusions
In this work we presented the algorithms to produce, and the first examples of,
K-means landscapes. Such landscapes provide unique insight into the topography
of cost function surfaces, which is essential to understanding K-means performance.
There are many general features of K-means landscapes that are retained for varied
datasets, and we present an overview in the following section.
8.1 Overview
All landscapes are relatively funnelled, and such organisation is characteristic of
efficient relaxation to the global minimum in physical systems. Therefore, the K-
means cost function generally has an appropriate structure for successful application
of global optimisation algorithms. The funnelled structures are underpinned by re-
markably small barriers separating the majority of minima from the global minimum.
However, the funnelled landscapes are rarely devoid of fine structure.
Almost all landscapes contain kinetic traps, which are regions of the cost function
above the global minimum that are separated from it by a significant barrier. Kinetic
traps hinder relaxation to the global minimum in physical systems and, consequently,
they are a key feature in predicting the challenge to global optimisation posed by
a given landscape. We apply kinetic analysis to provide rates and pathways that
quantify the challenge of traps to global optimisation. We see that for kinetic traps,
and transitions within the main funnel, the rates largely correlate with the similarity
of the cluster labels between two minima, irrespective of the cost function difference.
Pathways between dissimilar clusterings have much more significant intermediate
barriers that slow the rates.
The landscape organisation into funnels and kinetic traps can be broadly un-
derstood by the partitioning of the clusters within each minimum. Partitionings
Conclusions
that differ from the global minimum can have significant barriers between them,
and frequently belong to kinetic traps. However, many different partitionings also
lie within the main funnel, without large barriers separating them from the global
minimum. For outlier landscapes we additionally need to consider how the outliers
are incorporated into the clustering solutions, which becomes more important as the
number of outliers increases.
The diversity, calculated as the similarity between each minimum and the global
minimum, is largely controlled by the availability of these different partitionings
at varying cost function values. Therefore, the minima in kinetic traps frequently
have significantly different diversity to minima of equivalent cost function within the
global minimum funnel, and in the outlier landscapes this effect leads to some of the
most accurate solutions lying exclusively within kinetic traps. However, the main
funnel itself can be highly diverse due to the many partitionings it can contain.
The intermediate behaviour of the cost function between the minima can be elu-
cidated by the distribution of barrier heights, which is calculated as the cost function
difference between each transition state and its two connected minima. Such dis-
tributions are strongly peaked at very small barriers, with a decreasing probability
of observing larger barrier heights. Small barriers, and the associated difference in
position and cluster labels between transition states and connected minima, indicate
that the landscapes are organised such that many minimal clusterings are directly
connected by a transition state lying at their intersection seam. Larger barriers are
less common, but arise when the transition state is at an intersection seam where
one, or two, of the participating clusterings are non-minimal. The steepest-descent
paths pass through one, or more, non-minimal clusterings before attaining a mini-
mal clustering that is much lower in cost function. These transition states permit
larger movements across clustering space, such as changes between partitionings.
The barrier height distribution is strongly conserved for all dataset modifications,
indicating that K-means landscapes retain similar intermediate behaviour of the
cost function between minima, in addition to their funnelled organisation.
The distribution of minima over the cost function range is strongly affected
by dataset properties and cluster number, even when all other properties remain
similar. We have applied concepts from chemical physics, such as the heat capacity
and landscape entropy, to provide greater insight into minima distributions. The
interpretation of these two properties has been characterised for physical systems,
and we can transfer these concepts directly to this new application of the K-means
cost function.
Furthermore, each K-means landscape defines a network, and we can apply
Conclusions
network algorithms to understand their connectivity. K-means landscapes are highly
linear, with a low average degree resulting from the organisation of minima into long
sequential chains. Therefore, there are many regions of the cost function surface that
can only be reached by a single pathway. The low-valued minima tend to have higher
connectivity, and in the glass landscapes with many minima they form small dense
regions, to which the linear regions join. The network analysis complements the
topographical view.
In the following sections, we highlight specific conclusions about each of the
dataset modifications, and provide practical considerations for clustering success,
using the topographical information now available. We address both global op-
timisation algorithms and multiple initialisation methods, with a focus on global
optimisation, as this application is heavily dependent on landscape structure.
8.2 Cluster number
For all K, the Iris landscapes are organised by partitioning, and kinetic traps cor-
respond to sets of minima that differ in partitioning to the global minimum. The
increase in the number of partitionings with K, increases the number of kinetic
traps, and these traps become separated from the global minimum by smaller barri-
ers. However, a kinetic analysis shows that it becomes more challenging to transition
between different partitionings, despite the smaller barriers, due to longer pathways.
Hence, the landscapes become more challenging for global optimisation algo-
rithms due to the increasing pathway length between partitionings. However, each
region of the landscape associated with a particular partitioning remains locally fun-
nelled, at all K. Global optimisation, within a given partitioning, should be efficient
using local perturbations. Such moves will need to be paired with larger, and more
frequent, perturbations that allow for changes in partitionings as K increases, to
cover the growing number of distinct partitionings. Initialisation of cluster centres
also becomes more difficult as K increases, due to the growing number of different
partitionings, and the decreasing number of minima associated with the global min-
imum partitioning. Therefore, there will be a much larger number of initialisations
required as K increases.
8.3 Outliers
As the number of outliers increases the landscapes become more funnelled, and less
frustrated. However, rate calculations reveal that the more funnelled topography
Conclusions
does not result in more efficient global optimisation, as it becomes more difficult to
escape kinetic traps. Furthermore, transitions from minima within the main funnel
to the global minimum become more challenging too. A higher effective temperature
is needed as the number of outliers increases, due to the increase in cost function
range, and difficulty escaping kinetic traps.
There is a sharp increase in the number of ways in which outliers can be accom-
modated into clusterings as the number of outliers increases. Therefore, the growth
in the number of distinct types of clusterings, and more difficult transitions be-
tween them, contribute to more challenging global optimisation. The perturbations
used in an effective global optimisation algorithm must pair local changes, which
optimise cluster boundaries, with those that allow it to sample different structure
types by exchanging outliers. Such outlier exchanges become more important for
landscapes with an increased number of outliers, and a larger number of distinct
starting configurations are needed to ensure sampling of all clustering space. How-
ever, the funnelled organisation of minima should allow relatively efficient global
optimisation, if the moves sample the space effectively, for any number of outliers.
Initialisation will find the increase in the number of outliers much more challeng-
ing due to the growth in the number of different structure types. Furthermore,
the placement of outliers is essential to locating the global minimum, and they are
highly unlikely to be selected at random. Many of the best solutions involve single
outlier clusters and these will be very rare to attain by initialisation.
The maximum accuracy of K-means decreases as we increase the number of
outliers, for both the Iris and glass datasets. The Iris dataset accommodates the
outliers with little change in cluster boundaries from the original dataset and, con-
sequently, little reduction in accuracy. The correlation between accuracy and cost
function also remains strong, and although the global minimum is not the most ac-
curate clustering for all outlier landscapes the cost function difference is very small.
For the glass datasets the global minimum is not the most accurate clustering in
any case. As the number of outliers increases the correlation between both accuracy
and diversity with the cost function is vastly reduced, and is largely structureless
after addition of outliers. This increase in accuracy and diversity ranges, over most
of the cost function, largely results from the increased number of structure types
within the main funnel, and their overlap in cost function.
Therefore, for datasets containing multiple outliers, obtaining the global mini-
mum does not necessarily give the most accurate clustering solution that K-means
can support. Without a clear way to distinguish accurate solutions in cost function
it is important to use an internal metric, such as the silhouette coefficient,28 to in-
Conclusions
dependently rank the generated solutions. However, for all but the case of three
outliers in the highly-overlapping glass dataset, we observe that the low-valued re-
gion does contain minima of comparable accuracy to the most accurate clustering.
Therefore, attaining low-valued solutions is valuable, but it becomes essential to
generate an ensemble of clustering solutions, which can capture the many possi-
ble arrangements of the various outliers. Such an ensemble could be located using
global optimisation from several different starting configurations, which, as described
above, could be used to find low-valued minima efficiently thanks to the funnelled
landscape organisation even for multiple outliers. It could also be generated via
multiple initialisations, but these do not provide the same guarantee of low-valued
solutions.
8.4 Standardisation
Standardisations were shown to strongly retain landscape structure for the Iris
datasets. The glass datasets exhibit much more variation: the Z1-standardised
landscapes remain similar in all cases, but the Z2 and Z3 standardisations differ
significantly for the outlier landscapes. Z1 standardisation decreases escape rates
from kinetic traps in all datasets, whereas Z2 and Z3 increase the rates. Visualising
the pathway profiles we have seen that the increased rates are largely due to the
smaller cost function range, and do not correlate with better global optimisation
properties in the glass datasets.
For the Z2- and Z3-standardised datasets global optimisation should use a lower
effective temperature to accept new minima, to reflect the reduced cost function
ranges of these landscapes. Again, in all but the Z2-standardised glass dataset, with
outliers, the landscapes remain single-funnelled, and well-structured, which admits
efficient global optimisation. The perturbations must again pair local boundary
changes, with larger moves that exchange partitionings. For the Iris dataset this
effect likely results in more efficient global optimisation for standardised datasets,
and comparable efficiency for the glass. Initialisation is likely to perform similarly
after standardisation. The reduced cost function range will make little difference,
but there are specific structural changes that may affect performance. In general,
the reduced feature ranges shorten the separation between clusters, which allows bad
initialisations to minimise to better solutions due to greater intermediate density.
Network properties of the K-means landscapes show that they are highly linear,
with lower linearity for the glass datasets due to the formation of dense hubs. These
hubs appear to preferentially include low-valued minima. Within the hubs multiple
Conclusions
alternative pathways are possible, and they each connect to multiple linear regions.
The networks explain the long pathways between clusterings, which results from
moving along linear regions without alternative pathways. Such linearity means
that global optimisation will likely require multiple starting points, and long runs, to
ensure escape from linear regions and exploration of the relevant regions of clustering
space. The low-valued hubs, with many connections, provide an additional driving
force to the global minimum, in addition to the reduction in cost function.
For the Iris datasets the connectivity is strongly retained on standardisation,
along with the topography. The networks remains broadly similar for the glass
datasets too, with a small increase in linearity and hub density. These changes likely
provide a small improvement in global optimisation properties for the standardised
glass datasets, due to more alternative paths through cost function space between the
linear regions. Initialisation will be relatively unaffected by the changes in network
structure.
Standardisations can both raise and lower the maximum K-means accuracy, and
the position of this clustering in the cost function range. When the global minimum
is not the most accurate clustering this solution has lower accuracy, relative to the
unstandardised dataset. For all standardisations, the correlation between accuracy
and cost function is reduced, and the distributions become flatter. The loss of corre-
lation arises due to wider clustering diversity, where the reduced cost function range
allows multiple distinct partitionings to overlap in cost function. This effect is small
for the Iris datasets, due to the clear cluster structure, and it remains important to
attain low-valued solutions, which becomes easier for global optimisation with stan-
dardisations. The large overlap does not prevent the standardisation finding a higher
maximum accuracy for the glass datasets, but it does make the correlation between
accuracy and cost function much weaker. Therefore, without accurate solutions at
low cost function in many of the standardised datasets it becomes challenging to
find the most accurate solutions. It becomes necessary to evaluate minima using a
second internal metric, which provides a better estimate of clustering accuracy than
the cost function, for datasets of large overlap. Furthermore, a diverse ensemble
of clustering solutions need to be generated over much of the cost function range,
via either many initialisations, or multiple short global optimisation runs with local
perturbations.
Conclusions
8.5 Overlap
K-means landscapes become more funnelled as cluster overlap increases. Equivalent
kinetic traps decrease in depth, leading to faster escape rates that are mainly due to
smaller barriers in the corresponding pathways. The landscapes are largely organised
by partitionings, so kinetic traps decrease in depth due to the reduced distance
between the underlying clusters of the dataset. Transition rates between clusterings
within the main funnel also increase. Both kinetic and thermodynamic analysis
provides evidence that the landscape structure changes so that global optimisation
is less challenging as overlap increases. The significant kinetic traps in well-separated
datasets can pose a problem to global optimisation. However, in these datasets with
little overlap, each kinetic trap, corresponding to a different partitioning, is locally
funnelled. Therefore, the combination of local, and less frequent large, perturbations
should still favour efficient global optimisation. Initialisation methods are likely
to see a smaller increase in their ability to locate the global minimum as overlap
increases, as there remain a similar number of partitionings to search. However,
increased overlap may provide a small improvement, because the increasing lack of
cluster structure favours an even distribution of cluster centres.
As overlap increases we see a lower maximum accuracy for both datasets. The
Iris dataset, which is clearly-separated, shows a decrease in accuracy difference
between the global minimum and alternative solutions high in cost function, but we
still have a clear correlation between cost function and accuracy, even for the highest
overlap. For the glass datasets, with much greater overlap, the global minimum is
not the most accurate clustering in any case. Therefore, the ability to locate the
global minimum more easily does not necessarily improve K-means performance,
because the cost function is no longer a suitable surrogate for accuracy.
The diversity distributions also become more structureless and uniform as over-
lap increases and, as with accuracy, this effect is only seen for the Iris (K = 6) and
glass datasets. Kinetic traps can produce two significantly different sets of minima at
the same cost function value, but for little overlap these partitionings are localised,
and separate, in the diversity-cost function space. As overlap increases the parti-
tionings overlap more in cost function, and each partitioning itself becomes more
diverse, due to more possible boundary variations. The increased diversity at high
overlap makes it possible to find many distinct clusterings, including those resem-
bling the most accurate and global minimum clusterings, across large cost function
ranges. Furthermore, we require an ensemble of minima that can be evaluated us-
ing an alternative internal clustering metric, as the cost function is insufficient to
select accurate clusterings. Without the need for clusterings low in cost function,
Conclusions
initialisation becomes more appropriate for generating a random ensemble of min-
ima as the overlap increases. Improvements may be made in this ensemble through
global optimisation algorithms applied to each clustering, with local perturbations
to preserve the partitioning.
8.6 Cluster distribution
The cluster distribution also provides relatively systematic changes in landscape
structure, and for the Iris dataset with simple structure, the changes can be largely
related to the populations of overlapping and non-overlapping clusters. The changes
to the glass landscapes are not so straightforward, as the datasets are significantly
more complex. For the Iris dataset we see changes in the kinetic trap depth, and
relative height compared to the global minimum. All pathways have similar charac-
ter, so the rates depend upon the kinetic trap depth. For the glass datasets we do
not have kinetic traps and we see that the transition rates between highly similar,
or different, clustering solutions are comparable at all cluster populations, with no
clear trends. Transitions from similar solutions to the global minimum are much
faster than those from diverse solutions in all landscapes, due to smaller barriers on
the pathways.
Uneven cluster distributions make initialisation more challenging, as it becomes
less likely to randomly select data points from small underlying clusters. Kinetic
traps can be removed by cluster population changes, indicating that these modi-
fications can increase, as well as decrease, the performance of global optimisation
algorithms. In contrast, initialisation is expected to get uniformly worse. A higher
effective temperature may be needed for accepting new minima in basin-hopping
global optimisation if uneven cluster populations are expected, to guarantee escape
from kinetic traps, should they exist. However, for the glass datasets there are very
few kinetic traps, so global optimisation algorithms should perform well on all these
landscapes, irrespective of the cluster populations.
In the Iris datasets it is only for the most uneven distributions that the global
minimum does not have the highest accuracy. Furthermore, the correlation be-
tween cost function and accuracy remains relatively strong in all datasets. The
largest modifications, and those that add kinetic traps, make the distributions less
clear. However, low-valued minima should remain the aim of K-means. In all glass
datasets, as seen for all other dataset modifications, the global minimum is not the
most accurate clustering. There is generally an increase in the maximum accuracy,
but the global minimum accuracy can be better or worse relative to the original
Conclusions
dataset. Adding data points to the least-overlapping clusters makes both the accu-
racy and diversity distributions more correlated, but all other changes give largely
structureless distributions. Therefore, for most modified datasets, accurate solutions
can be obtained across the whole cost function range, with little need for low-valued
solutions. However, the flat distributions ensure that the low-valued solutions con-
tain those of comparable accuracy to the most accurate clustering. Therefore, a
diverse ensemble of minima is needed for highly-overlapping datasets, but the aim
should remain low-valued solutions for all cluster population changes. The low-
valued solutions can be efficiently located using global optimisation algorithms, due
to the strongly funnelled landscapes, and evaluated using an alternative internal
metric.
8.7 Application to novel datasets
The K-means landscape methodology presented in this work is applied to two spe-
cific datasets, but the results should be applicable to the clustering of novel datasets.
K-means landscapes allow clear separation of clustering solutions based on their re-
lation on the cost function surface. Therefore, K-means landscapes can reveal the
presence of different partitionings, provide straightforward evaluation of the stability
and reproducibility of clustering solutions, and their global organisation can guide
more efficient global optimisation. Furthermore, the construction of K-means land-
scapes provides a route for the application of algorithms from different fields, which
can be used to gain further insight into the space of clustering solutions.
Constructing K-means landscapes for novel datasets, including solutions across
the full cost function range, is significantly more expensive than only generating
minima. One way to limit the additional computational cost of transition state lo-
cation is to employ more efficient initialisation schemes, such as K-means++. The
exploratory work presented here employed a poor initialisation scheme to ensure
samples were drawn across the full cost function range, but sampling the com-
plete range is not necessary for real applications. A landscape constructed from
K-means++ solutions would predominantly contain minima low in cost function,
even with a small number of initialisation steps, resulting in less computational cost
associated with exploring high-cost regions of space that contain poor clusterings.
Therefore, in constructing K-means landscapes for novel datasets we recommend
generating minima by K-means++, which leads to landscapes that describe only
the low-valued regions of cost function space in detail. All the analysis given in
this work is then possible, but at a significantly reduced computational cost for
Conclusions
generating the landscapes, and without loss of accurate clusterings.
Furthermore, the detailed landscapes generated for the two datasets in this study
can be used to understand novel datasets, even without calculation of their K-means
landscapes. K-means clustering has been shown to be highly dependent on a small
number of dataset properties, and the observed changes in K-means landscapes with
these properties can be considered as largely general trends, which will be followed
by other datasets of similar properties. If the characteristics of a novel dataset are
known (outliers, standardisation, cluster overlap, and cluster distribution) then the
expected landscape structure can be estimated, with reference to the landscapes
of the Iris and glass datasets, and the sampling and stability recommendations of
preceding sections in this chapter can be applied.
The main limitation of this approach is that the dataset properties are not known
prior to clustering. It is possible that expert knowledge could provide a good esti-
mate of the dataset properties for a novel dataset, but there can be much variation.
Therefore, to leverage the understanding of K-means landscapes, at varying dataset
properties, we recommend an iterative approach to K-means. First, K-means is
performed to generate a small number of clustering solutions, and from this subset
the dataset properties can be estimated. These characteristics can guide further
K-means sampling, and assessment of the quality of current solutions, by reference
with the expected landscape structure. The sampling and dataset evaluation can
be iterated until acceptable clustering solutions are obtained. Such an approach
adds little computational cost to K-means clustering, and allows the description of
solution landscapes to guide more efficient sampling. The construction of K-means
landscapes with K-means++, as described above, can also be performed using an it-
erative approach based on the current landscape structure. We expect the use of this
iterative approach to be valuable in many applications due to its low computational
cost.
Appendices
Appendix A
Outliers
(a) Original 1 (b) Outlier 1 1 (c) Outlier 2
1 (d) Outlier 3 1 (e) Outlier 4
Figure A.1: Disconnectivity graphs showing the K-means landscapes for Fishers Iris
dataset with K = 3. Minima are coloured according to the partitioning of the underlying
data, which is given by the number of clusters that are assigned Setosa data points. The
scalebar and colour scale are the same in all plots.
Outliers
(a) Original 1 (b) Outlier 1 1 (c) Outlier 2
1 (d) Outlier 3 1 (e) Outlier 4
Figure A.2: Disconnectivity graphs for K-means landscapes of Fishers Iris dataset with
six clusters. Minima are coloured according to the number of clusters assigned to the
Setosa data points. The cost function and colour scale are the same in all plots.
Outliers
(a) Original
(b) Outlier 1
(c) Outlier 2
(d) Outlier 3
(e) Outlier 4
Figure A.3: K-means landscapes for Fishers Iris dataset with K = 6, highlighting kinetic
traps. The lowest minimum from each kinetic trap is coloured red, and the associated
numerical label is given. The numbering corresponds to that used in Chapter 4.
Outliers
(a) Original
(b) Outlier 1
(c) Outlier 2
(d) Outlier 3
Figure A.4: K-means landscapes for the glass datasets with varying outlier number. Se-
lected kinetic traps are identified by red minima in the disconnectivity graph, with the
associated label. The label corresponds to the kinetic traps given in Chapter 4.
Outliers
(a) Minimum 1 (b) TS 1 (c) Minimum 2 (d) TS 2
(e) Minimum 3 (f) TS 3 (g) Minimum 4 (h) TS 4
(i) Minimum 5 (j) TS 5 (k) Minimum 6 (l) TS 6
(m) Minimum 7 (n) TS 7 (o) Minimum 8
Figure A.5: Stationary points on the fastest path between kinetic trap minima and the
global minimum in the KML of the original Iris dataset with K = 6. The pathway
progresses from the global minimum (Minimum 1) to a minimum from kinetic trap 2,
as labelled in Fig. A.3. Minima and transition states are visualised in three of the four
dimensions. The petal width contains the least information, so is excluded in these plots.
Outliers
(a) Minimum 1 (b) TS 1 (c) Minimum 2 (d) TS 2
(e) Minimum 3 (f) TS 3 (g) Minimum 4 (h) TS 4
(i) Minimum 5 (j) TS 5 (k) Minimum 6 (l) TS 6
(m) Minimum 7
Figure A.6: Stationary points on the fastest path between kinetic trap 1 and the global
minimum for the Iris dataset with six clusters and three outliers. All stationary points
are represented using three of the four features.
Outliers
(a) Original
(b) Outlier 1
(c) Outlier 2
(d) Outlier 3
(e) Outlier 4
(f) Original
(g) Outlier 1
(h) Outlier 2
(i) Outlier 3
(j) Outlier 4
(k) Original
(l) Outlier 1
(m) Outlier 2
(n) Outlier 3
(o) Outlier 4
Figure A.7: Disconnectivity graphs for K-means landscapes of Fishers Iris dataset with
K = 3 for the original dataset, and datasets with outliers. The minima are coloured
according to the ARI relative to the ground truth labels (first row), the global minimum
of the landscape (second row), and the global minimum for the original dataset (third
row). The scalebar and colour scale are the same for all panels in the same row.
Outliers
(a) Original 1 (b) Outlier 1 1 (c) Outlier 2
1 (d) Outlier 3 1 (e) Outlier 4
Figure A.8: K-means landscapes for Fishers Iris dataset with K = 6 are shown for the
original dataset, and datasets with up to four outliers. The scale bar is 40 in all plots, and
the colour range also remains the same. Minima are coloured by the ARI relative to the
global minimum.
Outliers
(a) Original 1 (b) Outlier 1 1 (c) Outlier 2
1 (d) Outlier 3 1 (e) Outlier 4
Figure A.9: Disconnectivity graphs showing the K-means landscapes for Fishers Iris
dataset with six clusters for the original dataset, and datasets with up to four outliers.
The scale bar is 40 in all plots. Minima are coloured according to the ARI between
their cluster labels, and those of the global minimum of the original dataset. The colours
represent the same numerical range in all plots.
Outliers
(a) Original 0.35 (b) Outlier 1
0.35 (c) Outlier 2 0.35 (d) Outlier 3
Figure A.10: Disconnectivity graphs showing the KMLs for the glass datasets with an
increasing number of outliers. The scale bar represents the same range in all plots. The
colouring of minima is dictated by the ARI between the cluster labels of each minimum
and the ground truth labels. The colour range is given in (a) and remains the same in all
plots.
Outliers
(a) Original 1 (b) Outlier 1
1 (c) Outlier 2 1 (d) Outlier 3
Figure A.11: Disconnectivity graphs of the KMLs for the glass datasets with varying
number of outliers. The minima are coloured according to the ARI between their cluster
labels and those of the global minimum of their landscape. The same scale bar range is
used in all plots, and the colour range is also the same.
Outliers
(a) Original 1 (b) Outlier 1
1 (c) Outlier 2 1 (d) Outlier 3
Figure A.12: Disconnectivity graphs of the K-means landscapes for the glass identification
dataset, including the original datasets and those generated by addition of up to three
outliers. Each minimum is coloured according to the difference in cluster labels between
the global minimum clustering in the original dataset, as measured by ARI. The colour
bar and scale bar retain the same ranges across all plots.
Appendix B
Standardisation
(a) Original (b) Z1 (c) Z2
(d) Outlier 2
(e) Z1
(f) Z2
(g) Z3
Figure B.1: Disconnectivity graphs for the K-means landscapes of the original, and stan-
dardised, Iris datasets with three clusters. Kinetic trap minima are highlighted in red.
The top row shows the landscapes of the original dataset, and its standardisations with
a single kinetic trap. The second rows show landscapes for the dataset with two outliers,
which have two kinetic traps, as labelled.
Standardisation
Figure B.2: Pathway profiles for transitions from the kinetic trap to the global minimum
for the Iris dataset, and its standardisations.
(a) Original 0.35 (b) Z1
0.35 (c) Z2 0.35 (d) Z3
(e) Outlier 2 0.35 (f) Z1 1 (g) Z2 1 (h) Z3
Figure B.3: The same disconnectivity graphs as presented in Fig. 5.1. The top row are
variations on the original dataset, and the second row variations on the Iris dataset with
two outliers. In these plots the minima are coloured according to their similarity to the
global minimum, as measured by the ARI between the two cluster labels. The colour scale
remains the same in all plots of the same row.
Standardisation
(a) Original
(b) Z1
(c) Z2
(d) Z3
(e) Outlier 2
(f) Z1
(g) Z2
(h) Z3
Figure B.4: Disconnectivity graphs for the K-means landscapes of the glass identification
dataset, including the original datasets (first row) and those with two outliers (second
row). Kinetic traps are highlighted by minima coloured red. All kinetic traps have an
associated label that matches those used in Chapter 5.
Standardisation
(a) Outlier 2 (b) Z1
(c) Z2 (d) Z3
Figure B.5: Network visualisation of the KMLs constructed from the glass datasets with
two outliers. These graphs use a force-directed representation, with gravity applied to
prevent excess expansion.
Standardisation
(a) Original
(b) Z2
(c) Z3
0.35 (d) Z5
(e) Original
(f) Z1
(g) Z2 1 (h) Z3
Figure B.6: Disconnectivity graphs for the K-means landscapes corresponding to the glass
identification datasets. Landscapes are shown for the original datasets in the first row, and
the datasets with two outliers in the second row. Each minimum is coloured according to
the ARI difference between its cluster labels, and those of the global minimum clustering.
The colour scale is the same for all plots of the same row.
Appendix C
Overlap
(a) Separation 2
(b) Original
(c) Aggregation 1
(d) Aggregation 2
Figure C.1: Disconnectivity graphs for K-means landscapes corresponding to Fishers Iris
dataset with K = 3. Minima in each disconnectivity graph are coloured according to
the partitioning of the clusters, given by the number of clusters with Setosa data points
assigned to them. The scale bar and colour scale are the same in all plots.
Table C.1: The range of vibrational frequencies across all minima of a given KML. Vibra-
tional frequencies are calculated from the Hessian at each minimum.
Separation 2 Separation 1 Original Aggregation 1 Aggregation 2
Iris (K = 3) 51.6355.26 52.2255.06 53.4155.06 54.7354.94
Iris (K = 6) 67.4093.82 73.0093.82 76.42110.82 79.65110.83 71.1393.82
Glass 142.58228.08 137.15228.33 138.09217.18 124.71221.47 112.01223.10
Overlap
(a) Separation 2 4 (b) Separation 1 1 (c) Original
1 (d) Aggregation 1 1 (e) Aggregation 2
Figure C.2: Disconnectivity graphs showing the KMLs for Fishers Iris dataset with
K = 6, and variants of differing overlap. Minima are coloured according to the parti-
tioning of the clusters amongst the underlying data structure. The partitioning is given
by the number of clusters assigned to Setosa measurements, and the same range is used
in colouring all the graphs. The scale bar also retains the same range in all the plots.
Overlap
(a) Sep 1
(b) Original
(c) Agg 1
(d) Agg 2
Figure C.3: Disconnectivity graphs for the KMLs corresponding to Fishers Iris dataset,
and modified datasets of differing overlap, with K = 6. The lowest minimum from each
labelled kinetic trap is coloured red. The kinetic trap labels match those used in Chapter 6.
Overlap
(a) Minimum 1 (b) TS 1 (c) Minimum 2 (d) TS 2
(e) Minimum 3 (f) TS 3 (g) Minimum 4 (h) TS 4
(i) Minimum 5 (j) TS 5 (k) Minimum 6 (l) TS 6
(m) Minimum 7 (n) TS 7 (o) Minimum 8 (p) TS 8
(q) Minimum 9 (r) TS 9 (s) Minimum 10 (t) TS 10
(u) Minimum 11 (v) TS 11 (w) Minimum 12
Figure C.4: Stationary points on the fastest path between minima corresponding to kinetic
trap 2 of the original dataset and the global minimum for the Iris Separation 1 dataset
with K = 6. Each stationary point is visualised in three of the four dimensions, excluding
petal width. The pathway begins at the global minimum (Minimum 1) and progresses to
a minimum contained with the kinetic trap (Minimum 8).
Overlap
(a) Minimum 1 (b) TS 1 (c) Minimum 2 (d) TS 2
(e) Minimum 3 (f) TS 3 (g) Minimum 4 (h) TS 4
(i) Minimum 5
Figure C.5: The connected minima and transition states on the fastest path for escaping
kinetic trap 2 in the original Iris dataset with K = 6. The pathway begins with the
global minimum (Minimum 1), and all stationary points are visualised using three of four
features.
Overlap
(a) Minimum 1 (b) TS 1 (c) Minimum 2 (d) TS 2
(e) Minimum 3 (f) TS 3 (g) Minimum 4 (h) TS 4
(i) Minimum 5 (j) TS 5 (k) Minimum 6 (l) TS 6
(m) Minimum 7 (n) TS 7 (o) Minimum 8
Figure C.6: The stationary points encountered on the fastest path between minima within
kinetic trap 2 to the global minimum for the Iris Aggregation 1 dataset with K = 6. Min-
imum 1 is the global minimum, and the pathway progresses to the kinetic trap minimum
(Minimum 8). All stationary points are displayed in three of the four feaures.
Overlap
(a) Minimum 1 (b) TS 1 (c) Minimum 2 (d) TS 2
(e) Minimum 3
Figure C.7: A visualisation of the fastest path between minima corresponding to kinetic
trap 2 of the original dataset and the global minimum for the Iris Aggregation 2 dataset
with six clusters. All stationary points on the pathway are shown in three of the four
features.
Overlap
80 1 2
(a) Sep 2
(b) Sep 1
(c) Original
(d) Agg 1
(e) Agg 2
Figure C.8: Disconnectivity graphs for KMLs of the glass dataset, and variations of differ-
ent overlap. The lowest minimum of the kinetic traps is coloured red. Each kinetic trap
is labelled to correspond with those of Chapter 6.
Overlap
Figure C.9: The distribution of minima over the cost function range for Iris landscapes
of varying overlap, with six clusters.
Figure C.10: Distribution of minima over the cost function for landscapes of the glass
datasets.
Overlap
(a) Separation 2
(b) Original
(c) Aggregation 1
(d) Aggregation 2
Figure C.11: Disconnectivity graphs showing the KMLs for Iris datasets with K = 3.
Minima are coloured in order to show the clustering accuracy. The accuracy is calculated
as the ARI between the cluster labels of a minimum and the ground truth labels. The
colour range is the same in all plots, and the scale bar represents the same value.
Overlap
(a) Separation 2 5.07 (b) Separation 1 5.05 (c) Original
5.05 (d) Aggregation 1 5.07 (e) Aggregation 2
Figure C.12: Disconnectivity graphs showing the KMLs of the glass datasets. The accuracy
of each minimum, calculated as the ARI between its cluster labels and the ground truth
labels, is given by the colour of the minimum. The scale bar and colour range are the
same in all plots.
Appendix D
Cluster distribution
(a) Original (b) 1 (c) 2 (d) 3
(e) 4 (f) 5 (g) 6
(h) 7 (i) 8
Figure D.1: The lowest minimum in the kinetic trap for each Iris dataset with differing
cluster populations. For datasets 13 without a kinetic trap we simply select the lowest
minimum in the highest band of minima in cost function. Minima are visualised using
three of the four dimensions, with petal width excluded.
Cluster distribution
(a) Minimum 1 (b) TS 1 (c) Minimum 2 (d) TS 2
(e) Minimum 3 (f) TS 3 (g) Minimum 4 (h) TS 4
(i) Minimum 5
Figure D.2: The fastest pathway between kinetic trap 2 and the global minimum for the
original Iris dataset with K = 3. All stationary points of the fastest path are visualised
from the global minimum (Minimum 1) to a minimum of kinetic trap 2 (Minimum 5).
Clusterings are displayed using the sepal length, sepal width and petal length, with petal
width excluded.
(a) Minimum 1 (b) TS 1 (c) Minimum 2 (d) TS 2
(e) Minimum 3 (f) TS 3 (g) Minimum 4
Figure D.3: Stationary points of the fastest path for escape from kinetic trap 2 to the global
minimum. The pathway is calculated from the KML of Iris dataset 4. Each transition
state, and its two connected minima, are shown in sequence with three of the four features.
Cluster distribution
(a) Minimum 1 (b) TS 1 (c) Minimum 2 (d) TS 2
(e) Minimum 3
Figure D.4: The sequence of transition states, and connected minima, in the fastest path
between minima of kinetic trap 2 and the global minimum for Iris dataset 5. The pathway
progresses from the global minimum to the kinetic trap, with each clustering visualised in
the space of three features.
(a) Minimum 1 (b) TS 1 (c) Minimum 2 (d) TS 2
(e) Minimum 3
Figure D.5: Stationary points on the fastest path between kinetic trap 2 and the global
minimum for the Iris dataset 6. All minima and transition states on the pathway are
shown, moving sequentially from the global minimum (Minimum 1) to the kinetic trap
minimum (Minimum 3).
Cluster distribution
(a) Original (b) 1 (c) 2 (d) 3
(e) 4 (f) 5 (g) 6 (h) 7
(i) 8
Figure D.6: Barrier distrubutions for K-means landscapes constructed from Iris datasets
with varying cluster populations. Barriers are calculated for all the transition states.
Higher and lower barriers distinguish the cost function gap between the transition state
and the higher and lower connected minima, respectively.
Table D.1: The cluster overlap for the Iris datasets of varying cluster populations, as
measured by the misclassification probability.
Dataset Overlap
Original 7.33
1 7.33
2 6.67
3 7.33
4 6.67
5 6.67
6 5.33
7 6.67
8 4.67
Cluster distribution
(a) Original (b) 1 (c) 2 (d) 3
(e) 4 (f) 5 (g) 6 (h) 7
(i) 8
Figure D.7: Cluster label differences between each transition state and its higher and lower
connected minima on the KMLs for Iris datasets varying in cluster population.
Cluster distribution
(a) Original (b) 1 (c) 2 (d) 3
(e) 4 (f) 5 (g) 6 (h) 7
(i) 8
Figure D.8: Distributions showing the distance between each transition state and its two
connected minima for the Iris landscapes with varying cluster populations.
(a) Original
(b) 1
(c) 2
(d) 3
Figure D.9: K-means landscapes for Iris datasets with varying cluster populations. The
datasets have an increased population in one overlapping cluster, obtained by removing
data points evenly from the remaining two clusters. Minima are coloured according to
the ARI difference between each minimum and the global minimum clustering for their
landscape.
Cluster distribution
(a) 4
(b) 5
(c) 6
Figure D.10: K-means landscapes, visualised using disconnectivity graphs for Iris dataset
variations with different cluster populations. The datasets here have additional population
in the non-overlapping cluster. Minima are coloured to reflect the difference from the global
minimum, given by the ARI between the clusters labels.
(a) 7
(b) 8
Figure D.11: Disconnectivity graphs displaying the KMLs for Iris datasets with an in-
creased population of one overlapping cluster, and a reduced population for the other
overlapping cluster. Minima are coloured by the ARI between their cluster labels, and
those of the global minimum.
Cluster distribution
(a) Original (b) 1 (c) 2 (d) 3
(e) 4 (f) 5 (g) 6
Figure D.12: The distances between transition states and their connected minima for
landscapes of glass datasets with different cluster populations.
(a) Original (b) 1 (c) 2 (d) 3
(e) 4 (f) 5 (g) 6
Figure D.13: The cluster label difference between each transition state and its two con-
nected minima for KMLs of glass datasets with different cluster populations.
Cluster distribution
(a) Original 5.07 (b) 1 5.07 (c) 2
Figure D.14: Disconnectivity graphs for the K-means landscapes of glass datasets with dif-
ferent cluster populations. The original dataset is shown, in addition to two datasets with
more even distribution of data points amongst clusters. Minima are coloured according
to the ARI difference relative to the global minimum.
(a) 3 5.07 (b) 4
Figure D.15: Disconnectivity graphs for landscapes of glass datasets with different cluster
populations. These datasets have an increased population of minima in the clusters of
highest overlap. Minima are coloured according to the ARI between the clusters labels
and those of the global minimum.
Cluster distribution
(a) 5 5.07 (b) 6
Figure D.16: K-means landscapes for glass datasets with an increased number of data
points in the least-overlapping clusters. The minima are coloured to show the similarity
of each minimum to the global minimum. The similarity is quantified using the ARI.
References
[1] G. Carleo, I. Cirac, K. Cranmer, L. Daudet, M. Schuld, N. Tishby, L. Vogt-
Maranto, and L. Zdeborova. Machine learning and the physical sciences. Rev.
Mod. Phys., 91:045002, 2019.
[2] A. Tkatchenko. Machine learning for chemical discovery. Nat. Commun.,
11:4125, 2020.
[3] C. Angermueller, T. Parnamaa, L. Parts, and O. Stegle. Deep learning for
computational biology. Mol. Syst. Biol., 12:878, 2016.
[4] H. Steinhaus. Sur la division des crops materiels en parties. Bull. Acad. Polon.
Sci., 4:801804, 1957.
[5] E. W. Forgy. Cluster analysis of multivariate data: efficiency versus inter-
pretability of classifications. Biometrics, 21:768769, 1965.
[6] J. B. MacQueen. Some methods for classification and analysis of multivariate
observations. In Proc. of the 5th Berkeley Symp. Math. Stat. Prob., pages
281297, 1967.
[7] S. Lloyd. Least squares quantization in PCM. IEEE Trans. Inf. Theory,
28:129137, 1982.
[8] M. Ahmed, A. N. Mahmood, and M. R. Islam. A survey of anomaly detection
techniques in financial domain. Future Gener. Comput. Syst., 55:278288,
2016.
[9] M. Ahmed, N. Choudhury, and S. Uddin. Anomaly detection on big data in
financial markets. In Proc. of the 2017 IEEE/ACM Int. Conf. Adv. Soc. Netw.
Anal. Mining, pages 9981001, 2017.
[10] Z. Kakushadze and W. Yu. Statistical industry classification. J. Risk &
Control, 3:1765, 2016.
[11] F. Cai, N-A. Le-Khac, and M-T. Kechadi. Clustering approaches for financial
data analysis: a survey. In Proc. of the 8th Int. Conf. on Data Mining, pages
105111, 2016.
[12] V. Schellekens and L. Jacques. Quantized compressive K-means. IEEE Signal
Process. Lett., 25:12111215, 2018.
REFERENCES
[13] M. Alhawarat and M. Hegazi. Revisiting K-means and topic modeling, a
comparison study to cluster arabic documents. IEEE Access, 6:4274042749,
2018.
[14] J. Paek and J. Ko. k-means clustering-based data compression scheme for
wireless imaging sensor networks. IEEE Syst. J., 11:26522662, 2017.
[15] W. Kwedlo and P. J. Czochanski. A hybrid MPI/OpenMP parallelization of
K-means algorithms accelerated using the triangle inequality. IEEE Access,
7:4228042297, 2019.
[16] I. Cabria and I. Gondra. Potential-K-means for load balancing and cost min-
imization in mobile recycling network. IEEE Syst. J., 11:242249, 2014.
[17] W. Wu and M. Peng. A data mining approach combining K-means clustering
with bagging neural network for short-term wind power forecasting. IEEE
Internet Things J., 4:979986, 2017.
[18] U. Sengupta, M. Carballo-Pacheco, and B. Strodel. Automated Markov state
models for molecular dynamics simulations of aggregation and self-assembly.
J. Chem. Phys., 150:115101115113, 2019.
[19] I. H. Jarman, T. A. Etchells, D. Bacciu, J. M. Garibaldi, I. O. Ellis, and P. J. G.
Lisboa. Clustering of protein expression data: a benchmark of statistical and
neural approaches. Soft Comput., 15:14591469, 2010.
[20] F-X. Wu, W-J. Zhang, and A. J. Kusalik. Determination of the minimum sam-
ple size in microarray experiments to cluster genes using K-means clustering.
In Proc. of the 3rd IEEE Symp. Bioinf. Bioeng., pages 401406, 2003.
[21] R. Suresh, K. Dinakaran, and P. Valarmathie. Model based modified K-means
clustering for microarray data. In Proc. of the 2009 Int. Conf. Inf. Manag.
Eng., pages 271273, 2009.
[22] S. Tavazoie, J. D. Hughes, M. J. Campbell, R. J. Cho, and G. M. Church. Sys-
tematic determination of genetic network architecture. Nat. Genet., 22:281
285, 1999.
[23] J. Bota, J. Vandrovcova, P. Forabosco, S. Guelfi, K. DSa, J. Hardy, C. M.
Lewis, M. Ryten, and M. E. Weale. An additional k-means clustering step im-
proves the biological features of WGCNA gene co-expression networks. BMC
Syst. Biol., 11:1291812934, 2017.
[24] P. B. Frandsen, B. Calcott, C. Mayer, and R. Lanfear. Automatic selection
of partitioning schemes for phylogenetic analyses using iterative k-means clus-
tering of site rates. BMC Evol. Biol., 15:1286212879, 2015.
[25] A. K. Jain, A. Topchy, M. H. C. Law, and J. M. Buhmann. Landscape of
clustering algorithms. In Proc. of the IAPR Int. Conf. Pattern Recognit.,
pages 260263, 2004.
REFERENCES
[26] M. C. P. de Souto, I. G. Costa, D. S. A. de Araujo, T. B. Ludemir, and
A. Schliep. Clustering cancer gene expression data: a comparative study.
BMC Bioinformatics, 9:497, 2008.
[27] J. C. Dunn. Well-separated clusters and optimal fuzzy partitions. J. Cybern.,
4:95104, 1974.
[28] P. J. Rousseeuw. Silhouettes: a graphical aid to the interpretation and vali-
dation of cluster analysis. Comput. Appl. Math., 20:5365, 1987.
[29] D. L. Davies and D. W. Bouldin. A cluster separation measure. IEEE Trans.
Pattern Anal. Mach. Intell., 1:224227, 1979.
[30] S. Saitta, B. Raphael, and I. F. C. Smith. A comprehensive validity index for
clustering. Intell. Data Anal., 12:529548, 2008.
[31] J. Mao and A. K. Jain. A self-organizing network for hyper-ellipsoidal clus-
tering (HEC). IEEE Trans. Neural Netw., 7:1629, 1996.
[32] Y. Linde, A. Buzo, and R. Gray. An algorithm for vector quantizer design.
IEEE Trans. Commun., 28:8495, 1980.
[33] H. Kashima, J. Hu, B. Ray, and M. Singh. K-means clustering of proportional
data using L1 distance. In 2008 19th Int. Conf. Pattern Recognit., pages 14,
2008.
[34] B. Scholkopf, A. Smola, and K-R. Muller. Nonlinear component analysis as a
kernel eigenvalue problem. Neural Comput., 10:12991319, 1998.
[35] J. Shawe-Taylor and N. Cristianini. Kernel methods for pattern analysis. Cam-
bridge University Press, 2004.
[36] R. Chitta, R. Jin, T. C. Havens, and A. K. Jain. Approximate kernel K-means:
solution to large scale kernel clustering. In Proc. of the 17th ACM SIGKDD
Int. Conf. Knowl. Disc. Data Mining, pages 895903, 2011.
[37] J. C. Dunn. A fuzzy relative of the ISODATA process and its use in detecting
compact well-separated clusters. J. Cybernet., 3:3257, 1973.
[38] J. C. Bezdek. Pattern recognition with fuzzy objective function algorithms.
Kluwer Academic Publishers, 1981.
[39] L. Kaufman and P. J. Rousseeuw. Finding groups in data: an introduction to
cluster analysis. John Wiley & Sons, 1990.
[40] D. Pelleg and A. Moore. X-means: extending K-means with efficient estima-
tion of the number of clusters. In Proc. of the 17th Int. Conf. Mach. Learn.,
pages 727734, 2000.
[41] D. Aloise, A. Deshpande, P. Hansen, and P. Popat. NP-hardness of Euclidean
sum-of-squares clustering. Mach. Learn., 75:245248, 2009.
REFERENCES
[42] P. Drineas, A. Frieze, R. Kannan, S. Vempala, and V. Vinay. Clustering large
graphs via the singular value decomposition. Mach. Learn., 56:933, 2004.
[43] M. Mahajan, P. Nimbhorkar, and K. Varadarajan. The planar K-means prob-
lem is NP-hard. Theor. Comput. Sci., 442:1321, 2012.
[44] M. Meila. The uniqueness of a good optimum for K-means. In Proc. of the
23rd Int. Conf. Mach. Learn., pages 625632, 2006.
[45] S. T. Hadjitodorov, L. I. Kuncheva, and L. P. Todorova. Moderate diversity
for better cluster ensembles. Inform. Fusion, 7:264275, 2006.
[46] P. J. G. Lisboa, T. A. Etchells, I. H. Jarman, and S. J. Chambers. Finding
reproducible cluster partitions for the K-means algorithm. BMC Bioinformat-
ics, 14:S8, 2013.
[47] M. S. Yang. A survey of fuzzy clustering. Math. Comput. Model., 18:116,
1993.
[48] D. Steinley and M. J. Brusco. Initializing K-means batch clustering: a critical
evaluation of several techniques. J. Classif., 24:99121, 2007.
[49] M. Filippone, F. Camastra, F. Masulli, and S. Rovetta. A survey of kernel
and spectral methods for clustering. Pattern Recognit., 41:176190, 2008.
[50] P. Rai and S. Singh. A survey of clustering techniques. Int. J. Comput. Appl.,
7:15, 2010.
[51] M. E. Celebi, H. A. Kingravi, and P. A. Vela. A comparative study of efficient
initialization methods for the K-means clustering algorithm. Expert Syst.
Appl., 40:200210, 2013.
[52] A. Strehl and J. Ghosh. Cluster ensembles  a knowledge reuse framework for
combining multiple partitions. J. Mach. Learn. Res., 3:583617, 2002.
[53] P. Hore, L. O. Hall, and D.B. Goldof. A scalable framework for cluster ensem-
bles. Pattern Recognit., 42:676688, 2009.
[54] T. Alqurashi and W. Wang. Clustering ensemble method. Int. J. Mach. Learn.
Cybern., 10:12271246, 2018.
[55] D. Steinley. Local optima in K-means clustering: what you dont know may
hurt you. Psychol. Methods, 8:294304, 2003.
[56] P. S. Bradley and U. M. Fayyad. Refining initial points for K-means clustering.
Int. Conf. Mach. Learn., 1:9199, 1998.
[57] V. Faber. Clustering and the continuous K-means algorithm. Los Alamos
Science, 22:138144, 1994.
REFERENCES
[58] D. Arthur and S. Vassilvitskii. k-means++: the advantages of careful seeding.
In Proc. of the 18th Ann. ACM-SIAM Symp. on Discrete Algorithms, pages
10271035, 2007.
[59] B. Bahmani, B. Moseley, A. Vattani, R. Kumar, and S. Vassilvitskii. Scalable
K-means++. In Proc. of the VLDB Endowment, pages 622633, 2012.
[60] O. Bachem, M. Lucic, S. Hamed Hassani, and A. Krause. Fast and prov-
ably good seedings for k-means. In Proc. of the 30th Int. Conf. on Neural
Information Processing Systems, pages 5563, 2016.
[61] G. W. Milligan and P. D. Isaac. The validation of four ultrametric clustering
algorithms. Pattern Recognit., 12:4150, 1980.
[62] T. Su and J. G. Dy. Another look at non-random methods for initializing
K-means clustering. In Proc. of the 16th IEEE Int. Conf. Tools Art. Intell.,
pages 784786, 2004.
[63] D. J. Hand and W. J. Krzanowski. Optimising K-means clustering results with
standard software packages. Comput. Stat. Data Anal., 49:969973, 2005.
[64] G. P. Babu and M. N. Murty. A near-optimal initial seed value selection in
K-means algorithm using a genetic algorithm. Pattern Recognit. Lett., 14:763
769, 1993.
[65] K. Krishna and M. N. Murty. Genetic K-means algorithm. IEEE Trans. Syst.
Man Cybern., 29:433439, 1999.
[66] P. Franti. Genetic algorithm with deterministic crossover for vector quantiza-
tion. Pattern Recognit. Lett., 21:6168, 2000.
[67] L. A. Shalabi, Z. Shaaban, and B. Kasasbeh. Data mining: a preprocessing
engine. J. Comput. Sci., 2:735739, 2006.
[68] J-S. Zhang and Y-W. Leung. Robust clustering by pruning outliers. IEEE
Trans. Sys. Man Cybern., 33:983999, 2003.
[69] V. Hautamaki, S. Cherednichenko, I. Karkkainen, T. Kinnunen, and P. Franti.
Improving K-means by outlier removal. In Scandinavian Conf. Image Anal.,
pages 978987, 2005.
[70] M. B. Al-Zoubi. An effective clustering-based approach for outlier detection.
Eur. J. Sci. Res., 28:310316, 2009.
[71] M. F. Jiang, S. S. Tseng, and C. M. Su. Two-phase clustering process for
outliers detection. Pattern Recognit. Lett., 22:691700, 2001.
[72] V. N. Karthikeyani and K. Thangavel. Impact of normalization in distributed
K-means clustering. Int. J. Soft Comput., 4:168172, 2009.
[73] G. W. Milligan and M. C. Cooper. A study of standardization of variables in
cluster analysis. J. Classif., 5:181204, 1988.
REFERENCES
[74] I. B. Mohamad and D. Usman. Standardization and its effects on K-means
clustering algorithm. Res. J. Appl. Sci. Eng. Tech., 6:32993303, 2013.
[75] M. J. Brusco. Clustering binary data in the presence of masking variables.
Psychol. Methods, 9:510523, 2004.
[76] D. Steinley. Profiling local optima in K-means clustering: developing a diag-
nostic technique. Psychol. Methods, 11:178192, 2006.
[77] P. Franti and S. Sieranoja. K-means properties on six clustering benchmark
datasets. Appl. Intell., 48:47434759, 2018.
[78] R. E. Bellman. Adaptive control processes: a guided tour. Princeton University
Press, 1961.
[79] K. Beyer, J. Goldstein, R. Ramakrishnan, and U. Shaft. When is nearest
neighbor meaningful? Int. Conf. Database Theory, pages 217235, 1999.
[80] D. Steinley. K-means clustering: a half-century synthesis. Br. J. Math. Stat.
Psychol., 59:134, 2006.
[81] L. Vidman, D. Kallberg, and P. Ryden. Cluster analysis on high dimensional
RNA-seq data with applications to cancer research  an evaluation study.
PLOS One, 14:e0219102, 2019.
[82] D. J. Wales. Energy Landscapes. Cambridge University Press, Cambridge,
2003.
[83] K. Roder, G. Stirnemann, A-C. Dock-Bregeon, D. J. Wales, and S. Pasquali.
Structural transitions in the RNA 7SK 5 hairpin and their effect on HEXIM
binding. Nucleic Acids Res., 48:373389, 2019.
[84] S. Xiao, D. J. Sharpe, D. Chakraborty, and D. J. Wales. Energy landscapes
and hybridization pathways for DNA hexamer duplexes. Phys. Chem. Lett.,
10:67716779, 2019.
[85] D. Chakraborty, Y. Chebaro, and D. J. Wales. A multifunnel energy landscape
encodes the competing -helix and -hairpin conformations for a designed
peptide. Phys. Chem. Chem. Phys., 22:13591370, 2020.
[86] D. F. Burke, R. G. Mantell, C. E. Pitt, and D. J. Wales. Energy landscape
for the membrane fusion pathway in influenza A hemagglutinin from discrete
path sampling. Phys. Chem. Chem. Phys., 22:13591370, 2020.
[87] C. L. Vaillant, S. C. Althorpe, and D. J. Wales. Path integral energy landscapes
for water clusters. J. Chem. Theory Comput., 15:3342, 2018.
[88] T. Xu, X. Bin, S. R. Kirk, D. J. Wales, and S. Jenkins. Flip rearrangement in
the water pentamer: analysis of electronic structure. Int. J. Quantum Chem.,
120:e26124, 2019.
REFERENCES
[89] A. Banerjee and D. J. Wales. Fragility and correlated dynamics in supercooled
liquids. J. Chem. Phys., 153:124501, 2020.
[90] A. J. Ballard, J. D. Stevenson, R. Das, and D. J. Wales. Energy landscapes
for a machine learning application to series data. J. Chem. Phys., 144:124119,
2016.
[91] R. Das and D. J. Wales. Machine learning prediction for classification of
outcomes in local minimisation. Chem. Phys. Lett., 667:158164, 2017.
[92] A. J. Ballard, R. Das, S. Martiniani, D. Mehta, L. Sagun, J. D. Stevenson,
and D. J. Wales. Energy landscapes for machine learning. Phys. Chem. Chem.
Phys., 19:1258512603, 2017.
[93] R. A. Fisher. The use of multiple measurements in taxonomic problems. Ann.
Eugen., 7:179188, 1936.
[94] I. W. Evett and E. J. Spiehler. Rule induction in forensic science. Knowl.
Based Sys., pages 152160, 1989.
[95] D. Dua and C. Graff. UCI machine learning repository, 2017.
[96] R. Maitra and V. Melynkov. Simulating data to study performance of finite
mixture modeling and clustering algorithms. J. Comput. Graph. Stat., 19:354
376, 2010.
[97] M. Inaba, N. Katoh, and H. Imai. Applications of weighted Voronoi diagrams
and randomization to variance-based k-clustering. In Proc. of the 10th Ann.
Symp. on Comput. Geometry, pages 332339, 1994.
[98] J. N. Murrell and K. J. Laidler. Symmetries of activated complexes. Trans.
Faraday Soc., 64:371377, 1968.
[99] L. J. Munro and D. J. Wales. Defect migration in crystalline silicon. Phys.
Rev. B, 59:39693980, 1999.
[100] G. Henkelman and H. Jonsson. A dimer method for finding saddle points
on high dimensional potential surfaces using only first derivatives. J. Chem.
Phys., 111:70107022, 1999.
[101] G. Henkelman, B. P. Uberuaga, and H. Jonsson. A climbing image nudged
elastic band method for finding saddle points and minimum energy paths. J.
Chem. Phys., 113:99019904, 2000.
[102] B. Peters, A. Heyden, A. T. Bell, and A. Chakraborty. A growing string
method for determining transition states: comparison to the nudged elastic
band and string methods. J. Chem. Phys., 120:78777886, 2004.
[103] B. G. Levine, J. D. Coe, and T. J. Martnez. Optimizing conical intersections
without derivative coupling vectors: application to multistate multireference
second-order perturbation theory (MS-CASPT2). J. Phys. Chem. B, 112:405
413, 2008.
REFERENCES
[104] C. G. Broyden. The convergence of a class of double-rank minimization algo-
rithms 1. General considerations. J. Inst. Math. Appl., 6:7690, 1970.
[105] R. Fletcher. A new approach to variable metric algorithms. Comput. J.,
13:317322, 1970.
[106] D. Goldfarb. A family of variable-metric methods derived by variational
means. Math. Comput., 24:2326, 1970.
[107] D. F. Shanno. Conditioning of quasi-Newton methods for function minimiza-
tion. Math. Comput., 24:647656, 1970.
[108] J. Nocedal. Updating quasi-Newton matrices with limited storage. Math.
Comput., 35:773782, 1980.
[109] D. C. Liu and J. Nocedal. On the limited memory BFGS method for large
scale optimization. Math. Program., 45:503528, 1989.
[110] D. J. Wales. Energy landscapes: calculating pathways and rates. Int. Rev.
Phys. Chem., 25:237282, 2006.
[111] F. Noe and S. Fischer. Transition networks for modelling the kinetics of con-
formational change in macromolecules. Curr. Opin. Struct. Biol., 18:154162,
2008.
[112] J. M. Carr and D. J. Wales. Refined kinetic transition networks for the GB1
hairpin peptide. Phys. Chem. Chem. Phys., 11:33413354, 2009.
[113] D. Prada-Gracia, J. Gomez-Gardenes, P. Echenique, and F. Falo. Explor-
ing the free energy landscape: from dynamics to networks and back. PLoS
Comput. Biol., 5:e1000415, 2009.
[114] K. Roder and D. J. Wales. Energy landscapes for the aggregation of A1742.
J. Am. Chem. Soc., 140:40184027, 2018.
[115] O. M. Becker and M. Karplus. The topology of multidimensional potential
energy surfaces: theory and application to peptide structure and kinetics. J.
Chem. Phys., 106:14951517, 1997.
[116] D. J. Wales, M. A. Miller, and T. R. Walsh. Archetypal energy landscapes.
Nature, 394:758760, 1998.
[117] N. D. Socci, J. N. Onuchic, and P. G. Wolynes. Protein folding mechanisms
and the multidimensional folding funnel. Proteins, 32:136158, 1998.
[118] J. P. K. Doye, M. A. Miller, and D. J. Wales. The double-funnel energy
landscape of the 38-atom Lennard-Jones cluster. J. Chem. Phys., 110:6896
6906, 1999.
[119] Y. Chebaro, A. J. Ballard, D. Chakraborty, and D. J. Wales. Intrinsically
disordered energy landscapes. Sci. Rep., 5:10386, 2015.
REFERENCES
[120] K Roder, J. A. Joseph, B. E. Husic, and D. J. Wales. Energy landscapes for
proteins: from single funnels to multifunctional systems. Adv. Theory Simul.,
2:1800175, 2019.
[121] V. K. de Souza and D. J. Wales. Energy landscapes for diffusion: analysis of
cage-breaking processes. J. Chem. Phys., 129:164507, 2008.
[122] J. P. K. Doye and D. J. Wales. On potential energy surfaces and relaxation
to the global minimum. J. Chem. Phys., 105:84288445, 1996.
[123] D. J. Wales. Decoding the energy landscape: extracting structure, dynamics
and thermodynamics. Phil. Trans. R. Soc. A, 370:28772899, 2012.
[124] S. P. Niblett, V. K. de Souza, J. D. Stevenson, and D. J. Wales. Dynamics of
a molecular glass former: energy landscapes for diffusion in ortho-terphenyl.
J. Chem. Phys., 145:024505, 2016.
[125] V. K. de Souza and D. J. Wales. The potential energy landscape for crys-
tallisation of a Lennard-Jones fluid. J. Stat. Mech. Theory Exp., 2016:074001,
2016.
[126] R. D. Luce and A. D. Perry. A method of matrix analysis of group structure.
Psychometrika, 14:95116, 1949.
[127] M. Bastian, S. Heymann, and J. Matheiu. Gephi: an open source software
for exploring and manipulating networks. In Third Int. Conf. on Weblogs and
Social Media, 2009.
[128] M. Jacomy, T. Venturini, S. Heymann, and M. Bastian. ForceAtlas2, a con-
tinuous graph layout algorithm for handy network visualization designed for
the Gephi software. PLoS One, 9:e98679, 2014.
[129] D. J. Wales. Decoding heat capacity features from the energy landscape. Phys.
Rev. E, 95:030105, 2017.
[130] M. R. Hoare and J. McInnes. Statistical mechanics and morphology of very
small atomic clusters. Faraday Discuss. Chem. Soc., 61:1224, 1976.
[131] M. R. Hoare. Structure and dynamics of simple microclusters. Adv. Chem.
Phys., 40:49135, 1979.
[132] D. J. Wales. Coexistence in small inert gas clusters. Mol. Phys., 78:151171,
1993.
[133] J. P. K. Doye and D. J. Wales. Calculation of thermodynamic properties of
small Lennard-Jones clusters incorporating anharmonicity. J. Chem. Phys.,
102:96599672, 1995.
[134] C. E. Shannon. A mathematical theory of communication. Bell Syst. Tech.
J., 27:379423, 1948.
REFERENCES
[135] V. K. de Souza, J. D. Stevenson, S. P. Niblett, J. D. Farrell, and D. J. Wales.
Defining and quantifying frustration in the energy landscape: applications to
atomic and molecular clusters, biomolecules, jammed and glassy systems. J.
Chem. Phys., 146:124103, 2017.
[136] J. D. Bryngelson, J. N. Onuchic, N. D. Socci, and P. G. Wolynes. Funnels,
pathways, and the energy landscape of protein folding: a synthesis. Proteins,
21:167195, 1995.
[137] J. N. Onuchic, Z. Luthey-Schulten, and P. G. Wolynes. Theory of protein
folding: the energy landscape perspective. Ann. Rev. Phys. Chem., 48:545
600, 1997.
[138] H. Eyring. The activated complex and the absolute rate of chemical reactions.
Chem. Rev., 17:6577, 1935.
[139] M. G. Evans and M. Polanyi. Some applications of the transition state method
to the calculation of reaction velocities, especially in solution. Trans. Faraday
Soc., 31:875894, 1935.
[140] D. J. Wales. Some further applications of discrete path sampling to cluster
isomerization. Mol. Phys., 102:891908, 2004.
[141] K. P. N. Murthy and K. W. Kehr. Mean first-passage time of random walks
on a random lattice. Phys. Rev. A, 40:20822087, 1989.
[142] M. Raykin. First-passage probability of a random walk on a disordered one-
dimensional lattice. J. Phys. A: Math. Gen., 26:449466, 1993.
[143] E. W. Dijkstra. A note on two problems in connexion with graphs. Numerische
Math., 1:269271, 1959.
[144] S. A. Trygubenko and D. J. Wales. Kinetic analysis of discrete path sampling
stationary point databases. Mol. Phys., 104:14971507, 2006.
[145] S. A. Trygubenko and D. J. Wales. Graph transformation method for calcu-
lating waiting times in Markov chains. J. Chem. Phys., 124:234110, 2006.
[146] D. J. Wales. Calculating rate constants and committor probabilities for tran-
sition networks by graph transformation. J. Chem. Phys., 130:204111204118,
2009.
[147] T. Lange, V. Roth, M. L. Braun, and J. M. Buhmann. Stability-based valida-
tion of clustering solutions. Neural Comput., 16:12991323, 2004.
[148] P. Jaccard. Nouvelles recherches sur la distribution florale. Bull. Soc. Vandoise
des Sci. Nat., 44:223270, 1908.
[149] W. M. Rand. Objective criteria for the evaluation of clustering methods. J.
Am. Stat. Assoc., 66:846850, 1971.
REFERENCES
[150] L. Hubert and P. Arabie. Comparing partitions. J. Classif., 2:193218, 1985.
[151] E. B. Fowlkes and C. L. Mallows. A method for comparing two hierarchical
clusterings. J. Am. Stat. Assoc., 78:553569, 1983.
[152] S. van Dongen. Performance criteria for graph clustering and Markov cluster
experiments. Tech. Rep. INS-R0012, 2000.
[153] M. Meila. Comparing clusterings  an information based distance. J. Multivar.
Anal., 98:873895, 2007.
[154] F. Cazals, D. Mazauric, R. Tetley, and R. Watrigant. Comparing two clus-
terings using matchings between clusters of clusters. J. Exp. Algorithmics,
24:141, 2019.
[155] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel,
M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn:
machine learning in Python. J. Mach. Learn. Res., 12:28252830, 2011.
[156] J. N. Onuchic, N. D. Socci, Z. Luthey-Schulten, and P. G. Wolynes. Protein
folding funnels: the nature of the transition state ensemble. Fold. Des., 1:441
450, 1996.
[157] K. A. Dill, D. O. V. Alonso, and K. Hutchinson. Thermal stabilities of globular
proteins. Biochemistry, 28:54395449, 1989.
[158] P. G. Wolynes, Z. Luthey-Schulten, and J. N. Onuchic. Fast-folding experi-
ments and the topography of protein folding energy landscapes. Chem. Biol.,
3:425432, 1996.
[159] C. M. Dobson. Protein folding and misfolding. Nature, 426:884890, 2003.
[160] S. Z. Selim and K. Alsultan. A simulated annealing algorithm for the clustering
problem. Pattern Recognit., 24:10031008, 1991.
[161] U. Maulik and S. Bandyopadhyay. Genetic algorithm-based clustering tech-
nique. Pattern Recognit., 33:14551465, 2000.
[162] T. Niknam and B. Amiri. An efficient hybrid approach based on PSO, ACO
and k-means for cluster analysis. Appl. Soft Comput., 10:183197, 2010.
[163] H. Xie, L. Zhang, C. P. Lim, Y. Yu, C. Liu, H. Liu, and J. Walters. Improving
K-means clustering with enhanced firefly algorithms. Appl. Soft Comput.,
84:105763105785, 2019.
[164] F. Cazals, T. Dreyfus, D. Mazauric, C-A. Roth, and C. H. Robert. Conforma-
tional ensembles and sampled energy landscapes: Analysis and comparison.
J. Comput. Chem., 36:12131231, 2015.
REFERENCES
[165] F. Sciortino, W. Kob, and P. Tartaglia. Thermodynamics of supercooled liq-
uids in the inherent-structure formalism: a case study. J. Phys. Condens.
Matter, 12:65256534, 2000.
[166] T. V. Bogdan, D. J. Wales, and F. Calvo. Equilibrium thermodynamics from
basin-sampling. J. Chem. Phys., 124:044102, 2006.
[167] D. J. Wales. Surveying a complex potential energy landscape: overcoming
broken ergodicity using basin-sampling. Chem. Phys. Lett., 584:19, 2013.
[168] W. Forst. Theory of Unimolecular Reactions. Academic Press, New York,
1973.
[169] K. J. Laidler. Chemical Kinetics. Harper & Row, New York, 1987.
[170] G. S. Hammond. A correlation of reaction rates. J. Am. Chem. Soc., 77:334
338, 1955.
[171] D. J. Wales. A microscopic basis for the global appearance of energy land-
scapes. Science, 293:20672069, 2001.
[172] T. V. Bogdan and D. J. Wales. New results for phase transitions from catas-
trophe theory. J. Chem. Phys., 120:1109011099, 2004.
[173] J. P. K. Doye. Network topology of a potential energy landscape: a static
scale-free network. Phys. Rev. Lett., 88:238701, 2002.
[174] J. P. K. Doye and C. P. Massen. Characterizing the network topology of the
energy landscapes of atomic clusters. J. Chem. Phys., 122:084105, 2005.
