Between Integrals and Optima:
New Methods for Scalable
Machine Learning
Chris J. Maddison
Exeter College
University of Oxford
A thesis submitted for the degree of
Doctor of Philosophy
Trinity 2020
To Maia.
Acknowledgements
The work in this thesis reflects the support of an entire community.
Foremost in this group are my supervisors, Arnaud Doucet and Yee Whye
Teh, and my mentors, Danny Tarlow and Ilya Sutskever. They provided
me with opportunities, shared with me their ideas, and inspired me by
their relentless vision and drive. I would also like to thank Geoffrey
Hinton, David Silver, and Tom Minka for their leadership and their support.
Many parts of this thesis reflect collaborative work. I would like to thank
Andriy Mnih, for defending the baselines; to thank Dieterich Lawson and
George Tucker, for their excitement and many long hours; and to thank
Daniel Paulin, for our year-long adventure into the depths of optimization.
This work is made stronger by the participation of many voices.
Many people provided thoughts, comments, and critique throughout the
research process. I would like to thank Jimmy Ba, David Balduzzi, Gabriel
Barth-Maron, Lars Buesing, Stefano Favarro, Patrick Rebeschini, Daniel
J. Rezende, Sushant Sachdeva, and Theophane Weber.
My work was supported by a DeepMind Graduate Scholarship, a Postgraduate
Scholarship from the Natural Sciences and Engineering Research Council
of Canada, and the Open Phil AI Fellowship. In particular, I would
like to thank DeepMind for inviting me to participate in their vibrant
research community, and Daniel Dewey at Open Philanthropy for his work
in building a space for fellowship.
Finally, for friendship in research and in life, I would like to thank Ryan
Adams, Noam Brown, George Dahl, Emily Denton, Jessica Forde, Marta
Garnelo, Erin Grant, Roger Grosse, Frauke Harms, Tamir Hazan, James
Martens, Emile Mathieu, Robert Nishihara, Aditi Raghunathan, Jasper
Snoek, Ioan Stefanovici, Kevin Swersky, and Wojciech Zaremba.
Abstract
The success of machine learning is due in part to the effectiveness of
scalable computational methods, like stochastic gradient descent or Monte
Carlo methods, that undergird learning algorithms. This thesis contributes
four new scalable methods for distinct problems that arise in machine
learning. It introduces a new method for gradient estimation in discrete
variable models, a new objective for maximum likelihood learning in the
presence of latent variables, and two new gradient-based differentiable
optimization methods. Although quite different, these contributions address
distinct, critical parts of a typical machine learning workflow. Furthermore,
each contribution is inspired by an interplay between the numerical problems
of optimization and integration, an interplay that forms the central theme
of this thesis.
Contents
1 Introduction 1
1.1 Integrals and Optima . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.2 Gradient Estimation and the Gumbel max trick . . . . . . . . . . . . 2
1.3 Maximum Likliehood via Variational Objectives . . . . . . . . . . . . 5
1.4 Gradient-based Optimization and Momentum . . . . . . . . . . . . . 6
1.5 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
2 The Concrete Distribution: A Continuous Relaxation of Discrete
Random Variables 10
3 Filtering Variational Objectives 24
4 Hamiltonian Descent 38
4.1 Abstract . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
4.2 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
4.2.1 Notation and Convex Analysis Review . . . . . . . . . . . . . 42
4.2.2 Related Literature . . . . . . . . . . . . . . . . . . . . . . . . 43
4.3 Continuous Dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
4.3.1 Hamiltonian Systems . . . . . . . . . . . . . . . . . . . . . . . 45
4.3.2 Continuously Descending the Hamiltonian . . . . . . . . . . . 45
4.3.3 Continuous Hamiltonian Descent on Convex Functions . . . . 47
4.3.4 Partial Lower Bounds . . . . . . . . . . . . . . . . . . . . . . . 53
4.4 Optimization Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . 56
4.4.1 Implicit Method . . . . . . . . . . . . . . . . . . . . . . . . . . 56
4.4.2 First Explicit Method, with Analysis via the Hessian of f . . . 58
4.4.3 Second Explicit Method, with Analysis via the Hessian of k . 59
4.4.4 First Explicit Method on Non-Convex f . . . . . . . . . . . . 62
4.5 Kinetic Maps for Functions with Power Behavior . . . . . . . . . . . 63
4.5.1 Power Kinetic Energies . . . . . . . . . . . . . . . . . . . . . . 63
4.5.2 Matching power kinetic k with assumptions on f . . . . . . 67
4.5.3 Matching relativistic kinetic k with assumptions on f . . . . 72
4.6 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78
5 Dual Space Preconditioning for Gradient Descent 81
5.1 Abstract . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
5.2 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
5.3 Related literature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84
5.4 Convex analysis background . . . . . . . . . . . . . . . . . . . . . . . 85
5.4.1 Convex conjugate and Legendre functions . . . . . . . . . . . 85
5.4.2 Relative smoothness and relative strong convexity . . . . . . . 87
5.5 Analysis of the dual preconditioned scheme . . . . . . . . . . . . . . . 90
5.5.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90
5.5.2 Relative conditions in the dual space . . . . . . . . . . . . . . 91
5.5.3 Convergence rates under dual relative smoothness . . . . . . . 95
5.6 Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99
5.6.1 Exponential Penalty Functions . . . . . . . . . . . . . . . . . . 99
5.6.2 p-norm Regression . . . . . . . . . . . . . . . . . . . . . . . . 103
5.7 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108
5.7.1 Special cases and related methods . . . . . . . . . . . . . . . . 108
5.7.2 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
6 Conclusions 113
A Appendix to Hamiltonian Descent 115
Ap.1 Proofs for convergence of continuous systems . . . . . . . . . . . . . . 115
Ap.2 Proofs for partial lower bounds . . . . . . . . . . . . . . . . . . . . . 117
Ap.3 Proofs of convergence for discrete systems . . . . . . . . . . . . . . . 125
Ap.3.1 Implicit Method . . . . . . . . . . . . . . . . . . . . . . . . . . 125
Ap.3.2 First Explicit Method . . . . . . . . . . . . . . . . . . . . . . 131
Ap.3.3 Second Explicit Method . . . . . . . . . . . . . . . . . . . . . 135
Ap.3.4 Explicit Method on Non-Convex f . . . . . . . . . . . . . . . 140
Ap.4 Proofs for power kinetic energies . . . . . . . . . . . . . . . . . . . . . 141
Bibliography 157
Chapter 1
Introduction
Machine learning enjoyed a series of high-profile achievements in, e.g., image classification
[140], speech recognition [118], automatic machine translation [269], speech synthesis
[253], and games playing [235]. These successes are enabled partly by software
packages that automate learning [245, 1, 200, 43]; partly by the use of expressive
black-box models and predictive accuracy as the metric for validating them [45,
147]; and partly by the unreasonable effectiveness of computational methods, like
stochastic gradient descent [37] or Monte Carlo methods [220], that undergird learning
algorithms.
This thesis contributes to the computational methodology of machine learning
through new methods for gradient estimation in discrete variable models (Chapter
2), new objectives for maximum likelihood learning in the presence of latent variables
(Chapter 3), and gradient-based differentiable optimization (Chapters 4 & 5). Although
quite distinct as methods, the problems of computing derivatives, defining objective
functions, and optimizing are all critical parts of a typical machine learning workflow.
Furthermore, each new method is thematically unified by an interplay between the
numerical problems of optimization and integration.
1.1 Integrals and Optima
Optimization and integration hide at the core of many machine learning methods.
Consider for example the maximum likelihood objective [79], which is one most
widely-used for fitting machine learning models [178]. In all but the simplest models,
finding the maximum likelihood parameters requires an algorithm that searches over
the likelihood surface for an extreme point [86]. This is the problem of optimization,
and is usually accomplished using some kind of method for numerical optimization
[38]. Maximum likelihood in many graphical models requires computing marginal
distributions, which are essentially counts or volumes of high-dimensional spaces.
This is the problem of integration, and it is a well-studied topic for graphical models
[260].
At first, these two problems do not seem related. Integrals measure the volume
under a functions surface taken over its domain. Integration is fundamentally an
accumulative process; small changes anywhere in a functions surface will generally
lead to different values for its integral. Optima are the extreme points of a functions
surface. Many changes to the surface would not change the location of its optima,
and for differentiable functions, these optima are completely characterized by local
properties. Thus, optima appear to be mostly local phenomena, while integrals appear
to be mostly global ones.
Yet, this apparent distinction is only superficial. On the one hand, in some settings
integration can be reduced to optimization through a variational approach [128].
On the other hand, some optimization algorithms may be interpreted as numerical
integrators of ordinary differential equations [206, 238]. Consider a third specific
example in the problem of sampling from probability distributions, which is most
often used as a subroutine in the estimation of integrals via Monte Carlo estimation.
Markov chain Monte Carlo methods [46] are frequently used to solve this problem,
and they can be shown to reduce a real-valued objective function over probability
distributions [129, 221], i.e., to be performing optimization in the space of measures.
This view of sampling as optimization can be used to analyze rates [169] and design
methods [26, 262]. Each of the three contributions of this thesis are seeded by
an observation of this type. Chapter 2 is based on the Gumbel max trick, which
reduces random variate simulation to optimization. Chapter 3 is based on variational
inference, which reduces posterior inference to optimization. Chapters 4 and 5 are
inspired by continuous-time perspectives on optimization, which cast optimization
methods as numerical integrators of differential equations. We introduce each of
these contexts in turn.
1.2 Gradient Estimation and the Gumbel max trick
Software packages for automatic differentiation [245, 1, 200, 43] have been a driving
force in the deep learning revolution [147], because they liberate practitioners to
explore quickly in the space of models and objectives, without requiring separate
derivative derivations. Therefore, expanding the class of objectives or models that
are amenable to automatic differentiation can have an outsized impact on practice.
An important class of objectives in machine learning are integrals whose density
is the parameter we wish to optimize over. A typical example are the objectives of
reinforcement learning, in which the density in the integrand represents a distribution
over actions and the aim is to maximize the expected reward over the action distribution
[242]. Another example, which we will cover in the next section, are variational
objectives over approximate posteriors in models with latent variables [137, 218]. For
objectives over densities in the integrand, gradient estimators [177, 10] may be used in
lieu of exact gradients under the Robbins-Monro conditions [219]. In this section we
introduce two popular generic strategies for designing gradient estimators in machine
learning. We return to the interplay between optimization and integration at the end
of the section with the Gumbel max trick.
REINFORCE [264] and reparameterization tricks [137, 218] are two of the most
widely-used gradient estimation techniques in machine learning. Let ()Rn be a
family of probability distribution on Rd (equipped with the Borel -algebra B(Rd))
parameterized by   Rn. Assume that each  admits a density p with respect to
the Lebesgue measure dx. Suppose we are given a differentiable objective function
f : Rd  R, then our goal is to approximate the following vector of first partial
derivatives for X  :
 E[f(X)] = 
f(x)p(x) dx
f(x)p(x) dx
. (1.1)
The REINFORCE estimator [264], also known as the score function estimator [98], is
based on the following observation that (assuming that we may exchange derivatives
and integrals):
f(x)p(x) dx =
f(x)p(x) dx =
f(x) log p(x)p(x) dx, (1.2)
for p differentiable and positive a.e. This suggests the REINFORCE estimator,
f(X)  log p(X) for X  , which may be used a Monte Carlo estimator of
the true gradient. A sufficient condition guaranteeing that one can exchange the
derivative and integral in REINFORCE is that p be continuously differentiable in 
(a.e. in x), f  L2, and |p(x)/i|  Mi(x) (a.e. in x) for some Mi  L2 and all
  Rn [10, Prop. 3.5 of Chap. 7].
Reparameterization tricks [137, 218, 225], also known as infinitesimal perturbation
analysis [96, 52], are based on a change of variables. We consider a simple example
to introduce the idea. Let X  N (, I) =  be a Gaussian random variable with
mean   Rd. The observation driving the reparameterization tricks is the following
(assuming again that we may exchange derivatives and integrals):
 E[f(X)] =  E[f(+ )] = E[ [f(+ )]], (1.3)
where   N (0, I). This suggests the reparameterization gradient estimator, [f(+ )].
More generally, these estimators can be derived when there exists a differentiable
change of variables g : Rn  Rm  Rd such that X d= g(, ) for some random
variable  with range Rm. The specific requirements on g are that f(g(, )) is
differentiable in  (a.e. in ) and that f(g(, )) satisfies a local Lipschitz condition
for some Lipschitz constant in L2 [10, Prop. 2.3, Chap. 7]. The rediscovery of
reparameterization gradient estimators in machine learning [137, 218] revolutionized
probabilistic methods for deep learning, because these estimators are simple to implement
and generally lower variance than the REINFORCE estimator [88].
Reparameterization tricks are not always readily available. Consider when the
base measure of (1.1) is a counting measure and not Lebesgue. The REINFORCE
estimators are typically applicable, but these tend to have prohibitively high variance.
Control variates can be used to reduce the variance of REINFORCE [e.g., 172], but
one may prefer a reparameterization estimator. Discrete distributions admit a generic
change of variables (reparameterization), known in machine learning as the Gumbel
max trick [164], with a long history in random choice theory as the Plackett-Luce
model [205, 273, 158]. Let X   be a discrete random variable whose range
X = {x  {0, 1}n :
i xi = 1} are the vertices of the (n 1)-dimensional simplex. If
p is its mass function and Gi  Gumbel [110] i.i.d., then
= arg max
{xT (G+ log p(x))}. (1.4)
Unfortunately, this reparameterization of X does not obviously get us closer to our
goal of deriving a reparameterization gradient estimator for E[f(X)]. The partial
derivatives in i of the right hand side of (1.4) are 0 almost surely, and the exchange
of derivative and integral cannot be justified.
Nevertheless, (1.4) is the starting point of the first contribution of this thesis:
improved gradient estimators for discrete variable models (Chapter 2). The Gumbel
max trick is also a return to our central theme. (1.4) shows that simulating a
discrete random variable (an integration-type procedure) can be be accomplished by
optimizing a random function (an optimization-type procedure). Indeed, [164, 160]
show that this is not unique to counting measures. One can define a certain random
function whose optimum is distributed according to any given probability distribution,
and generalize the Gumbel max trick to continuous distributions.
1.3 Maximum Likliehood via Variational Objectives
In section 1.2, we considered the problem of computing derivatives. A major application
of the gradient estimators of the previous section occurs in the development of
large, non-linear, black-box statistical models for high dimensional natural data.
For example, [253] designed an autoregressive model of audio waveform data, called
WaveNet. When fit to human speech, WaveNet is capable of simulating a novel,
realistic-sounding utterance in the voice of a given speaker, even famous celebrities.
Collectively called generative modeling, these machine learning techniques seek
to fit some parametric family of distribution to the empirical distribution of a given
dataset [137, 218, 67, 100, 196, 248, 176]. Many of these models incorporate unobserved
latent variables, which poses a considerable challenge for methods like maximum
likelihood estimation. A set of scalable approaches [137, 218, 213], collectively called
variational autoencoders, based on variational inference (VI) [128] and amortized
inference have recently made these models trainable. Although VI has a long history
[32], our introduction will emphasize VI as seen from recent developments in statistical
deep learning. Furthermore, we will see that VI is another example of integration
reducing to optimization.
First, we define a typical maximum likelihood objective for latent variable models.
Denote the observations of a dataset by Y , a realization of an Rd1-valued random
variable. Let us assume that Y is jointly distributed with a random variable X  Rd2
with joint density p(x, y) (with respect to the Lebesgue measure) for some parameter
  Rn. It is typical that p is specified via a parametric prior density p(x) and a
conditional likelihood density p(y|x) parameterized by some parametric function
of x. The goal of maximum likelihood estimation (MLE) [79] is to recover   Rn
that maximizes the marginal log-likelihood,
log p(Y ) = log
p(x, Y ) dx
. (1.5)
In such generality, this model is not particularly new, e.g. mixture models are
captured in this family. It is the expressivity of the likelihood function (generally
computed by some neural network) that make such models suitable for complex
natural data and the likelihood is a point of major innovation, e.g., [55]. The
assumption that the observed data is actually distributed according to this distribution
for some   Rn is essentially never verified, but approaches that approximately
optimize the likelihood (1.5) of these large, non-linear, black-box models tend to
work well in practice, e.g. [142].
Ideally, one would compute derivatives of the objective (1.5) and run an optimization
routine to approximate the MLE. Unfortunately, even computing the derivative is
intractable, owing to the integral inside the logarithm. VI is a strategy for overcoming
this intractability [128, 17]. Let q(x|y) be a conditional density in some parametric
family, defined for all y in the support of p(x, ). The variational lower bound is
defined as the following objective,
L(Y, , ) =
p(x, Y )
q(x|Y )
q(x|Y ) dx  log p(Y ) (1.6)
The bound is tight when q(x|y) is the true conditional p(x|y) under the model
p. Assuming the parametric family of q contains the true posteriors of p, the
joint optimum of (1.6) in  and  is the MLE. The EM algorithm [66] is a classical
algorithm used to optimize (1.6), but the updates of the EM algorithm are generally
too expensive for the models considered in statistical deep learning.
There are two insights behind the variational autoencoder approach to using (1.6)
to perform approximate MLE in deep, non-linear models. The first is that (1.6) is in
the same form as (1.1) and its gradients in ,  may be estimated with the approaches
of section 1.2. The second, is that a single, highly-expressive q can approximate the
true conditional p(x|y) over an entire dataset. The second insight is typically called
amortized inference, because the cost of inference is amortized in the learning of
the parameters . In practice, in deep learning, this objective is then optimized with
some gradient-based optimization routine jointly over  and  [137, 218].
Returning now to the theme of the thesis, note that, when the variational bound
(1.6) is tight, the problem of integrating out the latent variables (computing the
marginal log-likelihood) can be cast as an optimization problem (maximizing over q).
In Chapter 3, we design generalizations of (1.6) specialized for sequential models.
1.4 Gradient-based Optimization and Momentum
In section 1.3 we introduced variational objectives for maximum likelihood estimation,
and in section 1.2 we introduced gradient estimators for such objectives. In this
section, we complete the picture by introducing the basics of gradient-based optimization.
We will consider a simplified scenario. Let f : Rd  R be a convex differentiable
function that is bounded below. We are interested in finding a minimizing argument,
xmin = arg min
f(x). (1.7)
The gradient descent algorithm is one of the simplest algorithms for this problem.
It begins with the observation that the negative gradient (the vector of first partial
derivatives, f(x) = (f(x)/x(n))) points in the direction of greatest instantaneous
decrease on the functions surface. Given some initial point x0  Rd and a step size
1/L > 0, the iterates of gradient descent are given by,
xi+1 = xi f(xi)/L. (1.8)
If the step size is small enough and the function smooth enough, the iterates of this
scheme will iteratively descend the functions surface. Methods like gradient descent,
which only use the first-order derivatives [190, 207, 193], are among the most popular
in machine learning. This is partly due to their ease of implementation and scalability
[130]. One of the first questions to ask about any optimization scheme is, when can
one guarantee its convergence and at what rate? This is the topic of the final two
chapters of this thesis (Chapters 4 & 5).
Smoothness and strong convexity are two typical conditions used to guarantee the
convergence of gradient descent. For smooth convex f , the iterates f(xi)  f(xmin)
of gradient descent converge at a rate of O(1/i). For smooth, strongly convex f , the
iterates f(xi) f(xmin) of gradient descent converge geometrically at a rate of O(i)
for some 0 <  < 1. Smoothness is a condition that guarantees that f grows no faster
than a quadratic, and strong convexity is a condition that guarantees that f grows no
slower than a quadratic. Intuitively, smoothness guarantees a descent in f and strong
convexity guarantees a sufficient descent for fast convergence. See [190, 207, 193] for
these classical results.
Gradient-based optimizers are generally improved by incorporating momentum
into the dynamics of the iterates. Although there is flexibility in how it is implemented,
momentum adds a persistence of motion that improves convergence in directions of
persistent gradient signal. The first introduction of momentum is due to Boris Polyak
[206], and his iteration is given by,
xi+1 = xi  f(xi) + (xi  xi1), (1.9)
where ,  > 0. Note that the only distinction between (1.9) and the gradient method
(1.8) is the last term, which is exactly the term that produces a persistence of motion.
Polyak showed that (1.9) is capable of achieving optimal local convergence rates (an
improvement over the gradient method) on twice-differentiable, smooth, and strongly
convex f . Nesterovs celebrated accelerated gradient algorithms are another form of
momentum, and these achieve optimal global rates for smooth convex f and smooth,
strongly convex f [195].
In the final two chapters of this thesis we ask whether the momentum method
(Chapters 4) and the gradient method (Chapters 5) can be generalized to converge
under conditions that generalize smoothness or strong convexity. As we show, this
generalization is possible, and the inspiration comes from the Monte Carlo literature
(another return to the central theme of the interplay between optiization and integration).
First, consider what happens to the sequence of iterates defined by (1.9) as  0
and  =
(1  ) for 0 <  < 1. The iterates (under reasonable smoothness
conditions) approach solutions of the following ordinary differential equation [233].
xt = pt
pt = f(xt) pt
(1.10)
Very similar differential equations describe dynamics used in the Monte Carlo literature.
In particular, the Hamiltonian Monte Carlo algorithm (HMC) is one of the most
successful Monte Carlo methods for continuous distributions and it uses discretizations
of the Hamiltonian differential equations (below) to propose moves in a Metropolis-
Hastings scheme [185],
xt = k(pt)
pt = f(xt)
(1.11)
where k : Rd  R is generally a convex differentiable function and f is the log-density
of the measure whose integrals we wish to approximate. The dynamics described by
(1.11) are used in physics as a model of frictionless dynamics that preserve energy.
(1.10) describes dynamics exposed to a constant source of friction, which dissipates
energy [165]. This dissipation is important for optimization, because otherwise the
system would fail to converge. Indeed, (1.10) and (1.11) describe the same system
when  = 0 and k(p) = p. Therefore, momentum optimizers may be seen as the
dissipative cousin of the dynamics used in HMC.
Drawing parallels between optimization and Monte Carlo methods can inspire
methodological progress. This is because some questions, more naturally posed in
one literature (Monte Carlo or optimization), may also be relevant for the other. For
example, the function k in (1.11) is a free parameter that a user may design, and recent
work in the HMC literature [149] argued that a sensible choice of k achieves k 
(f)1. This suggests considering following differential equation for optimization,
xt = k(pt)
pt = f(xt) pt
(1.12)
where k : Rd  R is a convex differentiable function designed by the user. The
natural question to ask of (1.12) is, how should k and f relate for the convergence of
discretizations of (1.12) to the minimum of f? This is the starting point of Chapter
4, and we find a rather satisfying answer; k should approximate (f)1 xmin in a
sense made precise by conditions that are similar to smoothness and strong convexity.
In Chapter 5 we simplify these conditions to ones that exactly generalize smoothness
and strong convexity, and we present a version of non-linear preconditioning of the
gradient descent algorithm that exploits them. The conditions that we are arrive
at are closely related to, but distinct from, conditions recently introduced to study
mirror descent [16, 156].
1.5 Overview
This thesis presents four methodological contributions to distinct, complementary
parts of a standard machine learning workflow. It is an integrated thesis formed of
four chapters, each corresponding to a separate paper and presented in the form that
they were published or submitted. Two papers were published at machine learning
conferences, one is in review, and one is in preprint. * indicates joint first authorship.
1. Chris J. Maddison*, Dieterich Lawson*, George Tucker*, Nicolas Heess, Mohammad
Norouzi, Andriy Mnih, Arnaud Doucet, Yee Whye Teh. Filtering Variational
Objectives. In Advances in Neural Information Processing Systems, 2017.
2. Chris J. Maddison, Andriy Mnih, and Yee Whye Teh. The Concrete Distribution:
A Continuous Relaxation of Discrete Random Variables. In International Conference
on Learning Representations, 2017.
3. Chris J. Maddison*, Daniel Paulin*, Yee Whye Teh, Arnaud Doucet. Hamiltonian
Descent Methods. In Preprint, 2018.
4. Chris J. Maddison*, Daniel Paulin*, Yee Whye Teh, Arnaud Doucet. Dual
Space Preconditioning for Gradient Descent. In Review, 2019.
Published as a conference paper at ICLR 2017
THE CONCRETE DISTRIBUTION:
A CONTINUOUS RELAXATION OF
DISCRETE RANDOM VARIABLES
Chris J. Maddison1,2, Andriy Mnih1, & Yee Whye Teh1
1DeepMind, London, United Kingdom
2University of Oxford, Oxford, United Kingdom
cmaddis@stats.ox.ac.uk
ABSTRACT
The reparameterization trick enables optimizing large scale stochastic computa-
tion graphs via gradient descent. The essence of the trick is to refactor each
stochastic node into a differentiable function of its parameters and a random vari-
able with fixed distribution. After refactoring, the gradients of the loss propa-
gated by the chain rule through the graph are low variance unbiased estimators
of the gradients of the expected loss. While many continuous random variables
have such reparameterizations, discrete random variables lack useful reparame-
terizations due to the discontinuous nature of discrete states. In this work we
introduce CONCRETE random variablesCONtinuous relaxations of disCRETE
random variables. The Concrete distribution is a new family of distributions
with closed form densities and a simple reparameterization. Whenever a discrete
stochastic node of a computation graph can be refactored into a one-hot bit rep-
resentation that is treated continuously, Concrete stochastic nodes can be used
with automatic differentiation to produce low-variance biased gradients of objec-
tives (including objectives that depend on the log-probability of latent stochastic
nodes) on the corresponding discrete graph. We demonstrate the effectiveness of
Concrete relaxations on density estimation and structured prediction tasks using
neural networks.
1 INTRODUCTION
Software libraries for automatic differentiation (AD) [1, 245] are enjoying broad use, spurred on by
the success of neural networks on some of the most challenging problems of machine learning. The
dominant mode of development in these libraries is to define a forward parametric computation, in
the form of a directed acyclic graph, that computes the desired objective. If the components of the
graph are differentiable, then a backwards computation for the gradient of the objective can be de-
rived automatically with the chain rule. The ease of use and unreasonable effectiveness of gradient
descent has led to an explosion in the diversity of architectures and objective functions. Thus, ex-
panding the range of useful continuous operations can have an outsized impact on the development
of new models. For example, a topic of recent attention has been the optimization of stochastic
computation graphs from samples of their states. Here, the observation that AD just works when
stochastic nodes1 can be reparameterized into deterministic functions of their parameters and a fixed
noise distribution [137, 218], has liberated researchers in the development of large complex stochas-
tic architectures [106].
Computing with discrete stochastic nodes still poses a significant challenge for AD libraries. Deter-
ministic discreteness can be relaxed and approximated reasonably well with sigmoidal functions or
the softmax [104, 102], but, if a distribution over discrete states is needed, there is no clear solution.
There are well known unbiased estimators for the gradients of the parameters of a discrete stochas-
tic node from samples. While these can be made to work with AD, they involve special casing
1For our purposes a stochastic node of a computation graph is just a random variable whose distribution
depends in some deterministic way on the values of the parent nodes.
Published as a conference paper at ICLR 2017
and defining surrogate objectives [231], and even then they can have high variance. Still, reason-
ing about discrete computation comes naturally to humans, and so, despite the difficulty associated,
many modern architectures incorporate discrete stochasticity [271, 138].
This work is inspired by the observation that many architectures treat discrete nodes continuously,
and gradients rich with counterfactual information are available for each of their possible states.
We introduce a CONtinuous relaxation of disCRETE random variables, CONCRETE for short, which
allow gradients to flow through their states. The Concrete distribution is a new parametric family
of continuous distributions on the simplex with closed form densities. Sampling from the Concrete
distribution is as simple as taking the softmax of logits perturbed by fixed additive noise. This
reparameterization means that Concrete stochastic nodes are quick to implement in a way that just
works with AD. Crucially, every discrete random variable corresponds to the zero temperature
limit of a Concrete one. In this view optimizing an objective over an architecture with discrete
stochastic nodes can be accomplished by gradient descent on the samples of the corresponding
Concrete relaxation. When the objective depends, as in variational inference, on the log-probability
of discrete nodes, the Concrete density is used during training in place of the discrete mass. At test
time, the graph with discrete nodes is evaluated.
The paper is organized as follows. We provide a background on stochastic computation graphs and
their optimization in Section 2. Section 3 reviews a reparameterization for discrete random vari-
ables, introduces the Concrete distribution, and discusses its application as a relaxation. Section 4
reviews related work. In Section 5 we present results on a density estimation task and a structured
prediction task on the MNIST and Omniglot datasets. When comparing the effectiveness of gra-
dients obtained via Concrete relaxations to a state-of-the-art-method (VIMCO) [175], we find that
they are competitiveoccasionally outperforming and occasionally underperformingall the while
being implemented in an AD library without special casing.
2 BACKGROUND
2.1 OPTIMIZING STOCHASTIC COMPUTATION GRAPHS
Stochastic computation graphs (SCGs) provide a formalism for specifying input-output mappings,
potentially stochastic, with learnable parameters using directed acyclic graphs (see [231] for a re-
view). The state of each non-input node in such a graph is obtained from the states of its parent
nodes by either evaluating a deterministic function or sampling from a conditional distribution.
Many training objectives in supervised, unsupervised, and reinforcement learning can be expressed
in terms of SCGs.
To optimize an objective represented as a SCG, we need estimates of its parameter gradients. We will
concentrate on graphs with some stochastic nodes (backpropagation covers the rest). For simplicity,
we restrict our attention to graphs with a single stochastic node X . We can interpret the forward
pass in the graph as first sampling X from the conditional distribution p(x) of the stochastic node
given its parents, then evaluating a deterministic function f(x) at X . We can think of f(X) as a
noisy objective, and we are interested in optimizing its expected value L(, ) = EXp(x)[f(X)]
w.r.t. parameters , .
In general, both the objective and its gradients are intractable. We will side-step this issue by esti-
mating them with samples from p(x). The gradient w.r.t. to the parameters  has the form
L(, ) = EXp(x)[f(X)] = EXp(x)[f(X)] (1)
and can be easily estimated using Monte Carlo sampling:
L(, ) '
f(Xs), (2)
where Xs  p(x) i.i.d. The more challenging task is to compute the gradient w.r.t. the parameters
 of p(x). The expression obtained by differentiating the expected objective,
L(, ) = 
p(x)f(x) dx =
f(x)p(x) dx, (3)
does not have the form of an expectation w.r.t. x and thus does not directly lead to a Monte Carlo
gradient estimator. However, there are two ways of getting around this difficulty which lead to the
two classes of estimators we will now discuss.
Published as a conference paper at ICLR 2017
2.2 SCORE FUNCTION ESTIMATORS
The score function estimator (SFE) [87], also known as the REINFORCE [264] or likelihood-ratio
estimator [98], is based on the identity p(x) = p(x) log p(x), which allows the gradient
in Eq. 3 to be written as an expectation:
L(, ) = EXp(x) [f(X) log p(X)] . (4)
Estimating this expectation using naive Monte Carlo gives the estimator
L(, ) '
s) log p(Xs), (5)
where Xs  p(x) i.i.d. This is a very general estimator that is applicable whenever log p(x)
is differentiable w.r.t. . As it does not require f(x) to be differentiable or even continuous as a
function of x, the SFE can be used with both discrete and continuous random variables.
Though the basic version of the estimator can suffer from high variance, various variance reduction
techniques can be used to make the estimator much more effective [103]. Baselines are the most
important and widely used of these techniques [264]. A number of score function estimators have
been developed in machine learning [198, 213, 172, 247, 109], which differ primarily in the variance
reduction techniques used.
2.3 REPARAMETERIZATION TRICK
In many cases we can sample from p(x) by first sampling Z from some fixed distribution
q(z) and then transforming the sample using some function g(z). For example, a sample
from Normal(, 2) can be obtained by sampling Z from the standard form of the distribution
Normal(0, 1) and then transforming it using g,(Z) =  + Z. This two-stage reformulation of
the sampling process, called the reparameterization trick, allows us to transfer the dependence on 
from p into f by writing f(x) = f(g(z)) for x = g(z), making it possible to reduce the problem
of estimating the gradient w.r.t. parameters of a distribution to the simpler problem of estimating the
gradient w.r.t. parameters of a deterministic function.
Having reparameterized p(x), we can now express the objective as an expectation w.r.t. q(z):
L(, ) = EXp(x)[f(X)] = EZq(z)[f(g(Z))]. (6)
As q(z) does not depend on , we can estimate the gradient w.r.t.  in exactly the same way we
estimated the gradient w.r.t.  in Eq. 1. Assuming differentiability of f(x) w.r.t. x and of g(z)
w.r.t.  and using the chain rule gives
L(, ) = EZq(z)[f(g(Z))] = EZq(z) [f (g(Z))g(Z)] . (7)
The reparameterization trick, introduced in the context of variational inference independently by
[137], [218], and [246], is usually the estimator of choice when it is applicable. For continuous
latent variables which are not directly reparameterizable, new hybrid estimators have also been
developed, by combining partial reparameterizations with score function estimators [225, 179].
2.4 APPLICATION: VARIATIONAL TRAINING OF LATENT VARIABLE MODELS
We will now see how the task of training latent variable models can be formulated in the SCG
framework. Such models assume that each observation x is obtained by first sampling a vector
of latent variables Z from the prior p(z) before sampling the observation itself from p(x | z).
Thus the probability of observation x is p(x) =
z p(z)p(x | z). Maximum likelihood train-
ing of such models is infeasible, because the log-likelihood (LL) objective L() = log p(x) =
logEZp(z)[p(x | Z)] is typically intractable and does not fit into the above framework due to the
expectation being inside the log. The multi-sample variational objective [50],
Lm(, ) = E
Ziq(z|x)
i, x)
q(Zi | x)
. (8)
Published as a conference paper at ICLR 2017
log1log1 log2log2 log3log3
argmaxi{xi}argmaxi{xi}
G1G1 G3G3
(a) Discrete()
exp(xi/)P
i exp(xi/)
exp(xi/)P
i exp(xi/)
log1log1 log2log2 log3log3 G1G1 G3G3
(b) Concrete(, )
Figure 1: Visualization of sampling graphs for 3-ary discrete D  Discrete() and 3-ary Con-
crete X  Concrete(, ). White operations are deterministic, blue are stochastic, rounded are
continuous, square discrete. The top node is an example state; brightness indicates a value in [0,1].
provides a convenient alternative which has precisely the form we considered in Section 2.1. This
approach relies on introducing an auxiliary distribution q(z | x) with its own parameters, which
serves as approximation to the intractable posterior p(z | x). The model is trained by jointly max-
imizing the objective w.r.t. to the parameters of p and q. The number of samples used inside the
objectivem allows trading off the computational cost against the tightness of the bound. Form = 1,
Lm(, ) becomes is the widely used evidence lower bound (ELBO) [120] on log p(x), while for
m > 1, it is known as the importance weighted bound [50].
3 THE CONCRETE DISTRIBUTION
3.1 DISCRETE RANDOM VARIABLES AND THE GUMBEL-MAX TRICK
To motivate the construction of Concrete random variables, we review a method for sampling from
discrete distributions called the Gumbel-Max trick [273, 116, 164]. We restrict ourselves to a rep-
resentation of discrete states as vectors d  {0, 1}n of bits that are one-hot, or
k=1 dk = 1. This
is a flexible representation in a computation graph; to achieve an integral representation take the
inner product of d with (1, . . . , n), and to achieve a point mass representation in Rm takeWd where
W  Rmn.
Consider an unnormalized parameterization (1, . . . , n) where k  (0,) of a discrete distribu-
tion D  Discrete()we can assume that states with 0 probability are excluded. The Gumbel-
Max trick proceeds as follows: sample Uk  Uniform(0, 1) i.i.d. for each k, find k that maximizes
{logk  log( logUk)}, set Dk = 1 and the remaining Di = 0 for i 6= k. Then
P(Dk = 1) =
i=1 i
. (9)
In other words, the sampling of a discrete random variable can be refactored into a deterministic
functioncomponentwise addition followed by argmaxof the parameters logk and fixed dis-
tribution  log( logUk). See Figure 1a for a visualization.
The apparently arbitrary choice of noise gives the trick its name, as  log( logU) has a Gumbel
distribution. This distribution features in extreme value theory [110] where it plays a central role
similar to the Normal distribution: the Gumbel distribution is stable under max operations, and for
some distributions, the order statistics (suitably normalized) of i.i.d. draws approach the Gumbel
in distribution. The Gumbel can also be recognized as a  log-transformed exponential random
variable. So, the correctness of (9) also reduces to a well known result regarding the argmin of
exponential random variables.
3.2 CONCRETE RANDOM VARIABLES
The derivative of the argmax is 0 everywhere except at the boundary of state changes, where it is
undefined. For this reason the Gumbel-Max trick is not a suitable reparameterization for use in SCGs
with AD. Here we introduce the Concrete distribution motivated by considering a graph, which is
the same as Figure 1a up to a continuous relaxation of the argmax computation, see Figure 1b. This
will ultimately allow the optimization of parameters k via gradients.
Published as a conference paper at ICLR 2017
(a)  = 0 (b)  = 1/2 (c)  = 1 (d)  = 2
Figure 2: A discrete distribution with unnormalized probabilities (1, 2, 3) = (2, 0.5, 1) and
three corresponding Concrete densities at increasing temperatures . Each triangle represents the
set of points (y1, y2, y3) in the simplex 2 = {(y1, y2, y3) | yk  (0, 1), y1 + y2 + y3 = 1}. For
 = 0 the size of white circles represents the mass assigned to each vertex of the simplex under the
discrete distribution. For   {2, 1, 0.5} the intensity of the shading represents the value of p,(y).
The argmax computation returns states on the vertices of the simplex n1 = {x  Rn | xk 
[0, 1],
k=1 xk = 1}. The idea behind Concrete random variables is to relax the state of a discrete
variable from the vertices into the interior where it is a random probability vectora vector of
numbers between 0 and 1 that sum to 1. To sample a Concrete random variable X  n1 at
temperature   (0,) with parameters k  (0,), sample Gk  Gumbel i.i.d. and set
exp((logk +Gk)/)n
i=1 exp((logi +Gi)/)
. (10)
The softmax computation of (10) smoothly approaches the discrete argmax computation as   0
while preserving the relative order of the Gumbels logk + Gk. So, imagine making a series of
forward passes on the graphs of Figure 1. Both graphs return a stochastic value for each forward
pass, but for smaller temperatures the outputs of Figure 1b become more discrete and eventually
indistinguishable from a typical forward pass of Figure 1a.
The distribution ofX sampled via (10) has a closed form density on the simplex. Because there may
be other ways to sample a Concrete random variable, we take the density to be its definition.
Definition 1 (Concrete Random Variables). Let   (0,)n and   (0,). X  n1 has a
Concrete distribution X  Concrete(, ) with location  and temperature , if its density is:
p,(x) = (n 1) n1
i=1 ix
. (11)
Proposition 1 lists a few properties of the Concrete distribution. (a) is confirmation that our def-
inition corresponds to the sampling routine (10). (b) confirms that rounding a Concrete random
variable results in the discrete random variable whose distribution is described by the logits logk,
(c) confirms that taking the zero temperature limit of a Concrete random variable is the same as
rounding. Finally, (d) is a convexity result on the density. We prove these results in Appendix A.
Proposition 1 (Some Properties of Concrete Random Variables). Let X  Concrete(, ) with
location parameters   (0,)n and temperature   (0,), then
(a) (Reparameterization) If Gk  Gumbel i.i.d., then Xk
exp((logk+Gk)/)
exp((logi+Gi)/)
(b) (Rounding) P (Xk > Xi for i 6= k) = k/(
i=1 i),
(c) (Zero temperature) P (lim0Xk = 1) = k/(
i=1 i),
(d) (Convex eventually) If   (n 1)1, then p,(x) is log-convex in x.
The binary case of the Gumbel-Max trick simplifies to passing additive noise through a step func-
tion. The corresponding Concrete relaxation is implemented by passing additive noise through a
sigmoidsee Figure 3. We cover this more thoroughly in Appendix B.
Published as a conference paper at ICLR 2017
(a)  = 0 (b)  = 1/2 (c)  = 1 (d)  = 2
Figure 3: A visualization of the binary special case. (a) shows the discrete trick, which works by
passing a noisy logit through the unit step function. (b), (c), (d) show Concrete relaxations; the
horizontal blue densities show the density of the input distribution and the vertical densities show
the corresponding Binary Concrete density on (0, 1) for varying .
3.3 CONCRETE RELAXATIONS
Concrete random variables may have some intrinsic value, but we investigate them simply as surro-
gates for optimizing a SCG with discrete nodes. When it is computationally feasible to integrate over
the discreteness, that will always be a better choice. Thus, we consider the use case of optimizing a
large graph with discrete stochastic nodes from samples.
First, we outline our proposal for how to use Concrete relaxations by considering a variational
autoencoder with a single discrete latent variable. Let Pa(d) be the mass function of some n-
dimensional one-hot discrete random variable with unnormalized probabilities a  (0,)n and
p(x|d) some distribution over a data point x given d  (0, 1)n one-hot. The generative model is
then p,a(x, d) = p(x|d)Pa(d). Let Q(d|x) be an approximating posterior over d  (0, 1)n one-
hot whose unnormalized probabilities (x)  (0,)n depend on x. All together the variational
lowerbound we care about stochastically optimizing is
L1(, a, ) = E
DQ(d|x)
p(x|D)Pa(D)
Q(D|x)
, (12)
with respect to , a, and any parameters of . First, we relax the stochastic computation
D  Discrete((x)) by replacing D with a Concrete random variable Z  Concrete((x), 1)
with density q,1(z|x). Simply replacing every instance of D with Z in Eq. 12 will re-
sult in a non-interpretable objective, which does not necessarily lowerbound log p(x), because
EZq,1 (a|x)[ logQ(Z|x)/Pa(Z)] is not a KL divergence. Thus we propose relaxing the
terms Pa(d) and Q(d|x) to reflect the true sampling distribution. Thus, the relaxed objective is:
L1(, a, )
relax E
Zq,1 (z|x)
p(x|Z)pa,2(Z)
q,1(Z|x)
where pa,2(z) is a Concrete density with location a and temperature 2. At test time we evaluate
the discrete lowerbound L1(, a, ).
Thus, the basic paradigm we propose is the following: during training replace every discrete node
with a Concrete node at some fixed temperature (or with an annealing schedule). The graphs are
identical up to the softmax / argmax computations, so the parameters of the relaxed graph and
discrete graph are the same. When an objective depends on the log-probability of discrete variables
in the SCG, as the variational lowerbound does, we propose that the log-probability terms are also
relaxed to represent the true distribution of the relaxed node. At test time the original discrete loss
is evaluated. This is possible, because the discretization of any Concrete distribution has a closed
form mass function, and the relaxation of any discrete distribution into a Concrete distribution has a
closed form density. This is not always possible. For example, the multinomial probit modelthe
Gumbel-Max trick with Gaussians replacing Gumbelsdoes not have a closed form mass.
The success of Concrete relaxations will depend on the choice of temperature during training. It is
important that the relaxed nodes are not able to represent a precise real valued mode in the interior
of the simplex as in Figure 2d. If this is the case, it is possible for the relaxed random variable to
communicate much more than log2(n) bits of information about its  parameters. This might lead
the relaxation to prefer the interior of the simplex to the vertices, and as a result there will be a large
integrality gap in the overall performance of the discrete graph. Therefore Proposition 1 (d) is a
conservative guideline for generic n-ary Concrete relaxations; at temperatures lower than (n 1)1
we are guaranteed not to have any modes in the interior for any   (0,)n. Ultimately the best
choice of  and the performance of the relaxation for any specific n will be an empirical question.
Published as a conference paper at ICLR 2017
4 RELATED WORK
Perhaps the most common distribution over the simplex is the Dirichlet with density p(x) n
k=1 x
k on x  n1. The Dirichlet can be characterized by strong independence proper-
ties, and a great deal of work has been done to generalize it [61, 6, 215, 77]. Of note is the Logistic
Normal distribution [11], which can be simulated by taking the softmax of n  1 normal random
variables and an nth logit that is deterministically zero. The Logistic Normal is an important dis-
tribution, because it can effectively model correlations within the simplex. To our knowledge the
Concrete distribution does not fall completely into any family of distributions previously described.
For   1 the Concrete is in a class of normalized infinitely divisible distributions (S. Favaro,
personal communication), and the results of [77] apply.
The idea of using a softmax of Gumbels as a relaxation for a discrete random variable was concur-
rently considered by [124], where it was called the Gumbel-Softmax. They do not use the density
in the relaxed objective, opting instead to compute all aspects of the graph, including discrete log-
probability computations, with the relaxed stochastic state of the graph. In the case of variational
inference, this relaxed objective is not a lower bound on the marginal likelihood of the observations,
and care needs to be taken when optimizing it. The idea of using sigmoidal functions with additive
input noise to approximate discreteness is also not a new idea. [85] introduced nonlinear Gaussian
units which computed their activation by passing Gaussian noise with the mean and variance spec-
ified by the input to the unit through a nonlinearity, such as the logistic function. [227] binarized
real-valued codes of an autoencoder by adding (Gaussian) noise to the logits before passing them
through the logistic function. Most recently, to avoid the difficulty associated with likelihood-ratio
methods [138] relaxed the discrete sampling operation by sampling a vector of Gaussians instead
and passing those through a softmax.
There is another family of gradient estimators that have been studied in the context of training neural
networks with discrete units. These are usually collected under the umbrella of straight-through
estimators [212]. The basic idea they use is passing forward discrete values, but taking gradients
through the expected value. They have good empirical performance, but have not been shown to be
the estimators of any loss function. This is in contrast to gradients from Concrete relaxations, which
are biased with respect to the discrete graph, but unbiased with respect to the continuous one.
5 EXPERIMENTS
5.1 PROTOCOL
The aim of our experiments was to evaluate the effectiveness of the gradients of Concrete relax-
ations for optimizing SCGs with discrete nodes. We considered the tasks in [175]: structured output
prediction and density estimation. Both tasks are difficult optimization problems involving fitting
probability distributions with hundreds of latent discrete nodes. We compared the performance
of Concrete reparameterizations to two state-of-the-art score function estimators: VIMCO [175] for
optimizing the multisample variational objective (m > 1) and NVIL [172] for optimizing the single-
sample one (m = 1). We performed the experiments using the MNIST and Omniglot datasets.
These are datasets of 28  28 images of handwritten digits (MNIST) or letters (Omniglot). For
MNIST we used the fixed binarization of [228] and the standard 50,000/10,000/10,000 split into
training/validation/testing sets. For Omniglot we sampled a fixed binarization and used the stan-
dard 24,345/8,070 split into training/testing sets. We report the negative log-likelihood (NLL) of the
discrete graph on the test data as the performance metric.
All of our models were neural networks with layers of n-ary discrete stochastic nodes with values on
the corners of the hypercube {1, 1}log2(n). The distributions were parameterized by n real values
logk  R, which we took to be the logits of a discrete random variable D  Discrete() with
n states. Model descriptions are of the form (200V200H784V), read from left to right. This
describes the order of conditional sampling, again from left to right, with each integer representing
the number of stochastic units in a layer. The letters V and H represent observed and latent variables,
respectively. If the leftmost layer is H, then it was sampled unconditionally from some parameters.
Conditioning functions are described by {, }, where  means a linear function of the previous
layer and  means a non-linear function. A layer of these units is simply the concatenation
Published as a conference paper at ICLR 2017
MNIST NLL Omniglot NLL
binary
model
Test Train Test Train
m Concrete VIMCO Concrete VIMCO Concrete VIMCO Concrete VIMCO
(200H
 784V)
1 107.3 104.4 107.5 104.2 118.7 115.7 117.0 112.2
5 104.9 101.9 104.9 101.5 118.0 113.5 115.8 110.8
50 104.3 98.8 104.2 98.3 118.9 113.0 115.8 110.0
(200H
 200H
 784V)
1 102.1 92.9 102.3 91.7 116.3 109.2 114.4 104.8
5 99.9 91.7 100.0 90.8 116.0 107.5 113.5 103.6
50 99.5 90.7 99.4 89.7 117.0 108.1 113.9 103.6
(200H
784V)
1 92.1 93.8 91.2 91.5 108.4 116.4 103.6 110.3
5 89.5 91.4 88.1 88.6 107.5 118.2 101.4 102.3
50 88.5 89.3 86.4 86.5 108.1 116.0 100.5 100.8
(200H
200H
784V)
1 87.9 88.4 86.5 85.8 105.9 111.7 100.2 105.7
5 86.3 86.4 84.1 82.5 105.8 108.2 98.6 101.1
50 85.7 85.5 83.1 81.8 106.8 113.2 97.5 95.2
Table 1: Density estimation with binary latent variables. When m = 1, VIMCO stands for NVIL.
of some number of independent nodes whose parameters are determined as a function the previous
layer. For example a 240 binary layer is a factored distribution over the {1, 1}240 hypercube.
Whereas a 240 8-ary layer can be seen as a distribution over the same hypercube where each of the
80 triples of units are sampled independently from an 8 way discrete distribution over {1, 1}3. All
models were initialized with the heuristic of [97] and optimized using Adam [135]. All temperatures
were fixed throughout training. Appendix C for hyperparameter details.
5.2 DENSITY ESTIMATION
Density estimation, or generative modelling, is the problem of fitting the distribution of data. We
took the latent variable approach described in Section 2.4 and trained the models by optimizing the
variational objective Lm(, ) given by Eq. 8 averaged uniformly over minibatches of data points
x. Both our generative models p(z, x) and variational distributions q(z | x) were parameterized
with neural networks as described above. We trained models with Lm(, ) for m  {1, 5, 50} and
approximated the NLL with L50,000(, ) averaged uniformly over the whole dataset.
The results are shown in Table 1. In general, VIMCO outperformed Concrete relaxations for linear
models and Concrete relaxations outperformed VIMCO for non-linear models. We also tested the
effectiveness of Concrete relaxations on generative models with n-ary layers on the L5(, ) ob-
jective. The best 4-ary model achieved test/train NLL 86.7/83.3, the best 8-ary achieved 87.4/84.6
with Concrete relaxations. The relatively poor performance of the 8-ary model may be because
moving from 4 to 8 results in a more difficult objective without much added capacity. As a control
we trained n-ary models using logistic normals as relaxations of discrete distributions (with retuned
temperature hyperparameters). Because the discrete zero temperature limit of logistic Normals is a
multinomial probit whose mass function is not known, we evaluated the discrete model by sampling
from the discrete distribution parameterized by the logits learned during training. The best 4-ary
model achieved test/train NLL of 88.7/85.0, the best 8-ary model achieved 89.1/85.1.
5.3 STRUCTURED OUTPUT PREDICTION
Structured output prediction is concerned with modelling the high-dimensional distribution of the
observation given a context and can be seen as conditional density estimation. We considered the
task of predicting the bottom half x1 of an image of an MNIST digit given its top half x2, as
introduced by [212]. We followed [212] in using a model with layers of discrete stochastic units
between the context and the observation. Conditioned on the top half x2 the network samples from
a distribution p(z | x2) over layers of stochastic units z then predicts x1 by sampling from a
distribution p(x1 | z). The training objective for a single pair (x1, x2) is
LSPm (, ) = E
Zip(z|x2)
p(x1 | Zi)
Published as a conference paper at ICLR 2017
binary
model
Test NLL Train NLL
m Concrete VIMCO Concrete VIMCO
(392V240H
240H392V)
1 58.5 61.4 54.2 59.3
5 54.3 54.5 49.2 52.7
50 53.4 51.8 48.2 49.6
(392V240H
240H240H
392V)
1 56.3 59.7 51.6 58.4
5 52.7 53.5 46.9 51.6
50 52.0 50.2 45.9 47.9
100 101 102
prefers interiorprefers {1, 1}
Continuous
Discrete
Figure 4: Results for structured prediction on MNIST comparing Concrete relaxations to VIMCO.
When m = 1 VIMCO stands for NVIL. The plot on the right shows the objective (lower is better)
for the continuous and discrete graph trained at temperatures . In the shaded region, units prefer to
communicate real values in the interior of (1, 1) and the discretization suffers an integrality gap.
This objective is a special case ofLm(, ) (Eq. 8) where we use the prior p(z|x2) as the variational
distribution. Thus, the objective is a lower bound on log p,(x1 | x2).
We trained the models by optimizing LSPm (, ) for m  {1, 5, 50} averaged uniformly over mini-
batches and evaluated them by computingLSP100(, ) averaged uniformly over the entire dataset. The
results are shown in Figure 4. Concrete relaxations more uniformly outperformed VIMCO in this
instance. We also trained n-ary (392V240H240H240H392V) models on the LSP1 (, ) objec-
tive using the best temperature hyperparameters from density estimation. 4-ary achieved a test/train
NLL of 55.4/46.0 and 8-ary achieved 54.7/44.8. As opposed to density estimation, increasing arity
uniformly improved the models. We also investigated the hypothesis that for higher temperatures
Concrete relaxations might prefer the interior of the interval to the boundary points {1, 1}. Figure
4 was generated with binary (392V240H240H240H392V) model trained on LSP1 (, ).
6 CONCLUSION
We introduced the Concrete distribution, a continuous relaxation of discrete random variables. The
Concrete distribution is a new distribution on the simplex with a closed form density parameterized
by a vector of positive location parameters and a positive temperature. Crucially, the zero temper-
ature limit of every Concrete distribution corresponds to a discrete distribution, and any discrete
distribution can be seen as the discretization of a Concrete one. The application we considered was
training stochastic computation graphs with discrete stochastic nodes. The gradients of Concrete
relaxations are biased with respect to the original discrete objective, but they are low variance un-
biased estimators of a continuous surrogate objective. We showed in a series of experiments that
stochastic nodes with Concrete distributions can be used effectively to optimize the parameters of
a stochastic computation graph with discrete stochastic nodes. We did not find that annealing or
automatically tuning the temperature was important for these experiments, but it remains interesting
and possibly valuable future work.
ACKNOWLEDGMENTS
We thank Jimmy Ba for the excitement and ideas in the early days, Stefano Favarro for some analysis
of the distribution. We also thank Gabriel Barth-Maron and Roger Grosse.
A PROOF OF PROPOSITION 1
Let X  Concrete(, ) with location parameters   (0,)n and temperature   (0,).
1. Let Gk  Gumbel i.i.d., consider
exp((logk +Gk)/)n
i=1 exp((logi +Gi)/)
Let Zk = logk +Gk, which has density
k exp(zk) exp(k exp(zk))
Published as a conference paper at ICLR 2017
We will consider the invertible transformation
F (z1, . . . , zn) = (y1, . . . , yn1, c)
where
yk = exp(zk/)c
exp(zi/)
F1(y1, . . . , yn1, c) = ((log y1 + log c), . . . , (log yn1 + log c), (log yn + log c))
where yn = 1
i=1 yi. This has Jacobian

y11 0 0 0 . . . 0 c
0 y12 0 0 . . . 0 c
0 0 y13 0 . . . 0 c
y1n y1n y1n y1n . . . y1n c1

by adding yi/yn times each of the top n1 rows to the bottom row we see that this Jacobian
has the same determinant as

y11 0 0 0 . . . 0 c
0 y12 0 0 . . . 0 c
0 0 y13 0 . . . 0 c
0 0 0 0 . . . 0 (cyn)

and thus the determinant is equal to
i=1 yi
all together we have the density
k=1 k exp( log yk   log c) exp(k exp( log yk   log c))
i=1 yi
with r = log c change of variables we have density
k=1 k exp(r) exp(k exp( log yk  r))n
i=1 y
k=1 kn
i=1 y
exp(nr) exp(
i exp( log yi  r)) =
letting  = log(
n=1 ky
k=1 kn
i=1 y
i exp()
exp(nr + ) exp( exp(r + )) =
integrating out r
k=1 kn
i=1 y
i exp()
exp(n+ )(n)
k=1 kn
i=1 y
(exp(n)(n)) =
(n 1)n1
k=1 ky
n=1 ky
Thus Y
= X .
Published as a conference paper at ICLR 2017
2. Follows directly from (a) and the Gumbel-Max trick [160].
3. Follows directly from (a) and the Gumbel-Max trick [160].
4. Let   (n 1)1. The density of X can be rewritten as
p,(x) 
i=1 iy
(n1)1
i=1 i
j 6=i y
Thus, the log density is up to an additive constant C
log p,(x) =
((n 1) 1) log yk  n log
j 6=k
If   (n  1)1, then the first n terms are convex, because  log is convex. For the
last term,  log is convex and non-increasing and
j 6=k y
j is concave for   (n 1)1.
Thus, their composition is convex. The sum of convex terms is convex, finishing the proof.
B THE BINARY SPECIAL CASE
Bernoulli random variables are an important special case of discrete distributions taking states in
{0, 1}. Here we consider the binary special case of the Gumbel-Max trick from Figure 1a along
with the corresponding Concrete relaxation.
Let D  Discrete() for   (0,)2 be a two state discrete random variable on {0, 1}2 such that
D1 +D2 = 1, parameterized as in Figure 1a by 1, 2 > 0:
P(D1 = 1) =
1 + 2
The distribution is degenerate, because D1 = 1  D2. Therefore we consider just D1. Under
the Gumbel-Max reparameterization, the event that D1 = 1 is the event that {G1 + log1 >
G2 + log2} where Gk  Gumbel i.i.d. The difference of two Gumbels is a Logistic distribution
G1 G2  Logistic, which can be sampled in the following way, G1 G2
= logU  log(1 U)
where U  Uniform(0, 1). So, if  = 1/2, then we have
P(D1 = 1) = P(G1 + log1 > G2 + log2) = P(logU  log(1 U) + log > 0) (15)
Thus, D1
= H(log+ logU  log(1 U)), where H is the unit step function.
Correspondingly, we can consider the Binary Concrete relaxation that results from this process.
As in the n-ary case, we consider the sampling routine for a Binary Concrete random variable
X  (0, 1) first. To sample X , sample L  Logistic and set
1 + exp((log+ L)/) (16)
We define the Binary Concrete random variable X by its density on the unit interval.
Definition 2 (Binary Concrete Random Variables). Let   (0,) and   (0,). X  (0, 1)
has a Binary Concrete distribution X  BinConcrete(, ) with location  and temperature , if
its density is:
p,(x) =
x1(1 x)1
(x + (1 x))2 . (17)
We state without proof the special case of Proposition 1 for Binary Concrete distributions
Proposition 2 (Some Properties of Binary Concrete Random Variables). Let X 
BinConcrete(, ) with location parameter   (0,) and temperature   (0,), then
Published as a conference paper at ICLR 2017
(a) (Reparameterization) If L  Logistic, then X d= 1
1+exp((log+L)/) ,
(b) (Rounding) P (X > 0.5) = /(1 + ),
(c) (Zero temperature) P (lim0X = 1) = /(1 + ),
(d) (Convex eventually) If   1, then p,(x) is log-convex in x.
We can generalize the binary circuit beyond Logistic random variables. Consider an arbitrary ran-
dom variable X with infinite support on R. If  : R [0, 1] is the CDF of X , then
P(H(X) = 1) = 1 (0)
If we want this to have a Bernoulli distribution with probability /(1 +), then we should solve the
equation
1 (0) = 
1 + 
This gives (0) = 1/(1 +), which can be accomplished by relocating the random variable Y with
CDF  to be X = Y  1(1/(1 + )).
C EXPERIMENTAL DETAILS
The basic model architectures we considered are exactly analogous to those in [50] with Con-
crete/discrete random variables replacing Gaussians.
C.1  VS 
The conditioning functions we used were either linear or non-linear. Non-linear consisted of two
tanh layers of the same size as the preceding stochastic layer in the computation graph.
C.2 n-ARY LAYERS
All our models are neural networks with layers of n-ary discrete stochastic nodes with log2(n)-
dimensional states on the corners of the hypercube {1, 1}log2(n). For a generic n-ary node
sampling proceeds as follows. Sample a n-ary discrete random variable D  Discrete() for
  (0,)n. If C is the log2(n)nmatrix, which lists the corners of the hypercube {1, 1}log2(n)
as columns, then we took Y = CD as downstream computation on D. The corresponding Con-
crete relaxation is to take X  Concrete(, ) for some fixed temperature   (0,) and set
Y = CX . For the binary case, this amounts to simply sampling U  Uniform(0, 1) and taking
Y = 2H(logU  log(1  U) + log)  1. The corresponding Binary Concrete relaxation is
Y = 2((logU  log(1 U) + log)/) 1.
C.3 BIAS INITIALIZATION
All biases were initialized to 0 with the exception of the biases in the prior decoder distribution over
the 784 or 392 observed units. These were initialized to the logit of the base rate averaged over the
respective dataset (MNIST or Omniglot).
C.4 CENTERING
We also found it beneficial to center the layers of the inference network during training. The activity
in (1, 1)d of each stochastic layer was centered during training by maintaining a exponentially
decaying average with rate 0.9 over minibatches. This running average was subtracted from the
activity of the layer before it was updated. Gradients did not flow throw this computation, so it
simply amounted to a dynamic offset. The averages were not updated during the evaluation.
Published as a conference paper at ICLR 2017
C.5 HYPERPARAMETER SELECTION
All models were initialized with the heuristic of [97] and optimized using Adam [135] with pa-
rameters 1 = 0.9, 2 = 0.999 for 107 steps on minibatches of size 64. Hyperparameters were
selected on the MNIST dataset by grid search taking the values that performed best on the val-
idation set. Learning rates were chosen from {104, 3  104, 103} and weight decay from
{0, 102, 101, 1}. Two sets of hyperparameters were selected, one for linear models and one for
non-linear models. The linear models hyperparameters were selected with the 200H200H784V
density model on the L5(, ) objective. The non-linear models hyperparameters were selected
with the 200H200H784V density model on the L5(, ) objective. For density estimation, the
Concrete relaxation hyperparameters were (weight decay = 0, learning rate = 3  104) for linear
and (weight decay = 0, learning rate = 104) for non-linear. For structured prediction Concrete
relaxations used (weight decay = 103, learning rate = 3  104).
In addition to tuning learning rate and weight decay, we tuned temperatures for the Concrete relax-
ations on the density estimation task. We found it valuable to have different values for the prior and
posterior distributions. In particular, for binary we found that (prior 2 = 1/2, posterior 1 = 2/3)
was best, for 4-ary we found (prior 2 = 2/3, posterior 1 = 1) was best, and (prior 2 = 2/5,
posterior 1 = 2/3) for 8-ary. No temperature annealing was used. For structured prediction we
used just the corresponding posterior 1 as the temperature for the whole graph, as there was no
variational posterior.
We performed early stopping when training with the score function estimators (VIMCO/NVIL) as
they were much more prone to overfitting.
Statement of Authorship for joint/multi-authored papers for PGR thesis 
To appear at the end of each thesis chapter submitted as an article/paper 
The statement shall describe the candidates and co-authors independent research contributions in the thesis 
publications. For each publication there should exist a complete statement that is to be filled out and signed by the 
candidate and supervisor (only required where there isnt already a statement of contribution within the paper 
itself). 
Title of Paper 
The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables 
Publication Status 
  Published                                   Accepted for Publication 
  Submitted for Publication          Unpublished and unsubmitted work written 
                         in a manuscript style 
Publication Details 
Chris J. Maddison, Andriy Mnih, and Yee Whye Teh. The Concrete Distribution: 
A Continuous Relaxation of Discrete Random Variables. In International 
Conference on Learning Representations, 2017. 
Student Confirmation 
Student Name: 
Chris J. Maddison 
Contribution to the 
Paper 
 I proposed the idea, and did all of the analysis in the paper. 
 I wrote all of the experimental code with the exception of the VIMCO and NVIL 
baselines, which were written by Andriy Mnih. 
 I wrote the majority of the paper, with the exception of the background section, 
which was written by Andriy Mnih. 
 All authors contributed to the development of the paper through discussions 
and ideas, and all authors reviewed the final draft. 
Signature  
05 May 2020 
Supervisor Confirmation 
By signing the Statement of Authorship, you are certifying that the candidate made a substantial contribution to the 
publication, and that the description described above is accurate. 
Supervisor name and title: 
Professor Arnaud Doucet 
Supervisor comments 
I agree that the candidate has made a substantial contribution to the publication. 
Signature 
05 May 2020 
This completed form should be included in the thesis, at the end of the relevant chapter. 
Filtering Variational Objectives
Chris J. Maddison1,3,*, Dieterich Lawson,2,* George Tucker2,*
Nicolas Heess1, Mohammad Norouzi2, Andriy Mnih1, Arnaud Doucet3, Yee Whye Teh1
1DeepMind, 2Google Brain, 3University of Oxford
{cmaddis, dieterichl, gjt}@google.com
Abstract
When used as a surrogate objective for maximum likelihood estimation in latent
variable models, the evidence lower bound (ELBO) produces state-of-the-art results.
Inspired by this, we consider the extension of the ELBO to a family of lower bounds
defined by a particle filters estimator of the marginal likelihood, the filtering
variational objectives (FIVOs). FIVOs take the same arguments as the ELBO,
but can exploit a models sequential structure to form tighter bounds. We present
results that relate the tightness of FIVOs bound to the variance of the particle filters
estimator by considering the generic case of bounds defined as log-transformed
likelihood estimators. Experimentally, we show that training with FIVO results
in substantial improvements over training the same model architecture with the
ELBO on sequential data.
1 Introduction
Learning in statistical models via gradient descent is straightforward when the objective function
and its gradients are tractable. In the presence of latent variables, however, many objectives become
intractable. For neural generative models with latent variables, there are currently a few dominant
approaches: optimizing lower bounds on the marginal log-likelihood [137, 218], restricting to a class
of invertible models [67], or using likelihood-free methods [100, 196, 248, 176]. In this work, we
focus on the first approach and introduce filtering variational objectives (FIVOs), a tractable family
of objectives for maximum likelihood estimation (MLE) in latent variable models with sequential
structure.
Specifically, let x denote an observation of an X -valued random variable. We assume that the
process generating x involves an unobserved Z-valued random variable z with joint density p(x, z)
in some family P . The goal of MLE is to recover p  P that maximizes the marginal log-likelihood,
log p(x) = log
p(x, z) dz
1 . The difficulty in carrying out this optimization is that the log-
likelihood function is defined via a generally intractable integral. To circumvent marginalization, a
common approach [137, 218] is to optimize a variational lower bound on the marginal log-likelihood
[128, 17]. The evidence lower bound L(x, p, q) (ELBO) is the most common such bound and is
defined by a variational posterior distribution q(z|x) whose support includes ps,
L(x, p, q) = E
q(z|x)
p(x, z)
q(z|x)
= log p(x)KL(q(z|x)  p(z|x))  log p(x) . (1)
L(x, p, q) lower-bounds the marginal log-likelihood for any choice of q, and the bound is tight when
q is the true posterior p(z|x). Thus, the joint optimum of L(x, p, q) in p and q is the MLE. In practice,
*Equal contribution.
1We reuse p to denote the conditionals and marginals of the joint density.
31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.
it is common to restrict q to a tractable family of distributions (e.g., a factored distribution) and to
jointly optimize the ELBO over p and q with stochastic gradient ascent [137, 218, 213, 141]. Because
of the KL penalty from q to p, optimizing (1) under these assumptions tends to force ps posterior to
satisfy the factorizing assumptions of the variational family which reduces the capacity of the model
p. One strategy for addressing this is to decouple the tightness of the bound from the quality of q.
For example, [50] observed that Eq. (1) can be interpreted as the log of an unnormalized importance
weight with the proposal given by q, and that using N samples from the same proposal produces a
tighter bound, known as the importance weighted auto-encoder bound, or IWAE.
Indeed, it follows from Jensens inequality that the log of any unbiased positive Monte Carlo estimator
of the marginal likelihood results in a lower bound that can be optimized for MLE. The filtering
variational objectives (FIVOs) build on this idea by treating the log of a particle filters likelihood
estimator as an objective function. Following [174], we call objectives defined as log-transformed
likelihood estimators Monte Carlo objectives (MCOs). In this work, we show that the tightness
of an MCO scales like the relative variance of the estimator from which it is constructed. It is
well-known that the variance of a particle filters likelihood estimator scales more favourably than
simple importance sampling for models with sequential structure [53, 25]. Thus, FIVO can potentially
form a much tighter bound on the marginal log-likelihood than IWAE.
The main contributions of this work are introducing filtering variational objectives and a more
careful study of Monte Carlo objectives. In Section 2, we review maximum likelihood estimation via
maximizing the ELBO. In Section 3, we study Monte Carlo objectives and provide some of their basic
properties. We define filtering variational objectives in Section 4, discuss details of their optimization,
and present a sharpness result. Finally, we cover related work and present experiments showing that
sequential models trained with FIVO outperform models trained with ELBO or IWAE in practice.
2 Background
We briefly review techniques for optimizing the ELBO as a surrogate MLE objective. We restrict our
focus to latent variable models in which the model p(x, z) factors into tractable conditionals p(z)
and p(x|z) that are parameterized differentiably by parameters . MLE in these models is then the
problem of optimizing log p(x) in . The expectation-maximization (EM) algorithm is an approach
to this problem which can be seen as coordinate ascent, fully maximizing L(x, p, q) alternately in q
and  at each iteration [66, 268, 186]. Yet, EM rarely applies in general, because maximizing over q
for a fixed  corresponds to a generally intractable inference problem.
Instead, an approach with mild assumptions on the model is to perform gradient ascent following a
Monte Carlo estimator of the ELBOs gradient [120, 213]. We assume that q is taken from a family of
distributions parameterized differentiably by parameters . We can follow an unbiased estimator of the
ELBOs gradient by sampling z  q(z|x) and updating the parameters by  = + log p(x, z)
and  =  + (log p(x, z)  log q(z|x)) log q(z|x), where the gradients are computed
conditional on the sample z and  is a learning rate. Such estimators follow the ELBOs gradient in
expectation, but variance reduction techniques are usually necessary [213, 173, 174].
A lower variance gradient estimator can be derived if q is a reparameterizable distribution [137,
218, 88]. Reparameterizable distributions are those that can be simulated by sampling from a
distribution   d(), which does not depend on , and then applying a deterministic transformation
z = f(x, ). When p, q, and f are differentiable, an unbiased estimator of the ELBO gradient
consists of sampling  and updating the parameter by (, ) = (, )+(,)(log p(x, f(x, ))
log q(f(x, )|x)). Given , the gradients of the sampling process can flow through z = f(x, ).
Unfortunately, when the variational family of q is restricted, following gradients of
KL(q(z|x)  p(z|x)) tends to reduce the capacity of the model p to match the assumptions
of the variational family. This KL penalty can be removed by considering generalizations of the
ELBO whose tightness can be controlled by means other than the closenesss of p and q, e.g., [50].
We consider this in the next section.
3 Monte Carlo Objectives (MCOs)
Monte Carlo objectives (MCOs) [174] generalize the ELBO to objectives defined by taking the log
of a positive, unbiased estimator of the marginal likelihood. The key property of MCOs is that
they are lower bounds on the marginal log-likelihood, and thus can be used for MLE. Motivated
by the previous section, we present results on the convergence of generic MCOs to the marginal
log-likelihood and show that the tightness of an MCO is closely related to the variance of the estimator
that defines it.
One can verify that the ELBO is a lower bound by using the concavity of log and Jensens inequality,
q(z|x)
p(x, z)
q(z|x)
 log
p(x, z)
q(z|x) q(z|x) dz = log p(x). (2)
This argument only relies only on unbiasedness of p(x, z)/q(z|x) when z  q(z|x). Thus, we
can generalize this by considering any unbiased marginal likelihood estimator pN (x) and treating
E[log pN (x)] as an objective function over models p. HereN  N indexes the amount of computation
needed to simulate pN (x), e.g., the number of samples or particles.
Definition 1. Monte Carlo Objectives. Let pN (x) be an unbiased positive estimator of p(x),
E[pN (x)] = p(x), then the Monte Carlo objective LN(x, p) over p  P defined by pN (x) is
LN(x, p) = E[log pN (x)] (3)
For example, the ELBO is constructed from a single unnormalized importance weight p(x) =
p(x, z)/q(z|x). The IWAE bound [50] takes pN (x) to be N averaged i.i.d. importance weights,
LIWAEN (x, p, q) = E
q(zi|x)
p(x, zi)
q(zi|x)
We consider additional examples in the Appendix. To avoid notational clutter, we omit the arguments
to an MCO, e.g., the observations x or model p, when the default arguments are clear from context.
Whether we can compute stochastic gradients of LN efficiently depends on the specific form of the
estimator and the underlying random variables that define it.
Many likelihood estimators pN (x) converge to p(x) almost surely as N   (known as strong
consistency). The advantage of a consistent estimator is that its MCO can be driven towards log p(x)
by increasing N . We present sufficient conditions for this convergence and a description of the rate:
Proposition 1. Properties of Monte Carlo Objectives. Let LN(x, p) be a Monte Carlo objective
defined by an unbiased positive estimator pN (x) of p(x). Then,
(a) (Bound) LN(x, p)  log p(x).
(b) (Consistency) If log pN (x) is uniformly integrable (see Appendix for definition) and pN (x)
is strongly consistent, then LN(x, p) log p(x) as N .
(c) (Asymptotic Bias) Let g(N) = E[(pN (x)  p(x))6] be the 6th central moment. If the 1st
inverse moment is bounded, lim supN E[pN (x)
1] <, then
log p(x) LN(x, p) =
pN (x)
g(N)). (5)
Proof. See the Appendix for the proof and a sufficient condition for controlling the first inverse
moment when pN (x) is the average of i.i.d. random variables.
In some cases, convergence of the bound to log p(x) is monotonic, e.g., IWAE [50], but this is not
true in general. The relative variance of estimators, var(pN (x)/p(x)), tends to be well studied, so
property (c) gives us a tool for comparing the convergence rate of distinct MCOs. For example,
[53, 25] study marginal likelihood estimators defined by particle filters and find that the relative
variance of these estimators scales favorably in comparison to naive importance sampling. This
suggests that a particle filters MCO, introduced in the next section, will generally be a tighter bound
than IWAE.
Algorithm 1 Simulating LFIVON (x1:T , p, q)
1: FIVO(x1:T , p, q,N):
2: {wi0}Ni=1 = {1/N}Ni=1
3: for t  {1, . . . , T} do
4: for i  {1, . . . , N} do
5: zit  qt(zt|x1:t, zi1:t1)
6: zi1:t = CONCAT(z
1:t1, z
7: pt =
i=1 w
t1t(z
8: pN (x1:t) = pN (x1:t1)pt
9: {wit}Ni=1 = {wit1t(zi1:t)/pt}Ni=1
10: if resampling criteria satisfied by {wit}Ni=1 then
11: {wit, zi1:t}Ni=1 = RSAMP({wit, zi1:t}Ni=1)
12: return log pN (x1:T )
13: RSAMP({wi, zi}Ni=1):
14: for i  {1, . . . , N} do
15: a  Categorical({wi}Ni=1)
16: yi = za
17: return { 1
, yi}Ni=1
4 Filtering Variational Objectives (FIVOs)
The filtering variational objectives (FIVOs) are a family of MCOs defined by the marginal likelihood
estimator of a particle filter. For models with sequential structure, e.g., latent variable models of audio
and text, the relative variance of a naive importance sampling estimator tends to scale exponentially
in the number of steps. In contrast, the relative variance of particle filter estimators can scale more
favorably with the number of stepslinearly in some cases [53, 25]. Thus, the results of Section 3
suggest that FIVOs can serve as tighter objectives than IWAE for MLE in sequential models.
Let our observations be sequences of T X -valued random variables denoted x1:T , where xi:j 
(xi, . . . , xj). We also assume that the data generation process relies on a sequence of T unobserved
Z-valued latent variables denoted z1:T . We focus on sequential latent variable models that factor as a
series of tractable conditionals, p(x1:T , z1:T ) = p1(x1, z1)
t=2 pt(xt, zt|x1:t1, z1:t1).
A particle filter is a sequential Monte Carlo algorithm, which propagates a population of N weighted
particles for T steps using a combination of importance sampling and resampling steps, see Alg. 1.
In detail, the particle filter takes as arguments an observation x1:T , the number of particles N , the
model distribution p, and a variational posterior q(z1:T |x1:T ) factored over t,
q(z1:T |x1:T ) =
qt(zt|x1:t, z1:t1) . (6)
The particle filter maintains a population {wit1, zi1:t1}Ni=1 of particles zi1:t1 with weights wit1.
At step t, the filter independently proposes an extension zit  qt(zt|x1:t, zi1:t1) to each particles
trajectory zi1:t1. The weights w
t1 are multiplied by the incremental importance weights,
1:t) =
pt(xt, z
t|x1:t1, zi1:t1)
t|x1:t, zi1:t1)
, (7)
and renormalized. If the current weights wit satisfy a resampling criteria, then a resampling step is
performed and N particles zi1:t are sampled in proportion to their weights from the current population
with replacement. Common resampling schemes include resampling at every step and resampling
if the effective sample size (ESS) of the population (
i=1(w
2)1 drops below N/2 [68]. After
resampling the weights are reset to 1. Otherwise, the particles zi1:t are copied to the next step along
with the accumulated weights. See Fig. 1 for a visualization.
Instead of viewing Alg. 1 as an inference algorithm, we treat the quantity E[log pN (x1:T )] as an
objective function over p. Because pN (x1:T ) is an unbiased estimator of p(x1:T ), proven in the
Appendix and in [63, 64, 8, 204], it defines an MCO, which we call FIVO:
Definition 2. Filtering Variational Objectives. Let log pN (x1:T ) be the output of Alg. 1 with inputs
(x1:T , p, q,N), then LFIVON (x1:T , p, q) = E[log pN (x1:T )] is a filtering variational objective.
pN (x1:T ) is a strongly consistent estimator [63, 64]. So if log pN (x1:T ) is uniformly integrable, then
LFIVON (x1:T , p, q) log p(x1:T ) as N . Resampling is the distinguishing feature of LFIVON ; if
resampling is removed, then FIVO reduces to IWAE. Resampling does add an amount of immediate
variance, but it allows the filter to discard low weight particles with high probability. This has the
log p1 log p2 log p3
resample {zi1:3}3i=1  wi3
z22 z
log p1 log p2 log p3 log p4
propose zi4  q4(z4|x1:4, zi1:3)
z22 z
 log p4log p4 logwi3
gradients
Figure 1: Visualizing FIVO; (Left) Resample from particle trajectories to determine inheritance in next
step, (middle) propose with qt and accumulate loss log pt, (right) gradients (in the reparameterized
case) flow through the lattice, objective gradients in solid red and resampling gradients in dotted blue.
effect of refocusing the distribution of particles to regions of higher mass under the posterior, and in
some sequential models can reduce the variance from exponential to linear in the number of time
steps [53, 25]. Resampling is a greedy process, and it is possible that a particle discarded at step t,
could have attained a high mass at step T . In practice, the best trade-off is to use adaptive resampling
schemes [68]. If for a given x1:T , p, q a particle filters likelihood estimator improves over simple
importance sampling in terms of variance, we expect LFIVON to be a tighter bound than L or LIWAEN .
4.1 Optimization
The FIVO bound can be optimized with the same stochastic gradient ascent framework used for
the ELBO. We found in practice it was effective simply to follow a Monte Carlo estimator of the
biased gradient E[(,) log pN (x1:T )] with reparameterized zit. This gradient estimator is biased,
as the full FIVO gradient has three kinds of terms: it has the term E[, log pN (x1:T )], where
, log pN (x1:T ) is defined conditional on the random variables of Alg. 1; it has gradient terms for
every distribution of Alg. 1 that depends on the parameters; and, if adaptive resampling is used, then
it has additional terms that account for the change in FIVO with respect to the decision to resample.
In this section, we derive the FIVO gradient when zit are reparameterized and a fixed resampling
schedule is followed. We derive the full gradient in the Appendix.
In more detail, we assume that p and q are parameterized in a differentiable way by  and . Assume
that q is from a reparameterizable family and that zit of Alg. 1 are reparameterized. Assume that we
use a fixed resampling schedule, and let I(resampling at step t) be an indicator function indicating
whether a resampling occured at step t. Now, LFIVON depends on the parameters via log pN (x1:T ) and
the resampling probabilities wit in the density. Thus,(,) LFIVON =
(,) log pN (x1:T ) +
I(resampling at step t) log
pN (x1:T )
pN (x1:t)
(,) logwit
Given a single forward pass of Alg. 1 with reparameterized zit , the terms inside the expectation form
a Monte Carlo estimator of Eq. (8). However, the terms from resampling events contribute to the
majority of the variance of the estimator. Thus, the gradient estimator that we found most effective
in practice consists only of the gradient (,) log pN (x1:T ), the solid red arrows of Figure 1. We
explore this experimentally in Section 6.3.
4.2 Sharpness
As with the ELBO, FIVO is a variational objective taking a variational posterior q as an argument.
An important question is whether FIVO achieves the marginal log-likelihood at its optimal q. We can
only guarantee this for models in which z1:t1 and xt are independent given x1:t1.
Proposition 2. Sharpness of Filtering Variational Objectives. Let LFIVON (x1:T , p, q) be a FIVO, and
q(x1:T , p) = argmaxq LFIVON (x1:T , p, q). If p has independence structure such that p(z1:t1|x1:t) =
p(z1:t1|x1:t1) for t  {2, . . . , T}, then
q(x1:T , p)(z1:T ) = p(z1:T |x1:T ) and LFIVON (x1:T , p, q(x1:T , p)) = log p(x1:T ) .
Proof. See Appendix.
Most models do not satisfy this assumption, and deriving the optimal q in general is complicated by
the resampling dynamics. For the restricted the model class in Proposition 2, the optimal qt does
not condition on future observations xt+1:T . We explored this experimentally with richer models
in Section 6.4, and found that allowing qt to condition on xt+1:T does not reliably improve FIVO.
This is consistent with the view of resampling as a greedy process that responds to each intermediate
distribution as if it were the final. Still, we found that the impact of this effect was outweighed by the
advantage of optimizing a tighter bound.
5 Related Work
The marginal log-likelihood is a central quantity in statistics and probability, and there has long
been an interest in bounding it [260]. The literature relating to the bounds we call Monte Carlo
objectives has typically focused on the problem of estimating the marginal likelihood itself. [107, 49]
use Jensens inequality in a forward and reverse estimator to detect the failure of inference methods.
IWAE [50] is a clear influence on this work, and FIVO can be seen as an extension of this bound.
The ELBO enjoys a long history [128] and there have been efforts to improve the ELBO itself. [214]
generalize the ELBO by considering arbitrary operators of the model and variational posterior. More
closely related to this work is a body of work improving the ELBO by increasing the expressiveness of
the variational posterior. For example, [217, 136] augment the variational posterior with deterministic
transformations with fixed Jacobians, and [229] extend the variational posterior to admit a Markov
chain.
Other approaches to learning in neural latent variable models include [35], who use importance
sampling to approximate gradients under the posterior, and [108], who use sequential Monte Carlo
to approximate gradients under the posterior. These are distinct from our contribution in the sense
that for them inference for the sake of estimation is the ultimate goal. To our knowledge the idea
of treating the output of inference as an objective in and of itself, while not completely novel, has
not been fully appreciated in the literature. Although, this idea shares inspiration with methods that
optimize the convergence of Markov chains [23].
We note that the idea to optimize the log estimator of a particle filter was independently and
concurrently considered in [180, 146]. In [180] the bound we call FIVO is cast as a tractable
lower bound on the ELBO defined by the particle filters non-parameteric approximation to the
posterior. [146] additionally derive an expression for FIVOs bias as the KL between the filters
distribution and a certain target process. Our work is distinguished by our study of the convergence
of MCOs in N , which includes FIVO, our investigation of FIVO sharpness, and our experimental
results on stochastic RNNs.
6 Experiments
In our experiments, we sought to: (a) compare models trained with ELBO, IWAE, and FIVO bounds
in terms of final test log-likelihoods, (b) explore the effect of the resampling gradient terms on FIVO,
(c) investigate how the lack of sharpness affects FIVO, and (d) consider how models trained with
FIVO use the stochastic state. To explore these questions, we trained variational recurrent neural
networks (VRNN) [58] with the ELBO, IWAE, and FIVO bounds using TensorFlow [1] on two
benchmark sequential modeling tasks: natural speech waveforms and polyphonic music. These
datasets are known to be difficult to model without stochastic latent states [82].
The VRNN is a sequential latent variable model that combines a deterministic recurrent neu-
ral network (RNN) with stochastic latent states zt at each step. The observation distri-
bution over xt is conditioned directly on zt and indirectly on z1:t1 via the RNNs state
ht(zt1, xt1, ht1). For a length T sequence, the models posterior factors into the condition-
t=1 pt(zt|ht(zt1, xt1, ht1))gt(xt|zt, ht(zt1, xt1, ht1)), and the variational posterior
factors as
t=1 qt(zt|ht(zt1, xt1, ht1), xt). All distributions over latent variables are factorized
Gaussians, and the output distributions gt depend on the dataset. The RNN is a single-layer LSTM
and the conditionals are parameterized by fully connected neural networks with one hidden layer
of the same size as the LSTM hidden layer. We used the residual parameterization [82] for the
variational posterior.
N Bound Nottingham JSB MuseData Piano-midi.de
ELBO -3.00 -8.60 -7.15 -7.81
IWAE -2.75 -7.86 -7.20 -7.86
FIVO -2.68 -6.90 -6.20 -7.76
ELBO -3.01 -8.61 -7.19 -7.83
IWAE -2.90 -7.40 -7.15 -7.84
FIVO -2.77 -6.79 -6.12 -7.45
ELBO -3.02 -8.63 -7.18 -7.85
IWAE -2.85 -7.41 -7.13 -7.79
FIVO -2.58 -6.72 -5.89 -7.43
TIMIT
N Bound 64 units 256 units
ELBO 0 10,438
IWAE -160 11,054
FIVO 5,691 17,822
ELBO 2,771 9,819
IWAE 3,977 11,623
FIVO 6,023 21,449
ELBO 1,676 9,918
IWAE 3,236 13,069
FIVO 8,630 21,536
Table 1: Test set marginal log-likelihood bounds for models trained with ELBO, IWAE, and FIVO.
For ELBO and IWAE models, we report max{L,LIWAE128 ,LFIVO128 }. For FIVO models, we report LFIVO128 .
Pianoroll results are in nats per timestep, TIMIT results are in nats per sequence relative to ELBO
with N = 4. For details on our evaluation methodology and absolute numbers see the Appendix.
For FIVO we resampled when the ESS of the particles dropped below N/2. For FIVO and IWAE we
used a batch size of 4, and for the ELBO, we used batch sizes of 4N to match computational budgets
(resampling is O(N) with the alias method). For all models we report bounds using the variational
posterior trained jointly with the model. For models trained with FIVO we report LFIVO128 . To provide
strong baselines, we report the maximum across bounds, max{L,LIWAE128 ,LFIVO128 }, for models trained
with ELBO and IWAE. Additional details in the Appendix.
6.1 Polyphonic Music
We evaluated VRNNs trained with the ELBO, IWAE, and FIVO bounds on 4 polyphonic music
datasets: the Nottingham folk tunes, the JSB chorales, the MuseData library of classical piano and
orchestral music, and the Piano-midi.de MIDI archive [40]. Each dataset is split into standard train,
valid, and test sets and is represented as a sequence of 88-dimensional binary vectors denoting the
notes active at the current timestep. We mean-centered the input data and modeled the output as a set
of 88 factorized Bernoulli variables. We used 64 units for the RNN hidden state and latent state size
for all polyphonic music models except for JSB chorales models, which used 32 units. We report
bounds on average log-likelihood per timestep in Table 1. Models trained with the FIVO bound
significantly outperformed models trained with either the ELBO or the IWAE bounds on all four
datasets. In some cases, the improvements exceeded 1 nat per timestep, and in all cases optimizing
FIVO with N = 4 outperformed optimizing IWAE or ELBO for N = {4, 8, 16}.
6.2 Speech
The TIMIT dataset is a standard benchmark for sequential models that contains 6300 utterances
with an average duration of 3.1 seconds spoken by 630 different speakers. The 6300 utterances are
divided into a training set of size 4620 and a test set of size 1680. We further divided the training
set into a validation set of size 231 and a training set of size 4389, with the splits exactly as in
[82]. Each TIMIT utterance is represented as a sequence of real-valued amplitudes which we split
into a sequence of 200-dimensional frames, as in [58, 82]. Data preprocessing was limited to mean
centering and variance normalization as in [82]. For TIMIT, the output distribution was a factorized
Gaussian, and we report the average log-likelihood bound per sequence relative to models trained
with ELBO. Again, models trained with FIVO significantly outperformed models trained with IWAE
or ELBO, see Table 1.
6.3 Resampling Gradients
All models in this work (except those in this section) were trained with gradients that did not include
the term in Eq. (8) that comes from resampling steps. We omitted this term because it has an outsized
effect on gradient variance, often increasing it by 6 orders of magnitude. To explore the effects of this
term experimentally, we trained VRNNs with and without the resampling gradient term on the TIMIT
and polyphonic music datasets. When using the resampling term, we attempted to control its variance
0 1 2 3 4 5 6
1M Gradient Updates
Without Resampling Gradient Term
With Resampling Gradient Term
0 10 20 30 40 50 60
1k Gradient Updates
Figure 2: (Left) Graph of LFIVO128 over training comparing models trained with and without the
resampling gradient terms on TIMIT with N = 4. (Right) KL divergence from q(z1:T |x1:T ) to
p(z1:T ) for models trained on the JSB chorales with N = 16.
Bound Nottingham JSB MuseData Piano-midi.de TIMIT
ELBO -2.40 -5.48 -6.54 -6.68 0
ELBO+s -2.59 -5.53 -6.48 -6.77 -925
IWAE -2.52 -5.77 -6.54 -6.74 1,469
IWAE+s -2.37 -4.63 -6.47 -6.74 2,630
FIVO -2.29 -4.08 -5.80 -6.41 6,991
FIVO+s -2.34 -3.83 -5.87 -6.34 9,773
Table 2: Train set marginal log-likelihood bounds for models comparing smoothing (+s) and non-
smoothing variational posteriors. We report max{L,LIWAE128 ,LFIVO128 } for ELBO and IWAE models
and LFIVO128 for FIVO models. All models were trained with N = 4. Pianoroll results are in nats per
timestep, TIMIT results are in nats per sequence relative to non-smoothing ELBO. For details on our
evaluation methodology and absolute numbers see the Appendix.
using a moving-average baseline linear in the number of timesteps. For all datasets, models trained
without the resampling gradient term outperformed models trained with the term by a large margin
on both the training set and held-out data. Many runs with resampling gradients failed to improve
beyond random initialization. A representative pair of train log-likelihood curves is shown in Figure
2  gradients without the resampling term led to earlier convergence and a better solution. We stress
that this is an empirical result  in principle biased gradients can lead to divergent behaviour. We
leave exploring strategies to reduce the variance of the unbiased estimator to future work.
6.4 Sharpness
FIVO does not achieve the marginal log-likelihood at its optimal variational posterior q, because the
optimal q does not condition on future observations (see Section 4.2). In contrast, ELBO and IWAE
are sharp, and their qs depend on future observations. To investigate the effects of this, we defined a
smoothing variant of the VRNN in which q takes as additional input the hidden state of a deterministic
RNN run backwards over the observations, allowing q to condition on future observations. We trained
smoothing VRNNs using ELBO, IWAE, and FIVO, and report evaluation on the training set (to
isolate the effect on optimization performance) in Table 2 . Smoothing helped models trained with
IWAE, but not enough to outperform models trained with FIVO. As expected, smoothing did not
reliably improve models trained with FIVO. Test set performance was similar, see the Appendix for
details.
6.5 Use of Stochastic State
A known pathology when training stochastic latent variable models with the ELBO is that stochastic
states can go unused. Empirically, this is associated with the collapse of variational posterior
q(z|x) network to the model prior p(z) [41]. To investigate this, we plot the KL divergence from
q(z1:T |x1:T ) to p(z1:T ) averaged over the dataset (Figure 2). Indeed, the KL of models trained with
ELBO collapsed during training, whereas the KL of models trained with FIVO remained high, even
while achieving a higher log-likelihood bound.
7 Conclusions
We introduced the family of filtering variational objectives, a class of lower bounds on the log
marginal likelihood that extend the evidence lower bound. FIVOs are suited for MLE in neural latent
variable models. We trained models with the ELBO, IWAE, and FIVO bounds and found that the
models trained with FIVO significantly outperformed other models across four polyphonic music
modeling tasks and a speech waveform modeling task. Future work will include exploring control
variates for the resampling gradients, FIVOs defined by more sophisticated filtering algorithms, and
new MCOs based on differentiable operators like leapfrog operators with deterministically annealed
temperatures. In general, we hope that this paper inspires the machine learning community to take a
fresh look at the literature of marginal likelihood estimatorsseeing them as objectives instead of
algorithms for inference.
Acknowledgments
We thank Matt Hoffman, Matt Johnson, Danilo J. Rezende, Jascha Sohl-Dickstein, and Theophane
Weber for helpful discussions and support in this project. A. Doucet was partially supported by the
EPSRC grant EP/K000276/1. Y. W. Tehs research leading to these results has received funding
from the European Research Council under the European Unions Seventh Framework Programme
(FP7/2007-2013) ERC grant agreement no. 617071.
Appendix to Filtering Variational Objectives
Proof of Proposition 1.
Let E[pN (x)] = p(x) and define LN (x, p) = E[log pN (x)] as the Monte Carlo objective defined by
pN (x).
(a) By the concavity of log and Jensens inequality,
LN (x, p) = E[log pN (x)]  logE[pN (x)] = log p(x)
(b) Assume
 pN (x) is strongly consistent, i.e. pN (x)
a.s. p(x) as N .
 log pN (x) is uniformly integrable. That is, let (,F , ) be the probability space on
which log pN (x) is defined. The random variables {log pN (x)}N=1 are uniformly
integrable if E[| log pN (x)|] <  and if for any  > 0, there exists  > 0, such that
for all N and E  F , (E) <  implies E[| log pN (x)|I(E)] < , where I(E) is an
indicator function of the set E.
Then by continuity of log, log pN (x) converges almost surely to log p(x). By Vitalis
convergence theorem (using the uniform integrability assumption), we get LN (x, p) =
E[log pN (x)] log p(x) as N .
(c) Let g(N) = E[(pN (x)  p(x))6], and assume lim supN E[(pN (x))1] < . Define
the relative error
pN (x) p(x)
Then the bias log p(x)  LN (x, p) = E[log(1 + )]. Now, Taylor expand log(1 + )
about 0,
log(1 + ) =  1
1 + x
 1 + x
dx (10)
=  1
1 + x
dx (11)
and in expectation
E[log(1 + )] = 1
2  E
1 + x
Our aim is to show
E
1 + x
]  O(g(N)
1/2) (13)
In particular, by Cauchy-Schwarz
E
1 + x
]  E

(1 + x)2

1/2 
x4 dx

 (14)
[
1 + 
1/2 
[
1 + 
1/2 
and again by Cauchy-Schwarz
[
1 + 
])1/2(
])1/2
. (17)
This concludes the proof.
Controlling the first inverse moment.
We provide a sufficient condition that guarantees that the inverse moment of the average of i.i.d.
random variables is bounded, a condition used in Proposition 1 (c). Intuitively, this is a fairly weak
condition, because it only requires that the mass in an arbitrarily small neighbourhood of zero is
bounded.
Lemma 3. Let wi be i.i.d. positive random variables and pN (x) = 1N
i=1 wi. If there exist
M,C,  > 0 such that P(wi < w)  Cw1+ for w  [0,M), then E[pN (x)1]  CM
Proof. Let M,C,  > 0 be such that P(wi < w)  Cw1+ for w  [0,M). We proceed in two cases.
If N = 1, then
E[pN (x)1] =
P(w11 > u) du
P(w1 < 1/u) du
P(w1 < w)
P(w1 < w)
Cw1+
For N > 1, we show that E[pN (x)1]  E[p1(x)1], so the same condition is sufficient for any N .
The AM-GM inequality tells us that
E[pN (x)1]  E
)1/N
and by Lyapunovs inequality, we have
= E[p1(x)1]
This concludes the proof.
Gradients of LFIVON (x1:T , p, q).
We formulate unbiased gradients of LFIVON (x1:T , p, q) by considering Algorithm 1 as a method for
simulating FIVO. We consider the cases when the sampling of zit is and is not reparameterized. We
also consider the case where we make adaptive resampling decisions.
First, we assume that the decision to resample is not adaptive (i.e., depends in some way on the
random variables already produced until that point in Algorithm 1), and are fixed ahead of time.
When the sampling zit is not reparameterized there are three terms to the gradient: (1) the gradients
of log pN (x1:T ) with respect to the parameters conditional on the latent states, (2) gradients of the
densities qt with respect to their parameters, and (3) gradients of the resampling probabilities with
respect to the parameters. All together, the following is a gradient of FIVO,
, log pN (x1:T ) +
pN (x1:T )
pN (x1:t1)
 log qt,(zit|x1:t, zi1:t1) +
I(resampling at step t) log
pN (x1:T )
pN (x1:t)
, logwit
)] (18)
where I(A) is an indicator function. If zit is reparameterized, then the first and third terms suffice for
an unbiased gradient,
, log pN (x1:T ) +
I(resampling at step t) log
pN (x1:T )
pN (x1:t)
, logwit
In this work we only considered reparameterized qts, and we dropped the terms of the gradient that
arise from resampling.
Second, when the decision to resample is adaptive, the domain of the random variables involved
in simulating log pN (x1:T ) can be partitioned into 2T regions, over each of which the density is
differentiable. Between those regions, the density experiences a jump discontinuity. Thus, there
are additional terms to the gradient of LFIVON (x1:T , p, q) that correspond to the change in the regions
of continuity as the parameters change. These terms can be written as surface integrals over the
boundaries of the regions. We drop these terms in practice.
Proof of Proposition 2.
Assume p(z1:t1|x1:t) = p(z1:t1|x1:t1) for all t  {2, . . . , T}. We will show LFIVON (x1:T , p, q) =
log p(x1:T ) at q(zt|z1:t1, x1:t) = p(zt|z1:t1, x1:t). We will do this by induction, showing that
every particle has a constant weight and that pN (x1:T ) = p(x1:T ) is a constant. For t = 1 we have
i1(z1) =
p1(x1, z1)
p(z1|x1)
= p1(x1) (20)
Thus, all particles have the same weight and p1 = p1(x1). Now for any t we have that the weights
must be 1/N since the particles all have the same weight and
it(z1:t) =
pt(xt, zt|z1:t1, x1:t1)
p(zt|z1:t1, x1:t)
p(z1:t, x1:t)
p(z1:t1, x1:t1)p(zt|z1:t1, x1:t)
p(x1:t)
p(x1:t1)
p(z1:t|x1:t)
p(z1:t1|x1:t1)p(zt|z1:t1, x1:t)
p(x1:t)
p(x1:t1)
p(z1:t|x1:t)
p(z1:t1|x1:t)p(zt|z1:t1, x1:t)
p(x1:t)
p(x1:t1)
and thus,
pN (x1:T ) = p1(x1)
p(x1:t)
p(x1:t1)
= p(x1:T ) (26)
Implementation details
We initialized weights using the Xavier initialization [97] and used the Adam optimizer [135] with a
batch size of 4. During training, we did not truncate sequences and performed full backpropagation
through time for all datasets. For the results presented in Sections 6.1 and 6.2 we performed a grid
search over learning rates {3 104, 1 104, 3 105, 1 105} and picked the run and early
stopping step by the validation performance.
Evaluation and Comparison of Bounds
Comparing models trained with different log-likelihood lower bounds is challenging because calculat-
ing the actual log-likelihood is intractable. Burda et al. [50] showed that the IWAE bound is at least
as tight as the ELBO and monotonically increases with N . This suggests comparing models based on
the IWAE bound evaluated with a large N . However, we found that IWAE and ELBO bounds tended
to diverge for models trained with FIVO.
Although FIVO is not provably a tighter bound than the ELBO or IWAE, our experiments suggest
that this tends to be the case in practice. In Figure 3, we plotted all three bounds over training for
a representative experiment. All plots use the same model architecture, but the training objective
changes in each panel. For the model trained with IWAE, the FIVO and IWAE bounds are tighter than
their counterparts on the model trained with ELBO, suggesting that the model trained with IWAE is
superior. The ELBO bound evaluated on the model trained with IWAE, however, is lower than its
counterpart on the model trained with the ELBO. For the model trained with FIVO, both IWAE and
ELBO bounds seem to diverge, but the FIVO bound outperforms the FIVO bounds on both of the
other models. As in the figure, we generally found that the same model evaluated with FIVO, IWAE,
and ELBO produced values descending in that order.
We suspect that q distributions trained under the FIVO bound are more entropic than those trained
under ELBO or IWAE because of the resampling operation. During training under FIVO, q is able to
propose state transitions that could poorly explain the observations because the bad states will be
resampled away without harming the final bound value. Then, when a FIVO-trained q is evaluated
with ELBO or IWAE it proposes poor states that are not resampled away, leading to a poor final bound
value. Conversely, qs trained with ELBO and IWAE are not able to fully leverage the resampling
operation when evaluated with the FIVO bound.
Because of this behavior, we chose to optimistically evaluate models trained with IWAE and ELBO
by reporting the maximum across all the bounds. For models trained with FIVO, we reported only the
FIVO bound. We felt this evaluation scheme provided the strongest comparison to existing bounds.
0 50 100 150 200 250 300 350
1k Gradient Updates
Trained with FIVO
0 100 200 300 400
1k Gradient Updates
Trained with IWAE
0 20 40 60 80 100 120 140 160
1k Gradient Updates
Trained with ELBO
Figure 3: Comparison of ELBO, IWAE, and FIVO bounds. We plot the ELBO (L), IWAE (LIWAE128 ),
and FIVO (LFIVO128 ) test log-likelihood lower bounds for a fixed model architecture trained with FIVO
(left), IWAE (middle), and ELBO (right). The models are VRNNs trained on the Nottingham dataset
with 64 units, N = 16, and learning rate 3 105.
Evaluating TIMIT Log-Likelihoods
We reported log-likelihood scores for TIMIT relative to an ELBO baseline instead of raw log-
likelihoods. Previous papers (e.g., [58, 82]) report the log-likelihood of data that have been mean
centered and variance normalized, but it would be more proper to report the results on the un-
standardized data. Specifically, if the training set has mean  and variance 2 and the model outputs 
and 2, then the un-standardized test data would be evaluated under a N ( + , 22) distribution.
Log-likelihoods produced by these approaches differ by a constant offset that depends on . Because
the offset is a function of only training set statistics, it does not affect relative comparison between
methods. Because of this we chose to report log-likelihoods relative to a baseline instead of absolute
numbers. Absolute numbers calculated on standardized data are reported in Tables 3, 4, and 5 to
allow for comparisons with other papers.
Statement of Authorship for joint/multi-authored papers for PGR thesis 
To appear at the end of each thesis chapter submitted as an article/paper 
The statement shall describe the candidates and co-authors independent research contributions in the thesis 
publications. For each publication there should exist a complete statement that is to be filled out and signed by the 
candidate and supervisor (only required where there isnt already a statement of contribution within the paper 
itself). 
Title of Paper 
Filtering Variational Objectives 
Publication Status 
  Published                                   Accepted for Publication 
  Submitted for Publication          Unpublished and unsubmitted work written 
                         in a manuscript style 
Publication Details 
Chris J. Maddison*, Dieterich Lawson*, George Tucker*, Nicolas Heess, 
Mohammad Norouzi, Andriy Mnih, Arnaud Doucet, Yee Whye Teh. Filtering 
Variational Objectives. In Advances in Neural Information Processing Systems, 
2017. 
Student Confirmation 
Student Name: 
Chris J. Maddison 
Contribution to the 
Paper 
 I proposed the idea and worked out the correctness of the approach. 
 The proofs were joint work with Arnaud Doucet. Arnaud helped me complete 
the proof of the rate of convergence and he provided the proof of conditions 
that control the first inverse moment. 
 I wrote a full set of experimental code for an RL application, but the 
experiments that I performed were not included in the final version. A separate 
codebase was used for the generative modeling experiments in the paper, and 
the reported experiments were all run by Dieterich Lawson. 
 I wrote the majority of the paper, with the exception of the experimental 
section, which was written by Dieterich Lawson and George Tucker. 
 All authors contributed to the development of the paper through discussions 
and ideas, and all authors reviewed the final draft. 
Signature  
05 May 2020 
Supervisor Confirmation 
By signing the Statement of Authorship, you are certifying that the candidate made a substantial contribution to the 
publication, and that the description described above is accurate. 
Supervisor name and title: 
Professor Arnaud Doucet 
Supervisor comments 
I agree that the candidate has made a substantial contribution to the publication. 
Signature 
05 May 2020 
This completed form should be included in the thesis, at the end of the relevant chapter. 
Chapter 4
Hamiltonian Descent
4.1 Abstract
We propose a family of optimization methods that achieve linear convergence using
first-order gradient information and constant step sizes on a class of convex functions
much larger than the smooth and strongly convex ones. This larger class includes
functions whose second derivatives may be singular or unbounded at their minima or
near infinity. Our methods are discretizations of conformal Hamiltonian dynamics,
which generalizes the classical momentum method to model the motion of a particle
with non-standard kinetic energy exposed to a dissipative force and the gradient field
of the function of interest. They are first-order in the sense that they require only
gradient computation. Yet, crucially the kinetic gradient map can be designed to
incorporate global information about the convex conjugate in a fashion that allows
for linear convergence on convex functions that may be non-smooth or non-strongly
convex. We study in detail one implicit and two explicit methods. For one explicit
method, we provide conditions under which it converges to stationary points of non-
convex functions. For all, we provide conditions on the convex function and kinetic
energy pair that guarantee linear convergence, and show that these conditions can
be satisfied by functions with power growth. In sum, these methods expand the
class of convex functions on which linear convergence is possible with first-order
computation.
4.2 Introduction
We consider the problem of unconstrained minimization of a differentiable function
f : Rd  R,
f(x), (4.1)
by iterative methods that require only the partial derivativesf(x) = (f(x)/x(n)) 
Rd of f , known also as first-order methods [190, 207, 193]. These methods produce
a sequence of iterates xi  Rd, and our emphasis is on those that achieve linear
convergence, i.e., as a function of the iteration i they satisfy f(xi)f(xmin) = O(i)
for some rate  > 1 and xmin  Rd a global minimizer. We briefly consider non-
convex differentiable f , but the bulk of our analysis focuses on the case of convex
differentiable f . Our results will also occasionally require twice differentiability of f .
The convergence rates of first-order methods on convex functions can be broadly
separated by the properties of strong convexity and Lipschitz smoothness. Taken
together these properties for convex f are equivalent to the conditions that the
following left hand bound (strong convexity) and right hand bound (smoothness)
hold for some , L  (0,) and all x, y  Rd,
x y22  f(x) f(y) f(y), x y 
x y22 , (4.2)
where x, y =
n=1 x
(n)y(n) is the standard inner product and x2 =
x, x
is the Euclidean norm. For twice differentiable f , these properties are equivalent
to the conditions that eigenvalues of the matrix of second-order partial derivatives
2 f(x) = (2f(x)/x(n)x(m))  Rdd are everywhere lower bounded by  and
upper bounded by L, respectively. Thus, functions whose second derivatives are
continuously unbounded or approaching 0, cannot be both strongly convex and smooth.
Both bounds play an important role in the performance of first-order methods. On the
one hand, for smooth and strongly convex f , the iterates of many first-order methods
converge linearly. On the other hand, for any first-order method, there exist smooth
convex functions and non-smooth strongly convex functions on which its convergence
is sub-linear, i.e., f(xi)  f(xmin)  O(i2) for any first-order method on smooth
convex functions. See [190, 207, 193] for these classical results and [131] for other more
exotic scenarios. Moreover, for a given method it can sometimes be very easy to find
examples on which its convergence is slow; see Figure 4.1, in which gradient descent
with a fixed step size converges slowly on f(x) = [x(1) + x(2)]4 + [x(1)/2  x(2)/2]4,
which is not strongly convex as its Hessian is singular at (0, 0).
The central assumption in the worst case analyses of first-order methods is that
information about f is restricted to black box evaluations of f and f locally at
points x  Rd, see [190, 193]. In this paper we assume additional access to first-order
information of a second differentiable function k : Rd  R and show how k can be
designed to incorporate information about f to yield practical methods that converge
linearly on convex functions. These methods are derived by discretizing the conformal
Iterates
iteration i
Objective
Gradient descent
Classical momentum
Hamiltonian descent
Figure 4.1: Optimizing f(x) = [x(1) + x(2)]4 + [x(1)/2  x(2)/2]4 with three methods:
gradient descent with fixed step size equal to 1/L0 where L0 = max(2 f(x0)) is
the maximum eigenvalue of the Hessian 2f at x0; classical momentum, which is a
particular case of our first explicit method with k(p) = [(p(1))2 + (p(2))2]/2 and fixed
step size equal to 1/L0; and Hamiltonian descent, which is our first explicit method
with k(p) = (3/4)[(p(1))4/3 + (p(2))4/3] and a fixed step size.
Hamiltonian system [165]. These systems are parameterized by f, k : Rd  R and
  (0,) with solutions (xt, pt)  R2d,
xt = k(pt)
pt = f(xt) pt.
(4.3)
From a physical perspective, these systems model the dynamics of a single particle
located at xt with momentum pt and kinetic energy k(pt) being exposed to a force
field f and a dissipative force. For this reason we refer to k as, the kinetic energy,
and k, the kinetic map. When the kinetic map k is the identity, k(p) = p,
these dynamics are the continuous time analog of Polyaks heavy ball method [206].
Let fc(x) = f(x + xmin)  f(xmin) denote the centered version of f , which takes its
minimum at 0, with minimum value 0. Our key observation in this regard is that
when f is convex, and k is chosen as k(p) = (f c (p) + f
c (p))/2 (where f c (p) =
sup{x, p  fc(x) : x  Rd} is the convex conjugate of fc), these dynamics have
linear convergence with rate independent of f . In other words, this choice of k acts as
a preconditioner, a generalization of using k(p) = p,A1p /2 for f(x) = x,Ax /2.
Thus k can exploit global information provided by the conjugate f c to condition
convergence for generic convex functions.
To preview the flavor of our results in detail, consider the special case of optimizing
the power function f(x) = |x|b/b for x  R and b  (1,) initialized at x0 > 0 using
system (4.3) (or discretizations of it) with k(p) = |p|a/a for p  R and a  (1,).
1 2 3 4
linear
convergence
in continuous time
sub-linear
convergence
in continuous time
f(x) = |x|b/b
k(p) = |p|a/a
linear convergence
of 1st explicit method
linear convergence
of 2nd explicit method
quadratic suitable for
strongly convex and smooth
Figure 4.2: Convergence Regions for Power Functions. Shown are regions of distinct
convergence types for Hamiltonian descent systems with f(x) = |x|b/b, k(p) = |p|a/a
for x, p  R and a, b  (1,). We show in Section 4.3 convergence is linear in
continuous time iff 1/a + 1/b  1. In Section 4.5 we show that the assumptions
of the explicit discretizations can be satisfied if 1/a + 1/b = 1, leaving this as the
only suitable pairing for linear convergence. Light dotted line is the line occupied by
classical momentum with k(p) = p2/2.
FOr this choice of f , it can be shown that f c (p) = f
c (p) = k(p) when a = b/(b1).
In line with this, in Section 4.3 we show that (4.3) exhibits linear convergence in
continuous time if and only if 1/a + 1/b  1. In Section 4.4 we propose two explicit
discretizations with fixed step sizes; in Section 4.5 we show that the first explicit
discretization converges if 1/a + 1/b = 1 and b  2, and the second converges if
1/a+ 1/b = 1 and 1 < b  2. This means that the only suitable pairing corresponds
in this case to the choice k(p)  f c (p)+f c (p). Figure 4.2 summarizes this discussion.
Returning to Figure 4.1, we can compare the use of the kinetic energy of Polyaks
heavy ball with a kinetic energy that relates appropriately to the convex conjugate
of f(x) = [x(1) + x(2)]4 + [x(1)/2 x(2)/2]4.
Most convex functions are not simple power functions, and computing f c (p) +
f c (p) exactly is rarely feasible. To make our observations useful for numerical
optimization, we show that linear convergence is still achievable in continuous time
even if k(p)  max{f c (p), f c (p)} for some 0 <   1 within a region defined by
x0. We study three discretizations of (4.3), one implicit method and two explicit ones
(which are suitable for functions that grow asymptotically fast or slow, respectively).
We prove linear convergence rates for these under appropriate additional assumptions.
We introduce a family of kinetic energies that generalize the power functions to
capture distinct power growth near zero and asymptotically far from zero. We show
that the additional assumptions of discretization can be satisfied for this family of
k. We derive conditions on f that guarantee the linear convergence of our methods
when paired with a specific choice of k from this family. These conditions generalize
the quadratic growth implied by smoothness and strong convexity, extending it to
general power growth that may be distinct near the minimum and asymptotically
far from the minimum, which we refer to as tail and body behavior, respectively.
Step sizes can be fixed independently of the initial position (and often dimension),
and do not require adaptation, which often leads to convergence problems, see [267].
Indeed, we analyze a kinetic map k that resembles the iterate updates of some
popular adaptive gradient methods [75, 274, 90, 135], and show that it conditions the
optimization of strongly convex functions with very fast growing tails (non-smooth).
Thus, our methods provide a framework optimizing potentially non-smooth or non-
strongly convex functions with linear rates using first-order computation.
The organization of the paper is as follows. In the rest of this section, we cover
notation, review a few results from convex analysis, and give an overview of the related
literature. In Section 4.3, we show the linear convergence of (4.3) under conditions
on the relation between the kinetic energy k and f . We show a partial converse that
in some settings our conditions are necessary. In Section 4.4, we present the three
discretizations of the continuous dynamics and study the assumptions under which
linear rates can be guaranteed for convex functions. For one of the discretizations, we
also provide conditions under which it converges to stationary points of non-convex
functions. In Section 4.5, we study a family of kinetic energies suitable for functions
with power growth. We describe the class of functions for which the assumptions of
the discretizations can be satisfied when using these kinetic energies.
4.2.1 Notation and Convex Analysis Review
We let x, y =
n=1 x
(n)y(n) denote the standard inner product for x, y  Rd and
x2 =
x, x the Euclidean norm. For a differentiable function f : Rd  R,
the gradient f(x) = (f(x)/x(n))  Rd is the vector of partial derivatives at x.
For twice-differentiable f , the Hessian 2h(x) = (2f(x)/x(n)x(m))  Rdd is the
matrix of second-order partial derivatives at x. The notation xt denotes the solution
xt : [0,) Rd to a differential equation with derivative in t denoted xt. xi denotes
the iterates xi : {0, 1, . . .}  Rd of a discrete system.
Consider a convex function h : C  R that is defined on a convex domain C  Rd
and differentiable on the interior int(C). The convex conjugate h : Rd  R is defined
h(p) = sup{x, p  h(x) : x  C} (4.4)
and it is itself convex. It is easy to show from the definition that if g : C  R is
another convex function such that g(x)  h(x) for all x  C, then h(p)  g(p)
for all p  Rd. Because we make such extensive use of it, we remind readers of the
Fenchel-Young inequality: for x  C and p  Rd,
x, p  h(x) + h(p), (4.5)
which is easily derived from the definition of h, or see Section 12 of [222]. For
x  int(C) by Theorem 26.4 of [222],
x,h(x) = h(x) + h(h(x)). (4.6)
Let y  Rd, c  R \ {0}. If g(x) = h(x + y)  c, then g(p) = h(p)  p, y + c
(Theorem 12.3 [222]). If h(x) = |x|b/b for x  R and b  (1,), then h(p) = |p|a/a
where a = b/(b1) (page 106 of [222]). If g(x) = ch(x), then g(p) = ch(p/c) (Table
3.2 [36]). For these and more on h, we refer readers to [222, 42, 36].
4.2.2 Related Literature
Standard references on convex optimization and the convergence analysis of first-order
methods include [190, 207, 28, 42, 193, 47].
The heavy ball method was introduced by Polyak in his seminal paper [206]. In
this paper, local convergence with linear rate was shown (i.e., when the initial position
is sufficiently close to the local minimum). For quadratic functions, it can be shown
that the convergence rate for optimally chosen step sizes is proportional to the square
root of the conditional number of the Hessian, similarly to conjugate gradient descent
(see e.g., [216]). As far as we know, global convergence of the heavy ball method
for non-quadratic functions was only recently established in [91] and [148], see [111]
for an extension to stochastic average gradients. The heavy ball method forms the
basis of the some of the most successful optimization methods for deep learning, see
e.g., [240, 135], and the recent review [38]. Hereafter, classical momentum refers to
any first-order discretization of the continuous analog of Polyaks heavy ball (with
possibly suboptimal step sizes).
Nesterov obtained upper and lower bounds of matching order for first-order methods
for smooth convex functions and smooth strongly convex functions, see [193]. In
Necoara et al. [187], the assumption of strong convexity was relaxed, and under a
weaker quadratic growth condition, linear rates were obtained by several well known
optimization methods. Several other authors obtained linear rates for various classes
of non-strongly convex or non-uniformly smooth functions, see e.g., [188, 133, 73, 272,
78, 223].
In recent years, there has been interest in the optimization community in looking
at the continuous time ODE limit of optimization methods, when the step size tends
to zero. Su et al. [238, 239] have found the continuous time limit of Nesterovs
accelerated gradient descent. This result improves the intuition about Nesterovs
method, as the proofs of convergence rates in continuous time are rather elegant and
clear, while the previous proofs in discrete time are not as transparent. Follow-ups
have studied the continuous time counterparts to accelerated mirror descent [139] as
well as higher order discretizations of such systems [263, 266]. Studying continuous
time systems for optimization can separate the concerns of designing an optimizer
from the difficulties of discretization. This perspective has resulted in numerous
other recent works that propose new optimization methods, and study existing ones
via their continuous time limit, see e.g., [29, 7, 80, 126, 72, 83, 84].
Conformal Hamiltonian systems (4.3) are studied in geometry [165, 30], because
their solutions preserve symplectic area up to a constant; when  = 0 symplectic
area is exactly preserved, when  > 0 symplectic area dissipates uniformly at an
exponential rate [165]. In classical mechanics, Hamiltonian dynamics (system (4.3)
with  = 0) are used to describe the motion of a particle exposed to the force field
f . Here, the most common form for k is k(p) = p, p /2m, where m is the mass,
or in relativistic mechanics, k(p) = c
p, p+m2c2 where c is the speed of light, see
[99]. In the Markov Chain Monte Carlo literature, where (discretized) Hamiltonian
dynamics (again  = 0) are used to propose moves in a MetropolisHastings algorithm
[170, 115, 74, 185], k is viewed as a degree of freedom that can be used to improve
the mixing properties of the Markov chain [93, 149]. Stochastic differential equations
similar to (4.3) with  > 0 have been studied from the perspective of designing k
[157, 237].
4.3 Continuous Dynamics
In this section, we motivate the discrete optimization algorithms by introducing their
continuous time counterparts. These systems are differential equations described by a
Hamiltonian vector field plus a dissipation field. Thus, we briefly review Hamiltonian
dynamics, the continuous dynamics of Hamiltonian descent, and derive convergence
rates for convex f in continuous time.
Hamiltonian Field
position x
Dissipation Field
Conformal Hamiltonian Field
Figure 4.3: A visualization of a conformal Hamiltonian system.
4.3.1 Hamiltonian Systems
In the Hamiltonian formulation of mechanics, the evolution of a particle exposed
to a force field f is described by its location xt : [0,)  Rd and momentum
pt : [0,)  Rd as functions of time. The system is characterized by the total
energy, or Hamiltonian,
H(x, p) = k(p) + f(x) f(xmin), (4.7)
where xmin is one of the global minimizers of f and k : Rd  R is called the kinetic
energy. Throughout, we consider kinetic energies k that are a strictly convex functions
with minimum at k(0) = 0. The Hamiltonian H defines the trajectory of a particle
xt and its momentum pt via the ordinary differential equation,
xt = pH(xt, pt) = k(pt)
pt = xH(xt, pt) = f(xt).
(4.8)
For any solution of this system, the value of the total energy over time Ht = H(xt, pt)
is conserved as Ht = k(pt), pt + f(xt), xt = 0. Thus, the solutions of the
Hamiltonian field oscillate, exchanging energy from x to p and back again.
4.3.2 Continuously Descending the Hamiltonian
The solutions of a Hamiltonian system remain in the level set {(xt, pt) : Ht = H0}. To
drive such a system towards stationary points, the total energy must reduce over time.
Consider as a motivating example the continuous system xt = f(xt) xt, which
describes Polyaks heavy ball algorithm in continuous time [206]. Letting xt = pt, the
heavy ball system can be rewritten as
xt = pt
pt = f(xt) pt.
(4.9)
Note that this system can be viewed as a combination of a Hamiltonian field with
k(p) = p, p /2 and a dissipation field, i.e., (xt, pt) = F (xt, pt) + G(xt, pt) where
F (xt, pt) = (pt,f(xt)) and G(xt, pt) = (0,pt), see Figure 4.3 for a visualization.
This is naturally extended to define the more general conformal Hamiltonian system
[165],
xt = k(pt)
pt = f(xt) pt.
(4.3 revisited)
with   (0,). When k is convex with a minimum k(0) = 0, these systems descend
the level sets of the Hamiltonian. We can see this by showing that the total energy
Ht is reduced along the trajectory (xt, pt),
Ht = k(pt), pt+ f(xt), xt =  k(pt), pt  k(pt)  0, (4.10)
where we have used the convexity of k, and the fact that it is minimised at k(0) = 0.
The following proposition shows some existence and uniqueness results for the
dynamics (4.3). We say thatH is radially unbounded ifH(x, p) when (x, p)2 
, e.g., this would be implied if f and k were strictly convex with unique minima.
Proposition 4.3.1 (Existence and uniqueness). If f and k are continuous, k is
convex with a minimum k(0) = 0, and H is radially unbounded, then for every x, p 
Rd, there exists a solution (xt, pt) of (4.3) defined for every t  0 with (x0, p0) = (x, p).
If in addition, f and k are continuously differentiable, then this solution is unique.
Proof. First, only assuming continuity, it follows from Peanos existence theorem
[201] that there exists a local solution on an interval t  [a, a] for some a > 0.
Let [0, A) denote the right maximal interval where a solution of (4.3) satisfying that
x0 = x and p0 = p exist. From (4.10), it follows that Ht  0, and hence Ht  H0
for every t  [0, A). Now by the radial unboundedness of H, and the fact that
Ht  H0, it follows that the compact set {(x, p) : H(x, p)  H0} is never left by
the dynamics, and hence by Theorem 10.1 of [114] (page 140), we must have A =
. The uniqueness under continuous differentiability follows from the Fundamental
ExistenceUniqueness Theorem on page 74 of [202].
As shown in the next proposition, (4.10) implies that conformal Hamiltonian
systems approach stationary points of f .
Proposition 4.3.2 (Convergence to a stationary point). Let (xt, pt) be a solution
to the system (4.3) with initial conditions (x0, p0) = (x, p)  R2d, f continuously
differentiable, and k continuously differentiable, strictly convex with minimum at 0
and k(0) = 0. If f is bounded below andH is radially unbounded, then f(xt)2  0.
Proof. Since f is bounded below, Ht  0. Since H is radially unbounded, the set
B := {(x, p)  R2d : H(x, p)  H(x0, p0) + 1} is a compact set that contains (x0, p0)
in its interior. Moreover, by (4.10), we also have (xt, pt)  B for all t > 0. Consider
the set M = {(xt, pt) : Ht = 0}  B. Since k is strictly convex, this set is equivalent
to {(xt, pt) : pt2 = 0}  B. The largest invariant set of the dynamics (4.3) inside
M is I = {(x, p)  R2d : p2 = 0, f(x)2 = 0}  B. By LaSalles principle [145],
all trajectories started from B must approach I. Since f is a continuous bounded
function on the compact set B, there is a point x  B such that f(x)  f(x) for
every x  B (i.e. the minimum is attained in B) by the extreme value theorem (see
[224]). Moreover, due to the definition of B, x is in its interior, hence f(x)2 = 0
and therefore (x, 0)  I. Thus the set I is non-empty (note that I might contain
other local minima as well).
Remark 4.3.3. This construction can be generalized by modifying thept component
of (4.3) to a more general dissipation field D(pt). If the dissipation field is
everywhere aligned with the kinetic map, k(p), D(p)  0, then these systems
dissipate energy. We have not found alternatives to D(p) = p that result in linear
convergence in general.
4.3.3 Continuous Hamiltonian Descent on Convex Functions
In this section we study how k can be designed to condition the system (4.3) for
linear convergence in log(f(xt)  f(xmin)). Although the solutions xt, pt of (4.3)
approach stationary points under weak conditions, to derive rates we consider the
case when f is convex. To motivate our choice of k, consider the quadratic function
f(x) = x,Ax /2 with k(p) = p,A1p /2 for positive definite symmetric A  Rdd.
Now (4.3) becomes,
xt = A
pt = Axt  pt.
(4.11)
By the change of variables vt = A
1pt, this is equivalent to
xt = vt
vt = xt  vt,
(4.12)
which is a universal equation and hence the convergence rate of (4.11) is independent
of A. Although this kinetic energy implements a constant preconditioner for any f ,
for this specific f k is its convex conjugate f . This suggests the core idea of this
paper: taking k related in some sense to f  for more general convex functions may
condition the convergence of (4.3). Indeed, we show in this section that, if the kinetic
energy k(p) upper bounds a centered version of f (p), then the convergence of (4.3)
is linear.
More precisely, define the following centered function fc : Rd  R,
fc(x) = f(x+ xmin) f(xmin). (4.13)
The convex conjugate of fc is given by f
c (p) = f
(p)  xmin, p + f(xmin) and is
minimized at f c (0) = 0. Importantly, as we will show in the final lemma of this
section, taking a kinetic energy such that k(p)  max(f c (p), f c (p)) for some
  (0, 1] suffices to achieve linear rates on any differentiable convex f in continuous
time. The constant  is included to capture the fact that k may under estimate f c
by some constant factor, so long as it is positive. If  does not depend in any fashion
on f , then the convergence rate of (4.3) is independent of f . In Section 4.3.4 we also
show a partial converse  for some simple problems taking a k not satisfying those
assumptions results in sub-linear convergence for almost every path (except for one
unique curve and its mirror).
Remark 4.3.4. There is an interesting connection to duality theory for a specific choice
of k. In a slight abuse of representation, consider rewriting the original problem as
f(x) = min
(f(x) + f(x)).
The Fenchel dual of this problem is equivalent to the following problem after a small
reparameterization of p (see Chapter 31 of [222]),
(f (p) f (p)).
The Fenchel duality theorem guarantees that for a given pair of primal-dual variables
(x, p)  Rd, the duality gap between the primal objective f(x) and the dual objective
(f (p) f (p))/2 is positive. Thus,
f(x) (f (p) f (p))/2 = f(x) f(xmin) + (f (p) + f (p))/2 + f(xmin)
= f(x) f(xmin) + (f c (p) + f c (p))/2  0.
Thus, for the choice k(p) = (f c (p) + f
c (p))/2, which as we will show implies linear
convergence of (4.3), the Hamiltonian H(x, p) is exactly the duality gap between the
primal and dual objectives.
Linear rates in continuous time can be derived by a Lyapunov function V : Rdd 
[0,) that summarizes the total energy of the system, contracts exponentially (or
linearly in log-space), and is positive unless (xt, pt) = (xmin, 0). Ultimately we are
trying to prove a result of the form V t  Vt for some rate  > 0. As the energy
Ht is decreasing, it suggests using Ht as a Lyapunov function. Unfortunately, this
will not suffice, as Ht plateaus instantaneously (Ht = 0) at points on the trajectory
where pt = 0 despite xt possibly being far from xmin. However, when pt = 0, the
momentum field reduces to the term f(xt) and the derivative of xt  xmin, pt in
t is instantaneously strictly negative xt  xmin,f(xt) < 0 for convex f (unless
we are at (xmin, 0)). This suggests the family of Lyapunov functions that we study in
this paper,
V(x, p) = H(x, p) +  x xmin, p , (4.14)
where   (0, ) (see the next lemma for conditions that guarantee that it is non-
negative). As with H, Vt is used to indicate V(xt, pt) at time t along a solution to
(4.3). Before moving on to the final lemma of the section, we prove two technical
lemmas that will give us useful control over V throughout the paper.
The first lemma describes how  must be constrained for V to be positive and
to track H closely, so that it is useful for the analysis of the convergence of H and
ultimately f .
Lemma 4.3.5 (Bounding the ratio of H and V). Let x  Rd, f : Rd  R convex with
unique minimum xmin, k : Rd  R strictly convex with minimum k(0) = 0,   (0, 1]
and   (0, ].
If p  Rd is such that k(p)  f c (p), then
x xmin, p   (k(p)/ + f(x) f(xmin))  
H(x, p)
, (4.15)
H(x, p)  V(x, p). (4.16)
If p  Rd is such that k(p)  f c (p), then
x xmin, p  k(p)/ + f(x) f(xmin) 
H(x, p)
, (4.17)
V(x, p)  +
H(x, p). (4.18)
Proof. Assuming that k(p)  f c (p), we have
k(p)/ + fc(x xmin)  f c (p) + fc(x xmin)
 x xmin,p  fc(x xmin) + fc(x xmin)
= x xmin, p ,
hence we have (4.15). (4.16) follows by rearrangement. The proof of (4.17) and (4.18)
is similar.
Lemma 4.3.5 constrains  in terms of . For a result like V t  Vt , we will
need to control  in terms of the magnitude  of the dissipation field. The following
lemma provides constraints on  and, under those constraints, the optimal . The
proof can be found in Section Ap.1 of the Appendix.
Lemma 4.3.6 (Convergence rates in continuous time for fixed ). Given   (0, 1),
f : Rd  R differentiable and convex with unique minimum xmin, k : Rd  R
differentiable and strictly convex with minimum k(0) = 0. Let xt, pt  Rd be the value
at time t of a solution to the system (4.3) such that there exists   (0, 1] where
k(pt)  f c (pt). Define
(, , ) = min
    
  ,
(1 )
. (4.19)
If   (0,min(, )], then
V t  (, , )Vt.
Finally,
1. The optimal   (0,min(, )], ? = arg max (, , ) and ? = (, ?, )
are given by,
? = 1
 + 
(1 )2 + 2
, (4.20)
(1 ) + 
(1 )2 + 2
for 0 <  < 1,
(1)
2 for  = 1,
(4.21)
2. If   (0, /2], then
(, , ) =
(1 )
1  , and (4.22)
    2(1 )/4
k(pt)  xt  xmin, pt   xt  xmin,f(xt)
 (1 )(k(pt) + f(xt) f(xmin) +  xt  xmin, pt). (4.23)
These two lemmas are sufficient to prove the linear contraction of V and the
contraction f(xt)  f(xmin)  ?H0 exp(
?t) under the assumption of constant
 and . Still, the constant , which controls our approximation of f c may be
quite pessimistic if it must hold globally along xt, pt as the system converges to its
minimum. Instead, in the final lemma that collects the convergence result for this
section, we consider the case where  may increase as convergence proceeds. To
support an improving , our constant  will now have to vary with time and we will
be forced to take slightly suboptimal  and  given by (4.22) of Lemma 4.3.6. Still,
the improving  will be important in future sections for ensuring that we are able to
achieve position independent step sizes.
We are now ready to present the central result of this section. Under Assumptions
A we show linear convergence of (4.3). In general, the dependence of the rate of linear
convergence on f is via the function  and the constant C, in our analysis.
Assumption A. A.1 f : Rd  R differentiable and convex with unique minimum
xmin.
A.2 k : Rd  R differentiable and strictly convex with minimum k(0) = 0.
A.3   (0, 1).
A.4 There exists some differentiable non-increasing convex function  : [0,) 
(0, 1] and constant C,  (0,  ] such that for every p  Rd,
k(p)  (k(p)) max(f c (p), f c (p)) (4.24)
and that for every y  [0,)
C,(y)y < (y). (4.25)
In particular, if k(p)  ? max(f c (p), f c (p)) for a constant ?  (0, 1], then
the constant function (y) = ? serves as a valid, but pessimistic choice.
Remark 4.3.7. Assumption A.4 can be satisfied if a symmetric lower bound on f is
known. For example, strong convexity implies
f(x+ xmin) f(xmin) 
x22 .
This in turn implies f c (p)  p
2 /(2). Because k(p) = p
2 /(2) is symmetric, it
satisfies A.4 which explains why conditions relating to strong convexity are necessary
for linear convergence of Polyaks heavy ball.
Theorem 4.3.8 (Convergence bound in continuous time with general ). Given f ,
k, , , C, satisfying Assumptions A. Let (xt, pt) be a solution to the system (4.3)
with initial states (x0, p0) = (x, 0) where x  Rd. Let ? = (3H0),  = (1)C,4 ,
and W : [0,) [0,) be the solution of
W t =   (2Wt)Wt,
with W0 := H0 = f(x0) f(xmin). Then for every t  [0,), we have
f(xt) f(xmin)  2H0 exp
(2Wt)
 2H0 exp (?t) . (4.26)
Proof. By (4.24) in assumption A.4, the conditions of Lemma 4.3.5 hold, and by
(4.15) and (4.17) we have
| xt  xmin, pt |  k(pt)/(k(pt)) + f(xt) f(xmin) 
(k(pt))
. (4.27)
Instead of defining the Lyapunov function Vt exactly as in (4.14) we take a time-
dependent t. Specifically, for every t  0 let Vt be the unique solution v of the
equation
v = Ht +
C,(2v)
xt  xmin, pt (4.28)
in the interval v  [Ht/2, 3Ht/2]. To see why this equation has a unique solution in
v  [Ht/2, 3Ht/2], note that from (4.27) it follows that
| (2v) xt  xmin, pt |  Ht for every v 
and hence for any such v, we have
 Ht +
C,(2v)
xt  xmin, pt 
Ht. (4.29)
This means that for v = Ht
, the left hand side of (4.28) is smaller than the right hand
side, while for v = 3Ht
, it is the other way around. Now using (4.25) in assumption
A.4 and (4.27), we have
|C, (2Vt) xt  xmin, pt| 
C,
(2Vt)2Vt
(2Vt)
 < 1, (4.30)
Thus, by differentiation, we can see that (4.30) implies that
v Ht  C,2 (2v) xt  xmin, pt
which implies that (4.28) has a unique solution Vt in [H2 ,
]. Let t = (2Vt) and
 (2Vt). By the implicit function theorem, it follows that Vt is differentiable
in t. Morover, since
Vt = Ht +
C,(2Vt)
xt  xmin, pt (4.31)
for every t  0, by differentiating both sides, we obtain that
V t = (  t) k(pt), pt  t xt  xmin, pt  t xt  xmin,f(xt)
+ t xt  xmin, pt
The first three terms are equivalent to the temporal derivative of Vt with constant
 = t. Since t  (k(pt)) and t  , the assumptions of Lemma 4.3.6 are satisfied
locally for t, t and we get
V t  (t, t, )Vt + t xt  xmin, pt = (t, t, )Vt + C,t xt  xmin, pt V t.
Using (4.22) of Lemma 4.3.6 for t, t, we have (t, t, ) =
t(1)
1t  t(1 ) and
V t  t(1 )Vt + C,t xt  xmin, pt V t.
Using (4.30) we have V t  
t(1)
Vt. Notice that V0 = H0 since we have assumed
that p0 = 0, and the claim of the lemma follows by Gronwalls inequality. The final
inequality (4.26) follows from the fact that (2Vt)  (3H0) = ?.
4.3.4 Partial Lower Bounds
In this section we consider a partial converse of Proposition 4.3.8, showing in a simple
setting that if the assumption k(p)  max(f c (p), f c (p)) of A.4 is violated, then
the ODE (4.3) contracts sub-linearly. Figure 4.4 considers the example f(x) = x4/4.
If k(p) = |p|a/a, then assumptions A cannot be satisfied for small p unless b  4/3.
Figure 4.4 shows that an inappropriate choice of k(p) = p2/2 leads to sub-linear
convergence both in continuous time and for one of the discretizations of Section 4.4.
In contrast, the choice of k(p) = 3p4/3/4 results in linear convergence, as expected.
Let b, a > 1 and  > 0. For d = 1 dimension, with the choice f(x) := |x|b/b and
k(p) := |p|a/a, (4.3) takes the following form,
xt = |pt|a1 sign(pt),
pt = |xt|b1 sign(xt) pt.
(4.32)
Objective
log f(xt)
log f(xi)
0 10 20 30
time t
f(x) = x4/4
k(p) = 3p4/3/4
Solution & Iterates
0 10 20 30
time t
t f(x) = x
k(p) = p2/2
Figure 4.4: Importance of Assumptions A. Solutions xt and iterates xi of our first
explicit method on f(x) = x4/4 with two different choices of k. Notice that f c (p) =
3p4/3/4 and thus k(p) = p2/2 cannot be made to satisfy assumption A.4.
Since f(x) takes its minimum at 0, (xt, pt) are expected to converge to (0, 0) as
t. There is a trivial solution: xt = pt = 0 for every t  R. The following Lemma
shows an existence and uniqueness result for this equation. The proof is included in
Section Ap.2 of the Appendix.
Lemma 4.3.9 (Existence and uniqueness of solutions of the ODE). Let a, b,  
(0,). For every t0  R and (x, p)  R2, there is a unique solution (xt, pt)tR of
the ODE (4.32) with xt0 = x, pt0 = p. Either xt = pt = 0 for every t  R, or
(xt, pt) 6= (0, 0) for every t  R.
Note that if (xt, pt) is a solution, and   R, then (xt+, pt+) is also a solution
(time translation), and (xt,pt) is also a solution (central symmetry).
Note also that f (p) = f (p) = |p|b/b for b := (1  1
)1. Hence if a  b,
or equivalently, if 1
 1, the conditions of Proposition 4.3.8 are satisfied for
some  > 0 (in particular, if a = b, then  = 1 independently of x0, p0). Hence in
1 0 1
position x
Two typical paths
0 10 20 30
time t
1 0 1
position x
Unique fast paths
0 10 20 30
time t
Figure 4.5: Solutions to the Hamiltonian descent system with f(x) = x4/4 and k(p) =
x2/2. The right plots show a numerical approximation of (x
t , p
t ) and (x
t ,p
The left plots show a numerical approximation of (x
t , p
t ) and (x
t ,p
t ) for
 =  +   R, which represent typical paths.
such cases, the speed of convergence is linear. For a > b, limp0
f(p)
= 0, so the
conditions of Proposition 4.3.8 are violated.
Now we are ready to state the main result in this section, a theorem characterizing
the convergence speeds of (xt, pt) to (0, 0) in this situation. The proof is included in
Section Ap.2 of the Appendix.
Proposition 4.3.10 (Lower bounds on the convergence rate in continuous time).
Suppose that 1
< 1. For any   R, we denote by (x()t , p
t ) the unique solution
of (4.32) with x0 = , p0 = 0. Then there exists a constant   (0,) depending on a
and b such that the path (x
t , p
t ) and its mirrored version (x
t , p
t ) satisfy that
|x()t | = |x
t |  O(exp(t)) for every  < (a 1) as t.
For any path (xt, pt) that is not a time translation of (x
t , p
t ) or (x
t , p
t ), we
have x1t
 = O(t
baba ) as t,
so the speed of convergence is sub-linear and not linearly fast.
Figure 4.5 illustrates the two paths where the convergence is linearly fast for
a = 2, b = 4. The main idea in the proof of Proposition 4.3.10 is that we establish the
existence of a class of trapping sets, i.e. once the path of the ODE enters one of them,
it never escapes. Convergence rates within such sets can be shown to be logarithmic,
and it is established that only two paths (which are symmetric with respect to the
origin) avoid each one of the trapping sets, and they have linear convergence rate.
4.4 Optimization Algorithms
In this section we consider three discretizations of the continuous system (4.3), one
implicit and two explicit. For these discretizations we must assume more about the
relationship between f and k. The implicit method defines the iterates as solution of
a local subproblem. The first and second explicit methods are fully explicit, and we
must again make stronger assumptions on f and k. The proofs of all of the results in
this section are given in Section Ap.3 of the Appendix.
4.4.1 Implicit Method
Consider the following discrete approximation (xi, pi) to the continuous system, making
the fixed  > 0 finite difference approximation,
xi+1xi
= xt and
pi+1pi
= pt, which
approximates the field at the forward points.
xi+1  xi
= k(pi+1)
pi+1  pi
= pi+1 f(xi+1).
(4.33)
Since  k( k(p)) = p, this system of equations corresponds to the stationary
condition of the following subproblem iteration, which we introduce as our implicit
method.
Method 1 (Implicit Method). Given f, k : Rd  R, ,   (0,), x0, p0  Rd.
Let  = (1 + )1 and
xi+1 = arg min
k(xxi
) + f(x)  pi, x
pi+1 = pi  f(xi+1).
(4.34)
The following lemma shows that the formulation (4.34) is well defined. The proof
is included in Section Ap.3 of the Appendix.
Lemma 4.4.1 (Well-definedness of the implicit scheme). Suppose that f and k satisfy
assumptions A.1 and A.2, and ,   (0,). Then (4.34) has a unique solution for
every xi, pi  Rd, and this solution also satisfies (4.33).
As this discretization involves solving a potentially costly subproblem at each
iteration, it requires a relatively light assumption on the compatibility of f and k.
Assumption B. B.1 There exists Cf,k  (0,) such that for all x, p  Rd,
| f(x),k(p) |  Cf,kH(x, p). (4.35)
Remark 4.4.2. Smoothness of f implies 1
f(x)22  L(f(x)f(xmin)) (see (2.1.7) of
Theorem 2.1.5 of [193]). Thus, if f is smooth and k(p) = 1
p22, then the assumption
B.1 can be satisfied by Cf,k = max{1, L}, since
| f(x),k(p) |  1
f(x)22 + 12 k(p)
2  L(f(x) f(xmin)) + k(p).
The following proposition shows a convergence result for the implicit scheme.
Proposition 4.4.3 (Convergence bound for the implicit scheme). Given f , k, , ,
C,, and Cf,k satisfying assumptions A and B. Suppose that  <
2 max(Cf,k,1)
. Let
? = (3H0), and let W0 = f(x0) f(xmin) and for i  0,
Wi+1 =Wi [1 + C,(1   2Cf,k)(2Wi)/4]1 .
Then for any (x0, p0) with p0 = 0, the iterates of (4.33) satisfy for every i  0,
f(xi) f(xmin)  2Wi  2W0[1 + C,(1   2Cf,k)?/4]i.
Remark 4.4.4. Proposition 4.4.3 means that we can fix any step size 0 <  <
2 max(Cf,k,1)
independently of the initial point, and have linear convergence with contraction
rate that is proportional to (3H0) initially and possibly increasing as we get closer
to the optimum. In Section 4.5 we introduce kinetic energies k(p) that behave like
pa2 near 0 and p
2 in the tails. We will show that for functions f(x) that behave
like x xminb2 near their minima and x xmin
2 in the tails the conditions of
assumptions B are satisfied as long as 1
= 1 and 1
 1. In particular, if we
choose k(p) =
p22 + 1 1 (relativistic kinetic energy), then a = 2 and A = 1, and
assumptions B can be shown to hold for every f that has quadratic behavior near its
minimum and no faster than exponential growth in the tails.
4.4.2 First Explicit Method, with Analysis via the Hessian
The following discrete approximation (xi, pi) to the continuous system makes a similar
finite difference approximation,
xi+1xi
= xt and
pi+1pi
= pt for  > 0. In contrast to
the implicit method, it approximates the field at the point (xi, pi+1), making it fully
explicit without any costly subproblem,
xi+1  xi
= k(pi+1)
pi+1  pi
= pi+1 f(xi).
This method can be rewritten as our first explicit method.
Method 2 (First Explicit Method). Given f, k : Rd  R, ,   (0,), x0, p0  Rd.
Let  = (1 + )1 and
pi+1 = pi  f(xi)
xi+1 = xi + k(pi+1).
(4.36)
This discretization exploits the convexity of k by approximating the continuous
dynamics at the forward point pi+1, but is made explicit by approximating at the
backward point xi. Because this method approximates the field at the backward
point xi it requires a kind of smoothness assumption to prevent f from changing
too rapidly between iterates. This assumption is in the form of a condition on the
Hessian of f , and thus we require twice differentiability of f for the first explicit
method. Because the accumulation of gradients of f in the form of pi are modulated
by k, this condition in fact expresses a requirement on the interaction between k
and 2f , see assumption C.3.
Assumption C. C.1 There exists Ck  (0,) such that for every p  Rd,
k(p), p  Ckk(p). (4.37)
C.2 f : Rd  R convex with a unique minimum at xmin and twice continuously
differentiable for every x  Rd \ {xmin}.
C.3 There exists Df,k  (0,) such that for every p  Rd, x  Rd \ {xmin},
k(p),2f(x)k(p)
 Df,k(3H(x, p))H(x, p). (4.38)
Remark 4.4.5. If f smooth and twice differentiable then v,2f(x)v is everywhere
bounded by L for v  Rd such that v2 = 1 (see Theorem 2.1.6 of [193]). Thus,
using k(p) = 1
p22, this allows us to satisfy assumption C.3 with Df,k = max{1, 2L},
since
k(p),2f(x)k(p)
 L k(p)22 = 2Lk(p)  f(x) f(xmin) + 2Lk(p).
Assumption C.1 is clearly satisfied in this case by Ck = 2.
The following lemma shows a convergence result for this discretization.
Proposition 4.4.6 (Convergence bound for the first explicit scheme). Given f , k,
, , C,, Cf,k, Ck, Df,k satisfying assumptions A, B, and C, and that 0 <  <
2 max(Cf,k+6Df,k/C, ,1)
10Cf,k+5Ck
. Let ? = (3H0), W0 := f(x0)  f(xmin),
and for i  0, let
Wi+1 =Wi
C,
[1   2(Cf,k + 6Df,k/C,)](2Wi)
Then for any (x0, p0) with p0 = 0, the iterates (4.36) satisfy for every i  0,
f(xi) f(xmin)  2Wi  2W0
C,
[1   2(Cf,k + 6Df,k/C,)]?
Remark 4.4.7. Similar to Remark 4.4.4, Proposition 4.4.6 implies that, under suitable
assumptions and position independent step sizes, the first explicit method can achieve
linear convergence with contraction rate that is proportional to (3H0) initially
and possibly increasing as we get closer to the optimum. In particular, again as
remarked in Remark 4.4.4, for f(x) that behave like x xminb2 near their minima
and x xminB2 in the tails the conditions of assumptions C can be satisfied for
kinetic energies that grow like pa2 in the body and p
2 in the tails as long as
= 1, 1
 1. The distinction here is that for the first explicit method we
will require b, B  2.
4.4.3 Second Explicit Method, with Analysis via the Hessian
Our second explicit method inverts relationship between f and k from the first. Again,
it makes a fixed  step approximation
xi+1xi
= xt and
pi+1pi
= pt. In contrast to
the implicit (4.33) and first explicit (4.36) methods, it approximates the field at the
point (xi+1, pi).
Method 3 (Second Explicit Method). Given f, k : Rd  R, ,   (0,), x0, p0 
Rd. Let,
xi+1 = xi + k(pi)
pi+1 = (1 )pi  f(xi+1).
(4.39)
This discretization exploits the convexity of f by approximating the continuous
dynamics at the forward point xi+1, but is made explicit by approximating at the
backward point pi. As with the other explicit method, it requires a smoothness
assumption to prevent k from changing too rapidly between iterates, which is expressed
as a requirement on the interaction between f and 2k, see assumption D.5. These
assumptions can be satisfied for k that have quadratic or higher power growth and
are suitable for f that may have unbounded second derivatives at their minima (for
such f , Assumptions C can not hold).
Assumption D. D.1 k : Rd  R strictly convex with minimum k(0) = 0 and twice
continuously differentiable for every p  Rd \ {0}.
D.2 There exists Ck  (0,) such that for every p  Rd,
k(p), p  Ckk(p). (4.40)
D.3 There exists Dk  (0,) such that for every p  Rd \ {0},
p,2k(p)p
 Dkk(p). (4.41)
D.4 There exists Ek, Fk  (0,) such that for every p, q  Rd,
k(p) k(q)  Ekk(q) + Fk k(p)k(q), p q . (4.42)
D.5 There exists Df,k  (0,) such that for every x  Rd, p  Rd \ {0},
f(x),2k(p)f(x)
 Df,k(3H(x, p))H(x, p). (4.43)
Remark 4.4.8. Smoothness of f implies 1
f(x)22  L(f(x)f(xmin)) (see (2.1.7) of
Theorem 2.1.5 of [193]). Thus, if f is smooth and k(p) = 1
p22, then the assumption
D.5 can be satisfied by Df,k = max{1, 2L}, since 2k(p) = I and
f(x),2k(p)f(x)
= f(x)22  2L(f(x)f(xmin))  2L(f(x)f(xmin))+k(p).
The k-specific assumptions D.2 and D.3 can clearly be satisfied with Ck = Dk = 2 in
this case. We show that D.4 can be satisfied in Section 4.5.
Objective
log f(xt)
log f(xi)
f(x) = x4/4
k(p) = p8/77/8
Solution & Iterates
Figure 4.6: Importance of discretization assumptions. Solutions xt and iterates xi of
our first explicit method on f(x) = x4/4. With an inappropriate choice of kinetic
energy, k(p) = p8/77/8, the continuous solution converges at a linear rate but the
iterates do not.
Proposition 4.4.9 (Convergence bound for the second explicit scheme). Given f ,
k, , , C,, Cf,k, Ck, Dk, Df,k, Ek, Fk satisfying assumptions A, B, and D, and
0 <  < min
2(Cf,k + 6Df,k/C,)
8Dk(1 + Ek)
6(5Cf,k + 2Ck) + 12C,
62DkFk
Let ? = (3H0), W0 := f(x0) f(xmin), and for i  0, let
Wi+1 =Wi
1 C,
[1   2(Cf,k + 6Df,k/C,)](2Wi)
Then for any (x0, p0) with p0 = 0, the iterates (4.39) satisfy for every i  0,
f(xi) f(xmin)  2Wi  2W0 
1 C,
[1   2(Cf,k + 6Df,k/C,)]?
Remark 4.4.10. Similar to Remark 4.4.4, Proposition 4.4.9 implies that, under suitable
assumptions and for a fixed step size independent of the initial point, the second
explicit method can achieve linear convergence with contraction rate that is proportional
to (3H0) initially and possibly increasing as we get closer to the optimum. In
particular, again as remarked in Remark 4.4.4, for f(x) that behave like x xminb2
near their minima and x xminB2 in the tails the conditions of assumptions D can
be satisfied for kinetic energies that grow like pa2 in the body and p
2 in the tails
as long as 1
= 1, 1
 1. The distinction here is that for the second explicit
method we will require b, B  2.
To conclude the analysis of our methods on convex functions, consider the example
f(x) = x4/4 from Figure 4.4. If we take k(p) = |p|a/a, then assumption A.4 requires
that a  4/3. Assumptions B and C cannot be satisfied as long as a < 4/3, which
suggests that k(p) = f (p) is the only suitable choice in this case. Indeed, in Figure
4.6, we see that the choice of k(p) = p8/77/8 results in a system whose continuous
dynamics converge at a linear rate and whose discrete dynamics fail to converge.
Note that as the continuous systems converge the oscillation frequency increases
dramatically, making it difficult for a fixed step size scheme to approximate.
4.4.4 First Explicit Method on Non-Convex f
We close this section with a brief analysis of the convergence of the first explicit
method on non-convex f . A traditional requirement of discretizations is some degree
of smoothness to prevent the function changing too rapidly between points of approximation.
The notion of Lipschitz smoothness is the standard one, but the use of the kinetic
mapk to select iterates allows Hamiltonian descent methods to consider the broader
definition of uniform smoothness, as discussed in [277, 13, 278] but specialized here
for our purposes.
Uniform smoothness is defined by a norm  and a convex non-decreasing function
 : [0,) [0,] such that (0) = 0. A function f : Rd  R is -uniformly smooth,
if for all x, y  Rd,
f(y)  f(x) + f(x), y  x+ (y  x). (4.44)
Lipschitz smoothness corresponds to (t) = 1
t2, and generally speaking there exist
non-trivial uniformly smooth functions for (t) = 1
tb for 1 < b  2, see, e.g., [192,
277, 13, 278].
Assumption E. E.1 f : Rd  R differentiable.
E.2   (0,).
E.3 There exists a norm  on Rd, b  (1,), Dk  (0,), Df  (0,),  :
[0,)  [0,] non-decreasing convex such that (0) = 0 and (ct)  cb(t)
for c, t  (0,); for all p  Rd,
(k(p))  Dkk(p); (4.45)
and for all x, y  Rd,
f(y)  f(y) + f(x), y  x+Df(y  x). (4.46)
Lemma 4.4.11 (Convergence of the first explicit scheme without convexity). Given
, f , k, , b, Dk, Df ,  satisfying assumptions E and A.2. If   (0, b1
/DfDk],
then the iterates (4.36) of the first explicit method satisfy
Hi+1 Hi  (bDfDk  )k(pi+1)  0, (4.47)
and f(xi)2  0.
Remark 4.4.12. L-Lipschitz continuity of the gradients f(x)f(y)2  L x y2
for L > 0 with Euclidean norm 2 implies both f(y)  f(y) + f(x), y  x +
y  x22 and 12 f(x)
2  L(f(x)  f(xmin)). Thus, if f, k are Lf , Lk smooth,
respectively, then the condition for convergence simplifies to   /LfLk.
4.5 Kinetic Maps for Functions with Power Behavior
In this section we design a family of kinetic maps k suitable for a class of functions
f that exhibit power growth, which we will describe precisely as a set of assumptions.
This class includes strongly convex and smooth functions. However, it is much
broader, including functions with possibly non-quadratic power behavior and singular
or unbounded Hessians. First, we show that this family of kinetic energies satisfies
the k-specific assumptions of Section 4.4. Then we use the generic analysis of Section
4.4 to provide a specific set of assumptions on fs and their match to the choice of
k. As a consequence, this analysis greatly extends the class of functions for which
linear convergence is possible with fixed step size first order computation. Still, this
analysis is not meant to be an exhaustive catalogue of possible kinetic energies for
Hamiltonian descent. Instead, it serves as an example of how known properties of f
can be used to design k. Note that, with a few exceptions, the proofs of all of our
results in this section are deferred to Section Ap.4 of the Appendix.
4.5.1 Power Kinetic Energies
We assume a given norm x and its dual p = sup{x, p : x  1} for x, p  Rd.
Define the family of power kinetic energies k,
k(p) = Aa (p) where 
a (t) =
(ta + 1)
a  1
for t  [0,) and a,A  [1,).
(4.48)
For a = A we recover the standard power functions, aa(t) = t
a/a. For distinct
a 6= A, we have (Aa )(t)  tA1 for large t and (Aa )(t)  ta1 for small t. Thus,
2 1 0 1 2
'Aa (|x|) with a = 8/7
2 1 0 1 2
'Aa (|x|) with a = 2
2 1 0 1 2
'Aa (|x|) with a = 8
A = 8/7
A = 2
A = 8
Figure 4.7: Power kinetic energies in one dimension.
k(p)  pA /A as p   and k(p)  p
 /a as p  0. See Figure 4.7 for
examples from this family in one dimension.
Broadly speaking, this family of kinetic energies must be matched in a conjugate
fashion to the body and tail behavior of f . Informally, for this choice of k we will
require conditions on f that correspond to requiring that it grows like x xminb in
the body (as x xmin  0) and x xminB in the tails (as x xmin  ) for
some b, B  (1,). In particular, our growth conditions in the case of f growing
like x22 = x, x everywhere will be necessary conditions of strong convexity and
smoothness. More generally, a,A, b, B will be well-matched if 1/a+1/b = 1/A+1/B =
1, but other scenarios are possible. Of these, the conjugate relationship between a
and b is the most critical; it captures the asymptotic match between f and k as
(xi, pi) (xmin, 0), and our analysis requires that 1/a+ 1/b = 1. The match between
A and B is less critical. In the ideal case, B is known and A = B/(B1). In this case,
the discretizations will converge at a constant fast linear rate. If B is not known, it
suffices for 1/A+ 1/B  1. The consequence of underestimating A < B/(B  1) will
be reflected in a linear, but non-constant, rate of convergence (via  of Assumption
A.4), which depends on the initial x0 and slowly improves towards a fast rate as
the system converges and the regime switches. We present a complete analysis and
set of conditions on f for two of the most useful scenarios. In Proposition 4.5.8 we
consider the case that f grows like Bb (x xmin) where b, B > 1 are exactly known.
In this case convergence proceeds at a fast constant linear rate when matched with
k(p) = Aa (p) where a = b/(b  1) and A = B/(B  1). In Proposition 4.5.10
we consider the case that f grows like B2 (x xmin) where B  2 is unknown.
Here, the convergence is linear with a non-constant rate when matched with the
relativistic kinetic energy k(p) = 12(p). The case covered by relativistic kinetic
f(x) grows like Bb (x) appropriate k(p) = Aa (p)
method powers known? body power b tail power B body power a tail power A
implicit known b > 1 B > 1 a = b/(b 1) A = B/(B  1)
unknown b = 2 B  2 a = 2 A = 1
1st explicit known b  2 B  2 a = b/(b 1) A = B/(B  1)
unknown b = 2 B  2 a = 2 A = 1
2nd explicit known 1 < b  2 1 < B  2 a = b/(b 1) A = B/(B  1)
Table 4.1: A summary of the conditions on f and power kinetic k considered in this
section that satisfy the assumptions of Section 4.4. Here grows like is an imprecise
term meaning that fs growth can be bounded in an appropriate way by Bb (x) (Bb
is defined in (4.48)). The full precise assumptions on f are laid out in Propositions
4.5.8 and 4.5.10. In particular, b = B = 2 corresponds to assumptions similar in
spirit to strong convexity and smoothness. Other combinations of b, B and a,A are
possible.
k is particularly valuable, as it covers a large class of globally non-smooth, but
strongly convex functions. Table 4.1 summarizes this, and throughout the remaining
subsections we flesh out the details of these claims.
For these kinetic energies to be suitable in our analysis, they must at minimum
satisfy assumptions A.2, C.1, D.1, D.3, and D.4. Assumptions C.1 and D.3 are clearly
satisfied by k(p) = |p|a/a for p  R with constants Ck = a and Dk = a(a  1). In
the remainder of this subsection, we provide conditions on the norms and a,A under
which assumptions like these hold for Aa with multiple power behavior in any finite
dimension.
In general, the problematic terms ofk(p) and2k(p) that arise in high dimensions
involve the gradient and Hessian of the norm. The gradient of norm can be dealt with
cleanly, but our analysis requires additional control on the Hessian of the norm. To
control terms involving 2 p we define a generalization of the maximum eigenvalue
induced by the norm . Let max : Rdd  R be the function defined by
max(M) = sup{v,Mv : v  Rd, v = 1}. (4.49)
For symmetric M  Rdd and Euclidean  this is exactly the maximum eigenvalue
of M . Now we are able to state our lemma analyzing power kinetic energies.
Lemma 4.5.1 (Verifying assumptions on k). Given a norm p on p  Rd, a,A 
[1,), and Aa in (4.48). Define the constant,
Ca,A =
) a1
Aa +
. (4.50)
k(p) = Aa (p) satisfies the following.
1. Convexity. If a > 1 or A > 1, then k is strictly convex with a unique minimum
at 0  Rd.
2. Conjugate. For all x  Rd, k(x) = (Aa )(x).
3. Gradient. If p is differentiable at p  Rd \ {0} and a > 1, then k is
differentiable for all p  Rd, and for all p  Rd,
k(p), p  max{a,A}k(p), (4.51)
(Aa )
(k(p))  (max{a,A}  1)k(p). (4.52)
Additionally, if a,A > 1, define B = A/(A 1), b = a/(a 1), and then
Bb (k(p))  Ca,A(max{a,A}  1)k(p). (4.53)
Additionally, if a,A  2, then for all p, q  Rd,
k(p)  k(q), q+ k(p)k(q), p q . (4.54)
4. Hessian. If p is twice continuously differentiable at p  Rd \ {0}, then k is
twice continuously differentiable for all p  Rd \ {0}, and for all p  Rd \ {0},
p,2k(p)p
 max{a,A}(max{a,A}  1)k(p). (4.55)
Additionally, if a,A  2 and there exists N  [0,) such that p 
max(2 p) 
N for p  Rd \ {0}, then for all p  Rd \ {0}
max(2k(p))
max{a,A}  1 +N
 (max{a,A}  2)k(p). (4.56)
Remark 4.5.2. (4.51), (4.54), and (4.55) together directly confirm that these k satisfy
C.1, D.3, and D.4 with constants Ck = max{a,A}, Dk = max{a,A}(max{a,A} 1),
Ek = max{a,A}  1, and Fk = 1. The other results (4.52), (4.53), and (4.56) will
be used in subsequent lemmas along with assumptions on f to satisfy the remaining
assumptions of discretization.
The assumption that p 
max(2 p)  N in Lemma 4.5.1 is satisfied by b-
norms for b  [2,), as the following lemma confirms. It implies that if p = pb
for b  2, we can take N = b 1 in (4.56).
Lemma 4.5.3 (Bounds on 
max(2 p) for b-norms). Given b  [2,), let xb =(d
n=1 |x(n)|b
for x  Rd. Then for x  Rd \ {0},
xb 
2 xb
 (b 1).
The remaining assumptions B.1, C.3, and D.5 involve inner products between
derivatives of f and k. To control these terms we will use the Fenchel-Young inequality.
To this end, the conjugates of Aa will be a crucial component of our analysis.
Lemma 4.5.4 (Convex conjugates of Aa ). Given a,A  (1,) and Aa in (4.48).
Define B = A/(A 1), b = a/(a 1). The following hold.
1. Near Conjugate. Bb upper bounds the conjugate (
 for all t  [0,),
(Aa )
(t)  Bb (t). (4.57)
2. Conjugate. For all t  [0,),
(aa)
(t) = bb(t). (4.58)
(A1 )
(t) =
0 t  [0, 1]
tB  t+ 1
t  (1,)
. (4.59)
(1a)
(t) =
1 (1 tb)
b t  [0, 1]
 t  (1,)
. (4.60)
(11)
(t) =
0 t  [0, 1]
 t  (1,)
. (4.61)
4.5.2 Matching power kinetic k with assumptions on f
In this subsection and the next we study assumptions on f that imply the suitability
of k(p) = Aa (p) with the discretizations of Section 4.4. The preceding subsection
is an analysis that verifies that such k satisfy the k-specific assumptions A, C, and
D. We now consider the remaining assumptions of A, B, C, and D, which require an
appropriate match between f and k. This includes the derivation of  and control
of terms of the form f(x),k(p) and k(p),2f(x)k(p) by the total energy
Gradient descent
0 500 1000
k(p) = 22(p)
0 500 1000
iteration i
k(p) = 28(p)
0 500 1000
Figure 4.8: Optimizing f(x) = 28/7(x) with three different methods with fixed step
size: gradient descent, classical momentum, and our second explicit method. Because
the second derivative of f is infinite at its minimum, only second explicit method with
k(p) = 28(p) is able to converge with a fixed step size.
H(x, p). Here we consider the case that f exhibits power behavior with known, but
possibly distinct, powers in the body and the tails.
To see a complete example of this type of analysis, take the case f(x) = |x|b/b and
k(p) = |p|a/a with x, p  R, b > 2, a < 2, and 1/a+ 1/b = 1. For  the strategy will
be to find a lower bound on f that is symmetric about f s minimum. The conjugate
of the centered lower bound can be used to construct an upper bound on f c with
which the gap between k and f c can be studied. In this case it is simple, as we have
f c (p) = k(p) and  = 1. The strategy for terms of the form f(x),k(p) and
k(p),2f(x)k(p) will be a careful application of the Fenchel-Young inequality.
Using a1 = a/b, the conjugacy of b and b/(b1), and the Fenchel-Young inequality,
| f(x),k(p) | = |x|b1|p|a/b  b1
(|x|b1)
b1 + 1
(|p|a/b)b = (b 1)f(x) + (a 1)k(p)
 (max{a, b}  1)H(x, p).
Finally, using the conjugacy of b/2 and b/(b  2) and again the Fenchel-Young
inequality,
k(p),2f(x)k(p)
= (b 1)|x|b2|p|2a/b  (b1)(b2)
(|x|b2)
b2 +
(b1)2
(|p|2a/b) b2
= (b 1)(b 2)f(x) + 2(b 1)(a 1)k(p)
 (b 1) max{b 2, 2(a 1)}H(x, p).
Along with Lemma 4.5.1, this covers Assumptions A, B, and C. Thus, we can justify
the use of the first explicit method for this f, k. All of the analyses of this section
essentially follow this outline.
Remark 4.5.5. These strategies apply naturally when f is twice differentiable and
smooth. In this case, we have 1
f(x)22  L(f(x)f(xmin)) and 
max(2f(x))  L.
Thus, using k(p) = 1
p22 is appropriate and f(x),k(p)  max{L, 1}H(x, p) and
k(p),2f(x)k(p)  2Lk(p).
We are now ready to consider the case of f growing like Bb (x xmin) matched
with k(p) = Aa (p) for 1/a + 1/b = 1/A + 1/B = 1. Assumptions F, below, will
be used in different combinations to confirm that the assumptions of the different
discretizations are satisfied. Assumptions F.1, F.2, F.3, and F.4 are required for all
methods. The explicit methods each require an additional assumption: F.5 for the
first explicit method and F.6 for the second. Thus, for f : Rd  R and k(p) =
12(p), Proposition 4.5.8 can be summarised as
F.1  F.2  F.3  F.4 A  B,
F.1  F.2  F.3  F.4  F.5 A  B  C,
F.1  F.2  F.3  F.4  F.6 A  B D.
Note, that Lemma 4.5.1 implies that the power kinetic energies are themselves examples
of functions satisfying Assumptions F. Figure 4.8 illustrates a consequence of this
proposition; f(x) = 28/7(x) for x  R is a difficult function to optimize with a first
order method using a fixed step size; the second derivative grows without bound as
x  0. As shown, Hamiltonian descent with the matched k(p) = 28(p) converges,
while gradient descent and classical momentum do not.
Assumption F. F.1 f : Rd  R differentiable and convex with unique minimum
xmin.
F.2 p is differentiable at p  Rd \ {0} with dual norm x = sup{x, p : p =
F.3 B = A/(A 1), and b = a/(a 1).
F.4 There exist , L  (0,) such that for all x  Rd
f(x) f(xmin)  Bb (x xmin)
Aa (f(x))  L(f(x) f(xmin)).
(4.62)
F.5 b  2 and B  2. f : Rd  R is twice continuously differentiable for all
x  Rd\{xmin} and there exists Lf , Df  (0,) such that for all x  Rd\{xmin}
max(2f(x))
 Df (f(x) f(xmin)). (4.63)
F.6 b  2 and B  2. p is twice continuously differentiable at p  Rd \ {0}, and
there exists N  (0,) such that max(2 p)  N p
 for all p  Rd \{0}.
Remark 4.5.6. Assumption F.4 can be read as the requirement that f is bounded
above and below by Bb , and in the b = B = 2 case it is a necessary condition of
strong convexity and smoothness.
Remark 4.5.7. Assumption F.5 generalizes a sufficient condition for smoothness. Consider
for simplicity the Euclidean norm case  = 2 and let max(M) be the maximum
eigenvalue of M  Rdd. If b = B = 2, then (B/2
) is finite only on [0, 1] where it is
zero. Moreover, F.5 simplifies to there existing Lf  (0,) such that max(2f(x)) 
Lf everywhere, the standard smoothness condition. When b > 2, B = 2, (
) is
finite on [0, 1] where it behaves like a power b/(b  2) function for small arguments.
Thus, F.5 can be satisfied in the Euclidean norm case by a function whose maximum
eigenvalue is shrinking like x xminb22 as x  xmin; the balance of where the
behavior switches can be controlled by Lf . When b = 2, B > 2, the role is switched
and F.5 can be satisfied by a function whose maximum eigenvalue is bounded near
the minimum and grows like x xminB22 as x xmin2 . When b, B > 2, this
can be satisfied by a function whose maximum eigenvalue shrinks like x xminb22
in the body and grows like x xminB22 in the tail.
Proposition 4.5.8 (Verifying assumptions for f with known power behavior and
appropriate k). Given a norm  satisfying F.2 and a,A  (1,), take
k(p) = Aa (p).
with Aa defined in (4.48). The following cases hold with this choice of k on f : R
d  R
convex.
0 100 200 300 400 500
iterates xi
Gradient descent
0 100 200 300 400 500
iterates xi
k(p) = ||p||2
dimension d
Figure 4.9: Dimension dependence on f(x) = x24 /2 initialized at x0 = (2, 2, . . . , 2) 
Rd. Left: Gradient descent with fixed step size equal to the inverse of the smoothness
coefficient, L = 3. Right: Hamiltonian descent with k(p) = p24/3 /2 and a fixed
step size (same for all dimensions). Gradient descent convergences linearly with rate
 = 3/(3  1/
d) while Hamiltonian descent converges with dimension-free linear
rates.
1. For the implicit method (4.33), assumptions A, B hold with constants
 = min{a1, A1, 1} C, =  Cf,k = max{a 1, A 1, L}, (4.64)
if f, a, A, , L,  satisfy assumptions F.1, F.2, F.3, F.4.
2. For the first explicit method (4.36), assumptions A, B, and C hold with constants
(4.64) and
Ck = max{a,A} Df,k = Lf1 max {Df , 2Ca,A(max{a,A}  1)} , (4.65)
if f, a, A, , L, Lf , Df ,  satisfy assumptions F.1, F.2, F.3, F.4, and F.5.
3. For the second explicit method (4.39), assumptions A, B, and D hold with
constants (4.64) and
Ck = max{a,A} Dk = max{a,A}(max{a,A}  1)
Ek = max{a,A}  1 Fk = 1
Df,k = 
1(max{a,A}  1 +N) max {2L, a 2, A 2} ,
(4.66)
if f, a, A, , L,N,  satisfy assumptions F.1, F.2, F.3, F.4, and F.6.
We highlight an interesting consequence of Proposition 4.5.8 for high dimensional
problems. For many first-order methods using standard gradients on smooth f ,
linear convergence can be guaranteed by the Polyak- Lojasiewicz (PL) inequality,
f(x)22 /2  (f(x) f(xmin)), see e.g., [133]. The rates of convergence generally
depend on  and the smoothness constant L. Unfortunately, for some functions
the constant L or the constant  may depend on the dimensionality of the space.
Although smoothness and the PL inequality can be defined with respect to non-
Euclidean norms, this does not generally overcome the issue of dimension dependence
if standard gradients are used, see [131, 191] for a discussion and methods using non-
standard gradients. The situation is distinct for Hamiltonian descent. If f is smooth
with respect to a non-Euclidean norm , then, by taking k(p) = p2 /2, Proposition
4.5.8 may guarantee, under appropriate assumptions, dimension independent rates
when using standard gradients (dependence on the dimensionality is mediated by the
constant N). For example, consider f(x) = x24 /2 = (
n=1(x
(n))4)1/2/2 defined for
d-dimensional vectors x  Rd. It is possible to show that f is smooth with respect to
2 with constant L = 3 (our Lemma 4.5.3 together with an analysis analogous to
Lemma 14 in Appendix A of [232] and the fact that x4  x2) and that f satisfies
the PL inequality with  = 1/
d (the fact that x24/3 
d x22 and Lemma 4.5.1).
The iterates of a gradient descent algorithm with fixed step size 1/L on this f will
therefore satisfy the following,
f(xi+1) f(xi)  
f(xi)22  
f(xi).
From which we conclude that gradient descent converges linearly with rate  = 3/(3
d), worsening as d  . Figure 4.9 illustrates this, along with a comparison to
Hamiltonian descent with k(p) = p24/3, which enjoys dimension independence as
N = 3.
4.5.3 Matching relativistic kinetic k with assumptions on f
The strongest assumption of Proposition 4.5.8 is that the power behaviour of f
captured in the constant b is exactly known. This is generally not the case and usually
hard to determine. The only possible exception is b = 2, which can be guaranteed by
lower bounding the eigenvalues of the Hessian. In our second analysis, we consider
a kinetic energy generically suitable for such strongly convex functions that may not
be smooth. Crucially, less information needs to be known about f for this kinetic
energy to be applicable. The cost imposed by this lack of knowledge is a non-constant
rate of linear convergence, which begins slowly and improves towards a faster rate as
(xi, pi) (xmin, 0).
In particular, we consider the use of the relativistic kinetic energy,
k(p) = 12(p) =
p2 + 1 1,
which was studied by Lu et al. [157] and Livingstone et al. [149] in the context of
Hamiltonian Monte Carlo. Consider for the moment the Euclidean norm case. In
this case, we have
k(p) = p
p22 + 1
As noted by Lu et al. [157], this kinetic map resembles the iterate updates of popular
adaptive gradient methods [75, 274, 90, 135]. Because the iterate updates xi+1  xi
of Hamiltonian descent are proportional to k(p) from some p  Rd, this suggests
that the relativistic map may have favorable properties. Notice that k(p)22 =
p22 /(p
2 + 1) < 1, implying that xi+1  xi2 <  uniformly and regardless of
the magnitudes of f . The fact that the magnitude of iterate updates is uniformly
bounded makes the relativistic map suitable for functions with very fast growing tails,
even if the rate of growth is not exactly known.
More precisely, we consider the case of f growing like B2 (x xmin) matched
with k(p) = 12(p) for B  2. Assumptions G, below, will be used in different
combinations to confirm that the assumptions of the different discretizations are
satisfied. Assumptions G.1, G.2, G.3, and G.4 are required for all methods. The
first explicit method requires additional assumptions G.5 for B > 2 and G.6 for
B = 2. We do not include an analysis for the second explicit method. Thus, for
f : Rd  R and k(p) = 12(p), Proposition 4.5.10 can be summarised as
G.1 G.2 G.3 G.4 A  B
G.1 G.2 G.3 G.4 G.5 A  B  C
G.1 G.2 G.3 G.4 G.6 A  B  C
Figure 4.10 illustrates a consequence of this proposition; f(x) = 28(x) for x  R is
a difficult function to optimize with a first order method using a fixed step size; the
second derivative grows without bound as |x|  . Thus if the initial point is taken
to be very large, gradient descent must take a very conservative choice of step size.
As shown, Hamiltonian descent with the matched k(p) = 28/7(p) converges quickly
and uniformly, while gradient descent with a fixed step size suffers a very slow rate
for |x0|  0. In the middle panel, the relativistic choice converges slowly at first, but
speeds up as convergence proceeds, making it a suitable agnostic choice in cases such
as this.
Gradient descent
0 500 1000
k(p) = 12(|p|)
0 500 1000
iteration i
k(p) = 
2 (p)
0 500 1000
Figure 4.10: f(x) = 82(x) with three different methods: gradient descent with the
optimal fixed step size, Hamiltonian descent with relativistic kinetic energy, and
Hamiltonian descent with the near dual kinetic energy.
Assumption G. G.1 f : Rd  R differentiable and convex with unique minimum
xmin.
G.2 p is differentiable at p  Rd \ {0} with dual norm x = sup{x, p : p =
G.3 B  [2,) and A = B/(B  1).
G.4 There exist , L  (0,) such that for all x  Rd
f(x) f(xmin)  B2 (x xmin)
12(f(x))  L(f(x) f(xmin)).
(4.67)
G.5 B > 2. Define
(t) =
0 0  t < 1
t 3t
3 + 2 1  t
. (4.68)
f : Rd  R is twice continuously differentiable for all x  Rd \ {xmin} and there
exists Lf  (0,) such that for all x  Rd \ {xmin}
max(2f(x))
 3(f(x) f(xmin)). (4.69)
G.6 B = 2. f : Rd  R is twice continuously differentiable for all x  Rd \ {xmin}
and there exists Lf  (0,) such that for all x  Rd \ {xmin}
max
2f(x)
 Lf . (4.70)
Remark 4.5.9. Assumptions G hold in general for convex functions f that grow
quadratically at their minimum, and as power B in the tails, for some B  2.
We include the proof of this proposition below, as it highlights every aspect of our
analysis, including non-constant .
Proposition 4.5.10 (Verifying assumptions for f with unknown power behavior and
relativistic k). Given a norm  satisfying G.2, take
k(p) = 12(p)
with Aa in (4.48). The following cases hold with this choice of kinetic energy k on
f : Rd  R convex.
1. For the implicit method (4.33), assumptions A, B hold with constants
C, =  Cf,k = max{1, L}, (4.71)
and  non-constant, equal to
(y) = min{A1, , 1}(y + 1)1A, (4.72)
if f,B, , L,  satisfy assumptions G.1, G.2, G.3, G.4.
2. For the first explicit method (4.36), assumptions A, B, and C hold with constants
(4.71),  equal to (4.72), and
Ck = 2 Df,k =
min{A1, , 1} , (4.73)
if f,B, , L, Lf ,  satisfy assumptions G.1, G.2, G.3, G.4, and G.5.
3. For the first explicit method (4.36), assumptions A,B, and C hold with constants
(4.71),  equal to (4.72), and
Ck = 2 Df,k =
min{, 1} , (4.74)
if f,B, , L, Lf ,  satisfy assumptions G.1,G.2, G.3, G.4, and G.6.
Proof of Proposition 4.5.10. First, by Lemma 4.5.1, this choice of k satisfies assumptions
A.2 and C.1 with constant Ck = 2. We consider the remaining assumptions of A, B,
and C.
1. Our first goal is to derive . By assumption G.4, we have Bb (x)  fc(x).
Lemma Ap.4.2 in Appendix Ap.4 implies that A2 (
1t)  max{2, A}A2 (t)
for t  0. Since (Bb ()) = (Bb )(1 ) by Lemma 4.5.1 and the results
discussed in the review of convex analysis, we have by assumption G.3 and
Lemma 4.5.4,
f c (p)  (B2 )
1 p
 A2
1 p
 max{1, 1A}A2 (p).
Since A2 (0) = 
2(0), any  satisfies (4.24) for p = 0. Assume p 6= 0. First, for
y  [0,), we have by rearrangement and convexity,
A2 ((
1(y)) = 1
(y + 1)A  1
 y(y + 1)A1.
Thus,
A2 (p)
A2 ((
1(k(p)))
Ak(p)
(k(p) + 1)A  1  (k(p) + 1)
From this we conclude
k(p)  (k(p) + 1)1AA2 (p)  (k(p))f
c (p).
Since k is symmetric, we have (4.24) of assumption A.4. To see that  satisfies
the remaining conditions of assumption A.4, note that (y+ 1)1A is convex and
decreasing for A > 1; (y+1)1A is non-negative and (0) = min{A1, , 1}  1.
Finally, G.3 implies 1 < A  2, for which,
(y)y = min{A1, , 1}(A 1)(y + 1)Ay < (A 1)(y)  (y). (4.75)
So we can take C, =  and  satisfies assumptions A. This implies that
k satisfies assumptions A. Assumption G.1 is the same as assumption A.1,
therefore f and k satisfy assumptions A.
Now by Fenchel-Young, the symmetry of norms, Lemma 4.5.1, and assumption
| k(p),f(x) |  (12)(k(p)) + 12(f(x))  Cf,kH(x, p),
where Cf,k = max{1, L} for assumptions B.
2. Assume B > 2, so that A < 2. The analysis of case 1. follows and therefore
assumptions A and B hold along with the constants just derived. (4.75) implies
[(y)y] = (y)y + (y) = (y)y + (2 A)(y) + (A 1)(y)  (2 A)(y).
Thus, ((y + 1)2A  1)  (y)y. Since 2  A = B2
B1 and
1 (y) is the
inverse function of (y + 1)2A  1, it would be enough to show for p  Rd and
x  Rd \ {xmin} that
k(p)2 max(2f(x))
 3H(x, p),
for assumptions C to hold with constant Df,k = 3Lf/min{A1, , 1}. First,
for  in 4.68 note that for t  [0, 1),
(t) = 2(1 t)
2  2, (4.76)
and that ((12)
(t)2) = 212(t). Furthermore, by Lemma Ap.4.1 of Appendix
Ap.4, we have that k(p) = (12)(p) < 1. Lemma Ap.4.2 in Appendix
Ap.4 implies that 
1 (t)  
1 (t) for  < 1 and t  0. All together this
implies,
k(p)2 max(2f(x))
 k(p)2 B1
max(2f(x))
 (k(p)2) + 
max(2f(x))
 2k(p) + 
max(2f(x))
 3H(x, p).
3. For B = 2, the analysis of case 1. follows and therefore assumptions A and B
hold along with the constants just derived.. Here  is equal to
(y) =
min(, 1)
y + 1
. (4.77)
Considering that z/(1 z) is the inverse function of y/(y + 1) for z  [0, 1), it
would be enough to show for p  Rd and x  Rd \ {xmin} that
k(p)2 max(2f(x))
2Lf  k(p)2 max(2f(x))
 3H(x, p),
for assumptions C to hold with constant Df,k = 6Lf/min{, 1}. Indeed, taking
,  from (4.68) and (4.76), we have again ((12)
(t)2) = 212(t). Again, by
Lemma Ap.4.1 of Appendix Ap.4, we have that k(p) = (12)(p) < 1.
Moreover z/(2L z)  1 for z  L. All together,
k(p)2 max(2f(x))
2Lf  k(p)2 max(2f(x))
 k(p)2 
max(2f(x))
2Lf  max(2f(x))
 (k(p)2) + 
max(2f(x))
2Lf  max(2f(x))
 2k(p)  3H(x, p).
4.6 Conclusion
The conditions of strong convexity and smoothness guarantee the linear convergence
of most first-order methods. For a convex function f these conditions are essentially
quadratic growth conditions. In this work, we introduced a family of methods, which
require only first-order computation, yet extend the class of functions on which linear
convergence is achievable. This class of functions is broad enough to capture non-
quadratic power growth, and, in particular, functions f whose Hessians may be
singular or unbounded. Although our analysis provides ranges for the step size and
other parameters sufficient for linear convergence, it does not necessarily provide the
optimal choices. It is a valuable open question to identify those choices.
The insight motivating these methods is that the first-order information of a
second function, the kinetic energy k, can be used to incorporate global bounds on the
convex conjugate f  in a manner that achieves linear convergence on f . This opens
a series of theoretical questions about the computational complexity of optimization.
Can meaningful lower bounds be derived when we assume access to the first order
information of two functions f and k? Clearly, any meaningful answer would restrict
kotherwise the problem of minimizing f could be solved instantly by assuming
first-order access to k = f  and evaluating k(0) = f (0) = xmin. Exactly what
that restriction would be is unclear, but a satisfactory answer would open yet more
questions: is there a meaningful hierarchy of lower bounds when access is given to the
first-order information of N > 2 functions? When access is given to the second-order
information of N > 1 functions?
From an applied perspective, first-order methods are playing an increasingly important
role in the era of large datasets and high-dimensional non-convex problems. In these
contexts, it is often impractical for methods to require exact first-order information.
Instead, it is frequently assumed that access is limited to unbiased estimators of
derivative information. It is thus important to investigate the properties of the
Hamiltonian descent methods described in this paper under such stochastic assumptions.
For non-convex functions, the success of adaptive gradient methods, which bear a
resemblance to our methods using a relativistic kinetic energy, suggests there may be
gains from an exploration of other kinetic energies. Can kinetic energies be designed
to condition Hamiltonian descent methods when the Hessian of f is not positive semi-
definite everywhere and to encourage iterates to escape saddle points? Finally, the
main limitation of the work presented herein is the requirement that a practitioner
have knowledge about the behavior of f near its minimum. Therefore, it would be
valuable to investigate adaptive methods that do not require such knowledge, but
instead estimate it on-the-fly.
Statement of Authorship for joint/multi-authored papers for PGR thesis 
To appear at the end of each thesis chapter submitted as an article/paper 
The statement shall describe the candidates and co-authors independent research contributions in the thesis 
publications. For each publication there should exist a complete statement that is to be filled out and signed by the 
candidate and supervisor (only required where there isnt already a statement of contribution within the paper 
itself). 
Title of Paper 
Hamiltonian Descent Methods 
Publication Status 
  Published                                   Accepted for Publication 
  Submitted for Publication          Unpublished and unsubmitted work written 
                         in a manuscript style 
Publication Details 
Chris J. Maddison*, Daniel Paulin*, Yee Whye Teh, Arnaud Doucet. 
Hamiltonian Descent Methods. In Preprint, 2018. 
Student Confirmation 
Student Name: 
Chris J. Maddison 
Contribution to the 
Paper 
 I proposed the Hamiltonian descent differential equation and the use of the 
convex conjugate in the design of the kinetic energy. 
 Daniel Paulin and I collaborated jointly on the proofs of convergence in 
Hamiltonian descent. 
 I designed a Lyapunov function that proved the convergence of the continuous 
Hamiltonian descent ODE in the strongly convex case. 
 Daniel modified this Lyapunov function and proved the general convergence 
results presented in Sections 2 and 3 of the paper. 
 Daniel is completely responsible for the lower bounds proofs. 
 I proved the results of Section 4. 
 All authors contributed to the development of the paper through discussions 
and ideas, and all authors reviewed the final draft. 
Signature  
05 May 2020 
Supervisor Confirmation 
By signing the Statement of Authorship, you are certifying that the candidate made a substantial contribution to the 
publication, and that the description described above is accurate. 
Supervisor name and title: 
Professor Arnaud Doucet 
Supervisor comments 
I agree that the candidate has made a substantial contribution to the publication. 
Signature 
05 May 2020 
This completed form should be included in the thesis, at the end of the relevant chapter. 
Chapter 5
Dual Space Preconditioning for
Gradient Descent
5.1 Abstract
The conditions of relative smoothness and relative strong convexity were recently
introduced for the analysis of Bregman gradient methods for convex optimization. In
this chapter, we introduce a fully explicit descent scheme with relative smoothness
in the dual space between the convex conjugate of the objective function and a
designed dual reference function. For Legendre type convex functions under this
dual relative smoothness, our scheme naturally remains in the interior of the domain,
despite being fully explicit. We obtain linear convergence under dual relative strong
convexity with a condition number that is invariant under horizontal translations.
Our method is a non-linear preconditioning of gradient descent that can improve the
conditioning of explicit first-order methods on problems with non-smooth or non-
strongly convex structure. We show how this method can be applied to p-norm
regression and exponential penalty function minimization.
5.2 Introduction
We study the minimization of a proper, lower semi-continuous (lsc), strictly convex,
and differentiable function f : Rd  {R,},
xdom f
f(x), (P)
where dom f = {x  Rd : f(x) < }. Our primary focus is on functions f with
Legendre structure: int(dom f) 6=  and f(xi)   for xi converging to the
boundary of dom f . For such functions, a global minimizer xmin, if it exists, is unique
Algorithm 2.1 Dual preconditioned gradient descent.
Given f : Rd  {R,} Legendre convex, k : Rd  {R,} Legendre convex with
f(int(dom f))  int(dom k) and 0 = arg minx k(x), x0  int(dom f), and L > 0.
For all i  0,
xi+1 = xi 
k(f(xi)).
and in int(dom f). We introduce an iterative first-order method (Algorithm 2.1) for
(P). Iterative first-order methods produce a sequence of iterates xi  int(dom f)
converging to xmin using only the ability to compute f(x) or the gradient vector
f(x) of first partial derivatives at any point x  int(dom f). Our method may be
seen as a non-linear preconditioning of the classical gradient descent method. We
show that the convergence of our method is guaranteed under a generalization of the
standard Lipschitz continuity condition on f , and develop two applications that
show how this generalization can be used in practice.
In the analysis of first-order methods, it is standard to assume that the derivatives
of f at some order are globally bounded by constants. For example, consider the
classical gradient descent method, whose iterates satisfy
xi+1 = arg min
xdom f
f(xi), x+ L2 x xi
, (5.1)
where L > 0 and x0  int(dom f). A classical analysis shows that the iterates of
gradient descent converge linearly in i, i.e., f(xi) f(xmin) = O(i) for  = 1 /L,
when f is assumed to be  > 0 strongly convex and f is assumed to be L-Lipschitz
continuous (traditionally called smoothness). Taken together for twice continuously
differentiable f , these conditions are equivalent to the conditions that the eigenvalues
of the Hessian matrix of second-order partial derivatives 2f(x) are everywhere lower
bounded by the constant  > 0 (strong convexity) and upper bounded by the constant
L > 0 (smoothness),
I  2f(x)  LI for all x  int(dom f), (5.2)
where  indicates the partial order of positive semi-definite matrices. Under these
classical assumptions, closely matching lower and upper bounds are available on the
number of gradient evaluations needed for a certain level of precision (see, e.g., [194]).
Analyses of first-order methods using only non-constant models of the derivatives
of f have recently been discovered [16, 244, 156, 153]. In particular, [16] studied the
following generalized gradient method that takes a designed Legendre convex reference
function h : Rd  {R,} with int(dom f)  int(domh). Given x0  int(dom f),
this methods iterates satisfy
xi+1 = arg min
xdom f
{f(xi), x+ LDh(x, xi)} (5.3)
where L > 0, ,  is the Euclidean inner product, and Dh(x, y) = h(x)  h(y) 
h(y), x y for x, y  int(domh). (5.3) is due to [189] and falls in a family of
so-called Bregman proximal gradient methods. A standard analysis (see, e.g., [19])
of (5.3) makes the absolute assumptions that f is Lipschitz continuous and that
h is strongly convex. In contrast, Bauschke, Bolte, and Teboulle [16] show that
the following relative smoothness condition between f and h is sufficient for the
convergence of (5.3),
2f(x)  L2h(x) for all x  int(dom f). (5.4)
It is possible for (5.4) to hold for f and h that are both non-smooth. For example,
[16] study a Poisson inverse problem f whose derivatives of all orders are unbounded
as x  0. They design an appropriate h, whose Hessian is also unbounded at
0, but which satisfies (5.4). [244, 156] extend the analysis of (5.3) to show that
lower bounding the Hessian of f(x) with the Hessian of h(x) for  > 0 (relative
strong convexity) is sufficient for the linear convergence of f(xi)  f(xmin). To
summarize, (5.4) generalizes smoothness and admits optimization methods for non-
smooth differentiable f provided that the kind of non-smoothness can be captured
by (5.4) and that (5.3) has a solution that can be efficiently computed.
In this paper we introduce a method (Algorithm 2.1) that exploits an application
of relative smoothness in the dual space through a Legendre convex dual reference
function k : Rd  {R,} withf(int(dom f))  int(dom k) and 0 = arg minx k(x).
The method is a generalization of gradient descent, in which the update direction
of the iterates is preconditioned by  k. In section 5.5 we consider in detail the
conditions under which we can provide convergence rates for Algorithm 2.1. The
central condition is the existence of L > 0 such that
2f(x)  L[2k(f(x))]1 for all x  int(dom f). (5.5)
Under this condition we show that along the iterates of Algorithm 2.1, k(f(xi)) 
k(0) converges sub-linearly with rate O(i1) (and thus xi  xmin for Legendre f).
When the lower bound analog of (5.5) holds up to a constant factor  > 0, we show
that f(xi)f(xmin) converges linearly with rate  = 1/L. As we show in section
5.5.2, (5.5) is a relative smoothness condition in the dual space. It relates the growth
of the second derivatives of f to the growth of the first derivatives of f , modulated
by the choice of preconditioner k. In contrast to the primal application of relative
smoothness (5.4), the class of f satisfying (5.5) for a fixed k is closed under horizontal
translations. With the exception of quadratic k or h, (5.4) and (5.5) are generally not
equivalent conditions and  6= , L 6= L. Thus, the global information encoded in
the dual reference function k is distinct from the information encoded in the reference
function h. In section 5.6, we design ks and globally convergent methods for p-norm
regression (see [48, 2] and references therein) and exponential penalty functions (see,
e.g., [60, 59]).
5.3 Related literature
Dual preconditioned gradient descent requires of f only the ability to evaluate f
locally. The complexity of optimization under such assumptions is well-understood
within the local oracle model of computation [190], which restricts access to information
about f . First-order methods are those that use only local evaluations of f or f
(see [18, 194] for excellent and recent introductions). [190] first derived sub-linear
lower bounds for first-order methods, i.e., f(xi)f(xmin)  (i2), within the class of
smooth convex functions. Shortly thereafter [195] obtained upper bounds of matching
order.
The recent works on relative smoothness [16, 156, 113] derive first-order methods
that do not require classical smoothness. As far as we know these conditions were
first studied in [31] and rediscovered multiple times, e.g. [256]. Bauschke et al. [16]
provided a general analysis of mirror descent under these generalized smoothness
conditions for first-order methods. [156] provided the proof of linear convergence of
the primal gradient and dual averaging schemes under both relative strong convexity
and smoothness. Analyses of first-order methods under relative smoothness have
been extended to non-convex f [34, 69], and an analogous notion of relative Lipschitz
continuity has been developed for continuous convex optimization [153]. Accelerated
versions of the primal schemes have been proposed in [113] and [112]. Relative
smoothness has been used in the analysis of stochastic composite least-squares problems
[81], symmetric non-negative matrix factorization [69], and the Sinkhorn algorithm
[171]. These results do not contradict the classical lower bounds [190], because relative
smoothness is a global condition that provides non-black-box information about f .
Dual preconditioned gradient descent extends linear preconditioning of gradient
descent (see, e.g., [42, sect. 9.4]). Linear preconditioning improves dual gradient
methods [94, 95], and is a classical tool in the study of iterative methods for linear
systems [255, Chap. 13]. Non-linear preconditioning methods have recently been
shown to stabilize Euler discretization schemes of stochastic differential equations
[122, 226]. In fact, the non-linear preconditioning of [122] is the same as the one
we consider for exponential penalty functions. We discuss the relationship of dual
preconditioned gradient descent to existing methods in more detail in section 5.7.1.
5.4 Convex analysis background
5.4.1 Convex conjugate and Legendre functions
In this section we review some basic facts of convex analysis that will be used
throughout. Let h : Rd  {R,} be a proper, lsc, convex function with domain
domh = {x : Rd : h(x) <}. To indicate domh = Rd, we simply define h : Rd  R
as ranging only over the reals. Let  and ,  indicate the Euclidean norm and
inner product, respectively, unless otherwise specified. The convex conjugate h :
Rd  {R,} of a proper, lsc convex function h is given by
h(x) = sup{x, x  h(x) : x  domh}. (5.6)
h is also a proper, lsc, convex function, and (h) = h [222, Cor. 12.2.1]. If g :
Rd  {R,} is another proper, lsc, convex function and g(x)  h(x) for all x  Rd,
then h(x)  g(x) for all x  Rd follows by definition. For h differentiable on
int(domh), we have by [222, Thm. 26.4] for x  int(domh),
x,h(x) = h(x) + h(h(x)). (5.7)
For more on h, we refer readers to [222, 42, 36].
We make heavy use of Legendre type convex functions [222, Chap. 25]. Intuitively,
these functions can be thought of as generalizations of positive definite quadratics and
their gradient maps as generalizations of positive definite linear maps.
Definition 1 (Legendre convex functions). Let h : Rd  {R,} be proper, lsc, and
convex. h is Legendre, if
1. int(domh) 6= ,
2. h is differentiable on int(domh), with h(xi)   for every sequence xi 
int(domh) converging to a boundary point x  (domh),
3. h is strictly convex on int(domh).
A key consequence of property 2 of Legendre convex functions is that they can
only be minimized in their interior. We confirm this in Lemma 5.4.1 below.
Lemma 5.4.1. Let h : Rd  {R,} be a Legendre convex function with a minimum
at xmin  domh. xmin is unique and furthermore xmin  int(domh).
Proof. First, we argue that xmin cannot be found on the boundary by contradiction.
Suppose that xmin is a boundary point. Since int(domh) 6= , by convexity there
exists a line segment connecting the boundary point xmin and any other interior point
a. However, by [222, Lem. 26.2], we know that the directional derivative converges
to  as we tend towards the boundary point on this line segment, hence xmin could
not be a minimum of h. Thus we conclude that xmin  int(dom(h)). By property 3.,
Legendre functions are strictly convex on their interior, and thus xmin is unique.
Property 2 together with Lemma 5.4.1 implies that Legendre convex functions grow
without bound for sequences xi  Rd where xi  xmin  . We present a
specialization of this fact in Lemma 5.4.2, which will be used in our analysis to
show that the dual reference function k is radially unbounded.
Lemma 5.4.2. Let h : Rd  {R,} be a Legendre convex function achieving its
minimum at 0  domh. Then h is radially unbounded, i.e., if xi  Rd is a sequence
such that xi  , then h(xi).
Proof. First, by Lemma 5.4.1 it follows that 0  int(domh) and it is the unique
minimum of h. Thus, we can define the sphere S = {x  Rd : x = r} for some
r > 0 such that S  int(domh). By continuity of h in the interior of its domain, and
the uniqueness of the minimum at zero, we have infxS h(x) > h(0). Now, assume
without loss of generality that xi > r. By strict convexity of Legendre functions,
property 3, we have
h(0) +
 h(0)
< h(0) + (h(xi) h(0)) (5.8)
and thus
h(xi) > h(0) +
h(x) h(0)
. (5.9)
Our result follows by taking i.
A second key consequence of Legendre structure is that the gradient map h is
invertible and given by (h)1 = h, which also gives a characterization of the
inverse of 2h(x). We summarize both of these properties in Lemma 5.4.3.
Lemma 5.4.3. Let h : Rd  {R,} be a Legendre convex function, then h is
also Legendre. The gradient map  h is one-to-one and onto from the open set
int(domh) onto the open set int(domh), continuous in both directions, and for all
x  int(domh)
h(h(x)) = x. (5.10)
If h is twice continuously differentiable on an open set containing x, then
2h(h(x))2h(x) = 2h(x)2h(h(x)) = I. (5.11)
Proof. For the first part see Rockafellar [222, Thm. 26.5]. For (5.11), note that, by
the inverse function theorem, h is continuously differentiable at h(x) under the
assumption that h is continuously differentiable on an open set containing x. The
remainder follows by the chain rule applied to (5.10).
5.4.2 Relative smoothness and relative strong convexity
Analyses of first-order methods for differentiable optimization typically require that
f is Lipschitz continuous (smooth). Recently [16, 156] discovered that certain so-
called Bregman proximal gradient methods (mirror descent due to [189] is the first
such method) require a generalized relative smoothness condition, which admits
f that have non-Lipschitz f . These relative smoothness conditions generalize the
classical smoothness condition and are defined via the Bregman divergence [44] of a
given a proper, lsc, convex function h : Rd  {R,} that is differentiable on the
interior of its domain. The Bregman divergence (see [15] for a review) is defined
x  domh,y  int(domh),
Dh(x, y) = h(x) h(y) h(y), x y . (5.12)
In the special case of h(x) = x22 /2, Dh is the classical Euclidean distance squared.
The relative conditions of relative strong convexity and relative smoothness [16, 156]
relate two convex functions via their respective Bregman divergences.
Definition 2 (Relative smoothness and strong convexity). Let g, h : Rd  {R,} be
proper, lsc, convex functions that are differentiable on the interior of their domains.
g is L-smooth relative to h on a convex set Q if Q  int(domh)  int(dom g), and
there exists L > 0 such that for every x, y  Q,
Dg(x, y)  LDh(x, y).
g is -strongly convex relative to h on a convex set Q if Q  int(dom g) int(domh),
and there exists  > 0 such that for every x, y  Q,
Dg(x, y)  Dh(x, y).
Here, again, the special cases of relative strong convexity and smoothness with respect
to h(x) = x22 /2 are exactly the classical conditions of strong convexity and smoothness,
the first-order equivalents of (5.2). Lemma 5.4.4 (a re-statement of [156, Prop. 1.1])
below describes a variety of equivalent definitions for relative strong convexity and
smoothness.
Lemma 5.4.4. (Equivalent definitions of relative conditions [156, Prop. 1.1]). Let
g, h : Rd  {R,} be proper, lsc, convex functions that are differentiable on the
interior of their domains. The following are equivalent
1. g is L-smooth relative to h on Q.
2. Lh g is convex on Q.
3. g(x)g(y), x y  L h(x)h(y), x y for all x, y  Q.
The following are equivalent
1. g is -strongly convex relative to h on Q.
2. g  h is convex on Q.
3.  h(x)h(y), x y  g(x)g(y), x y for all x, y  Q.
Just as Lipschitz continuity ofg can be characterized by a bound on2g, relative
smoothness and strong convexity can be characterized by the second derivatives of
g and h. In particular, Lh(x)  g(x) is convex if and only if L2 h(x)  2 g(x)
is positive semi-definite for all x. In this way, it is clear how relative smoothness
generalizes classical smoothness (the 2h(x) = I case). We present the second-order
characterization of the relative conditions in Lemma 5.4.5, generalized slightly to
allow the bound to fail at a single point. This is useful for cases in which 2f is
not continuous at xmin. Typically, it is easiest to prove relative smoothness or strong
convexity via these second-order equivalents.
Lemma 5.4.5 (Second-order characterizations of relative conditions). Let g, h : Rd 
{R,} be proper, lsc, convex functions that are differentiable on the interior of their
domains. Let g, h be twice continuously differentiable at all x  Q \ {z} for z  Q 
int(dom g)  int(domh).
1. g is L-smooth relative to h on Q iff L > 0,
2g(x)  L2h(x) x  Q \ {z}.
2. g is -strongly convex relative to h on Q iff  > 0,
2h(x)  2g(x) x  Q \ {z}.
Proof. For relative smoothness, () follows directly from part one of [194, Thm.
2.1.4] applied to f(x) = Lh(x)g(x). For (), we have f(x) = Lh(x)g(x) convex.
Now, let x, y  Q and t  [0, 1] and define xt = y + t(x  y). There can be at most
one time a  [0, 1] such that xa = z. Take a to be that time, if it exists, or some
arbitrary a  [0, 1], otherwise. We have
Lh(x)g(x) Lh(y) +g(y), x y
= f(x)f(y), x y
= lim
f(x)f(x ), x y+ lim
f(x )f(y), x y
= lim
x y,2f(xt)(x y)
dt+ lim
x y,2f(xt)(x y)
dt  0,
(a) follows by the continuity of f and (b) by the fundamental theorem of calculus.
The relative strong convexity result follows analogously.
The analyses of Bregman proximal gradient methods under relative smoothness
rely on some standard manipulations of Bregman divergences. In Lemma 5.4.6 we
summarize the ones used in our analysis.
Lemma 5.4.6. Let h : Rd  {R,} be a convex Legendre function.
1. (Dual divergence) For all x, y  int (domh),
Dh(x, y) = Dh(h(y),h(x)).
2. (Three-point property) [54, Lem. 3.1] For all x, y, z  int (domh),
Dh(x, y) = Dh(x, z) +Dh(z, y) x z,h(y)h(z) .
3. (Bregman proximal inequality) [54, Lem. 3.2] Given a proper, lsc, convex  :
Rd  {R,} with int(domh)  dom, and for y  int(domh),
zmin = arg min
zint(domh)
{(z) +Dh(z, y)}.
Then for all x  int(domh)
(x) +Dh(x, y)  (zmin) +Dh(zmin, y) +Dh(x, zmin).
Proof. For the dual divergence property,
h(x) h(y) h(y), x y
= h(h(x)) + h(h(y)) h(y)h(x), x
= h(h(y)) h(h(x)) h(h(x)),h(y)h(x) ,
Here (a) follows from (5.7) and (b) follows from Lemma 5.4.3. The other two properties
are given in [54, Lem. 3.1, Lem. 3.2].
5.5 Analysis of the dual preconditioned scheme
5.5.1 Motivation
Relative smoothness (Def. 2) is the key condition under which [16, 244, 156] analyzed
the convergence of Bregman proximal gradient methods. In this section we show that
the dual space preconditioned gradient descent method (Algorithm (2.1)) converges
under a distinct relative smoothness condition. To motivate this, we consider two
idealizations: one of the Bregman proximal gradient method and another of the dual
space preconditioned gradient method.
First, consider the Bregman proximal gradient method update (5.3), which can
be rewritten in the following form.
xi+1 = arg min
xdom f
{f(xi) Lh(xi), x+ Lh(x)} (5.13)
In this form, it is clear that, if h = f and L = 1, then the iteration would converge in
a single step. This is an idealization, because a single iteration would be as expensive
to compute as the original problem. The spirit behind relative smoothness is that the
condition h = f can be relaxed to admit h for which the update (5.13) is efficiently
solvable and the iterates still converge.
Now, consider the case that f is Legendre convex with a minimum at xmin, and
let f c (x
) = f (x)  x, xmin for x  Rd. Notice that f c (f(x)) = x  xmin
by Lemma 5.4.3 and that Algorithm 2.1 with k = f c and Li = 1 would converge in
a single step. Thus, in analogy to relative smoothness analysis of [16] in the primal
space, the spirit behind our analysis under relative smoothness in the dual space is
that the requirement k = f c can be relaxed while maintaining the convergence of
Algorithm 2.1. In particular, the essential features of f c that we require of k are that
it is minimized at 0 and smooth relative to f .
5.5.2 Relative conditions in the dual space
Our analysis guaranteeing the convergence of the dual preconditioned method uses the
relative smoothness (Def. 2) of k relative to f . We call this condition dual relative
smoothness, to contrast it with the typical application of relative smoothness [16, 156,
244], which we henceforth call primal relative smoothness. Similarly, we distinguish
dual relative strong convexity from the condition of relative strong convexity applied
in [156, 244] (henceforth called primal relative smoothness).
Definition 3 (Dual relative smoothness and dual relative strong convexity). Let
f, k : Rd  {R,} be Legendre convex functions. We say f is dual L-smooth (dual
-strongly convex, resp.) relative to k on Q  int (dom f), if k is L-smooth (-
strongly convex, resp.) relative to f  on f(Q)  int(dom f ). We abbreviate this
condition to dual relative smoothness (dual relative strong convexity, resp.).
Our dual relative conditions are defined via the convex conjugate f , which is
generally inaccessible. Lemma 5.5.1 below gives equivalent definitions of dual relative
smoothness and strong convexity in terms of objects that are more accessible.
Lemma 5.5.1 (Equivalent definitions of dual relative conditions). Let f, k : Rd 
{R,} be Legendre convex functions. The following are equivalent.
1. f is dual L-smooth relative to k on int(dom f).
2. For all x, y  int(dom f),
Dk(f(y),f(x))  LDf (x, y).
3. For all x, y  int(dom f),
k(f(x))k(f(y)),f(x)f(y)  L f(x)f(y), x y .
The following are equivalent.
1. f is dual -strongly convex relative to k on int(dom f).
2. For all x, y  int(dom f),
Df (x, y)  Dk(f(y),f(x)).
3. For all x, y  int(dom f),
 f(x)f(y), x y  k(f(x))k(f(y)),f(x)f(y)
Proof. We prove the relative smoothness results, and the relative strong convexity
ones follow similarly. First, notice that thatf(int(dom f)) = int(dom f ) by Lemma
5.4.3. So, by definition of dual relative smoothness we can apply relative smoothness
in the dual space over int (dom f ). For (1  2), we have by Lemma 5.4.4 that for
all x, y  int(dom f ),
, x)  LDf(y, x). (5.14)
By Lemmas 5.4.3 and 5.4.6, this implies
Dk(f(y),f(x))  LDf(f(y),f(x)) = LDf (x, y), (5.15)
for all x, y  int(dom f). For (2  3), simply sum over a permutation of x, y in 2.
For (3  1), we have by Lemma 5.4.3 for all x, y  int(dom f ),
k(x)k(y), x  y  L f (x)f (y), x  y (5.16)
This is equivalent to k being L-smooth relative to f  on int(dom f ) by Lemma
5.4.4.
The dual relative conditions have a natural second-order characterization, which
reveals the structure of the difference between them and primal relative conditions.
Again, typically it is easiest to prove dual relative smoothness (or strong convexity)
via these second-order conditions.
Lemma 5.5.2 (Second-order characterizations of dual relative conditions). Let f :
Rd  {R,} be Legendre convex, minimized at xmin  int(dom f), and twice continuously
differentiable at all x  int(dom f)\{xmin}. Let k : Rd  {R,} be Legendre convex,
and twice continuously differentiable at all x  int (dom f ) \ {0}.
1. f is dual L-smooth relative to k on int(dom f) iff L > 0,
2f(x)  L[2k(f(x))]1 x  int(dom f) \ {xmin}.
2. f is dual -strongly convex relative to k on int(dom f) iff  > 0,
[2k(f(x))]1  2f(x) x  int(dom f) \ {xmin}.
Remark 5.5.3. It is well-known that the primal and dual relative conditions are
equivalent in the case of 2 h(x) = I = 2 k(x) (see, e.g., [278, 232, 132, 276]).
In particular, if f is -strongly convex and L-smooth on int(dom f), then its convex
conjugate f  is (1/L)-strongly convex and (1/)-smooth on int(dom f ). In fact,
for twice continuously differentiable f , the equivalence is a simple consequence of
Lemmas 5.4.5 and 5.5.2. However, this is equivalence is not true in general.
Given a Legendre convex g : R {R,} define the following sets of functions
Fg = {f : f is smooth and strongly convex relative to g}, (5.17)
F g = {f : f is dual smooth and dual strongly convex relative to g}. (5.18)
Let k(x) = |x|q/q for x  R and 1 < q < 2. A simple argument by contradiction
shows that F k * Fh for all twice continuously differentiable h : R  R, implying
that the primal and dual relative conditions are not equivalent in general. Consider
fb(x) = |x b|p/p, (5.19)
for p = q
q1 and x  R. First fb  F
k for all b, which follows from [k
(f b(x))]
(p  1)|x  b|p2 = f b (x) and Lemma 5.5.2. On the other hand, suppose there is
some twice continuously differentiable h : R  R such that fb  Fh for all b. Then
there exists  > 0 such that h(b)  f b (b) = 0 for all b. This implies that h(x)  0
and thus h(x)  0. However, this leads to a contradiction, because smoothness is
violated: f b (b+ ) > 0 = Lh
(x) for any L,  > 0.
Proof of Lemma 5.5.2. Again, we prove the relative smoothness result, and the relative
strong convexity one follows similarly. By Lemma 5.4.3, if  f is continuously
differentiable for x  int(dom f) \ {xmin}, then f  is continuously differentiable
for x  int(dom f ) \ {0} by the inverse function theorem. Thus, by Lemma 5.4.5
relative smoothness in the dual space is equivalent to: for all x  int(dom f ) \ {0},
2k(x)  L2f (x). (5.20)
By (5.11) of Lemma 5.4.3, these matrices are invertible (and thus positive definite).
Thus, (5.20) is equivalent to for all x  int(dom f) \ {xmin},
2k(f(x))  L[2f(x)]1. (5.21)
Since A1  B1 is equivalent to B  A for positive definite matrices, we are
done.
As Remark 5.5.3 shows, the primal relative conditions (Def. 2) and the dual
relative conditions (Def. 3) are not generally equivalent concepts. One major difference
is the fact that dual relative conditions are invariant under horizontal translations of
f . To see why, let f, k satisfy the dual relative smoothness condition (Def. 3) with
constant L on int(dom f). Define g(x) = f(x  z) for z  Rd. Then, by Theorem
12.3 of [222], g(x) = f (x) + z, x. First, note that dom g = dom f . Bregman
divergences of functions that differ only in affine terms are identical [15], so we have
for all x, y  int(dom g) = int(dom f )
, y)  LDf(x, y) = LDg(x, y). (5.22)
Thus g is dual L-smooth relative to k on int(dom g). Invariance under horizontal
translation is clearly easy to violate in the case of primal relative smoothness (see
previous remark).
Even if h is allowed to translate with f , the primal and dual relative conditions
can lead to distinct conditioning. Given a positive definite A  0, let
f(x) = Ax bp /p, h(x) =
x A1b
p /p, k(x) = xq /q, (5.23)
for 1/p + 1/q = 1 and p > 2. It is not hard to show that f satisfies both the dual
(with respect to k) and primal (with respect to h) relative conditions. Nonethless,
the condition numbers are distinct. A simple calculation reveals that for this choice
of k and h,
max(A)
min(A)
= (p 1)2
max(A)
min(A)
, (5.24)
where min and max are the smallest and largest singular values of A, respectively.
Thus, the primal condition number is larger than the dual number (since 4 q = 3
(p1)1 < p when p > 2). Similarly, the example f(x) = Axb44/4+Cxd22/2 of
[156, p. 339] can be shown to have better conditioning under the dual preconditioned
method than under the Bregman proximal gradient method.
To close this subsection, we consider a natural sufficient condition for dual relative
smoothness: the Lipschitz continuity of the composition k  f .
Lemma 5.5.4. Let f : Rd  {R,} be a proper function with int(dom f) 6=  that
is twice continuously differentiable on int(dom f). Let k : Rd  {R,} be Legendre
convex and twice continuously differentiable on f(int(dom f)). If k  f is L-
Lipschitz continuous, then for all x  int(dom f),
2f(x)  L[2k(f(x))]1. (5.25)
In particular, if f is Legendre convex, then f is dual L-smooth relative to k on
int(dom f).
Proof. Let x  int(dom f), v  Rd, K(x) = 2k(f(x)), and F (x) = 2f(x). Note
that k is continuously differentiable at f(x). Hence, K(x) is invertible and thus
positive definite by (5.11) of Lemma 5.4.3. By L-Lipschitz continuity we also have
K(x)F (x)v = lim
k(f(x+ tv))k(f(x))
 L v . (5.26)
Thus, K(x)F (x)  L for the induced matrix norm. Now,
v, [K(x)]1/2F (x)[K(x)]1/2v
 ([K(x)]1/2F (x)[K(x)]1/2) v2
= (K(x)F (x)) v2
 K(x)F (x) v2  L v2
(5.27)
where (A) is the spectral radius of A. The result follows because B1/2AB1/2  I
implies A  B1 for positive definite B.
5.5.3 Convergence rates under dual relative smoothness
In this subsection we provide conditions under which convergence rates for Algorithm
2.1 can be established for Legendre convex f . The first key ingredient is the condition
of dual relative smoothness between f and k with constant L, developed in the
previous section. The second is the requirement that 0 = arg minx k(x
). In this
case, we find that k(f(xi))) k(0) converges with rate O(i1). When f is also dual
 > 0 strongly convex relative to k, we find that f(xi) f(xmin) converges with rate
O((1/L)i). Both of these results are derived from the following descent lemma.
Lemma 5.5.5 (Descent lemma). Given f : Rd  {R,} Legendre convex, k : Rd 
{R,} Legendre convex with 0 = arg minxdom k k(x), and x0  int(dom f). If
f is dual L-smooth relative to k on int(dom f), then for all i > 0, the iterates of
Algorithm 2.1 satisfy
1. xi  int(dom f),
2. for all x  int(dom f),
k(f(xi))  k(f(x))Dk(f(x),f(xi1))+LDf (xi1, x)LDf (xi, x). (5.28)
In particular, we have for i > 0,
3. k(f(xi)) + LDf (xi, xi1)  k(f(xi1)),
4. LDf (xi, xmin) +Dk(f(xi), 0) +Dk(0,f(xi1))  LDf (xi1, xmin).
Proof. First, note int(dom f )  int(dom k) by relative smoothness. Thus, by Lemma
5.4.3 we have f(x)  int(dom k) for all x  int(dom f).
We proceed by induction. For i = 0 we have x0  int(dom f) by assumption.
Now, for i > 0, assume the induction hypothesis for xi1. First, define
x = xi1 
k(f(xi1)) (5.29)
for  > 0. Because xi1  int(dom f) 6= , the following set is not empty,
S = {  L : x  int(dom f)}. (5.30)
Let xi1 = f(xi1) and x = f(x) for all   S. By Lemma 5.4.3, we have
f (x) = f (xi1)
k(xi1). (5.31)
Therefore x satisfies the stationary condition of the following subproblem,
xint(dom f)
k(xi1), x  xi1
+Df(x
, xi1)
. (5.32)
From the Bregman proximal inequality of Lemma 5.4.6 applied with h = f , (x) =
k(xi1), x  xi1
, x = x, y = xi1 and zmin = x
, we have
k(xi1), x  xi1
+ Df(x
, xi1) 
k(xi1), x  xi1
+ Df(x
i1) + Df(x
, x).
(5.33)
Putting everything together, we have for all x  int(dom f )
k(x)
 k(xi1) +
k(xi1), x  xi1
+ LDf(x
 k(xi1) +
k(xi1), x  xi1
+ Df(x
 k(xi1) +
k(xi1), x  xi1
+ Df(x
, xi1) Df(x, x)
 k(x)Dk(x, xi1) + Df(x, xi1) Df(x, x). (5.34)
(a) follows from dual L-smoothness, (b) from L   and the non-negativity of the
Bregman divergence, (c) from (5.33), and (d) by definition and simple algebra. Taking
x = xi1 and recalling the definition of x
i1 and x
 reveals that
k(f(x)) + Df(f(xi1),f(x))  k(f(xi1)). (5.35)
Now, our goal is to show that xi = xL  int(dom f) by showing that L  S. We
proceed by contradiction, so suppose L / S. Then xL  Rd \ int(dom f). Hence we
can find   L such that x  (dom f). Now take a sequence j   such that
j > . By the above discussion for all j  0 we have k(f(xj))  k(f(xi1)). k
being minimized at 0 means it satisfies Lemma 5.4.2 and thus is radially unbounded.
This implies that
f(xj)
  C for some C > 0 and all j  0. But this contradicts
the requirement from property 2 of Legendre functions that
f(xj)
   since
xj  x  (dom f) by assumption. This completes the proof that xi = xL 
int(dom f). Since L  S, (5.34) along with the dual divergence property of Lemma
5.4.6 ensures that 2. holds. Taking x = xi1 in 2. ensures that 3. holds while 4.
follows by taking x = xmin.
We are ready to analyze the convergence rates of the dual preconditioned gradient
descent method.
Theorem 5.5.6. Given f : Rd  {R,} Legendre convex, xmin = arg minx f(x),
k : Rd  {R,} Legendre convex, 0 = arg minpdom k k(p), and x0  int(dom f). If f
is dual L-smooth relative to k on int(dom f), then for all i > 0 and x  int(dom f)
the iterates of Algorithm 2.1 satisfy
k(f(xi)) k(0) 
(f(x0) f(xmin)). (5.36)
In particular, f(xi)  0. If additionally f is dual -strongly convex relative to k
on int(dom f) with  > 0, then for all i > 0 the iterates of Algorithm 2.1 satisfy
f(xi) f(xmin) 
(f(x0) f(xmin)). (5.37)
Remark 5.5.7. Ensuring that k is minimized at 0 is not difficult. Let l satisfy the
requirements on k in Theorem 5.5.6 and 0  int(dom l), but with a minimum other
than 0. Then k(p) = l(p) l(0), p will suffice for Theorem 5.5.6.
Proof of Theorem 5.5.6. First, we have xi  int(dom f) and k(f(xi)) is non-increasing
by 1. and 3. of the Descent Lemma 5.5.5. Thus, by 2. of the same lemma, for all
x  int(dom f) and i > 0
i(k(f(xi)) k(f(x))) 
k(f(xi)) k(f(x))
 LDf (x0, x) LDf (xi, x).
(5.38)
Dropping the negative term on the right hand side, dividing by i, and taking x =
xmin gives our first result. This implies that k(f(xi))  k(0), which implies that
f(xi)  0 by continuity and the uniqueness of ks minimum. Now, assume that f
is dual -strongly convex relative to k on int(dom f) with  > 0. For all i > 0,
L(f(xi) f(xmin))
 L(f(xi1) f(xmin))Dk(0,f(xi1))
 L(f(xi1) f(xmin)) (f(xi1) f(xmin)),
(5.39)
where (a) follows as an implication of 4. in the Descent Lemma 5.5.5 and (b) follows
from dual relative strong convexity. This inequality implies our desired result.
Theorem 5.5.6 guarantees the convergence of the iterates of Algorithm 2.1 under
the assumption that dual relative smoothness hold globally for a fixed L. Unfortunately
it may be difficult to to derive a tight bound on L or small L may be appropriate
locally. In this case, it may be useful to use a line search to choose L. Consider the
following generalization of the update rule of Algorithm 2.1,
xi+1 = xi 
k(f(xi)) (5.40)
where Li > 0 is allowed to depend on the iteration. The next proposition shows that,
under suitable assumptions, (5.40) converges with rates analogous to Theorem 5.5.6.
Proposition 5.5.8 (Adaptive step sizes). Given f : Rd  {R,} Legendre convex,
k : Rd  {R,} Legendre convex with 0 = arg minxdom k k(x), and x0  int(dom f).
If, for all i > 0 the iterates defined by (5.40) satisfy
1. xi  int(dom f),
2. k(f(xi))  k(f(xi1)),
3. k(f(xi)) k(0)  Li1(f(xi1) f(xi)),
then we have
k(f(xi)) k(0) 
max0ji1 L
(f(x0) f(xmin)). (5.41)
Remark 5.5.9. In practice, a possible choice of step sizes is
Li1 = min{2r, r  Z : 1., 2., and 3. of Proposition 5.5.8 are satisfied}. (5.42)
If L is the smallest real number such that f is dual L-smooth relative to k (see
Lemma 5.5.1 for an equivalent condition), then this scheme satisfies that Li1 < 2L
for every i > 0 (hence we are making steps that are almost as large or larger as if we
would use the smallest possible fixed L, without knowing the value of L in advance).
The search through the set in (5.42) for finding Li can be initialized at L
Proof of Proposition 5.5.8. The proof follows similar lines as in the previous case.
First, by summing up the inequalities from 3, we obtain that
1ji
[k(f(xj)) k(0)] 
1ji
Li1(f(xi1) f(xi))  (f(x0) f(xmin)) max
0ji1
Lj ,
and using 2., it follows that
1ji[k(f(xj))  k(0)]  i(k(f(xi))  k(0)). The
result follows directly.
An important question that we do not address in this section is whether the sub-
linear convergence of k(f(xi))  k(0) implies specific rates of convergence of other
quantities of interest. These might be, for example, xi  xmin or f(xi)  f(xmin).
Rates for these will likely depend on both f and k.
5.6 Applications
5.6.1 Exponential Penalty Functions
Consider the following linear programming problem.
{cTx : Ax  b}, (LP)
where c  Rd, b  Rn, and A  Rnd. Associate with this linear program the following
relaxation into an unconstrained problem: minxRd f (x) for
f (x) = c
Tx+ 
exp((Aix bi)/), (5.43)
where  > 0 and Ai is the ith row of A (a row vector). This approximation of (LP)
with exponential penalty functions was studied by several authors (see [251, 60, 209,
12]) and is directly useful in the machine learning literature for boosting (see, e.g.,
[167]). Derivatives of all orders for this problem are unbounded as x  , and
analyses of optimization methods, which rely on global smoothness constants, do not
provide global convergence rates. In this section we design a dual reference function
for f under the following assumptions on (LP).
Assumption H. Suppose that the following hold for problem (LP).
1. Ai = 1 for 1  i  n.
2. A  Rnd is of full rank d  n.
3. P = {x  Rn : Ax  b} is a polytope, which is contained in a Euclidean ball of
radius R > 0 and contains a Euclidean ball of radius r > 0.
The dual reference function will be designed so that f is dual smooth relative
to it and Algorithm 2.1, with appropriate step-size choices, converges with global
guarantees.
Define the dual reference function k : Rd  R,
k(x) = x  log(x+ 1). (5.44)
This behaves like a quadratic x2/2 near its minimum x = 0 and like x, i.e.,
grows linearly, at infinity. It is also possible to verify that k is Legendre convex.
Furthermore, we have:
k(x) = x
x+ 1 , 
2k(x) =
x+ 1 
xxT
(x+ 1)2x . (5.45)
Hence, [2k(x)]1  (1 + x)I. From Lemma 5.5.4 and this inequality it follows
that the fact that f is dual L-smooth relative to k is implied by
2f (x)  L [1 + f (x)] I x  Rd. (5.46)
This is the strategy of the following theorem, which shows that f is dual smooth to
this choice of k under our assumptions.
Proposition 5.6.1. Under Assumption H for f defined in (5.43) and k defined in
(5.44), we have that
2f (x)  L [2k(f (x))]1 x  Rd, (5.47)
where the dual relative smoothness constant is given by
L =
ATA
( + c). (5.48)
Here,
ATA
 is the induced matrix norm, and
 = sup
s1
AT s
 . (5.49)
Because f and k are Legendre convex, f is dual smooth relative to k and Theorem
5.5.6 implies that Algorithm 2.1 converges with k(f(xi)) converging at a rate O(1/i).
Remark 5.6.2. From Theorem 5.5.6, we have
k(f (x)) 
L (f (x0) f (xmin))
. (5.50)
This suggests that, if we can start from an initial point within the polytope, then we
can reach a point where f (x) is significantly less than c (which is expected to
be near the minimum) in polynomial amount of steps, depending on the conditioning
R/r and the value of  . The step-size 1/Li can also be chosen adaptively, as explained
in Proposition 5.5.8. Near the minimum, both f (x) and k(x
) behave like quadratic
functions, so local linear convergence rates hold. We believe that this iterative scheme
is reasonably efficient for high dimensional well-conditioned polytopes, but in other
less well conditioned instances it is outperformed by existing algorithms such as
multiplicative weights [9] or [59], which is based on Newtons method (hence uses
second-order information).
Proof of Proposition 5.6.1. Note that 1    n, because Ai = 1. Let (x) :=
maxi[n](Aix bi). Then (x) < 0 inside the polytope and (x) > 0 outside of it. By
differentiation, we have
f (x) =
Ai exp((Aix bi)/) + c, (5.51)
2f (x) =
ATi Ai
exp((Aix bi)/). (5.52)
Note that f is defined everywhere and differentiable. Furthermore, under our assumption
that rank(ATA) = rank(A) = d, it is evidently strictly convex and therefore Legendre.
The Hessian of f satisfies
2f (x)  exp((x)/)
 exp ((x)/)
ATA
I. (5.53)
Because   1, it is clear that the claim of the theorem holds for every x where
(x)  0 (i.e. inside the polytope or on its boundary). From now on we will assume
that x is such that (x) > 0 (outside of the polytope). Let xc be a minimizer of (x)
(at least one exists since the polytope is compact and (x) is a continuous function),
then using the assumption Ai = 1 it follows that (xc) = r < 0. Hence x 6= xc.
We are going to need an upper bound on x xc, which we will obtain as follows.
By the definitions, we have Aixc  r+ bi and Aix = Aix bi + bi  (x) + bi, hence
(x) + r
(x xc)
(x) + r
(x) + r
(x) + r
((x) + bi) +
(x) + r
(r + bi) = bi.
Therefore xc +
(x)+r
(x xc)  P  Bxc(2R), so
0 < x xc  2
(x) + r
R and x xc1 
(x) + r
. (5.54)
Let I = {i  [n]; Aix bi > 0}, J = {i  [n]; Aix bi  0}, and
GI(x) =
(Aixbi)Ai GJ (x) =
(Aixbi)Ai. (5.55)
Then f (x) = GI(x) +GJ (x) + c. We have
GI(x) 
GI(x)
T (x xc)
x xc
= x xc1
(Aixbi)Ai(x xc)
 x xc1 e
 ((x) + r)
Here, (a) follows from the facts that there is a j  I such that Aj(x xc) = (x) +
bj  Ajxc  (x) + r and the fact that Ai(x  xc)  bi + r  bi > 0 holds for every
i  I. (b) follows from (5.54). From (5.53) we obtain that
2f (x)  exp ((x)/)
ATA
ATA
GI(x) I
ATA
(f (x)+ GJ (x)+ c)I.
(5.56)
Hence (5.46) follows from the facts that GJ (x)   and  + c  1. As discussed
(5.47) follows from [2k(x)]1  (1 + x)I.
5.6.2 p-norm Regression
Consider the following p-norm regression problem,
Ax bpp , (pnorm)
where A  Rnd, d n, b  Rn, and p  1. This problem is a useful abstraction for
some important graph problems, including Lipschitz learning on graphs [143] and `p-
norm minimizing flows [4]. Algorithms specialized for p-norm regression have recently
been studied in the theoretical computer science literature by several authors (see,
e.g., [48, 2] and references therein). In this subsection, we design an appropriate dual
reference function for (pnorm) under the following assumptions. Let Ai denote the
rows of A (as row vectors).
Assumption I. Suppose that the following hold for problem (pnorm).
1. 2  p <.
2. A is full rank d, and for all x  Rd there is a subset I(x)  [n] such that
Aix 6= bi for all i  I(x), and span{Ai : i  I(x)} = Rd.
3. cG = infs=1 Aspp > 0.
4. cH = infu,vRd:u=1,v=1
i=1 |Aiu|
(Aiv)
2 > 0.
Remark 5.6.3. Although these assumptions seem restrictive, we can show that, if
n  2d  1 and (Ai)1in and (bi)1in are chosen as independent random variables
with densities that are absolutely continuous with respect to the Lebesgue measure
on Rd and R, then the assumptions hold with probability 1. Assumption 2 is implied
by the stronger assumption that any d rows of A define a full rank d matrix, and the
maximal number of equalities Aix = bi that hold for any x is no more than d. This
stronger version of Assumption 2, and Assumption 3 holds with probability 1 under
the random allocation due to the fact that the set of real valued d d matrices with
determinant 0 has Lebesgue-measure 0 in Rdd (due to the fact that the determinant
is a multivariate polynomial of the entries, and the zero set of such polynomials
has Lebesgue measure zero unless they are constant 0, see [51]). The minimum in
Assumption 4 is achieved for some umin and vmin due to continuity and compactness
of the unit sphere. Since any d rows of A form an independent basis with probability
1, it follows that u and v can be orthogonal to at most d  1 of them, respectively,
so using n  2d  1 there exists an i in the sum
i=1 |Aiumin|
(Aivmin)
2 that is
non-zero, hence Assumption 4 holds.
Consider the dual reference function k : Rd  R,
k(x) = 1
x2 + 1
2  1
, (5.57)
for q = p
p1 (hence
= 1). This behaves like a quadratic x2/2 near its minimum
x = 0 and like xq/q at infinity. For this k, we have
k(x) = x(1 + x2)
2 (5.58)
As the next theorem shows dual relative strong convexity and smoothness of (pnorm)
relative to this k hold under our assumptions.
Proposition 5.6.4. Let f(x) = Ax bpp be the p-norm objective. Under Assumption
I for k defined in (5.57), there exists , L > 0 such that
[2k(f(x))]1  2f(x)  L[2k(f(x))]1 x  Rd. (5.59)
See (5.69) and (5.70) for the definitions of  and L. Because f and k are Legendre
convex, f is dual smooth and dual strongly convex relative to k and Theorem 5.5.6
implies that Algorithm 2.1 converges with f(xi) f(xmin) converging at a linear rate
O((1 /L)i).
To test the empirical performance of this method, we have implemented it with
Ai, b, and x0 i.i.d. as standard normals for power p = 4, d  {102, 103, 104},
and n = 10d. The inverse step-size L0 was chosen to be L
0 = 1 initially, and
multiplied by 2 if the function value would increase due to too large steps (hence
this was chosen adaptively in the beginning, but Li was never decreased later on).
As Figure 5.1 shows, empirically our method seems to be performing well, with
high precision achieved after 50-80 gradient evaluations, and the convergence rate
seems to be mostly unaffected by the dimension d. Hence in this random setting
dual space preconditioning is indeed very efficient, and competitive with previous
works [48, 2, 4] which had dimension dependent convergence rates. We think that
based on Proposition 5.6.4, it can be shown that with high probability, dimension-
free convergence rates hold in this random scenario when the number of vectors n
tends to infinity (the proof would be based on concentration inequalities for empirical
processes, see e.g. [39] for an overview of such inequalities). Note however that
we do not believe this always to be the case for general non-random A and b, and
there could be instances of very poor conditioning (such as when n  d) where the
homotopy method of [48] or the IRLS method of [3] could perform better. The proof
of Proposition 5.6.4 is based on the following two lemmas.
0 20 40 60 80
iteration i
n = 103, d = 102
0 20 40 60 80
iteration i
n = 104, d = 103
0 20 40 60 80
iteration i
n = 105, d = 104
Figure 5.1: Convergence rates for p-norm regression are mostly unaffected by the
dimension d for these random instances with p = 4.
Lemma 5.6.5 (Bounds on the gradient). Let f(x) = Ax bpp be the p-norm
objective for (pnorm). Under Assumption I, we have
LGxp1  CG  f(x)  UGxp1 +DG (5.60)
for all x  Rd, with constants
LG = 2
p+1cG = 2
p+1 inf
s=1
Aspp , CG =
|bi|p
)(p1)/p
 c1/pG ,
UG = 2
p2(p+ 1) sup
s=1
Aspp , DG = 2
p2(p 1)
|bi|p
Proof. By differentiation, we have
f(x) = p
|Aix bi|p2 (Aix bi)Ai, (5.61)
f(x) = p

|Aix bi|p2 (Aix bi)Ai

 max
|Aix bi|p2 (Aix bi)Aix, 0
= max
|Aix bi|p2 (Aix bi)2 + |Aix bi|p2 (Aix bi)bi
 max
|Aix bi|p  |Aix bi|p1 |bi|
now by Youngs inequality |Aix bi|p1 |bi|  |Aix bi|p p1p +
|bi|p
, hence
 max
(|Aix bi|p  |bi|p) , 0
using the fact that |a+ b|p  (|a|+ |b|)p =
2|a|+2|b|
 2p1(|a|p + |b|p) by convexity
(this is so-called the Cp inequality), so |Aix bi|p + |bi|p  2p+1 |Aix|p, hence
 max
2p+1 |Aix|p  2|bi|p
 max
2p+1
s=1
Aspp
 xp1  2
i=1 |bi|p
x , 0
and the lower bound follows from Assumption I by straightforward rearrangement.
For the upper bound, notice that
f(x)  p sup
v=1
|Aix bi|p1 |Aiv|
 2p2p sup
v=1
|Aix|p1 |Aiv|+ |bi|p1 |Aiv|
 2p2p
xp1 sup
s=1,v=1
|Ais|p1 |Aiv|
+ sup
v=1
|bi|p1 |Aiv|
by Fenchel-Young, and rearrangement
 2p2p
s=1
Aspp +
|bi|p
hence the result follow.
Lemma 5.6.6 (Bounds on the Hessian). Let f(x) = Ax bpp be the p-norm
objective. Suppose that Assumption I holds, and let

|bi|p2ATi Ai

1/(p2)
/(cH2
p)1/(p2), (5.62)
H = inf
xRH
min(2f(x)) = inf
x1,u=1
p(p 1)
|Aix bi|p2 (Aiu)2. (5.63)
Then H > 0, and we have
(LHxp2 + CH)I  2f(x)  (UHxp2 +DH)I (5.64)
for all x  Rd, with constants
LH = min
p(p 1)2p1cH ,
CH = min
, p(p 1)2p1cHRp2H
UH = 2
p3p(p 1) sup
u=1,v=1
|Aiu|p2 (Aiv)2,
DH = p(p 1)2p3

|bi|p2ATi Ai
 .
Proof. We have by differentiation
2f(x) = p(p 1)
|Aix bi|p2ATi Ai. (5.65)
Notice that using the fact that |a b|p2 + |b|p2  2(p1)|a|p2, we have
2f(x) = p(p 1)
|Aix bi|p2ATi Ai
 p(p 1)
2(p1)|Aix|p2  |bi|p2
ATi Ai
 p(p 1)2(p1)cHxp2  p(p 1)

|bi|p2ATi Ai
 .
Let RH be as in (5.62), then using the above bound, we can see that for x  RH ,
we have
2f(x)  p(p 1)2pcHxp2I
 p(p 1)2p1cHxp2 + p(p 1)2p1cHRp2H .
(5.66)
Since the minimum of the continuous function min(2 f(x)) is achieved on the
compact set BRH , and by the second part of Assumption I, it cannot be zero, and
hence H > 0 and 2f(x)  HI for every x  BRH . The lower bound in (5.64)
follows by combining this with (5.66). For the upper bound, using the inequality
|a+ b|p2  2p3(|a|p2 + |b|p2), we obtain that
2f(x)  p(p 1)2p3 sup
s=1

|Ais|p2ATi Ai
  x
+ p(p 1)2p3

|bi|p2ATi Ai
 .
Now we are ready to prove our main result in this section.
Proof of Proposition 5.6.4. First, both f and k are Legendre convex in this case. This
is easy to verify for k, and evidently f is differentiable everywhere. To verify strict
convexity of f , note that 2f(x)  0 under part two of Assumption I. Since both f
and k are twice differentiable, by Lemma 5.5.2, it suffices to check that (5.59) holds
for the linear convergence of Algorithm 2.1. We have by differentiation,
2k(x) = (1 + x2)
2 I + (q  2)(1 + x2)
2 xxT . (5.67)
Now it is easy to see that for p  [2,), we have q = p/(p 1)  (1, 2] and it is not
difficult to verify that 2k satisfies that for all x  Rd,
(1 + x2)
p1 I 
2k(x)
]1  (p 1)(1 + x2)
p1 I. (5.68)
The claim of the theorem now follows by some straightforward rearrangement using
Lemmas 5.6.5 and 5.6.6, with constants
 = min
2(p 1)(2 + 2DG)
4(p 1)U (p2)/(p1)G
, (5.69)
L = min
(LG/2)(p2)/(p1)
, 4UH
)(p2)/(p1)
+ 2DH
. (5.70)
5.7 Discussion
5.7.1 Special cases and related methods
Algorithm 2.1 is closely related to a number of existing methods, some of which
are subject to the analysis we provide. The most notable of these is the method of
steepest descent with respect to a given norm  (now not necessarily Euclidean).
Here we follow the exposition of Boyd and Vandenberghe [42, sect. 4.9]. The steepest
descent iteration is given by
xi+1 = xi +
f(xi) d, where d  arg max
x1
f(xi), x , (5.71)
and x = supx1 x, x is the dual norm of . It is possible to verify the
following equivalencies for all x  Rd.
x arg max{x
, x : x  1} = {x : x, x = x2 , x = x
= (x2 /2)
(5.72)
where k(x) is the subdifferential of k at x. In this form it is clear to see that for
strictly convex and differentiable , the steepest descent method (5.71) is a special
case of dual preconditioned gradient descent with k(x) = x2 /2. Our analysis does
not apply in the case of other norms or normalized steepest descent [42], although
they may be seen as close relatives of Algorithm 2.1.
Algorithm 2.1 also generalizes some recent work in machine learning. Using the
identityk = (k)1 and the fact that k is Legendre iff k is Legendre, the iterations
of dual preconditioned gradient descent may be written as
xi+1 = xi  1Lik(f(xi)) = arg min
{f(xi), x+ 1Lik
(Li(xi  x))}. (5.73)
In this form it is clear that the rescaled gradient descent method studied in [265,
sect. 2.2] is a special case of Algorithm 2.1 with k(x) = 2 x, B1xq/2 /q where B
is a positive definite, self-adjoint linear operator and 1  q <  with p = q/(q  1)
an integer. [265] study the convergence of this method under smoothness conditions
that require bounds on the derivatives of all orders up to p. In contrast, our analysis
under dual relative smoothness requires only a relationship between the derivatives of
first and second order and is applicable to rescaled gradient descent. To summarize,
Algorithm 2.1 may be seen as a generalization of the steepest descent method to
general convex regularizers or a generalization of polynomial rescalings of gradient
descent.
Dual preconditioning is more distantly related to the dual gradient methods [249,
21]. Dual gradient methods are suitable for the following composite model.
f(x) + g(x), (primal)
where f : Rd  {R,} is proper, lsc, and strongly convex and g : Rd  {R,}
is proper, lsc, and convex (see [18, Chap. 12] for a more general model and review).
The observation motivating the dual gradient methods is that the dual formulation,
xRd
f (x) + g(x), (dual)
admits gradient [249] and accelerated gradient methods [20], because f  is smooth
when f is strongly convex. Similarly, dual preconditioned gradient descent can be seen
as a move to the dual space, in which a dual problem k(x)  f (x)x, xmin (dual
to f(x)+x=xmin(x)) is minimized by a Bregman gradient method. Thus, dual gradient
methods and dual preconditioning are most easily applied when the dual structure is
relatively more benign to model than the primal structure, e.g., when f has super-
quadratic growth (and thus f c has sub-quadratic growth). Both of the applications
considered in this paper are of this kind. However, the two methods differ in terms
of what is assumed to be cheap to compute; dual gradient methods assume that it is
cheap to find points in f (x), whereas dual preconditioning explicitly avoids this
with two ideas: by designing a dual objective function k(x) with a minimum at 0
whose gradient map is cheap to compute and by using f  as the reference function
in a dual Bregman gradient scheme. When f is Legendre convex and k is relatively
smooth in the dual space to f , the primal iterates are cheap to compute and the
analysis over the dual iterates closely follows recent work on relative smoothness
[16, 156, 244].
5.7.2 Conclusions
In this paper we introduced a non-linear preconditioning scheme for gradient descent
on Legendre convex functions f that converges under generalizations of the standard
Lipschitz assumption on f . There are at least two interpretations of this method.
The first is as a generalization of gradient descent in which the update direction
is preconditioned by the gradient map k of a designed dual reference, Legendre
convex function k. The second interpretation is as a Bregman gradient method
in the dual space, which minimizes the designed k while the conjugate f  plays
the role of the reference function, see section 5.7.1. The choice of k affects the
conditioning of our method, which is made explicit in our analysis through a dual
relative smoothness condition between f and k. The dual relative conditions admit
non-smooth f and k, and are provably distinct dual cousins of the relative smoothness
conditions introduced by [16]. In the first interpretation of dual preconditioning, dual
relative smoothness is as a requirement that k  f is Lipschitz continuous. In the
second, k serves as a model of the convex conjugates f  in a certain problem class. In
section 5.6, we show how this method can be applied to exponential penalty functions
(see, e.g., [60, 59]) and p-norm regression (see [48, 2] and references therein) with
global convergence rate guarantees.
There are natural questions that arise from this work. First, it may be useful to
pursue the analogy with dual gradient methods further and to design methods for
the general composite model (dual) that exploit dual relative smoothness. Second,
it is natural to wonder whether dual relative smoothness can be exploited by an
accelerated method, which should be optimal in the class of functions dual smooth
relative to a fixed k. [156] raised this question for primal relative smoothness, and
Bregman methods converging at accelerated rates under primal relative smoothness
have been designed [113, 112]. Unfortunately, recent work suggests that it is not
possible to accelerate mirror descent under the generalized relative conditions [71].
Finally, some caution is warranted. There is no free lunch and the central difficulty
of this method is in the design of k. Still, the dual relative conditions studied in this
work provide new avenues for improving the conditioning of optimizers via hard-won
domain-specific knowledge.
Statement of Authorship for joint/multi-authored papers for PGR thesis 
To appear at the end of each thesis chapter submitted as an article/paper 
The statement shall describe the candidates and co-authors independent research contributions in the thesis 
publications. For each publication there should exist a complete statement that is to be filled out and signed by the 
candidate and supervisor (only required where there isnt already a statement of contribution within the paper 
itself). 
Title of Paper 
Dual Space Preconditioning for Gradient Descent 
Publication Status 
  Published                                   Accepted for Publication 
  Submitted for Publication          Unpublished and unsubmitted work written 
                         in a manuscript style 
Publication Details 
Chris J. Maddison*, Daniel Paulin*, Yee Whye Teh, Arnaud Doucet. Dual Space 
Preconditioning for Gradient Descent. In Review, 2019. 
Student Confirmation 
Student Name: 
Chris J. Maddison 
Contribution to the 
Paper 
 I proposed the application of relative smoothness and strong convexity for non-
linear preconditioning of gradient descent. 
 I proved all of the convergence results in the paper, with the exception of the 
adaptive step-size analysis. 
 I proposed the application to p-norm regression, but Daniel Paulin developed 
both of the applications and is responsible for all of the analysis in the 
applications section. 
 I wrote the entire paper, with some help from Daniel to incorporate his 
analyses. 
 All authors contributed to the development of the paper through discussions 
and ideas, and all authors reviewed the final draft. 
Signature  
05 May 2020 
Supervisor Confirmation 
By signing the Statement of Authorship, you are certifying that the candidate made a substantial contribution to the 
publication, and that the description described above is accurate. 
Supervisor name and title: 
Professor Arnaud Doucet 
Supervisor comments 
I agree that the candidate has made a substantial contribution to the publication. 
Signature 
05 May 2020 
This completed form should be included in the thesis, at the end of the relevant chapter. 
Chapter 6
Conclusions
We introduced four new methods that address distinct parts of the machine learning
workflow: designing objectives, computing gradients, and gradient-based optimization.
Future work is discussed in the conclusion sections of each chapter. To conclude, we
review the major contributions in turn.
In Chapter 2 we introduced biased gradient estimators for stochastic optimization
in the presence of discrete random variables. This is a challenging problem, and the
previous unbiased methods generally suffer from high variance. We took a different
tack, introducing bias to decrease variance. Our estimators are reparameterization-
based, but of a relaxed approximation to the discrete model. The primary advantage
of such estimators is that they are easy to implement and computationally inexpensive.
We showed improved performance on multiple benchmarks in statistical deep learning.
In Chapter 3 we introduced variational objectives for sequential models, called
filtering variational objectives (FIVOs). FIVO is a lower bound on the marginal log-
likelihood, constructed by taking the logarithm of a particle filters estimator for the
normalizing constant of the model. We showed that the tightness of FIVO is related
to the variance of the estimator from which it is constructed, suggesting that improved
bounds may be designed from more sophisticated normalizing constant estimators.
We developed a reparameterization scheme for estimating gradients of FIVO. We
showed that optimizing FIVO uniformly outperforms baseline objectives on a variety
of deep learning benchmarks.
In Chapter 4 we studied the impact of the choice of kinetic energy on the convergence
of mometum-based optimizers. We studied conformal Hamiltonian systems, which we
called Hamiltonian descent systems, and showed that the kinetic energy acts as a non-
linear preconditioner. We studied Hamiltonian descent systems in continuous time
and presented three discretization schemes. The choice of kinetic energy in all of
these schemes modifies the conditioning of the system to allow linear convergence on
strictly convex functions, even for functions that may be non-smooth or non-strongly
convex. In a rough sense, the conditions that we study require that the kinetic
energy approximate the curvature of the convex conjugate of the function of interest,
a generalization of strong convexity and smoothness. We proved partial lower bounds
for simple one-dimensional power functions, showing that when these conditions are
not satisfied the Hamiltonian descent system fails to converge linearly. We designed
and analyzed kinetic energies for functions with power growth.
In Chapter 5 we presented a non-linear preconditioning scheme for gradient descent.
This scheme is closely related to the work of Chapter 4, but its analysis is considerably
simpler. We called our scheme dual space preconditioning, and showed that it
converges under conditions (related to the conditions studied for Hamiltonian descent)
that are related to recent work on relative smoothness and strong convexity [16, 156].
We presented two applications to p-norm regression and exponential barrier functions.
We showed how our conditions may be satisfied in practice on these examples.
The central theme of this thesis is on the interplay between the problems of
numerical optimization and numerical integration. Although apparently distinct
problems, their methodology, analysis, and study are complementary, and there is
a productive research agenda at their intersection.
Appendix A
Appendix to Hamiltonian Descent
Ap.1 Proofs for convergence of continuous systems
Lemma 4.3.6 (Convergence rates in continuous time for fixed ). Given   (0, 1),
f : Rd  R differentiable and convex with unique minimum xmin, k : Rd  R
differentiable and strictly convex with minimum k(0) = 0. Let xt, pt  Rd be the value
at time t of a solution to the system (4.3) such that there exists   (0, 1] where
k(pt)  f c (pt). Define
(, , ) = min
    
  ,
(1 )
. (4.19)
If   (0,min(, )], then
V t  (, , )Vt.
Finally,
1. The optimal   (0,min(, )], ? = arg max (, , ) and ? = (, ?, )
are given by,
? = 1
 + 
(1 )2 + 2
, (4.20)
(1 ) + 
(1 )2 + 2
for 0 <  < 1,
(1)
2 for  = 1,
(4.21)
2. If   (0, /2], then
(, , ) =
(1 )
1  , and (4.22)
    2(1 )/4
k(pt)  xt  xmin, pt   xt  xmin,f(xt)
 (1 )(k(pt) + f(xt) f(xmin) +  xt  xmin, pt). (4.23)
Proof.
Vt =  k(pt), pt+  k(pt), pt   xt  xmin, pt   xt  xmin,f(xt)
= (  ) k(pt), pt   xt  xmin, pt   xt  xmin,f(xt)
 (  )k(pt)  xt  xmin, pt  (f(xt) f(xmin))
by convexity and   . Our goal is to show that Vt  Vt for some  > 0, which
would hold if
(  )k(pt)  xt  xmin, pt(f(xt) f(xmin)) 
 (k(pt) + f(xt) f(xmin) +  xt  xmin, pt)
which is equivalent by rearrangement to
(  ) xt  xmin, pt  (    )k(pt) + (  )(f(xt) f(xmin)). (Ap.1)
Assume that   . By assumption on f , k, and  we have (4.15), which implies by
rearrangement that k(pt)   xt  xmin, pt  (f(xt) f(xmin)), so
(  ) xt  xmin, pt 
(  )(k(pt) + (f(xt) f(xmin))), (Ap.2)
and k(pt)  0 and f(xt)f(xmin)  0, hence it is enough to have ()  
and ()   for showing (Ap.1). Thus we need   min(, 
 ,
(1)
1 ).
Here 
1 (1 )   for 0 <    < 1, therefore Vt
  (, , )Vt for
(, , ) = min
    
  ,
(1 )
In order to obtain the optimal contraction rate, we need to maximize (, , ) in .
Without loss of generality, we can assume that 0 <  < 
, and it is easy to see
that on this interval, 
 is strictly monotone decreasing, while
(1)
1 is strictly
monotone increasing. Therefore, the maximum will be taken when the two terms are
equal. This leads to a quadratic equation with two solutions
1 + 
(1 )2 + 
One can check that + >
, while 0 <  <
, hence
[0,]
(, , ) = (, , ) =
(1 ) + 
(1 )2 + 
for  < 1. For  = 1, we obtain that ? =  =
, and ? =
(1)
2 .
Now assume   (0, /2]. Since we have shown that (, , ) = (1)
1 for
 <  it is enough to show that  > /2 to get our result. Notice that  as a
function of , (), is strictly concave with (0) = 0, and (1) =
, thus
 = () > (1) =
1 + 
Finally, the proof of (4.23) is equivalent by rearrangement to showing that for  =
(1 ),
() xt  xmin, pt  ( 2(1 )/4)k(pt) + ()(f(xt) f(xmin)),
(Ap.3)
hence by (Ap.2) it suffices to show that we have 
(  )    2(1 )/4   
and (  )    . The latter one was already verified in the previous section,
and the first one is equivalent to
     
(  )  2(1 )/4 for every 0 <  < 1, 0 <   1, 0 <   /2.
It is easy to see that we only need to check this for  = /2, and in this case by
minimizing the left hand side for 0    1 and using the fact that  = (1 ), we
obtain the claimed result.
Ap.2 Proofs for partial lower bounds
In this section, we present the proofs of the lower bounds. First, we show the existence
and uniqueness of solutions.
Lemma 4.3.9 (Existence and uniqueness of solutions of the ODE). Let a, b,  
(0,). For every t0  R and (x, p)  R2, there is a unique solution (xt, pt)tR of
the ODE (4.32) with xt0 = x, pt0 = p. Either xt = pt = 0 for every t  R, or
(xt, pt) 6= (0, 0) for every t  R.
Proof. Let Ht := |xt|
|pt|a
, then Ht  0 and
Ht = |xt|b1 sign(xt)xt + |pt|a1 sign(p)pt = |pt|a,
so 0  Ht  aHt. By Gronwalls inequality, this implies that for any solution of
(4.3),
Ht  H0 for t  0, and (Ap.4)
Ht  H0 exp(at) for t < 0. (Ap.5)
The derivatives x, p are continuous functions of (x, p), and these functions are
locally Lipschitz if x 6= 0 and p 6= 0. So by the Picard-Lindelof theorem, if xt0 6= 0,
pt0 6= 0, then there exists a unique solution in the interval (t0  , t0 + ) for some
 > 0.
Now we will prove local existence and uniqueness for (xt0 , pt0) = (x0, 0) with
x0 6= 0, and for (xt0 , pt0) = (0, p0) with p0 6= 0. Because of the central symmetry, we
may assume that x0 > 0 and p0 > 0, and we may also assume that t0 = 0.
First let x0 = 0 and p0 > 0. We take t close enough to 0 so that pt > 0. Then
xt = p
t > 0 and p
t = |xt|b1 sign(xt)  pt. Then pt = (xt) for some function
 : (, ) R>0, where t is close enough to 0. Here (0) = p0 and pt = (xt)xt, so
(xt) =
= (|xt|b1 sign(xt) + pt)p1at
= (|xt|b1 sign(xt) + (xt))(xt)1a,
and hence
(u) = (|u|b1 sign(u) + (u))(u)1a and (0) = p0.
This ODE satisfies the conditions of the Picard-Lindelof theorem, so  exists and
is unique in a neighborhood of 0. Then x0 = 0 and x
t = (xt)
a1, so for the
Picard-Lindelof theorem we just need to check that u 7 (u)a1 is Lipschitz in a
neighborhood of 0. This is true, because  is C1 in a neighborhood of 0. So xt exists
and is unique when t is near 0, hence pt = (xt) also exists and is unique there.
Now let x0 = x0 > 0 and p0 = 0. We take t close enough to 0 so that xt > 0.
Then xt = |pt|a1 sign(pt) and pt = xb1t  pt < 0 for t close enough to 0. Then
xt = (pt) for some function  : (, )  R>0, where t is close enough to 0. Here
(0) = x0 and x
t = 
(pt)p
t, so
(pt) =
= |pt|
a1 sign(pt)
xb1t + pt
= |pt|
a1 sign(pt)
(pt)b1 + pt
and thus
(u) = |u|
a1 sign(u)
(u)b1 + u
and (0) = x0.
This ODE satisfies the conditions of the Picard-Lindelof theorem, so  exists and is
unique in a neighborhood of 0. Then p0 = 0 and p
t = (pt)b1  pt, so for the
Picard-Lindelof theorem we just need to check that u 7 (u)b1  u is Lipschitz
in a neighborhood of 0. This is true, because  is C1 in a neighborhood of 0. So pt
exists and is unique when t is near 0, hence xt = (pt) also exists and is unique there.
Let [0, tmax) and (tmin, 0] be the longest intervals where the solution exists and
unique. If tmax <  or tmin < , then by Theorem 3 of [202], page 91, the solution
would have to be able to leave any compact set K in the interval [0, tmax) or (tmax, 0],
respectively. However, due to the (Ap.4) and (Ap.5), the energy function Ht cannot
converge to infinity in finite amount of time, so this is not possible. Hence, the
existence and uniqueness for every t  R follows.
Before proving Theorem 4.3.10, we need to show a few preliminary results.
Lemma Ap.2.1. If (x, p) is not constant zero, then limtHt = and limtHt =
limt xt = limt pt = 0.
Proof. The limits ofH exist, becauseHt = |pt|a  0. First suppose that limtHt =
M < . Then Ht  M for every t, so x and p are bounded functions, therefore x
and p are also bounded by the differential equation. Then Ht = a|pt|a1 sign(pt)pt
is also bounded, so H is Lipschitz. This together with limtHt = M  R implies
that limtHt = 0. So limt pt = 0. Then we must have limt xt = x0 for
some x0  R \ {0}. But then limt pt = |x0|b1 sign(x0) 6= 0, which contradicts
limt pt = 0. So indeed limtHt =.
Now suppose that limtHt > 0. For t  [0,) we have Ht  H0, so for t  0
the functions x and p are bounded, hence also x, p, H, H are bounded there.
So limtHt  R, and H  is Lipschitz for t  0, therefore limtHt = 0, thus
limt pt = 0. Then we must have limt xt = x0 for some x0  R \ {0}. But
then limt p
t = |x0|b1 sign(x0) 6= 0, which contradicts limt pt = 0. So indeed
limtHt = 0, thus limt xt = limt pt = 0.
From now on we assume that 1
Lemma Ap.2.2. If (xt, pt) is a solution, then for every t0  R there is a t  t0 such
that pt = 0.
Proof. The statement is trivial for the constant zero solution, so assume that (x, p)
is not constant zero. Then limtHt = . Suppose indirectly that pt < 0 for
every t  t0. Then xt = |pt|a1 < 0 for t  t0. If limt xt = x < ,
then limt pt = , because limtHt = . Then x  x  R and x =
|p|a1   when t , which is impossible. So limt xt =, hence there
is a t1  t0 such that pt < 0 < xt for every t  t1. Let Gt := |pt|
a1  xt, then for
t  t1 we have
Gt = |pt|a2pt  xt = |pt|a2(xb1t  |pt|) + |pt|a1 = |pt|a2xb1t > 0.
So Gt  G(t1) for every t  t1. Thus
|pt|  ((a 1)(xt +G(t1)))
a1 = (Axt +B)
a1 ,
for t  t1, where A > 0. For big enough x we have (Ax + B)
a1 < 1
xb1, because
a1 < b  1, since ba > b + a. So there is a t2  t1 such that pt < 0 < xt
and |pt|  1x
t for every t  t2. Then pt = xb1t  pt  0, so pt < 0 is
monotone decreasing for t  (, t2], hence p = limt pt  R. But then
pt = xb1t  pt   when t , which together with p  R is impossible.
This contradiction shows that indeed there is a t  t0 such that pt  0. Applying
this for (x,p), we get that there is a t  t0 such that pt  0. So by continuity,
there is a t  t0 such that pt = 0.
For A > 1
let (A) := ( A1
(b1)Aa )
baba and
RA := {(x, p)  R2; 0 < x < (A), Axb1 < p < 0}, (Ap.6)
Lemma Ap.2.3. Let A > 1
. If (x, p) is a solution, t0  R and (xt0 , pt0)  RA, then
(xt, pt)  RA for every t  t0.
Proof. Suppose indirectly that there is a t > t0 such that (xt, pt) / RA. Let T be
the infimum of these ts. Then T > t0, and (x(T ), p(T )) is on the boundary of the
region RA. We cannot have (x(T ), p(T )) = (0, 0), because (0, 0) is unreachable in
finite time. Since xt = |pt|a1  0 for t  [t0, T ), we have x(T )  xt0 < (A). So
we have 0 < x(T ) < (A) and either p(T ) = 0 or p(T ) = Ax(T )b1.
Suppose that p(T ) = 0. Then p(T ) = x(T )b1 < 0, so if t  (t0, T ) is close
enough to T , then pt > 0, which contradicts (xt, pt)  RA. So p(T ) = Ax(T )b1.
Let Ut := pt + Ax
t . Then U(T ) = 0, and by the definition of T , we must have
U (T )  0. Using |p(T )| = Ax(T )b1 we get
0  U (T ) = p(T ) + (b 1)Ax(T )b2x(T )
= |p(T )|  x(T )b1  (b 1)Ax(T )b2|p(T )|a1
= x(T )b1(A 1 (b 1)Aax(T )baba),
so (T ) = ( A1
(b1)Aa )
baba  x(T ) < (T ). This contradiction proves that (xt, pt)  RA
for every t  t0.
The following lemma characterises the paths of every solution of the ODE in terms
of single parameter .
Lemma Ap.2.4. There is a constant  > 0 such that every solution (xt, pt) which
is not constant zero is of the form (x
t+, p
t+) for exactly one   [, ] \ {0} and
  R.
Proof. For u > 0 let us take the solution (xt, pt) with (xt0 , pt0) = (u, 0) for some
t0  R. Then pt0 = u
b1 > 0, so pt0 < 0 if t < t0 is close enough to t0. By
Lemma Ap.2.2, there is a smallest T (u)  R>0 such that p(t0  T (u)) = 0. We
may take t0 = T (u), and call this solution (X(u)(t), P (u)(t)). Then X(u)T (u) = u,
T (u) = P
0 = 0, and P
t < 0 for t  (0, T (u)). Let g(u) = X
0 . Here g(u) 6= 0,
because we cannot reach (0, 0) in finite time due to (Ap.4)-(Ap.5). We cannot have
g(u) < 0, because then P u(0) = |g(u)|b1 sign(g(u)) > 0. So g(u) > 0, thus we have
defined a function g : R>0  R>0. For u > 0 let us take the continuous path
Cu := {(X(u)t , P
t ); t  [0, T (u)]}.
Note that this path is below the x-axis except for the two endpoints, which are on
the x-axis. If 0 < u < v, then Cu  Cv = , so we must have g(u) < g(v) (otherwise
the two paths would have to cross). So g is strictly increasing. Let
 := lim
g(u)  R0.
If 0 < u < v and z  (g(u), g(v)), then going forward in time after the point (z, 0),
the solution must intersect the x-axis first somewhere between the points (v, 0) and
(u, 0), thus z is in the image of . So  : R>0  (,) is a strictly increasing
bijective function. We have g(u) > u for every u > 0, because if g(u)  u, then
for the solution (X
t , P
t ) we have H0  H(T (u)) and Ht = |P
t |a < 0 for
t  (0, T (u)), which is impossible.
Let A > 1
and 0 < z < (A), and take the solution (xt, pt) with (x0, p0) = (z, 0).
Then p0 < 0, so (xt, pt)  RA for t > 0 close enough to 0, and then by Lemma Ap.2.3,
(xt, pt)  RA for every t > 0. So z is not in the image of , hence   (A) > 0.
Let (xt, pt) be a solution, which is not constant zero. Let S = {t  R; pt = 0}.
This is a closed, nonempty subset of R. Suppose that sup(S) =. Since limt xt =
0, this means that there are infinitely many t  R such that pt = 0 and |xt|  (0, ).
This is impossible, since there can be only one such t. So sup(S) = max(S) = T  R.
We may translate time so that T = 0. Then p0 = 0 and pt 6= 0 for every t > 0. If
|x0| > , then later we again intersect the x-axis, so we must have 0 < |x0|  . So
the not constant zero solutions can be described by their last intersection with the
x-axis, and this intersection has its x-coordinate in [, ] \ {0}.
By symmetry, we have x
t = x
t and p
t = p
t . We now study the
solutions (x
t , p
t ) for 0 <    and t  0. Then p
t < 0 < x
t for t  0
and (x())t < 0 and limt x
t = 0, so x
t  (0, ) for t > 0. So we can write
t = ()(x
t ), where 
() : (0, )  R>0 and limz0 ()(z) = limz ()(z) = 0.
  (0, ]; (z,()(z))  RA for some A >
and z  (0, )
The following lemmas characterize this set.
Lemma Ap.2.5. If   (0, ] \, then
()(z)
((a 1)z)
Proof. By the definition of (),
(x()t )b1 + ()(x
t ) = (p
 = (())(x
()(x
a1, so
(())(z) = ()(z)1a(()(z) zb1).
Because the orbits are disjoint for different s, we have (1)(z) < (2)(z) for 0 <
1 < 2   and z  (0, 1). If (z0,()(z0))  RA for some z0  (0, ), then by
Lemma Ap.2.3, (z,()(z))  RA for every z  (0, z0]. So
  (0, ]; lim inf
z1b()(z) <
If 0 < 1 < 2 and 2  , then 1   too, since (1)(z) < (2)(z) for z  (0, 1).
If A > 1
and  < (A), then (x
t , p
t )  RA for t > 0, so (z,()(z))  RA for
z  (0, ). So (0, (A))   for every A > 1
. Let
F (z) := 1(a 1)1()(z)a1  z,
then limz0 F (z) = 0, and
F (z) =
(())(z)
()(z)2a
 1 = 1(z1b()(z))1,
so limz0 F
(z) = 0, because limz0 z
1b()(z) = , since  / . Then for every
 > 0 there is a  > 0 such that F is -Lipschitz in (0, ), and then |F (z)|  z for
z  (0, ). So limz0 F (z)z = 0.
Lemma Ap.2.6.  = (0, ).
Proof. Suppose indirectly that   . Then there is an A > 1
and a z  (0, ) such
that (z,(z))  RA. Then for  > 0 small enough we have (z,(z)  )  RA
too. Let (x, p) be the solution with (x0, p0) = (z,(z)  ). By Lemma Ap.2.2,
there is a T < 0 such that p(T ) = 0 and pt < 0 for t  (T, 0]. Then xt < 0 for
t  (T, 0]. Since this orbit cannot cross {(u,()(u)); u  (0, )}, we must have
x(T ) > . However (x0, p0)  RA, so (xt, pt)  RA for every t  0 by Lemma Ap.2.3.
So (x(T ), 0) is the last intersection of the solution (x, p) with the x-axis, hence x(T )
is not in the image of , so  < x(T )  . This contradiction proves that  / .
Now suppose indirectly that there is a   (0, ) \ . Let us write  =  and
 = () for simplicity. We have
(z) = (z)1a((z) zb1) > 0
for z > 0 close enough to 0, because limz0
= , since  / . So  has an
inverse function 1 near 0. So we can define a function G(z) := 1((z)) for
z  (0, c), for some c > 0. We have (z) < (z) for every z  (0, ), so G(z) > z for
z  (0, c). Then
G(z) = (G(z))1(z) =
(z)1a((z) zb1)
(G(z))1a((G(z))G(z)b1)
(z) zb1
(z)G(z)b1 .
Let h(z) = G(z) z for z  (0, c), then h(z) > 0, limz0 h(z) = 0, and
h(z) =
(z + h(z))b1  zb1
(z)G(z)b1 = z
b1 (1 +
)b1  1
(z)G(z)b1 .
If z  0, then (z)a1  (z)a1  (a 1)z by Lemma Ap.2.5. Since G(z) 0, we
also have (a 1)z  (z)a1 = (G(z))a1  (a 1)G(z). So limz0 G(z)z = 1 and
limz0
= 0. Then (z)/G(z)b1  ((a1))
a1 z
a1(b1), so limz0 (z)/G(z)
b1 =
, because 1
a1 < b 1. Therefore
h(z)  zb1
)b1  1
(z)
 zb1(b 1)1h(z)
((a 1)z)
= Cz1h(z),
where C = (b 1)1((a 1))
a1 > 0 and  = b 1 1
a1 > 0 are constants.
We know that (z)  ((a  1))
a1 z
a1 = 1. Note that 1
a1 < b  1, so
limz0 (z)/G(z)
b1 =. We also have limz0 h(z)z = 0 and (1 +
)b1 1  (b
. So h(z)  
1(b1)
((a1))
zb2
a1h(z). Thus h(z)  Cz1h(z), where C > 0 and
 = b2 1
a1 +1 > 0, because baba > 0. So log(h(z))
h(z)
 Cz1 = (C
z).
Applying LHospitals rule, we get
1 = lim
log(h(z))
= lim
log(h(z))
= .
This contradiction proves that  = (0, ).
Now we are ready to prove our lower bound.
Proposition 4.3.10 (Lower bounds on the convergence rate in continuous time).
Suppose that 1
< 1. For any   R, we denote by (x()t , p
t ) the unique solution
of (4.32) with x0 = , p0 = 0. Then there exists a constant   (0,) depending on a
and b such that the path (x
t , p
t ) and its mirrored version (x
t , p
t ) satisfy that
|x()t | = |x
t |  O(exp(t)) for every  < (a 1) as t.
For any path (xt, pt) that is not a time translation of (x
t , p
t ) or (x
t , p
t ), we
have x1t
 = O(t
baba ) as t,
so the speed of convergence is sub-linear and not linearly fast.
Proof. First let   (0, ). Then    by Lemma Ap.2.6, so there is an A > 1
and a
t0  R such that (x()t , p
t )  RA for t  t0. Then x
t > 0 and (x
 = |p()t |a1 
Aa1(x()t )(b1)(a1) for t  t0. Here (b 1)(a 1) > 1, so
(baba)) = (ba b a)(x()t )(b1)(a1)(x
  (ba b a)Aa1
for t  t0. Let K := (ba  b  a)Aa1, then (x()t )(baba)  Kt + L for t  t0 and
some L  R. So (x()t )1 = O(t
baba ) when t, therefore the convergence is not
linear.
By Lemma Ap.2.5, we have p
t  ((a 1)x
a1 when t. So (x())t =
|p()t |a1  (a  1)x
t , hence (log(x
  (a  1) = ((a  1)t), when
t  . So by LHospitals rule, log(x()t )  (a  1)t, thus |x
t | = O(et) when
t  , for every  < (a  1). Then also |p()t | = O(et) when t  , for every
 < . So the convergence is linear.
So up to time translation there are only two solutions, (x(), p()) and (x(),p()),
where the convergence to (0, 0) is linear.
Ap.3 Proofs of convergence for discrete systems
Ap.3.1 Implicit Method
Firstly, we show the well-definedness of the implicit scheme.
Lemma 4.4.1 (Well-definedness of the implicit scheme). Suppose that f and k satisfy
assumptions A.1 and A.2, and ,   (0,). Then (4.34) has a unique solution for
every xi, pi  Rd, and this solution also satisfies (4.33).
Proof. The proof is based on Theorem 26.3 of [222]. We start by introducing some
concepts from [222] that are useful for dealing with convex functions on Rn taking
values in [,]. We say that g : Rn  [,] is convex if the epigraph of g,
{(x, ) :   g(x), x  Rn,   [,]} is convex. A convex function g : Rn 
[,] is called proper convex if g(x) 6=  for every x  Rn, and there is at least
one x  Rn where g(x) <. We say that g : Rn  [,] is lower-semicontinuous
if limxix g(xi)  g(x) for every sequence xi  x such that limxix g(xi) exists. The
relative interior of a set S  Rn, denoted by riS, is the interior of the set within
its affine closure. We define the essential domain of a function g : Rn  [,],
denoted by dom g, as the set of points x  Rn where g(x) is finite. We call a proper
convex function g : Rn  [,] essentially smooth if it satisfies the following 3
conditions for C = int(dom g):
(a) C is non-empty
(b) g is differentiable throughout C
(c) limi g(xi) = + whenever x1, x2, . . . is a sequence in C converging to a
boundary point of C.
Let g(x) denote the subdifferential of g at x (which is the set of subgradients of g
at x), and denote dom g := {x|g(x) 6= }. We say that a proper convex function
g : Rn  [,] is essentially strictly convex if g is strictly convex for every convex
subset of dom g.
By assumption A.2, k is differentiable everywhere, and it is strictly convex, hence it
is both essentially smooth and essentially strictly convex (since its domain is dom k =
Rn). Moreover, since k is a proper convex function, and it is lower semicontinuous
everywhere (hence closed, see page 52 of [222]), it follows from Theorem 12.2 of
[222] that (k) = k. Therefore, by Theorem 26.3 of [222], it follows that k is both
essentially strictly convex and essentially smooth. Since f is convex and differentiable
everywhere in Rn, based on the definitions and the assumption ,   (0,), it is
straightforward to show that
F (x) := k(xxi
) + f(x)  pi, x
is also essentially strictly convex and essentially smooth. Now we are going to show
that its infimum is reached at a unique point in Rn. First, using the convexity of f ,
it follows that f(x)  f(xi) + f(xi), x xi, hence
F (x)  k(xxi
) +  f(xi), x xi   pi, x xi   pi, xi+ f(xi)
using the definition (5.6) of the convex conjugate k
 p, x xi  k(p) +  f(xi), x xi   pi, x xi   pi, xi+ f(xi)
= p+ f(xi) pi, x xi  k(p)  pi, xi+ f(xi),
for any p  Rn. By setting p = xxixxi  (f(xi) pi) for x  xi > 0, and
p =  (f(xi) pi) for x  xi = 0, using the continuity and finiteness of k, it
follows that F (x)  x  xi  c for some c <  depending only on , , xi and pi.
Together with the lower semicontinuity of F , this implies that there exists at least
one y  Rn such that F (y) = infxRn F (x), and  < infxRn F (x) <.
It remains to show that this y is unique. First, we are going to show that it falls
within the interior of the domain of F . Let C := int(domF ), then using the essential
smoothness of F , it follows that C  domF  clC is a non-empty convex set (cl
refers to closure). If y would fall on the boundary of C, then by Lemma 26.2 of [222],
F (y) could not be equal to the infimum of F . Hence every such y falls within C. By
Theorem 23.4 of [222], ri(domF )  dom F  domF . Since C is a non-empty open
convex set, C = int domF = ri(domF ), therefore from the definition of essential
strict convexity, it follows that F is strictly convex on C. This means that there the
infimum infxRn F (x) is achieved at a unique y  Rn, thus (4.34) is well-defined.
Finally, we show the equivalence with (4.33). First, note that using the fact that
k is essentially smooth and essentially convex, it follows from Theorem 26.5 of [222]
that (k)(x) = (k)1(x) for every x  int(dom k). Since F is differentiable in the
open set C = int(domF ), and the infimum of F is taken at some y  C, it follows
that F (y) = 0. From the fact that f(x) and pi, x are differentiable for every
x  Rn, it follows that for every point z  C, zxi
 int(dom k). Thus in particular,
using the definition xi+1 = y, we have
(k)
xi+1  xi
+ f(xi+1) pi = 0,
which can be rewritten equivalently using the second line of (4.34) as
(k)
xi+1  xi
= pi+1.
Using the expression (k)(x) = (k)1(x) for x = xi+1xi
 int(dom k), we obtain
that (k)1
xi+1xi
= pi+1, and hence the first line of (4.33) follows by applying
k on both sides. The second line follows by rearrangement of the second line of
(4.34).
The following two lemmas are preliminary results that will be used in deriving
convergence results for both this scheme and the two explicit schemes in the next
sections.
Lemma Ap.3.1. Given f , k, , , C,, and Cf,k satisfying Assumptions A and B,
and a sequence of points xi, pi  Rd for i  0, we define Hi := f(xi) f(xmin) + k(pi)
. Then the equation
v = Hi +
(2v) xi  xmin, pi . (Ap.7)
has a unique solution in the interval v  [Hi/2, 3Hi/2], which we denote by Vi. In
addition, let
i :=
(2Vi), (Ap.8)
then Vi = Hi + i xi  xmin, pi and the differences Vi+1  Vi can be expressed as
Vi+1  Vi
= Hi+1 Hi + i+1 xi+1  xmin, pi+1  i xi  xmin, pi (Ap.9)
= Hi+1 Hi + i(xi+1  xmin, pi+1  xi  xmin, pi) + (i+1  i) xi+1  xmin, pi+1
(Ap.10)
= Hi+1 Hi + i+1(xi+1  xmin, pi+1  xi  xmin, pi) + (i+1  i) xi  xmin, pi .
(Ap.11)
Proof. Similarly to (4.27), we have by Lemma 4.3.5,
| xi  xmin, pi |  k(pi)/(k(pi)) + f(xi) f(xmin) 
(k(pi))
. (Ap.12)
For every i  0, we define Vi as the unique solution v  [Hi/2, 3Hi/2] of the equation
v = Hi +
(2v) xi  xmin, pi . (Ap.13)
The existence and uniqueness of this solution was shown in the proof of Theorem
4.3.8. The fact that Vi = Hi + i xi  xmin, pi immediately follows from equation
(Ap.13), and (Ap.10)-(Ap.11) follow by rearrangement.
Lemma Ap.3.2. Under the same assumptions and definitions as in Lemma Ap.3.1,
if in addition we assume that for some constants C1, C2  0, for every i  0,
Vi+1  Vi  (  i+1  C1)k(pi+1) i+1 xi+1  xmin, pi+1  i+1(f(xi+1) f(xmin))
+ C2
2i+1Vi+1 + (i+1  i) xi  xmin, pi , and (Ap.14)
Vi+1  Vi  (  i  C1)k(pi+1) i xi+1  xmin, pi+1  i(f(xi+1) f(xmin))
+ C2
2iVi+1 + (i+1  i) xi+1  xmin, pi+1 , (Ap.15)
then for every 0 <   min
2(1)
, for every i  0, we have
Vi+1  [1 + i(1   C2)/2]1 Vi. (Ap.16)
Similarly, if in addition to the assumptions of Lemma Ap.3.1, we assume that for
some constants C1, C2  0, for every i  0,
Vi+1  Vi  (  i+1  C1)k(pi) i+1 xi  xmin, pi  i+1(f(xi) f(xmin))
+ C2
2i+1Vi + (i+1  i) xi  xmin, pi , and (Ap.17)
Vi+1  Vi  (  i  C1)k(pi) i xi  xmin, pi  i(f(xi) f(xmin))
+ C2
2iVi + (i+1  i) xi+1  xmin, pi+1 , (Ap.18)
then for every 0 <   min
2(1)
, we have
Vi+1  [1 i(1   C2)/2]Vi. (Ap.19)
Proof. First suppose that assumptions (Ap.14) and (Ap.15) hold. Using (4.23) of
Lemma 4.3.6 with  = (2Vi+1) and  = i+1, it follows that for   
2(1)
 (  i+1  C1)k(pi+1) i+1(f(xi+1) f(xmin)) i+1 xi+1  xmin, pi+1
 i+1(1 )Vi+1,
and by combining the terms in (Ap.14), we have
Vi+1  Vi  i+1[1   C2]Vi+1 + (i+1  i) xi  xmin, pi . (Ap.20)
Now we are going to prove that Vi+1  Vi under the assumptions of the lemma. We
argue by contradiction, suppose that Vi+1 > Vi. Then by the non-increasing property
of , and the definition i =
(2Vi), we have i+1  i. Using the convexity of
, we have (y) (x)  (y)(y  x) for any x, y  0, hence we obtain that
|i+1  i| = i  i+1 =
((2Vi) (2Vi+1))  C,(Vi+1  Vi)((2Vi)),
and by (Ap.12) and assumption A.4 we have
|i+1  i| |xi  xmin, pi|  C,(Vi+1  Vi)((2Vi))
(2Vi)
< Vi+1  Vi.
Combining this with (Ap.20) we obtain that Vi+1  Vi < Vi+1  Vi, which is a
contradiction. Hence we have shown that Vi+1  Vi, which implies that i+1  i.
Using (4.23) of Lemma 4.3.6 with  = (2Vi) and  = i, it follows that for
0 <   
2(1)
 (  i  C1)k(pi+1) i(f(xi+1) f(xmin)) i xi+1  xmin, pi+1
 i(1 )Vi+1,
and hence by substituting this to (Ap.15), it follows that
Vi+1  Vi  i[1   C2]Vi+1 + (i+1  i) xi+1  xmin, pi+1 . (Ap.21)
Now using the convexity of , and the fact that i+1  i, we have
|i+1  i| = i+1  i =
((2Vi+1) (2Vi))  C,(Vi  Vi+1)((2Vi+1)),
and by (Ap.12) and assumption A.4 we have
|i+1  i| |xi+1  xmin, pi+1|  C,(Vi  Vi+1)((2Vi+1))
2Vi+1
(2Vi+1)
< Vi  Vi+1.
By combining this with (Ap.21), we obtain that
Vi+1  Vi  
[1   C2]Vi+1,
and the first claim of the lemma follows by rearrangement and monotonicity.
The proof of the second claim based on assumptions (Ap.17) and (Ap.18) is as
follows. As previously, in the first step, we show that Vi+1  Vi by contradiction.
Suppose that Vi+1 > Vi, then i+1  i. Using (4.23) of Lemma 4.3.6 with  = (2Vi)
and  = i+1  i  2 , it follows that for  
2(1)
 (  i+1  C1)k(pi+1) i+1(f(xi+1) f(xmin)) i+1 xi+1  xmin, pi+1
 i+1(1 )Vi+1,
and by combining the terms in (Ap.17), we have
Vi+1  Vi  i+1[1   C2]Vi + (i+1  i) xi  xmin, pi . (Ap.22)
The rest of the proof follows the same steps as for assumptions (Ap.14) and (Ap.15),
hence it is omitted.
Now we are ready to prove the main result of this section.
Proposition 4.4.3 (Convergence bound for the implicit scheme). Given f , k, , ,
C,, and Cf,k satisfying assumptions A and B. Suppose that  <
2 max(Cf,k,1)
. Let
? = (3H0), and let W0 = f(x0) f(xmin) and for i  0,
Wi+1 =Wi [1 + C,(1   2Cf,k)(2Wi)/4]1 .
Then for any (x0, p0) with p0 = 0, the iterates of (4.33) satisfy for every i  0,
f(xi) f(xmin)  2Wi  2W0[1 + C,(1   2Cf,k)?/4]i.
Proof. We follow the notations of Lemma Ap.3.1, and the proof is based on Lemma
Ap.3.2. By rearrangement of the (4.33), we have
xi+1  xi = k(pi+1)
pi+1  pi = pi+1  f(xi+1)
(Ap.23)
For the Hamiltonian terms, by the convexity of f and k, we have
Hi+1 Hi  k(pi+1), pi+1  pi+ f(xi+1), xi+1  xi
= k(pi+1),pi+1  f(xi+1)+  f(xi+1),k(pi+1) (Ap.24)
=  k(pi+1), pi+1 (Ap.25)
For the inner product terms, we have
xi+1  xmin, pi+1  xi  xmin, pi
= xi+1  xmin, pi+1  xi+1  xmin  (xi+1  xi), pi+1  (pi+1  pi)
= xi+1  xmin, pi+1  xi+1  xmin  k(pi+1), pi+1 + pi+1 + f(xi+1)
= (+ 2) pi+1,k(pi+1)   xi+1  xmin,f(xi+1)
  xi+1  xmin, pi+1+ 2 k(pi+1),f(xi+1) ,
and by assumption B.1 we have
k(pi+1),f(xi+1)  Cf,kHi+1  2Cf,kVi+1, (Ap.26)
and hence
xi+1  xmin, pi+1  xi  xmin, pi  (+ 2) pi+1,k(pi+1)   xi+1  xmin,f(xi+1)
(Ap.27)
  xi+1  xmin, pi+1+ 22Cf,kVi+1.
By assumption A.4 on C, we have i+1  2 , and using the condition  <
2(Cf,k+)
of the lemma, we have
  i+1  i+1   
 (1 )
> 0. (Ap.28)
By (Ap.11), (Ap.25), (Ap.27), we have
Vi+1  Vi
 (  i+1  i+1) k(pi+1), pi+1  i+1 xi+1  xmin,f(xi+1)
 i+1 xi+1  xmin, pi+1+ 22Cf,ki+1Vi+1 + (i+1  i) xi  xmin, pi
using the convexity of f and k, and inequality (Ap.28)
 (  i+1  i+1)k(pi+1) i+1(f(xi+1) f(xmin)) i+1 xi+1  xmin, pi+1
+ 22Cf,ki+1Vi+1 + (i+1  i) xi  xmin, pi .
Using the fact that i+1  C,2 
, it follows that (Ap.14) holds with C1 =
C2 = 2Cf,k.
By (Ap.10), (Ap.25), (Ap.27), it follows that
Vi+1  Vi 
Hi+1 Hi + i(xi+1  xmin, pi+1  xi  xmin, pi) + (i+1  i) xi+1  xmin, pi+1
  k(pi+1), pi+1+ i(xi+1  xmin, pi+1  xi  xmin, pi) + (i+1  i) xi+1  xmin, pi+1
  k(pi+1), pi+1+ (i+1  i) xi+1  xmin, pi+1+ (i+ i2) pi+1,k(pi+1)
 i xi+1  xmin,f(xi+1)  i xi+1  xmin, pi+1+ 2i2Cf,kVi+1
using the convexity of f and k, and inequality (Ap.28)
 (  i  i)k(pi+1) i xi+1  xmin, pi+1  i(f(xi+1) f(xmin))
+ 22Cf,kiVi+1 + (i+1  i) xi+1  xmin, pi+1 ,
implying that (Ap.15) holds with C1 =
and C2 = 2Cf,k. The claim of the Lemma
now follows from Lemma Ap.3.2.
Ap.3.2 First Explicit Method
The following lemma is a preliminary result that will be useful for proving our
convergence bounds for this discretization.
Lemma Ap.3.3. Given f , k, , , C,, Cf,k, Ck, Df,k satisfying assumptions A,
B, and C, and 0 <   C,
10Cf,k+5Ck
, the iterates (4.36) satisfy that for every i  0,
f(xi+1)f(xi), xi+1  xi  32Df,k min((3Hi), (3Hi+1))Hi+1. (Ap.29)
Proof. Let x
i+1 := xi+1tk(pi+1) andH
i+1 := H(x
i+1, pi+1). Using the assumptions
that f is 2 times continuously differentiable, and assumption C.3, we have
f(xi+1)f(xi), xi+1  xi =
xi+1  xi,2f(xi+1  t(xi+1  xi))(xi+1  xi)
k(pi+1),2f(x(t)i+1)k(pi+1)
dt  2Df,k
(3H(t)i+1)H
i+1dt, (Ap.30)
where we have used the fundamental theorem of calculus, which is applicable since
k(pi+1),2f(x(t)i+1)k(pi+1)
is piecewise continuous by assumption C.2. We are
going to show the following inequalities based on the assumptions of the Lemma,
H(t)i+1 
1 Cf,k
Hi+1, (Ap.31)
(3H(t)i+1)  (3Hi+1) 
1 Cf,k
1 Cf,k(1 + 1/C,)
, (Ap.32)
(3H(t)i+1)  (3Hi) 
1 (2Cf,k + Ck)
1 [Cf,k(2 + 3/C,) + Ck(1 + 1/C,)]
. (Ap.33)
The claim of the lemma follows directly by combining these 3 inequalities with (Ap.30)
and using the assumptions on .
First, by convexity and assumption B.1, we have
H(t)i+1 Hi+1 = f(x
i+1) f(xi+1)  
f(x(t)i+1), tk(pi+1)
 tCf,kH(t)i+1,
and (Ap.31) follows by rearrangement. In the other direction, by convexity and
assumption B.1, we have
Hi+1 H(t)i+1 = f(xi+1) f(x
i+1)  f(xi+1), tk(pi+1)  tCf,kHi+1,
so by rearrangement, it follows that
Hi+1 H(t)i+1 
tCf,k
1 tCf,k
H(t)i+1.
Using this, and the convexity of , and Assumption A.4, we have
(3H(t)i+1) (3Hi+1)  3(3H
i+1)(Hi+1 H
i+1)  (3H
i+1)3H
tCf,k
1 tCf,k
tCf,k
1 tCf,k
(3H(t)i+1),
and (Ap.32) follows by rearrangement. Finally, using the convexity of f and k, we
Hi H(t)i+1 = k(pi) k(pi+1) + f(xi) f(x
k(pi),
1 + 
1 + 
f(xi)
+ f(xi),(1 t)k(pi+1)
using Assumptions B.1 and C.1
 Ckk(pi) + Cf,kHi + Cf,k(k(pi+1) + f(xi) f(xmin))
 [(2Cf,k + Ck)Hi + Cf,kH(t)i+1].
By rearrangement, this implies that
Hi H(t)i+1 
(3Cf,k + Ck)
1 (2Cf,k + Ck)
 H(t)i+1.
Using this, the convexity of , and Assumption A.4, we have
(3H(t)i+1) (3Hi)  3(3H
i+1)(Hi H
i+1)  (3H
i+1)3H
i+1 
(3Cf,k + Ck)
1 (2Cf,k + Ck)
 (3Cf,k + Ck)
1 (2Cf,k + Ck)
 (3H(t)i+1),
and (Ap.33) follows by rearrangement.
Now we are ready to prove our convergence bound for this discretization.
Proposition 4.4.6 (Convergence bound for the first explicit scheme). Given f , k,
, , C,, Cf,k, Ck, Df,k satisfying assumptions A, B, and C, and that 0 <  <
2 max(Cf,k+6Df,k/C, ,1)
10Cf,k+5Ck
. Let ? = (3H0), W0 := f(x0)  f(xmin),
and for i  0, let
Wi+1 =Wi
C,
[1   2(Cf,k + 6Df,k/C,)](2Wi)
Then for any (x0, p0) with p0 = 0, the iterates (4.36) satisfy for every i  0,
f(xi) f(xmin)  2Wi  2W0
C,
[1   2(Cf,k + 6Df,k/C,)]?
Proof. We follow the notations of Lemma Ap.3.1, and the proof is based on Lemma
Ap.3.2. For the Hamiltonian terms, by the convexity of f and k, we have
Hi+1 Hi
= f(xi+1) f(xi) + k(pi+1) k(pi) (Ap.34)
 f(xi+1), xi+1  xi+ k(pi+1), pi+1  pi
= f(xi), xi+1  xi+ k(pi+1), pi+1  pi+ f(xi+1)f(xi), xi+1  xi
=  f(xi),k(pi+1)   k(pi+1),f(xi) + pi+1+ f(xi+1)f(xi), xi+1  xi
=  k(pi+1), pi+1+ f(xi+1)f(xi), xi+1  xi (Ap.35)
for any  > 0. Note that by convexity and assumption B.1, we have
f(xi) = f(xi+1) + f(xi+1) f(xi)  f(xi+1) +  f(xi+1),k(pi+1)
 f(xi+1) + Cf,kHi+1  f(xi+1) + 2Cf,kVi+1.
For the inner product terms, using the above inequality and convexity, we have
xi+1  xmin, pi+1  xi  xmin, pi
= xi+1  xmin, pi+1  xi  xmin, pi+1+ xi  xmin, pi+1  xi  xmin, pi
=  k(pi+1), pi+1   xi  xmin,f(xi)   xi  xmin, pi+1
= (+ 2) k(pi+1), pi+1   xi  xmin,f(xi)   xi+1  xmin, pi+1
 (+ 2) k(pi+1), pi+1  (f(xi) f(xmin))  xi+1  xmin, pi+1
 (+ 2) k(pi+1), pi+1  (f(xi+1) f(xmin))  xi+1  xmin, pi+1+ 22Cf,kVi+1.
(Ap.36)
Since C,  , it follows that i+1 = C,2 (2Vi+1) 
, and using the assumption
on , we have
  i+1  i+1   
 1 
> 0. (Ap.37)
By (Ap.11), (Ap.35), and (Ap.36), it follows that
Vi+1  Vi
= Hi+1 Hi + i+1(xi+1  xmin, pi+1  xi  xmin, pi) + (i+1  i) xi  xmin, pi
  k(pi+1), pi+1+ f(xi+1)f(xi), xi+1  xi+ (i+1  i) xi  xmin, pi
+ i+1
(+ 2) k(pi+1), pi+1  (f(xi+1) f(xmin))  xi+1  xmin, pi+1+ 22Cf,kVi+1
 (  i+1  i+1) k(pi+1), pi+1  i+1f(xi+1) i+1 xi+1  xmin, pi+1
+ 22i+1Cf,kVi+1 + f(xi+1)f(xi), xi+1  xi+ (i+1  i) xi  xmin, pi
which can be further bounded using (Ap.37), the convexity of k, and Lemma Ap.3.3
 (  i+1  
)k(pi+1) i+1 xi+1  xmin, pi+1  i+1(f(xi+1) f(xmin))
+ 22(Cf,k + 6Df,k/C,)i+1Vi+1 + (i+1  i) xi  xmin, pi ,
implying that (Ap.14) holds with C1 =
and C2 = 2(Cf,k + 6Df,k/C,).
Since Hi  2Vi  3Hi, and by applying Lemma Ap.3.3 it follows that
f(xi+1)f(xi), xi+1  xi  62Df,k
Hi+1  122
iVi+1. (Ap.38)
By (Ap.10), (Ap.35), (Ap.36), and assumption B.1, we have
Vi+1  Vi
= Hi+1 Hi + i(xi+1  xmin, pi+1  xi  xmin, pi) + (i+1  i) xi+1  xmin, pi+1
  k(pi+1), pi+1+ f(xi+1)f(xi), xi+1  xi+ (i+1  i) xi+1  xmin, pi+1
+ (i+ i
2) k(pi+1), pi+1  i(f(xi+1) f(xmin)) i xi+1  xmin, pi+1
+ 22iCf,kVi+1
using (Ap.38) and the convexity of f and k
  i  
k(pi+1) i xi+1  xmin, pi+1  i(f(xi+1) f(xmin))
+ 22(Cf,k + 6Df,k/C,)iVi+1 + (i+1  i) xi+1  xmin, pi+1 ,
implying that (Ap.15) holds with C1 =
and C2 = 2(Cf,k + 6Df,k/C,). The claim
of the lemma now follows by Lemma Ap.3.2.
Ap.3.3 Second Explicit Method
The following preliminary result will be used in the proof of the convergence bound.
Lemma Ap.3.4. Given f , k, , , C,, Cf,k, Ck, Df,k satisfying assumptions A, B,
and D, and 0 <   min
6(5Cf,k+2Ck)+12C,
62DkFk
, the iterates (4.39) satisfy
that for every i  0,
k(pi+1)k(pi), pi+1  pi  2C min((3Hi), (3Hi+1))Hi + 2Dk(pi), (Ap.39)
where
C = 3Df,k, D = 2
2Dk(1 + Ek). (Ap.40)
Proof. For 0  t  1, let
i := pi + t(pi+1  pi) = (1 t)pi  tf(xi+1), (Ap.41)
H(t)i := H
xi+1, p
= f(xi+1) f(xmin) + k
, (Ap.42)
Pi,i+1 := k(pi+1)k(pi), pi+1  pi . (Ap.43)
Note that by rearrangement we have pi =
i + tf(xi+1)
/(1 t), and hence
pi+1  pi =
i  pi
p(t)i  f(xi+1)
1 t . (Ap.44)
Using assumption D.1, it follows that
pi+1  pi,2k
(pi+1  pi)
is piecewise
continuous, hence by the fundamental theorem of calculus, we have
Pi,i+1 =
pi+1  pi,2k
(pi+1  pi)
(1 t)2
i + f(xi+1),2k
i + f(xi+1)
(1 )2
i ,2k
(1 )2
f(xi+1),2k
f(xi+1)
(Ap.45)
For the first integral, using Assumptions D.3, the convexity of k, and then D.4, we
i ,2k
dt  Dk
dt  Dk
(k(pi) + k(pi+1))
((1 + Ek)k(pi) + FkPi,i+1) (Ap.46)
For the second integral, using Assumption D.5, we have
f(xi+1),2k
f(xi+1)
dt  Df,k
H(t)i (3H
i )dt. (Ap.47)
We are going to show the following 3 inequalities based on the assumptions of the
Lemma.
H(t)i 
1 
1 ( + 2Cf,k)
 Hi, (Ap.48)
(3H(t)i )  (3Hi+1) 
1 (Cf,k + )
1 (Cf,k +  + Cf,k/C,)
, (Ap.49)
(3H(t)i )  (3Hi) 
1 (2Cf,k + Ck)
1 [Cf,k(2 + 3/C,) + Ck(1 + 1/C,)]
. (Ap.50)
The claim of the lemma follows from substituting these bounds into (Ap.47), and
then substituting the bounds (Ap.46) and (Ap.47) into (Ap.45) and rearranging.
First, by the convexity of f and Assumption B.1, we have
f(xi+1) f(xi)   f(xi+1),k(pi)  Cf,k(f(xi+1) f(xmin) + k(pi))
= Cf,k((f(xi+1) f(xi)) +Hi)
so by rearrangement it follows that
f(xi+1) f(xi) 
Cf,k
1 Cf,k
 Hi, (Ap.51)
and similiarly
f(xi) f(xi+1)   f(xi),k(pi)  Cf,kHi. (Ap.52)
Using (Ap.51), and the convexity of k, we have
H(t)i Hi = f(xi+1) f(xi) + k
 k(pi)
 Cf,k
1 Cf,k
 Hi +
, t(pi+1  pi)
now using (Ap.44), and then Assumption B.1,
 Cf,k
1 Cf,k
 Hi  t
i +f(xi+1)
1 t
 Cf,k
1 Cf,k
 Hi +
Cf,k
1 H
and inequality (Ap.48) follows by rearrangement.
By the convexity of k, and using (Ap.44) for t = 1, we have
Hi+1 H(t)i = k(pi+1) k
k(pi+1), pi+1  p(t)i
= k(pi+1), (1 t)(pi+1  pi) = (1 t)
k(pi+1),
1  pi+1 +
1 f(xi+1)
using Assumption B.1,
 Cf,k
1   Hi+1,
so by rearrangement,
Hi+1 H(t)i 
Cf,k
1 (Cf,k + )
H(t)i . (Ap.53)
Using this, the convexity of , and Assumption A.4, we have
(3H(t)i ) (3Hi+1)  3(3H
i )(Hi+1 H
i )  (3H
i )3H
Cf,k
1 (Cf,k + )
 Cf,k
1 (Cf,k + )
 (3H(t)i ),
and (Ap.49) follows by rearrangement. Finally, using inequality (Ap.52), we have
Hi H(t)i = f(xi) f(xi+1) + k(pi) k
 Cf,kHi + k(pi),t(pi+1  pi)
 Cf,kHi + t k(pi), pi +f(xi+1)
now using Assumptions B.1 and D.2,
 (Cf,kHi + Ckk(pi) + Cf,kk(pi) + Cf,k(f(xi+1) f(xmin))
 ((2Cf,k + Ck)Hi + Cf,kH(t)i ),
and by rearrangement this implies that
Hi H(t)i 
(3Cf,k + Ck)
1 (2Cf,k + Ck)
 H(t)i . (Ap.54)
Using this, the convexity of , and Assumption A.4, we have
(3H(t)i ) (3Hi)  3(3H
i )(Hi H
i )  (3H
i )3H
(3Cf,k + Ck)
1 (2Cf,k + Ck)
 (3Cf,k + Ck)
1 (2Cf,k + Ck)
 (3H(t)i ),
and (Ap.50) follows by rearrangement.
Now we are ready to prove the convergence bound.
Proposition 4.4.9 (Convergence bound for the second explicit scheme). Given f ,
k, , , C,, Cf,k, Ck, Dk, Df,k, Ek, Fk satisfying assumptions A, B, and D, and
0 <  < min
2(Cf,k + 6Df,k/C,)
8Dk(1 + Ek)
6(5Cf,k + 2Ck) + 12C,
62DkFk
Let ? = (3H0), W0 := f(x0) f(xmin), and for i  0, let
Wi+1 =Wi
1 C,
[1   2(Cf,k + 6Df,k/C,)](2Wi)
Then for any (x0, p0) with p0 = 0, the iterates (4.39) satisfy for every i  0,
f(xi) f(xmin)  2Wi  2W0 
1 C,
[1   2(Cf,k + 6Df,k/C,)]?
Proof. We follow the notations of Lemma Ap.3.1, and the proof is based on Lemma
Ap.3.2. For the Hamiltonian terms, by the convexity of f and k, we have
Hi+1 Hi = f(xi+1) f(xi) + k(pi+1) k(pi)
 f(xi+1), xi+1  xi+ k(pi+1), pi+1  pi
= f(xi+1), xi+1  xi+ k(pi), pi+1  pi+ k(pi+1)k(pi), pi+1  pi
=  f(xi+1),k(pi)   k(pi),f(xi+1) + pi+ k(pi+1)k(pi), pi+1  pi
=  k(pi), pi+ k(pi+1)k(pi), pi+1  pi (Ap.55)
for any  > 0. For the inner product terms, we have
xi+1  xmin, pi+1  xi  xmin, pi
= xi+1  xmin, pi+1  xi+1  xmin, pi+ xi+1  xmin, pi  xi  xmin, pi
= xi+1  xmin,pi  f(xi+1)+ xi+1  xi, pi
=  f(xi+1), xi+1  xmin   xi+1  xmin  (xi+1  xi), pi+ (1 ) xi+1  xi, pi
=  f(xi+1), xi+1  xmin+ (1 ) k(pi), pi   xi  xmin, pi . (Ap.56)
Note that from assumption B.1 and the convexity of f it follows that
 (f(xi+1) f(xmin))  (f(xi) f(xmin)) + f(xi) f(xi+1)
 (f(xi) f(xmin)) + f(xi),k(pi)  (f(xi) f(xmin)) + Cf,KHi.
(Ap.57)
By combining (Ap.11), (Ap.55), and (Ap.56), it follows that
Vi+1  Vi = Hi+1 Hi + i+1(xi+1  xmin, pi+1  xi  xmin, pi) + (i+1  i) xi  xmin, pi
  k(pi), pi+ k(pi+1)k(pi), pi+1  pi+ (i+1  i) xi  xmin, pi
+ i+1 (f(xi+1), xi+1  xmin+ (1 ) k(pi), pi   xi  xmin, pi)
 (  i+1) k(pi), pi  i+1 f(xi+1), xi+1  xmin  i+1 xi  xmin, pi
+ k(pi+1)k(pi), pi+1  pi+ (i+1  i) xi  xmin, pi
which can be further bounded using i+1  2 , the convexity of k and f , and Lemma
Ap.3.4 as
 (  i+1  D)k(pi) i+1(f(xi+1) f(xmin)) i+1 xi  xmin, pi
+ 22i+1  C/C,  Hi + (i+1  i) xi  xmin, pi
and now using (Ap.57) and Hi  2Vi leads to
 (  i+1  D)k(pi) i+1(f(xi) f(xmin)) i+1 xi  xmin, pi
+ 2i+1  (4C/C, + 2Cf,k)  Vi + (i+1  i) xi  xmin, pi ,
implying that (Ap.17) holds with C1 = D and C2 = 4C/C, + 2Cf,k.
By combining (Ap.10), (Ap.55), and (Ap.56), it follows that
Vi+1  Vi
= Hi+1 Hi + i(xi+1  xmin, pi+1  xi  xmin, pi) + (i+1  i) xi+1  xmin, pi+1
  k(pi), pi+ k(pi+1)k(pi), pi+1  pi+ (i+1  i) xi+1  xmin, pi+1
+ i (f(xi+1), xi+1  xmin+ (1 ) k(pi), pi   xi  xmin, pi)
 (  i) k(pi), pi  i f(xi+1), xi+1  xmin  i xi  xmin, pi
+ k(pi+1)k(pi), pi+1  pi+ (i+1  i) xi+1  xmin, pi+1
which can be further bounded using i  2 , the convexity of k and f , and Lemma
Ap.3.4 as
 (  i  D)k(pi) i(f(xi+1) f(xmin)) i xi  xmin, pi
+ 22i  C/C,  Hi + (i+1  i) xi+1  xmin, pi+1
and now using (Ap.57) and Hi  2Vi leads to
 (  i  D)k(pi) i(f(xi) f(xmin)) i xi  xmin, pi
+ 2i  (4C/C, + 2Cf,k)  Vi + (i+1  i) xi+1  xmin, pi+1
implying that (Ap.18) holds with C1 = D and C2 = 4C/C, + 2Cf,k (see (Ap.40)
for the definition of C and D). The claim of the lemma now follows by Lemma
Ap.3.2.
Ap.3.4 Explicit Method on Non-Convex f
Lemma 4.4.11 (Convergence of the first explicit scheme without convexity). Given
, f , k, , b, Dk, Df ,  satisfying assumptions E and A.2. If   (0, b1
/DfDk],
then the iterates (4.36) of the first explicit method satisfy
Hi+1 Hi  (bDfDk  )k(pi+1)  0, (4.47)
and f(xi)2  0.
Proof. By assumption E.3
Hi+1 Hi  k(pi+1) k(pi) +  f(xi),k(pi+1)+Df( k(pi+1))
now with convexity of k
  k(pi+1), pi+1+Df( k(pi+1))
 k(pi+1) +Df( k(pi+1))
we have (t)  b(t) and (k(p))  Dkk(p) by assumption E.3
 (bDfDk  )k(pi+1)
If   b1
/DfDk, then Hi+1  Hi. If H is bounded below we get that xi, pi is
such that k(pi) 0 and thus pi  0. Since pi+12 +  pi2   f(xi)2, we get
f(xi)  0.
Ap.4 Proofs for power kinetic energies
The proofs of the results in this section will be based on the following three preliminary
lemmas.
Lemma Ap.4.1. Let  be a norm on Rd and x  Rd \ {0}. If x is differentiable,
x = 1 x , x = x , (Ap.58)
and if x is twice differentiable, then
(2 x)x = 0. (Ap.59)
Proof. Let x  Rd \ {0}. By the convexity of the norm we have
x , y  y  x , x  x
for all y  Rd. Thus
{x , y  y}  x , x  x
Because the right hand side is finite, we must have x  1 and the left hand
side equal to 0.
0  x , x  x  x x  x  0
forces x = 1 and x , x = x. In fact, this argument goes through
for non-differentiable , by definition of the subderivative. For twice differentiable
norms, take the derivative of x , x = x to get
(2 x)x+x = x
and our result follows.
Lemma Ap.4.2. Given a  [1,), A  [1,), and Aa in (4.48). Define B =
A/(A 1), b = a/(a 1). For convenience, define
(t) = Aa (t) (t) = 
b (t). (Ap.60)
The following hold.
1. Monotonicity. For t  (0,), (t) > 0. If a = A = 1, then for t  (0,),
(t) = 0, otherwise (t) > 0. This implies that  is strictly increasing on
[0,).
2. Subhomogeneity. For all t,   [0,),
(t)  max{a, A}(t) (Ap.61)
with equality iff a = A or t = 0 or  = 0 or  = 1.
3. Strict Convexity. If a > 1 or A > 1, then (t) is strictly convex on [0,) with
a unique minimum at 0.
4. Derivatives. For all t  (0,),
min{a,A}(t)  t(t)  max{a,A}(t), (Ap.62)
(min{a,A}  1)(t)  t(t)  (max{a,A}  1)(t). (Ap.63)
If a,A  2, then for all t, s  (0,),
(t)  s(s) + ((t) (s))(t s) (Ap.64)
Proof. First, for t  (0,), the following identities can be easily verified.
t(t) = (ta + 1)
a ta (Ap.65)
t(t) = (t)
a 1 + (A a) ta
(Ap.66)
1. Monotonicity. First, for t > 0 we have,
(t) = (ta + 1)
a ta1 > 0 (Ap.67)
For (t) for t > 0, we have the following with equality iff a = A = 1
(t) = t1(t)
a 1 + (A a) ta
 0. (Ap.68)
Finally, (t) is continuous at 0, which gives our result.
2. Subhomogeneity. If a = A or t = 0 or  = 0 or  = 1, then equality clearly
holds. Assume a 6= A, t,  > 0, and  6= 1. Assuming A > a, t
a is strictly
increasing. If  < 1, then
(t) = a(ata + 1)
a ta1 < a(ta + 1)
a ta1.
If  > 1, then
(t) = A(ta + a)
a ta1 < A(ta + 1)
a ta1.
Integrating both sides gives (t) < max{a, A}(t). The case A < a follows
similarly, using the fact that t
a is strictly decreasing.
3. Strict convexity. First, since  is strictly increasing we get (t) > (0) = 0,
which proves that 0 is the unique minimizer. Our goal is to prove that for
t, s  [0,) and   (0, 1) such that t 6= s,
(t+ (1 )s) < (t) + (1 )(s)
First, for t = 0 or s = 0, this reduces to a condition of the form (t) < (t)
for all t  [0,) and   (0, 1). Considering separately the cases A = 1, a >
1 and a = 1, A > 1 and a,A > 1, it is easy to see that this follows from
the subhomogeneity result (Ap.61). For s, t > 0, our result follows from the
positivity of , (Ap.68).
4. Derivatives. Since,
min{a,A}  1  a 1 + (A a) t
ta + 1
 max{a,A}  1,
we get the second derivative bound (Ap.63) from identity (Ap.82). The first
derivative bound (Ap.62) follows from (Ap.63), since
t(t) =
(t) + t(t) dt.
Our goal is now to prove the uniform gradient bound (Ap.64) for a,A  2. In
the case that 0 < t < s, the bound reduces to ((t) (s))(t s)  0, which
follows from convexity. The remaining case is 0 < s  t. Notice that for the
case 0 < s < t, convexity implies
(s)  (t) (s)
t s  
(t) (Ap.69)
Notice that in the case s = 0 for (Ap.69) we get the inequality (t)  t(t),
again a condition of convexity. On the other hand, we have just shown that for
our  the stronger inequality min{a,A}(t)  t(t) holds. This motivates a
strategy of searching for a stronger bound of form (Ap.69), and using this to
derive the uniform gradient bound (Ap.64). Indeed, let  = t/s > 1, then we
will show
()(s)  (s) (s)
s s  
(s)() (Ap.70)
where
() =
a(1) A  a
A(1) A  a
() =
(1a)
a(1) A  a
(1A)
A(1) A  a
(Ap.71)
First, assume A  a. We need to prove
a  1
s(s)  (s) (s)  1 
s(s)
We fix s > 0, and take F1() := (s)  (s)  
s(s) and F2() :=
1a
s(s)(s) +(s). We need to prove that F1()  0 and F2()  0
for   1. We have F1(1) = F2(1) = 0,
F 1() =
(s)a
(((s)a + 1)
a  (sa + 1)Aaa )  0
F 2() =
(1 a)s
((s) + s(s) a(s))
(1 a)s
(s)(A a) (s)
(s)a + 1
so indeed F1()  0 and F2()  0 for every  > 1.
Now let A  a. Then we need to prove that
A  1
s(s)  (s) (s)  1 
s(s).
We fix s > 0, and take F3() := (s)  (s)  
s(s) and F4() :=
1A
s(s)(s) +(s). We need to prove that F3()  0 and F4()  0
for   1. We have F3(1) = F4(1) = 0,
F 3() =
(s)a
(((s)a + 1)
a  ((s)a + a)aAa )  0
F 4() =
(1 A)s
((s) + s(s) A(s))
(1 A)s
(s)
a A+ (A a) (s)
(s)a + 1
so indeed F3()  0 and F4()  0 for every  > 1.
Now, we can prove (Ap.64). We have so far proven the following inequalities in
(t), (s), (t), (s):
(s)  0, s(s)min(a,A)(s)  0,
(t) (s) ( 1)()s(s)  0, ( 1)()s(t) (t) + (s)  0.
We try to express the inequality s(s) + (t s)((t) (s)) (t)  0 as a
linear combination of the above four inequalities with non-negative coefficients:
s(s) + (t s)((t) (s)) (t) = c1(s) + c2(s(s)min(a,A)(s))
+ c3((t) (s) ( 1)()s(s)) + c4(( 1)()s(t) (t) + (s)).
Comparing the coefficients of (s), (t), (s), (t), we get the following
equations:
c1 min(a,A)c2  c3 + c4 = 0,
c3  c4 = 1,
c2  ( 1)()c3 = 2 ,
( 1)()c4 =  1.
This system of equations has a unique solution: c4 =
, c3 =
c2 = 2 + ( 1)()( 1()  1) and c1 = min(a,A)c2 1. We will prove that
c1, c2, c3, c4  0. Clearly () > 0. We claim that ()  1. For this it is enough
to check that (1)  (1) for every  > 1 and   2. After reordering
the terms, we get 1 + (1 )( 1)  1 = (1 + ( 1))1, which follows
from the generalized Bernoulli inequality. So 0 < ()  1, therefore c3, c4  0.
We just need to prove now that min(a,A)c2  1, because then c1, c2  0. If
a  A, then c1 = min(a,A)c2  1 = a(2  + a  a1  
). If a  A, then
c1 = A(2 + A  A1  
). So the only remaining thing to show is
2 +   1    0
for every  > 1 and   2. Letting  = 1/, this is equivalent to showing
2  1 +   1  0
for   (0, 1). Let () = 21 + 1. To see that ()  0, note
() = 2(2  + 1) 
from which () < (1/2)  0 for  < 1/2 and () > ((  1)/)  0 for
 > /( 1). This implies that  is minimized on [1/2, ( 1)/]. Our result
follows then from the fact that for   [1/2, ( 1)/],
()    1  0
Lemma Ap.4.3. Given a  [1,), A  [1,), and Aa in (4.48). Define B =
A/(A 1), b = a/(a 1). For convenience, define
(t) = Aa (t) (t) = 
b (t). (Ap.72)
For t  [0,) define the function
(t) =
ta + 1
+ (ta + 1)
) aA
a(A1)
, (Ap.73)
and for a 6= A define the constant
Ca,A =
) a1
Aa +
. (Ap.74)
We have the following results. For all t  (0,),
((t)) = (t)t, (Ap.75)
which means that  captures the relative error between () and , because ()(t) =
()1(t). Finally,  is bounded for all t  (0,) between the constants,
1  (t)  Ca,A (Ap.76)
Proof. We will show the results backwards, starting with (Ap.76). By rearrangement,
(t) = ( t
+ (ta + 1)1(ta + 1)
a1 )
a(A1) .
If a  A we have (ta + 1)
a1  1 and t
a(A1) is increasing, so (t)  1. If a < A we
have (ta+1)
a1  1 and t
a(A1) is decreasing, so again (t)  1. This proves the left
hand inequality of (Ap.76). Now, assume that A 6= a. Looking at ta
+ (ta+ 1)
we have
+ (ta + 1)
(ta+1)2
 A1
a1 (t
a + 1)
a1 1ata1
Since t 6= 0 we see that it has a stationary point at
(ta + 1)1  A1
a1 (t
a + 1)
a1 = 0,
which is equivalent to (ta + 1) =
) a1
Aa . This is also a stationary point of (t).
Since (0) = () = 1 and (t)  1 this stationary point must be a maximum. Thus
(t) =
1 (ta + 1)1 + (ta + 1)
) a1
Aa +
This proves the right hand inequality of (Ap.76). For (Ap.75), since (b 1)(a 1) =
ab a b+ 1 = 1, we have,
((t)) = ([(ta + 1)
a ta1]b + 1)
b [(ta + 1)
a ta1]b1
= ((ta + 1)
a1 ta + 1)
b (ta + 1)
a(a1) t
we have Bb
A(a1)a(A1)
(a1)(A1)b =
a(A1) and thus
= ((ta + 1)
a1 ta + 1)
a(A1) (ta + 1)
a(a1) t
= (t)t
Now we are ready to prove the key results in this section.
Lemma 4.5.1 (Verifying assumptions on k). Given a norm p on p  Rd, a,A 
[1,), and Aa in (4.48). Define the constant,
Ca,A =
) a1
Aa +
. (4.50)
k(p) = Aa (p) satisfies the following.
1. Convexity. If a > 1 or A > 1, then k is strictly convex with a unique minimum
at 0  Rd.
2. Conjugate. For all x  Rd, k(x) = (Aa )(x).
3. Gradient. If p is differentiable at p  Rd \ {0} and a > 1, then k is
differentiable for all p  Rd, and for all p  Rd,
k(p), p  max{a,A}k(p), (4.51)
(Aa )
(k(p))  (max{a,A}  1)k(p). (4.52)
Additionally, if a,A > 1, define B = A/(A 1), b = a/(a 1), and then
Bb (k(p))  Ca,A(max{a,A}  1)k(p). (4.53)
Additionally, if a,A  2, then for all p, q  Rd,
k(p)  k(q), q+ k(p)k(q), p q . (4.54)
4. Hessian. If p is twice continuously differentiable at p  Rd \ {0}, then k is
twice continuously differentiable for all p  Rd \ {0}, and for all p  Rd \ {0},
p,2k(p)p
 max{a,A}(max{a,A}  1)k(p). (4.55)
Additionally, if a,A  2 and there exists N  [0,) such that p 
max(2 p) 
N for p  Rd \ {0}, then for all p  Rd \ {0}
max(2k(p))
max{a,A}  1 +N
 (max{a,A}  2)k(p). (4.56)
Proof. Again, for the purposes of this proof, let (t) = Aa (t) and (t) = 
b (t) for
t  [0,).
1. Convexity. First, since norms are positive definite and  uniquely minimized
at 0  R by Lemma Ap.4.2, this proves that 0  Rd is a unique minimizer of
k. Let   (0, 1) and p, q  Rd such that p 6= q. By the monotonicity proved in
Lemma Ap.4.2 and the triangle inequality
k(p+ (1 )q) = (p+ (1 )q)
 ( p + (1 ) q)
and finally, by the strict convexity proved in Lemma Ap.4.2
< k(p) + (1 )k(q).
2. Conjugate. Let x  Rd. First, by definition of the convex conjugate and the
dual norm,
k(x) = sup
{x, p  k(p)} = sup
{x, p  (p)}
= sup
p=t
{x, p  (t)} = sup
{t x  (t)} = (x)
3. Gradient. First we argue for differentiability. For p = 0 (or q = 0 in the case of
(4.54)), we have by the equivalence of the norms that there exists c > 0 such
that p < c p2. Thus, limp20 k(p) p
2  limp20 (c p2) p
c limt0 (t)t
1 = 0, and thus we have k(0) = 0. Now for p 6= 0, we have
p 6= 0. Since (t) is differentiable for t > 0 and p at p 6= 0, we have by
the chain rule k(p) = (p)p.
All four results follow trivially when p = 0. In particular, (4.54) reduces to
k(p)  k(p), p for q = 0 and 0  k(q), q for p = 0; both follow from
convexity.
Now, assume p 6= 0. For (4.51), (4.52), and (4.53) we have, by Lemma Ap.4.1,
k(p), p = p (p) and (k(p)) = ((p)) and (k(p)) =
((p)). Letting t = p > 0, (4.51) follows directly from (Ap.62) of
Lemma Ap.4.2. For (4.52), we have from convex analysis (5.7) that
((t)) = t(t) (t). (Ap.77)
This implies that ((t))  (max{a,A} 1)(t), again by (Ap.62) of Lemma
Ap.4.2. For (4.53) assume a,A > 1 and consider that by (Ap.63) of Lemma
Ap.4.2 and (Ap.76) of Lemma Ap.4.3,
[((t))] = ((t))(t) = (t)t(t)  (t)Ca,A(max{a,A}  1)
Integrating both sides of this inequality gives ((t))  Ca,A(max{a,A} 
1)(t).
Finally, for the uniform gradient bound (4.54), assume p 6= 0 and q 6= 0. Lemma
Ap.4.1 implies by Cauchy-Schwartz that p , q  q for any p, q 
Rd \ {0}. Thus by Lemma Ap.4.1,
k(q), q+ k(p)k(q), p q 
(q) q + (
(p) 
(q))(p  q)
and our result is implied by the one dimensional result (Ap.64) of Lemma Ap.4.2.
4. Hessian. Throughout, assume p  Rd\{0}. First we argue for twice differentiability.
We havek(p) = (p)p, which for p 6= 0 is a product of a differentiable
function and a composition of differentiable functions. Thus, we have differentiability,
and by the chain rule,
2k(p) = (p)pp
 + 
(p)
2 p (Ap.78)
All of these terms are continuous at p 6= 0 by assumption or inspection of
(Ap.82).
We study (Ap.78). For (4.55), we have by Lemma Ap.4.1 and (Ap.62),(Ap.63)
of Lemma Ap.4.2,
p,2k(p)p
= (p)
p,pp
+ (p)
p,2 p p
= (p) p
 max{a,A}(max{a,A}  1)(p).
For (4.56) first note, by Lemma Ap.4.1
v,pp
= (v,p)
2  v2
and further
p,pp
= p2. Thus 
pp
Together, along with our assumption on the Hessian of p, we have
max
2k(p)
 (p)
pp
+ (p)
2 p
 (p) +N
(p) p
and by (Ap.63) of Lemma Ap.4.2
 (p) p
 (max{a,A}  1 +N)
On the other hand, by Lemma Ap.4.1 and the monotonicity of Lemma Ap.4.2,
max
2k(p)
 (p)
,pp
+ (p)
,2 p
= (p) > 0
Taken together, we have
max(2k(p))
max{a,A}  1 +N  
(p) p
Now, assume a,A  2 and let t = p > 0 and (t) = 
(t). Our goal is to
show that
max(2k(p))
max{a,A}  1 +N
 (max{a,A}  2)(p)
To do this we argue that (t) is an non-decreasing function on (0,) and
((t)t1)  (max{a,A}  2)(t), from which our result would follow. First,
we have for r  [0,) and s  (0,) such that for t  s,
(t)  tr  (r)  sr  (r)
Taking the supremum in r returns the result that  is non-decreasing. Otherwise
it can be verified directly from (4.57)  (4.61). Thus, what remains to show is
((t)t1)  (max{a,A}2)(t). Note that (t2) = 2(t) and (t2)t = (t).
Using (5.7) of convex analysis and (Ap.62) of Lemma Ap.4.2,
((t)t1) = ((t2)) = t2(t2) (t2)
 (max{a
}  1)(t2) = (max{a,A}  2)(t)
from which our result follows.
Lemma 4.5.3 (Bounds on 
max(2 p) for b-norms). Given b  [2,), let xb =(d
n=1 |x(n)|b
for x  Rd. Then for x  Rd \ {0},
xb 
2 xb
 (b 1).
Proof. A short calculation reveals that
2 xb =
(b 1)
|x(n)|b2
xb2b
xbx
(Ap.79)
Thus, since
b, aaT b
= a, b2  0 for any a, b  Rd, we have bmax
(1 b)xbx
0 and
xb 
2 xb
 (b 1)bmax
|x(n)|b2 x2bb
First, consider the case b > 2. Given v  Rd such that vb = 1, we have by the
Holders inequality along with the conjugacy of b/2 and b/(b 2),
v, diag
|x(n)|b2 x2bb
|x(n)|b
xbb
) b2
|v(n)|b
Now, for the case b = 2 we get diag
|x(n)|b2 x2b2
= I and 
max(I) = 1. Our
result follows.
Lemma 4.5.4 (Convex conjugates of Aa ). Given a,A  (1,) and Aa in (4.48).
Define B = A/(A 1), b = a/(a 1). The following hold.
1. Near Conjugate. Bb upper bounds the conjugate (
 for all t  [0,),
(Aa )
(t)  Bb (t). (4.57)
2. Conjugate. For all t  [0,),
(aa)
(t) = bb(t). (4.58)
(A1 )
(t) =
0 t  [0, 1]
tB  t+ 1
t  (1,)
. (4.59)
(1a)
(t) =
1 (1 tb)
b t  [0, 1]
 t  (1,)
. (4.60)
(11)
(t) =
0 t  [0, 1]
 t  (1,)
. (4.61)
Proof. For convenience, define
(t) = Aa (t) (t) = 
b (t). (Ap.80)
As a reminder, for t  (0,), the following identities can be easily verified.
(t) = (ta + 1)
a ta1 (Ap.81)
(t) = t1(t)
a 1 + (A a) ta
(Ap.82)
1. Near Conjugate. As a reminder (t) = sups0{ts(s)}. First, since (0) =
 infs0{(s)} = 0, this result holds for t = 0. Assume t > 0. Our strategy will
be to show that for s  [0,) we have st (s)  (t). This is true for s = 0
by the monotonicity of  in Lemma Ap.4.2, so assume s > 0. Now, consider
s = ((r)) for some r  (0,). To see that this is a valid parametrization
for s, notice that limr0 
((r)) = 0 and
[((r))] = ((r))(r) > 0
Thus s(r) = ((r)) is one-to-one and onto (0,). Further we have by Lemma
Ap.4.3 that
t  (t)t = ((t)) (Ap.83)
and thus ()1(t)  (t), since (t) > 0. All together, using convexity we
(t)  ((r)) + ((r))(t (r)) = st+ (()1(s)) s()1(s)
taking the derivative of (()1(s))  s()1(s) we get ()1(s). Since
()1(s)  (s), we finally get
 st (s).
Taking the supremum in s gives us our result.
2. Conjugate. As a reminder (t) = sups0{ts(s)}. Since (0) =  infs0{(s)}
= 0, these results all hold when t = 0. (4.58) is a standard result, since
aa(t) =
ta. (4.61) is a standard result, since 11(s) = s. Thus, we assume
a,A > 1 and t > 0 for the remainder. For (4.60), assume just A = 1. The
stationary condition of the supremum of ts (s) in s is
t = (sa + 1)
b sa1
Raising both sides to b we get tb = s
, whose solution for t  [0, 1] is s =
. Thus,
(t) = t
1 tb
1 tb
(1 tb)
(1 tb)
= 1 (1 tb)1
When t > 1, ts dominates (s) and the supremum is infinite. Now for (4.59),
assume just a = 1. We have the stationary condition of the conjugate equal to
t = (s+ 1)A1, which corresponds to
s = max{t
A1  1, 0}
Thus, when t > 1 we have
(t) = t(t
A1  1) 1
tB + 1
tB  t+ 1
otherwise (s) = 0.
Proposition 4.5.8 (Verifying assumptions for f with known power behavior and
appropriate k). Given a norm  satisfying F.2 and a,A  (1,), take
k(p) = Aa (p).
with Aa defined in (4.48). The following cases hold with this choice of k on f : R
d  R
convex.
1. For the implicit method (4.33), assumptions A, B hold with constants
 = min{a1, A1, 1} C, =  Cf,k = max{a 1, A 1, L}, (4.64)
if f, a, A, , L,  satisfy assumptions F.1, F.2, F.3, F.4.
2. For the first explicit method (4.36), assumptions A, B, and C hold with constants
(4.64) and
Ck = max{a,A} Df,k = Lf1 max {Df , 2Ca,A(max{a,A}  1)} , (4.65)
if f, a, A, , L, Lf , Df ,  satisfy assumptions F.1, F.2, F.3, F.4, and F.5.
3. For the second explicit method (4.39), assumptions A, B, and D hold with
constants (4.64) and
Ck = max{a,A} Dk = max{a,A}(max{a,A}  1)
Ek = max{a,A}  1 Fk = 1
Df,k = 
1(max{a,A}  1 +N) max {2L, a 2, A 2} ,
(4.66)
if f, a, A, , L,N,  satisfy assumptions F.1, F.2, F.3, F.4, and F.6.
Proof. First, by Lemma 4.5.1, this choice of k satisfies assumptions A.2 and C.1 /
D.2 with constant Ck = max{a,A}. We consider the remaining assumptions of A, B,
C, and D..
1. Our first goal is to derive . By assumption F.4, we have Bb (x)  fc(x).
Since (Bb ()) = (Bb )(1 ) by Lemma 4.5.1 and the results discussed
in the review of convex analysis, we have by assumption F.3,
f c (p)  (Bb )
1 p
 max{1a, 1A}k(p)
Thus, we have  = min{a1, A1, 1} constant. Moreover, we can take C, =
. This along with F.1 implies that f and k satisfy assumptions A
By Lemma 4.5.1 and assumption F.4 we have
Aa (f(x))  L(f(x) f(xmin)),
(Aa )
(k(p)) = (max{a,A}  1)k(p),
By Fenchel-Young and the symmetry of norms, we have | f(x),k(p) | 
max{max{a,A}  1, L}H(x, p), from which we derive Df,k and the fact that
f, k satisfy assumptions B.
2. The analysis of 1. holds for assumptions A and B. Now, to derive the conditions
for assumptions C consider
k(p)2 = [(Aa )(p)]
Note that,
([(Aa )
(t)]2) = 2Bb ((
(t))
Thus 
(k(p)2)  2Ca,A(max{a,A}  1)k(p) for all p  Rd, by Lemma
4.5.1. Now all together, by the Fenchel-Young inequality and assumption F.5,
we have for p  Rd and x  Rd \ {xmin}
k(p),2f(x)k(p)
 k(p)2 max
2f(x)
 LfB/2b/2 (k(p)
) + Lf (
max(2f(x))
 Lf2Ca,A(max{a,A}  1)k(p) + Lf (B/2b/2 )
max(2f(x))
 Df,kH(x, p).
This gives us assumptions C.
3. The analysis of 1. holds again for assumptions A and B. Now, for assumptions D,
we first note that Lemma 4.5.1 gives us constants Dk = max{a,A}(max{a,A}
1), Ek = max{a,A}  1, Fk = 1.
For the remaining constant Df,k we follow a similar path as 2. First, note that
since b, B  2 we have that a,A  2. This, along with assumption F.6, lets us
use (4.56) of Lemma 4.5.1 for p  Rd \{0}. Now, letting M = (max{a,A}1+
N) and applying (4.56) of Lemma 4.5.1 along with the Fenchel-Young inequality,
we have for p  Rd \ {0} and x  Rd
f(x),2k(p)f(x)
 f(x)2 
2k(p)
MA/2
(f(x)2) +M(
max(2k(p))
 2LM(f(x) f(xmin)) +M(max{a,A}  2)k(p)
 Df,kH(x, p).
This gives us assumptions D.
Bibliography
[1] Martn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,
Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael
Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh
Levenberg, Dan Mane, Rajat Monga, Sherry Moore, Derek Murray, Chris
Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal
Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viegas,
Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and
Xiaoqiang Zheng. TensorFlow: Large-Scale Machine Learning on Heterogeneous
Distributed Systems. arXiv e-prints, page arXiv:1603.04467, March 2016.
[2] Deeksha Adil, Rasmus Kyng, Richard Peng, and Sushant Sachdeva. Iterative
refinement for `p-norm regression. In Proceedings of the Thirtieth Annual ACM-
SIAM Symposium on Discrete Algorithms, pages 14051424. SIAM, 2019.
[3] Deeksha Adil, Richard Peng, and Sushant Sachdeva. Fast, provably convergent
irls algorithm for p-norm linear regression. In Advances in Neural Information
Processing Systems, 2019.
[4] Deeksha Adil and Sushant Sachdeva. Faster p-norm minimizing flows, via
smoothed q-norm problems. arXiv e-prints, Oct 2019.
[5] Alekh Agarwal, Martin J Wainwright, Peter L Bartlett, and Pradeep K
Ravikumar. Information-theoretic lower bounds on the oracle complexity of
convex optimization. In Advances in Neural Information Processing Systems,
2009.
[6] J Aitchison. A general class of distributions on the simplex. Journal of the
Royal Statistical Society. Series B (Methodological), pages 136146, 1985.
[7] Zeyuan Allen-Zhu, Zheng Qu, Peter Richtarik, and Yang Yuan. Even faster
accelerated coordinate descent using non-uniform sampling. In International
Conference on Machine Learning, 2016.
[8] Christophe Andrieu, Arnaud Doucet, and Roman Holenstein. Particle Markov
chain Monte Carlo methods. J. R. Stat. Soc. Ser. B Stat. Methodol., 72(3):269
342, 2010.
[9] Sanjeev Arora, Elad Hazan, and Satyen Kale. The multiplicative weights update
method: a meta-algorithm and applications. Theory of Computing, 8(1):121
164, 2012.
[10] Sren Asmussen and Peter W Glynn. Stochastic simulation: algorithms and
analysis, volume 57. Springer Science & Business Media, 2007.
[11] J Atchison and Sheng M Shen. Logistic-normal distributions: Some properties
and uses. Biometrika, 67(2):261272, 1980.
[12] Alfred Auslender, Roberto Cominetti, and Mounir Haddou. Asymptotic
analysis for penalty and barrier methods in convex and linear programming.
Mathematics of Operations Research, 22(1):4362, 1997.
[13] Dominique Aze and Jean-Paul Penot. Uniformly convex and uniformly
smooth convex functions. Annales de la Faculte des Sciences de Toulouse :
Mathematiques, Serie 6, 4(4):705730, 1995.
[14] Matej Balog, Nilesh Tripuraneni, Zoubin Ghahramani, and Adrian Weller. Lost
relatives of the gumbel trick. In International Conference on Machine Learning,
2017.
[15] Arindam Banerjee, Srujana Merugu, Inderjit S Dhillon, and Joydeep Ghosh.
Clustering with Bregman divergences. Journal of Machine Learning Research,
6:17051749, 2005.
[16] Heinz H Bauschke, Jerome Bolte, and Marc Teboulle. A descent lemma beyond
Lipschitz gradient continuity: first-order methods revisited and applications.
Mathematics of Operations Research, 42(2):330348, 2016.
[17] Matthew J. Beal. Variational algorithms for approximate Bayesian inference.
2003.
[18] Amir Beck. First-Order Methods in Optimization. SIAM, 2017.
[19] Amir Beck and Marc Teboulle. Mirror descent and nonlinear projected
subgradient methods for convex optimization. Operations Research Letters,
31(3):167175, 2003.
[20] Amir Beck and Marc Teboulle. Fast gradient-based algorithms for constrained
total variation image denoising and deblurring problems. IEEE transactions on
image processing, 18(11):24192434, 2009.
[21] Amir Beck and Marc Teboulle. A fast dual proximal gradient algorithm for
convex minimization and applications. Operations Research Letters, 42(1):16,
2014.
[22] Richard Bellman. Dynamic Programming. Princeton University Press, 1957.
[23] Yoshua Bengio, Li Yao, Guillaume Alain, and Pascal Vincent. Generalized
denoising auto-encoders as generative models. In Advances in Neural
Information Processing Systems, 2013.
[24] Yoshua Bengio, Li Yao, and Kyunghyun Cho. Bounding the Test Log-Likelihood
of Generative Models. arXiv e-prints, page arXiv:1311.6184, November 2013.
[25] Jean Berard, Pierre Del Moral, and Arnaud Doucet. A lognormal central limit
theorem for particle approximations of normalizing constants. Electron. J.
Probab., 19(94):128, 2014.
[26] Espen Bernton. Langevin monte carlo and jko splitting. In Conference On
Learning Theory, pages 17771798, 2018.
[27] Dimitri P. Bertsekas. Proximal Algorithms and Temporal Differences for Large
Linear Systems: Extrapolation, Approximation, and Simulation. arXiv e-prints,
page arXiv:1610.05427, October 2016.
[28] Dimitri P Bertsekas, Angelia Nedi, and Asuman E Ozdaglar. Convex Analysis
and Optimization. Athena Scientific, 2003.
[29] Michael Betancourt, Michael I. Jordan, and Ashia C. Wilson. On Symplectic
Optimization. arXiv e-prints, page arXiv:1802.03653, February 2018.
[30] Ashish Bhatt, Dwayne Floyd, and Brian E Moore. Second order conformal
symplectic schemes for damped Hamiltonian systems. Journal of Scientific
Computing, 66(3):12341259, 2016.
[31] Benjamin Birnbaum, Nikhil R. Devanur, and Lin Xiao. New convex programs
and distributed algorithms for fisher markets with linear and spending
constraint utilities. Technical Report MSR-TR-2010-112, August 2010.
[32] David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A
review for statisticians. Journal of the American Statistical Association, (just-
accepted), 2017.
[33] Jerome Bolte, Aris Daniilidis, and Adrian Lewis. The lojasiewicz inequality for
nonsmooth subanalytic functions with applications to subgradient dynamical
systems. SIAM Journal on Optimization, 17(4):12051223, 2007.
[34] Jerome Bolte, Shoham Sabach, Marc Teboulle, and Yakov Vaisbourd. First
order methods beyond convexity and Lipschitz gradient continuity with
applications to quadratic inverse problems. SIAM Journal on Optimization,
28(3):21312151, 2018.
[35] Jorg Bornschein and Yoshua Bengio. Reweighted wake-sleep. In International
Conference on Learning Representations, 2015.
[36] Jonathan Borwein and Adrian S Lewis. Convex Analysis and Nonlinear
Optimization: Theory and Examples. Springer Science & Business Media, 2010.
[37] Leon Bottou. Large-scale machine learning with stochastic gradient descent. In
Proceedings of COMPSTAT2010, pages 177186. Springer, 2010.
[38] Leon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for
large-scale machine learning. SIAM Review, 60(2):223311, 2018.
[39] Stephane Boucheron, Gabor Lugosi, and Pascal Massart. Concentration
inequalities: A nonasymptotic theory of independence. Oxford university press,
2013.
[40] Nicolas Boulanger-Lewandowski, Yoshua Bengio, and Pascal Vincent. Modeling
temporal dependencies in high-dimensional sequences: Application to
polyphonic music generation and transcription. In International Conference
on Machine Learning, 2012.
[41] Samuel Bowman, Luke Vilnis, Oriol Vinyals, Andrew Dai, Rafal Jozefowicz, and
Samy Bengio. Generating sentences from a continuous space. In Proceedings of
The 20th SIGNLL Conference on Computational Natural Language Learning,
pages 1021, 2016.
[42] Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge
university press, 2004.
[43] James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris
Leary, Dougal Maclaurin, and Skye Wanderman-Milne. JAX: composable
transformations of Python+NumPy programs, 2018.
[44] Lev Bregman. The relaxation method of finding the common point of convex
sets and its application to the solution of problems in convex programming.
USSR computational mathematics and mathematical physics, 7(3):200217,
1967.
[45] Leo Breiman et al. Statistical modeling: The two cultures (with comments and
a rejoinder by the author). Statistical science, 16(3):199231, 2001.
[46] Steve Brooks, Andrew Gelman, Galin Jones, and Xiao-Li Meng. Handbook of
Markov chain Monte Carlo. CRC press, 2011.
[47] Sebastien Bubeck. Convex optimization: Algorithms and complexity.
Foundations and Trends R in Machine Learning, 8(3-4):231357, 2015.
[48] Sebastien Bubeck, Michael B Cohen, Yin Tat Lee, and Yuanzhi Li. An
homotopy method for lp regression provably beyond self-concordance and
in input-sparsity time. In Proceedings of the 50th Annual ACM SIGACT
Symposium on Theory of Computing, pages 11301137. ACM, 2018.
[49] Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Accurate and
conservative estimates of MRF log-likelihood using reverse annealing. In
International Conference on Artificial Intelligence and Statistics, 2015.
[50] Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance weighted
autoencoders. In International Conference on Learning Representations, 2016.
[51] Richard Caron and Tim Traynor. The zero set of a polynomial. Technical report,
2005. Available at http://www1.uwindsor.ca/math/sites/uwindsor.ca.
http://www1.uwindsor.ca/math/sites/uwindsor.ca.math/files/05-03.pdf
http://www1.uwindsor.ca/math/sites/uwindsor.ca.math/files/05-03.pdf
http://www1.uwindsor.ca/math/sites/uwindsor.ca.math/files/05-03.pdf
math/files/05-03.pdf and https://www.researchgate.net/publication/
281285245_The_Zero_Set_of_a_Polynomial.
[52] Christos G Cassandras, Yorai Wardi, Christos G Panayiotou, and Chen Yao.
Perturbation analysis and optimization of stochastic hybrid systems. European
Journal of Control, 16(6):642661, 2010.
[53] Frederic Cerou, Pierre Del Moral, and Arnaud Guyader. A nonasymptotic
theorem for unnormalized FeynmanKac particle models. Ann. Inst. H.
Poincare B, 47(3):629649, 2011.
[54] Gong Chen and Marc Teboulle. Convergence analysis of a proximal-
like minimization algorithm using bregman functions. SIAM Journal on
Optimization, 3(3):538543, 1993.
[55] Xi Chen, Diederik P Kingma, Tim Salimans, Yan Duan, Prafulla Dhariwal,
John Schulman, Ilya Sutskever, and Pieter Abbeel. Variational lossy
autoencoder. In International Conference on Learning Representations, 2017.
[56] Y. Chen and Z. Ghahramani. Scalable Discrete Sampling as a Multi-Armed
Bandit Problem. ArXiv e-prints, June 2015.
[57] Siddhartha Chib. Marginal likelihood from the Gibbs output. Journal of the
American Statistical Association, 90(432):13131321, 1995.
[58] Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron C
Courville, and Yoshua Bengio. A recurrent latent variable model for sequential
data. In Advances in Neural Information Processing Systems, 2015.
[59] Roberto Cominetti and Jean-Pierre Dussault. Stable exponential-penalty
algorithm with superlinear convergence. Journal of Optimization Theory and
Applications, 83(2):285309, 1994.
[60] Roberto Cominetti and Jaime San Martn. Asymptotic analysis of
the exponential penalty trajectory in linear programming. Mathematical
Programming, 67(1-3):169187, 1994.
[61] Robert J Connor and James E Mosimann. Concepts of independence for
proportions with a generalization of the dirichlet distribution. Journal of the
American Statistical Association, 64(325):194206, 1969.
http://www1.uwindsor.ca/math/sites/uwindsor.ca.math/files/05-03.pdf
http://www1.uwindsor.ca/math/sites/uwindsor.ca.math/files/05-03.pdf
http://www1.uwindsor.ca/math/sites/uwindsor.ca.math/files/05-03.pdf
https://www.researchgate.net/publication/281285245_The_Zero_Set_of_a_Polynomial
https://www.researchgate.net/publication/281285245_The_Zero_Set_of_a_Polynomial
[62] Ivo Danihelka, Balaji Lakshminarayanan, Benigno Uria, Daan Wierstra, and
Peter Dayan. Comparison of Maximum Likelihood and GAN-based training of
Real NVPs. arXiv e-prints, page arXiv:1705.05263, May 2017.
[63] Pierre Del Moral. Feynman-Kac formulae: genealogical and interacting particle
systems with applications. Springer Verlag, 2004.
[64] Pierre Del Moral. Mean field simulation for Monte Carlo integration. CRC
Press, 2013.
[65] Pierre Del Moral, Arnaud Doucet, and Ajay Jasra. Sequential Monte Carlo
samplers. J. R. Stat. Soc. Ser. B Stat. Methodol., 68(3):411436, 2006.
[66] Arthur P Dempster, Nan M Laird, and Donald B Rubin. Maximum likelihood
from incomplete data via the em algorithm. Journal of the Royal Statistical
Society: Series B (Methodological), 39(1):122, 1977.
[67] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation
using real nvp. In International Conference on Learning Representations, 2017.
[68] Arnaud Doucet and Adam M. Johansen. A tutorial on particle filtering and
smoothing: Fifteen years later. In D. Crisan and B. Rozovsky, editors, The
Oxford Handbook of Nonlinear Filtering, pages 656704. Oxford University
Press, 2011.
[69] Radu-Alexandru Dragomir, Jerome Bolte, and Alexandre dAspremont. Fast
Gradient Methods for Symmetric Nonnegative Matrix Factorization. arXiv e-
prints, page arXiv:1901.10791, January 2019.
[70] Radu-Alexandru Dragomir, Alexandre dAspremont, and Jerome Bolte. Quartic
First-Order Methods for Low Rank Minimization. arXiv e-prints, page
arXiv:1901.10791, January 2019.
[71] Radu-Alexandru Dragomir, Adrien Taylor, Alexand re dAspremont, and
Jerome Bolte. Optimal Complexity and Certification of Bregman First-Order
Methods. arXiv e-prints, page arXiv:1911.08510, November 2019.
[72] Dmitriy Drusvyatskiy, Maryam Fazel, and Scott Roy. An optimal first order
method based on optimal quadratic averaging. SIAM Journal on Optimization,
28(1):251271, 2018.
[73] Dmitriy Drusvyatskiy and Adrian S Lewis. Error bounds, quadratic growth, and
linear convergence of proximal methods. Mathematics of Operations Research,
2018.
[74] Simon Duane, Anthony D Kennedy, Brian J Pendleton, and Duncan Roweth.
Hybrid Monte Carlo. Physics Letters B, 195(2):216222, 1987.
[75] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods
for online learning and stochastic optimization. Journal of Machine Learning
Research, 12(Jul):21212159, 2011.
[76] Vctor Elvira, Luca Martino, David Luengo, Monica F Bugallo, et al.
Generalized multiple importance sampling. Statistical Science, 34(1):129155,
2019.
[77] Stefano Favaro, Georgia Hadjicharalambous, and Igor Prunster. On a class
of distributions on the simplex. Journal of Statistical Planning and Inference,
141(9):2987  3004, 2011.
[78] Mahyar Fazlyab, Alejandro Ribeiro, Manfred Morari, and Victor M Preciado.
Analysis of optimization algorithms via integral quadratic constraints:
Nonstrongly convex problems. SIAM Journal on Optimization, 28(3):2654
2689, 2018.
[79] Ronald A. Fisher. On the mathematical foundations of theoretical statistics.
Phil. Trans. R. Soc. Lond. A, 222(594-604):309368, 1922.
[80] Nicolas Flammarion and Francis Bach. From averaging to acceleration, there
is only a step-size. In Conference on Learning Theory, pages 658695, 2015.
[81] Nicolas Flammarion and Francis Bach. Stochastic composite least-squares
regression with convergence rate o(1/n). In Conference on Learning Theory,
2017.
[82] Marco Fraccaro, Sren Kaae Snderby, Ulrich Paquet, and Ole Winther.
Sequential neural models with stochastic layers. In Advances in Neural
Information Processing Systems, 2016.
[83] Guilherme Franca, Daniel P Robinson, and Rene Vidal. ADMM and accelerated
ADMM as continuous dynamical systems. International Conference on Machine
Learning, 2018.
[84] Guilherme Franca, Daniel P Robinson, and Rene Vidal. Relax, and accelerate:
A continuous perspective on ADMM. In International Conference on Machine
Learning, 2018.
[85] Brendan Frey. Continuous sigmoidal belief networks trained using slice
sampling. In Advances in Neural Information Processing Systems, 1997.
[86] Jerome Friedman, Trevor Hastie, and Robert Tibshirani. The elements of
statistical learning, volume 1. Springer series in statistics New York, 2001.
[87] Michael C Fu. Gradient estimation. Handbooks in operations research and
management science, 13:575616, 2006.
[88] Yarin Gal. Uncertainty in Deep Learning. PhD thesis, University of Cambridge,
2016.
[89] Andrew Gelman and Xiao-Li Meng. Simulating normalizing constants: From
importance sampling to bridge sampling to path sampling. Statistical science,
pages 163185, 1998.
[90] Geoffrey Hinton. Neural Networks for Machine Learning. url: http://www.
cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf, 2014.
Slides 26-31 of Lecture 6.
[91] Euhanna Ghadimi, Hamid Reza Feyzmahdavian, and Mikael Johansson. Global
convergence of the heavy-ball method for convex optimization. In Control
Conference (ECC), 2015 European, pages 310315. IEEE, 2015.
[92] Zoubin Ghahramani and Michael I Jordan. Factorial hidden Markov models.
In Advances in Neural Information Processing Systems, 1996.
[93] Mark Girolami and Ben Calderhead. Riemann manifold Langevin and
Hamiltonian Monte Carlo methods. Journal of the Royal Statistical Society:
Series B (Statistical Methodology), 73(2):123214, 2011.
[94] Pontus Giselsson. Improved fast dual gradient methods for embedded model
predictive control. IFAC Proceedings Volumes, 47(3):23032309, 2014.
[95] Pontus Giselsson and Stephen Boyd. Preconditioning in fast dual gradient
methods. In 53rd IEEE Conference on Decision and Control, pages 50405045.
IEEE, 2014.
http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf
http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf
[96] Paul Glasserman and Yu-Chi Ho. Gradient estimation via perturbation analysis,
volume 116. Springer Science & Business Media, 1991.
[97] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training
deep feedforward neural networks. In International Conference on Artificial
Intelligence and Statistics, 2010.
[98] Peter W Glynn. Likelihood ratio gradient estimation for stochastic systems.
Communications of the ACM, 33(10):7584, 1990.
[99] Herbert Goldstein, Charles P. Poole, and John Safko. Classical Mechanics.
Pearson Education, 2011.
[100] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-
Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative
adversarial nets. In Advances in Neural Information Processing Systems, 2014.
[101] Neil J Gordon, David J Salmond, and Adrian FM Smith. Novel approach to
nonlinear/non-gaussian bayesian state estimation. In IEE Proceedings F (Radar
and Signal Processing), volume 140, pages 107113. IET, 1993.
[102] Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka,
Agnieszka Grabska-Barwinska, Sergio Gomez Colmenarejo, Edward
Grefenstette, Tiago Ramalho, John Agapiou, et al. Hybrid computing using
a neural network with dynamic external memory. Nature, 538(7626):471476,
2016.
[103] Evan Greensmith, Peter L. Bartlett, and Jonathan Baxter. Variance reduction
techniques for gradient estimates in reinforcement learning. Journal of Machine
Learning Research, 5:14711530, 2004.
[104] Edward Grefenstette, Karl Moritz Hermann, Mustafa Suleyman, and Phil
Blunsom. Learning to transduce with unbounded memory. In Advances in
Neural Information Processing Systems, pages 18281836, 2015.
[105] Karol Gregor, Frederic Besse, Danilo Jimenez Rezende, Ivo Danihelka, and
Daan Wierstra. Towards conceptual compression. In Advances in Neural
Information Processing Systems, 2016.
[106] Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Jimenez Rezende, and
Daan Wierstra. Draw: A recurrent neural network for image generation. In
International Conference on Machine Learning, 2015.
[107] Roger B. Grosse, Zoubin Ghahramani, and Ryan P. Adams. Sandwiching
the marginal likelihood using bidirectional Monte Carlo. arXiv e-prints, page
arXiv:1511.02543, November 2015.
[108] Shixiang Gu, Zoubin Ghahramani, and Richard E Turner. Neural adaptive
sequential Monte Carlo. In Advances in Neural Information Processing Systems,
2015.
[109] Shixiang Gu, Sergey Levine, Ilya Sutskever, and Andriy Mnih. MuProp:
Unbiased backpropagation for stochastic neural networks. In International
Conference on Learning Representations, 2016.
[110] Emil Julius Gumbel. Statistical theory of extreme values and some practical
applications: a series of lectures. Number 33. US Govt. Print. Office, 1954.
[111] Mert Gurbuzbalaban, Asuman Ozdaglar, and Pablo A Parrilo. On the
convergence rate of incremental aggregated gradient algorithms. SIAM Journal
on Optimization, 27(2):10351048, 2017.
[112] David H. Gutman and Javier F. Pena. A unified framework for Bregman
proximal methods: subgradient, gradient, and accelerated gradient schemes.
arXiv e-prints, Dec 2018.
[113] Filip Hanzely, Peter Richtarik, and Lin Xiao. Accelerated Bregman Proximal
Gradient Methods for Relatively Smooth Convex Optimization. arXiv e-prints,
Aug 2018.
[114] P Hartman. Ordinary Differential Equations. Society for Industrial and Applied
Math, 2002.
[115] W Keith Hastings. Monte Carlo sampling methods using Markov chains and
their applications. Biometrika, 57(1):97109, 1970.
[116] Tamir Hazan and Tommi Jaakkola. On the partition function and random
maximum a-posteriori perturbations. In International Conference on Machine
Learning, 2012.
[117] Tamir Hazan, Subhransu Maji, and Tommi Jaakkola. On Sampling from the
Gibbs Distribution with Random Maximum A-Posteriori Perturbations. In
Advances in Neural Information Processing Systems, 2013.
[118] Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed,
Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N
Sainath, et al. Deep neural networks for acoustic modeling in speech recognition:
The shared views of four research groups. IEEE Signal Processing Magazine,
29(6):8297, 2012.
[119] Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning
algorithm for deep belief nets. Neural computation, 18(7):15271554, 2006.
[120] Matthew D Hoffman, David M Blei, Chong Wang, and John William Paisley.
Stochastic variational inference. Journal of Machine Learning Research,
14(1):13031347, 2013.
[121] Roger A. Horn and Charles R. Johnson. Matrix analysis. Cambridge University
Press, Cambridge, second edition, 2013.
[122] Martin Hutzenthaler, Arnulf Jentzen, Peter E Kloeden, et al. Strong
convergence of an explicit numerical method for sdes with nonglobally lipschitz
continuous coefficients. The Annals of Applied Probability, 22(4):16111641,
2012.
[123] Tommi S Jaakkola and Michael I Jordan. Computing upper and lower bounds
on likelihoods in intractable networks. In Proceedings of the twelfth international
conference on Uncertainty in Artificial Intelligence, pages 340348. Morgan
Kaufmann Publishers Inc., 1996.
[124] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with
gumbel-softmax. In International Conference on Learning Representations,
2016.
[125] Shanyu Ji, Janos Kollar, and Bernard Shiffman. A global lojasiewicz inequality
for algebraic varieties. Transactions of the American Mathematical Society,
329(2):813818, 1992.
[126] Chi Jin, Praneeth Netrapalli, and Michael I Jordan. Accelerated gradient
descent escapes saddle points faster than gradient descent. In Conference on
Learning Theory, 2018.
[127] Matthew J Johnson, David Duvenaud, Alexander B Wiltschko, Sandeep R
Datta, and Ryan P Adams. Composing graphical models with neural networks
for structured representations and fast inference. In Neural Information
Processing Systems, 2016.
[128] Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K
Saul. An introduction to variational methods for graphical models. Machine
learning, 37(2):183233, 1999.
[129] Richard Jordan, David Kinderlehrer, and Felix Otto. The variational
formulation of the FokkerPlanck equation. SIAM Journal on Mathematical
Analysis, 29(1):117, 1998.
[130] Anatoli Juditsky, Arkadi Nemirovski, et al. First order methods for nonsmooth
convex large-scale optimization, i: general purpose methods. Optimization for
Machine Learning, pages 121148, 2011.
[131] Anatoli Juditsky and Yurii Nesterov. Deterministic and stochastic primal-dual
subgradient algorithms for uniformly convex minimization. Stochastic Systems,
4(1):4480, 2014.
[132] Sham Kakade, Shai Shalev-Shwartz, and Ambuj Tewari. On the duality of
strong convexity and strong smoothness: Learning applications and matrix
regularization. 2009.
[133] Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient
and proximal-gradient methods under the Polyak-Lojasiewicz condition. In
Joint European Conference on Machine Learning and Knowledge Discovery in
Databases, pages 795811. Springer, 2016.
[134] Carolyn Kim, Ashish Sabharwal, and Stefano Ermon. Exact Sampling with
Integer Linear Programs and Random Perturbations. In AAAI, 2016.
[135] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic
optimization. International Conference on Learning Representations, 2015.
[136] Diederik P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever,
and Max Welling. Improved variational inference with inverse autoregressive
flow. In Advances in Neural Information Processing Systems, 2016.
[137] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In
International Conference on Learning Representations, 2014.
[138] Tomas Kocisky, Gabor Melis, Edward Grefenstette, Chris Dyer, Wang Ling,
Phil Blunsom, and Karl Moritz Hermann. Semantic parsing with semi-
supervised sequential autoencoders. In Conference on Empirical Methods in
Natural Language Processing, 2016.
[139] Walid Krichene, Alexandre Bayen, and Peter L Bartlett. Accelerated mirror
descent in continuous and discrete time. In Advances in Neural Information
Processing Systems, pages 28452853, 2015.
[140] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification
with deep convolutional neural networks. In Advances in Neural Information
Processing Systems, pages 10971105, 2012.
[141] Alp Kucukelbir, Dustin Tran, Rajesh Ranganath, Andrew Gelman, and
David M Blei. Automatic differentiation variational inference. The Journal
of Machine Learning Research, 18(1):430474, 2017.
[142] Manoj Kumar, Mohammad Babaeizadeh, Dumitru Erhan, Chelsea Finn, Sergey
Levine, Laurent Dinh, and Durk Kingma. Videoflow: A flow-based generative
model for video. In International Conference on Learning Representations,
2014.
[143] Rasmus Kyng, Anup Rao, Sushant Sachdeva, and Daniel A Spielman.
Algorithms for lipschitz learning on graphs. In Conference on Learning Theory,
pages 11901223, 2015.
[144] Hugo Larochelle and Iain Murray. The neural autoregressive distribution
estimator. In International Conference on Artificial Intelligence and Statistics,
2011.
[145] Joseph LaSalle. Some extensions of Liapunovs second method. IRE
Transactions on Circuit Theory, 7(4):520527, 1960.
[146] Tuan Anh Le, Maximilian Igl, Tom Jin, Tom Rainforth, and Frank Wood. Auto-
encoding sequential Monte Carlo. In International Conference on Learning
Representations, 2018.
[147] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature,
521(7553):436444, 2015.
[148] Laurent Lessard, Benjamin Recht, and Andrew Packard. Analysis and design
of optimization algorithms via integral quadratic constraints. SIAM Journal on
Optimization, 26(1):5795, 2016.
[149] Samuel Livingstone, Michael F. Faulkner, and Gareth O. Roberts. Kinetic
energy choice in Hamiltonian/hybrid Monte Carlo. arXiv e-prints, page
arXiv:1706.02649, June 2017.
[150] Stanislas Lojasiewicz. Sur la geometrie semi-et sous-analytique. Ann. Inst.
Fourier, 43(5):15751595, 1993.
[151] Stanislaw Lojasiewicz. Sur le probleme de la division. 1961.
[152] Stanislaw Lojasiewicz. Une propriete topologique des sous-ensembles
analytiques reels. Les equations aux derivees partielles, 117:8789, 1963.
[153] Haihao Lu. Relative Continuity for Non-Lipschitz Nonsmooth Convex
Optimization Using Stochastic (or Deterministic) Mirror Descent. INFORMS
Journal on Optimization, 2019.
[154] Haihao Lu. relative continuity for non-lipschitz nonsmooth convex
optimization using stochastic (or deterministic) mirror descent. Informs Journal
on Optimization, 1(4):288303, 2019.
[155] Haihao Lu and Robert M Freund. Generalized stochastic frankwolfe algorithm
with stochastic substitute gradient for structured convex optimization.
Mathematical Programming, pages 133, 2020.
[156] Haihao Lu, Robert M Freund, and Yurii Nesterov. Relatively smooth convex
optimization by first-order methods, and applications. SIAM Journal on
Optimization, 28(1):333354, 2018.
[157] Xiaoyu Lu, Valerio Perrone, Leonard Hasenclever, Yee Whye Teh, and Sebastian
Vollmer. Relativistic Monte Carlo. In International Conference on Artificial
Intelligence and Statistics, 2017.
[158] R Duncan Luce. Individual choice behavior: A theoretical analysis. Courier
Corporation, 2012.
[159] David JC MacKay. Information theory, inference and learning algorithms.
Cambridge university press, 2003.
[160] Chris J. Maddison. A Poisson process model for Monte Carlo. In Tamir Hazan,
George Papandreou, and Daniel Tarlow, editors, Perturbation, Optimization,
and Statistics. MIT Press, 2016.
[161] Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution:
A continuous relaxation of discrete random variables. In International
Conference on Learning Representations, 2017.
[162] Chris J. Maddison, Daniel Paulin, Yee Whye Teh, Brendan ODonoghue,
and Arnaud Doucet. Hamiltonian Descent Methods. arXiv e-prints, page
arXiv:1809.05042, September 2018.
[163] Chris J. Maddison, Daniel Paulin, Yee Whye Teh, Brendan ODonoghue,
and Arnaud Doucet. Hamiltonian descent methods. arXiv e-prints, page
arXiv:1809.05042, September 2018.
[164] Chris J Maddison, Daniel Tarlow, and Tom Minka. A Sampling. In Advances
in Neural Information Processing Systems, 2014.
[165] Robert McLachlan and Matthew Perlmutter. Conformal Hamiltonian systems.
Journal of Geometry and Physics, 39(4):276300, 2001.
[166] Wenjun Mei and Francesco Bullo. LaSalle Invariance Principle for Discrete-time
Dynamical Systems: A Concise and Self-contained Tutorial. arXiv e-prints,
page arXiv:1710.03710, October 2017.
[167] Ron Meir and Gunnar Ratsch. An introduction to boosting and leveraging. In
Advanced lectures on machine learning, pages 118183. Springer, 2003.
[168] Xiao-Li Meng and Wing Hung Wong. Simulating ratios of normalizing constants
via a simple identity: a theoretical exploration. Statistica Sinica, pages 831860,
1996.
[169] Kerrie L Mengersen and Richard L Tweedie. Rates of convergence of the
Hastings and Metropolis algorithms. The Annals of Statistics, 24(1):101121,
1996.
[170] Nicholas Metropolis, Arianna W Rosenbluth, Marshall N Rosenbluth,
Augusta H Teller, and Edward Teller. Equation of state calculations by fast
computing machines. The Journal of Chemical Physics, 21(6):10871092, 1953.
[171] Konstantin Mishchenko. Sinkhorn algorithm as a special case of stochastic
mirror descent. arXiv e-prints, Sep 2019.
[172] Andriy Mnih and Karol Gregor. Neural variational inference and learning in
belief networks. In International Conference on Machine Learning, 2014.
[173] Andriy Mnih and Karol Gregor. Neural variational inference and learning in
belief networks. In International Conference on Machine Learning, 2014.
[174] Andriy Mnih and Danilo J Rezende. Variational inference for Monte Carlo
objectives. In International Conference on Machine Learning, 2016.
[175] Andriy Mnih and Danilo Jimenez Rezende. Variational inference for monte
carlo objectives. In International Conference on Machine Learning, 2016.
[176] Shakir Mohamed and Balaji Lakshminarayanan. Learning in Implicit
Generative Models. arXiv e-prints, page arXiv:1610.03483, October 2016.
[177] Shakir Mohamed, Mihaela Rosca, Michael Figurnov, and Andriy Mnih.
Monte Carlo Gradient Estimation in Machine Learning. arXiv e-prints, page
arXiv:1906.10652, June 2019.
[178] Kevin P Murphy. Machine learning: a probabilistic perspective. MIT press,
2012.
[179] Christian Naesseth, Francisco Ruiz, Scott Linderman, and David Blei.
Reparameterization gradients through acceptance-rejection sampling
algorithms. In International Conference on Artificial Intelligence and
Statistics, 2017.
[180] Christian A Naesseth, Scott W Linderman, Rajesh Ranganath, and David M
Blei. Variational sequential Monte Carlo. In International Conference on
Artificial Intelligence and Statistics, 2018.
[181] Christian Andersson Naesseth, Fredrik Lindsten, and Thomas B Schon.
Sequential Monte Carlo for graphical models. In Advances in Neural
Information Processing Systems, 2014.
[182] Radford M Neal. Connectionist learning of belief networks. Artificial
intelligence, 56(1):71113, 1992.
[183] Radford M Neal. Annealed importance sampling. Statistics and computing,
11(2):125139, 2001.
[184] Radford M. Neal. Estimating Ratios of Normalizing Constants Using Linked
Importance Sampling. arXiv Mathematics e-prints, page math/0511216,
November 2005.
[185] Radford M Neal. MCMC using Hamiltonian dynamics. Handbook of Markov
Chain Monte Carlo, pages 113162, 2011.
[186] Radford M Neal and Geoffrey E Hinton. A view of the EM algorithm that
justifies incremental, sparse, and other variants. In Learning in graphical models,
pages 355368. Springer, 1998.
[187] Ion Necoara, Yurii Nesterov, and Francois Glineur. Linear convergence
of first order methods for non-strongly convex optimization. Mathematical
Programming, pages 139, 2018.
[188] Arkadi Nemirovski and Yurii Nesterov. Optimal methods of smooth convex
minimization. USSR Computational Mathematics and Mathematical Physics,
25(2):2130, 1985.
[189] Arkadi Nemirovski and David Yudin. Effective methods for the solution of
convex programming problems of large dimensions. Ekonom. i Mat. Metody,
15(1):135152, 1979.
[190] Arkadi Nemirovski and David B Yudin. Problem Complexity and Method
Efficiency in Optimization. Wiley Interscience, 1983.
[191] Yurii Nesterov. Smooth minimization of non-smooth functions. Mathematical
programming, 103(1):127152, 2005.
[192] Yurii Nesterov. Accelerating the cubic regularization of Newtons method on
convex problems. Mathematical Programming, 112(1):159181, 2008.
[193] Yurii Nesterov. Introductory Lectures on Convex Optimization: A Basic Course,
volume 87. Springer Science & Business Media, 2013.
[194] Yurii Nesterov. Lectures on convex optimization, volume 137. Springer, 2018.
[195] Yurii E Nesterov. A method for solving the convex programming problem with
convergence rate O(1/k2). Dokl. Akad. Nauk SSSR, 269:543547, 1983.
[196] Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training
generative neural samplers using variational divergence minimization. In
Advances in Neural Information Processing Systems, 2016.
[197] Brooks Paige and Frank Wood. Inference networks for sequential Monte Carlo
in graphical models. In International Conference on Machine Learning, 2016.
[198] John William Paisley, David M. Blei, and Michael I. Jordan. Variational
bayesian inference with stochastic search. In International Conference on
Machine Learning, 2012.
[199] G. Papandreou and A. Yuille. Perturb-and-MAP Random Fields: Using
Discrete Optimization to Learn and Sample from Energy Models. In
International Conference on Computer Vision, 2011.
[200] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang,
Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.
Automatic differentiation in pytorch. 2017.
[201] Giuseppe Peano. Demonstration de lintegrabilite des equations differentielles
ordinaires. In Arbeiten zur Analysis und zur mathematischen Logik, pages 76
126. Springer, 1990.
[202] Lawrence Perko. Differential Equations and Dynamical Systems, volume 7.
Springer Science & Business Media, 2013.
[203] Jan Peters and Stefan Schaal. Reinforcement learning of motor skills with policy
gradients. Neural networks, 21(4):682697, 2008.
[204] Michael K Pitt, Ralph dos Santos Silva, Paolo Giordani, and Robert Kohn. On
some properties of Markov chain Monte Carlo simulation methods based on the
particle filter. J. Econometrics, 171(2):134151, 2012.
[205] Robin L Plackett. The analysis of permutations. Journal of the Royal Statistical
Society: Series C (Applied Statistics), 24(2):193202, 1975.
[206] Boris T Polyak. Some methods of speeding up the convergence of iteration
methods. USSR Computational Mathematics and Mathematical Physics, 4(5):1
17, 1964.
[207] Boris T Polyak. Introduction to Optimization. Optimization Software, 1987.
[208] BT Polyak and PS Shcherbakov. Optimisation and asymptotic stability.
International Journal of Control, 91(11):24042410, 2018.
[209] Roman Polyak and Marc Teboulle. Nonlinear rescaling and proximal-like
methods in convex optimization. Mathematical programming, 76(2):265284,
1997.
[210] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation
learning with deep convolutional generative adversarial networks. In
International Conference on Learning Representations, 2016.
[211] Maxim Raginsky and Alexander Rakhlin. Information-based complexity,
feedback and dynamics in convex programming. IEEE Transactions on
Information Theory, 57(10):70367056, 2011.
[212] Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh.
Techniques for Learning Binary Stochastic Feedforward Neural Networks. arXiv
e-prints, page arXiv:1406.2989, June 2014.
[213] Rajesh Ranganath, Sean Gerrish, and David Blei. Black box variational
inference. In International Conference on Artificial Intelligence and Statistics,
2014.
[214] Rajesh Ranganath, Dustin Tran, Jaan Altosaar, and David Blei. Advances in
neural information processing systems. In NIPS, 2016.
[215] William S Rayens and Cidambi Srinivasan. Dependence properties of
generalized liouville distributions on the simplex. Journal of the American
Statistical Association, 89(428):14651470, 1994.
[216] Benjamin Recht. CS726 - Lyapunov analysis and the heavy ball method. Lecture
notes, 2012.
[217] Danilo Jimenez Rezende and Shakir Mohamed. Variational inference with
normalizing flows. International Conference on Machine Learning, 2015.
[218] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic
backpropagation and approximate inference in deep generative models. In
International Conference on Machine Learning, 2014.
[219] Herbert Robbins and Sutton Monro. A stochastic approximation method. The
annals of mathematical statistics, pages 400407, 1951.
[220] Christian Robert and George Casella. Monte Carlo statistical methods. Springer
Science & Business Media, 2013.
[221] Gareth O Roberts and Jeffrey S Rosenthal. General state space Markov chains
and MCMC algorithms. Probability Surveys, 1:2071, 2004.
[222] R. Tyrrell Rockafellar. Convex Analysis. Princeton University Press, 1970.
[223] Vincent Roulet and Alexandre dAspremont. Sharpness, restart and
acceleration. In Advances in Neural Information Processing Systems, pages
11191129, 2017.
[224] Walter Rudin. Real and Complex Analysis. McGraw-Hill Book Co., New York,
third edition, 1987.
[225] Francisco JR Ruiz, Michalis K Titsias, and David M Blei. The generalized
reparameterization gradient. In Advances in Neural Information Processing
Systems, 2016.
[226] Sotirios Sabanis. A note on tamed Euler approximations. Electronic
Communications in Probability, 18, 2013.
[227] Ruslan Salakhutdinov and Geoffrey Hinton. Semantic hashing. International
Journal of Approximate Reasoning, 50(7):969978, 2009.
[228] Ruslan Salakhutdinov and Iain Murray. On the quantitative analysis of deep
belief networks. In International Conference on Machine Learning, volume 25,
2008.
[229] Tim Salimans, Diederik Kingma, and Max Welling. Markov chain Monte Carlo
and variational inference: Bridging the gap. In International Conference on
Machine Learning, 2015.
[230] Lawrence K Saul, Tommi Jaakkola, and Michael I Jordan. Mean field theory for
sigmoid belief networks. Journal of Artificial Intelligence Research, 4(1):6176,
1996.
[231] John Schulman, Nicolas Heess, Theophane Weber, and Pieter Abbeel. Gradient
estimation using stochastic computation graphs. In Advances in Neural
Information Processing Systems, pages 35283536, 2015.
[232] Shai Shalev-Shwartz and Yoram Singer. Online learning: Theory, algorithms,
and applications. 2007.
[233] Bin Shi, Simon S. Du, Michael I. Jordan, and Weijie J. Su. Understanding the
Acceleration Phenomenon via High-Resolution Differential Equations. arXiv
e-prints, page arXiv:1810.08907, October 2018.
[234] Bin Shi, Simon S. Du, Weijie J. Su, and Michael I. Jordan. Acceleration
via symplectic discretization of high-resolution differential equations. arXiv
e-prints, Feb 2019.
[235] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre,
George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda
Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep
neural networks and tree search. Nature, 529(7587):484489, 2016.
[236] John Skilling. Nested sampling for general Bayesian computation. Bayesian
analysis, 1(4):833859, 2006.
[237] Gabriel Stoltz and Zofia Trstanova. Langevin dynamics with general kinetic
energies. Multiscale Modeling & Simulation, 16(2):777806, 2018.
[238] Weijie Su, Stephen Boyd, and Emmanuel Candes. A differential equation for
modeling Nesterovs accelerated gradient method: Theory and insights. In
Advances in Neural Information Processing Systems, pages 25102518, 2014.
[239] Weijie Su, Stephen Boyd, and Emmanuel J Candes. A differential equation for
modeling Nesterovs accelerated gradient method: theory and insights. Journal
of Machine Learning Research, 17(1):53125354, 2016.
[240] Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the
importance of initialization and momentum in deep learning. In International
Conference on Machine Learning, pages 11391147, 2013.
[241] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning
with neural networks. In Advances in Neural Information Processing Systems,
pages 31043112, 2014.
[242] Richard S Sutton and Andrew G Barto. Reinforcement learning: An
introduction. MIT press, 2018.
[243] Daniel Tarlow, Ryan Prescott Adams, and Richard S Zemel. Randomized
Optimum Models for Structured Prediction. In International Conference on
Artificial Intelligence and Statistics, 2012.
[244] Marc Teboulle. A simplified view of first order methods for optimization.
Mathematical Programming, 170(1):6796, 2018.
[245] The Theano Development Team, Rami Al-Rfou, Guillaume Alain, Amjad
Almahairi, Christof Angermueller, Dzmitry Bahdanau, Nicolas Ballas, Frederic
Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, Yoshua
Bengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher
Snyder, Nicolas Bouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier,
Alexandre de Brebisson, Olivier Breuleux, Pierre-Luc Carrier, Kyunghyun
Cho, Jan Chorowski, Paul Christiano, Tim Cooijmans, Marc-Alexandre Cote,
Myriam Cote, Aaron Courville, Yann N. Dauphin, Olivier Delalleau, Julien
Demouth, Guillaume Desjardins, Sander Dieleman, Laurent Dinh, Melanie
Ducoffe, Vincent Dumoulin, Samira Ebrahimi Kahou, Dumitru Erhan, Ziye
Fan, Orhan Firat, Mathieu Germain, Xavier Glorot, Ian Goodfellow, Matt
Graham, Caglar Gulcehre, Philippe Hamel, Iban Harlouchet, Jean-Philippe
Heng, Balazs Hidasi, Sina Honari, Arjun Jain, Sebastien Jean, Kai Jia, Mikhail
Korobov, Vivek Kulkarni, Alex Lamb, Pascal Lamblin, Eric Larsen, Cesar
Laurent, Sean Lee, Simon Lefrancois, Simon Lemieux, Nicholas Leonard,
Zhouhan Lin, Jesse A. Livezey, Cory Lorenz, Jeremiah Lowin, Qianli Ma,
Pierre-Antoine Manzagol, Olivier Mastropietro, Robert T. McGibbon, Roland
Memisevic, Bart van Merrienboer, Vincent Michalski, Mehdi Mirza, Alberto
Orlandi, Christopher Pal, Razvan Pascanu, Mohammad Pezeshki, Colin Raffel,
Daniel Renshaw, Matthew Rocklin, Adriana Romero, Markus Roth, Peter
Sadowski, John Salvatier, Francois Savard, Jan Schluter, John Schulman,
Gabriel Schwartz, Iulian Vlad Serban, Dmitriy Serdyuk, Samira Shabanian,
Etienne Simon, Sigurd Spieckermann, S. Ramana Subramanyam, Jakub
Sygnowski, Jeremie Tanguay, Gijs van Tulder, Joseph Turian, Sebastian Urban,
Pascal Vincent, Francesco Visin, Harm de Vries, David Warde-Farley, Dustin J.
Webb, Matthew Willson, Kelvin Xu, Lijun Xue, Li Yao, Saizheng Zhang,
and Ying Zhang. Theano: A Python framework for fast computation of
mathematical expressions. arXiv e-prints, abs/1605.02688:arXiv:1605.02688,
May 2016.
[246] Michalis Titsias and Miguel Lazaro-gredilla. Doubly stochastic variational bayes
for non-conjugate inference. In International Conference on Machine Learning,
2014.
[247] Michalis Titsias and Miguel Lazaro-Gredilla. Local expectation gradients for
black box variational inference. In Advances in Neural Information Processing
Systems, pages 26202628, 2015.
[248] Dustin Tran, Rajesh Ranganath, and David Blei. Hierarchical implicit models
and likelihood-free variational inference. In Advances in Neural Information
Processing Systems, pages 55235533, 2017.
[249] Paul Tseng. Applications of a splitting algorithm to decomposition in convex
programming and variational inequalities. SIAM Journal on Control and
Optimization, 29(1):119138, 1991.
[250] Paul Tseng. On accelerated proximal gradient methods for convex-concave
optimization. Technical report, MIT, 2008.
[251] Paul Tseng and Dimitri P Bertsekas. On the convergence of the exponential
multiplier method for convex programming. Mathematical Programming, 60(1-
3):119, 1993.
[252] Benigno Uria, Iain Murray, and Hugo Larochelle. A deep and tractable density
estimator. In International Conference on Machine Learning, 2014.
[253] Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan,
Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray
Kavukcuoglu. WaveNet: A Generative Model for Raw Audio. arXiv e-prints,
page arXiv:1609.03499, September 2016.
[254] Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel
Recurrent Neural Networks. arXiv e-prints, page arXiv:1601.06759, January
2016.
[255] Henk A Van der Vorst. Iterative Krylov methods for large linear systems,
volume 13. Cambridge University Press, 2003.
[256] Quang Van Nguyen. Forward-backward splitting with bregman distances.
Vietnam Journal of Mathematics, 45(3):519539, 2017.
[257] Vladimir Vapnik. Principles of risk minimization for learning theory. In
Advances in Neural Information Processing Systems, pages 831838, 1992.
[258] Eric Veach and Leonidas J Guibas. Optimally combining sampling techniques
for Monte Carlo rendering. In Proceedings of the 22nd annual conference on
Computer graphics and interactive techniques, pages 419428. ACM, 1995.
[259] Martin J Wainwright, Tommi S Jaakkola, and Alan S Willsky. A new class of
upper bounds on the log partition function. IEEE Transactions on Information
Theory, 51(7):23132335, 2005.
[260] Martin J Wainwright, Michael I Jordan, et al. Graphical models, exponential
families, and variational inference. Foundations and Trends in Machine
Learning, 1(12):1305, 2008.
[261] Nick Whiteley and Anthony Lee. Twisted particle filters. Ann. Statist.,
42(1):115141, 2014.
[262] Andre Wibisono. Sampling as optimization in the space of measures: The
langevin dynamics as a composite optimization problem. In Conference On
Learning Theory, pages 20933027, 2018.
[263] Andre Wibisono, Ashia C Wilson, and Michael I Jordan. A variational
perspective on accelerated methods in optimization. Proceedings of the National
Academy of Sciences, 113(47):E7351E7358, 2016.
[264] Ronald J Williams. Simple statistical gradient-following algorithms for
connectionist reinforcement learning. Machine learning, 8(3-4):229256, 1992.
[265] Ashia Wilson, Lester Mackey, and Andre Wibisono. Accelerating rescaled
gradient descent. In Advances in Neural Information Processing Systems, 2019.
[266] Ashia C. Wilson, Benjamin Recht, and Michael I. Jordan. A Lyapunov Analysis
of Momentum Methods in Optimization. arXiv e-prints, page arXiv:1611.02635,
November 2016.
[267] Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin
Recht. The marginal value of adaptive gradient methods in machine learning.
In Advances in Neural Information Processing Systems, pages 41484158, 2017.
[268] CF Jeff Wu. On the convergence properties of the EM algorithm. Ann. Stat.,
pages 95103, 1983.
[269] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi,
Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey,
Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu,  Lukasz Kaiser,
Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens,
George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason
Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey
Dean. Googles Neural Machine Translation System: Bridging the Gap between
Human and Machine Translation. arXiv e-prints, page arXiv:1609.08144,
September 2016.
[270] Yuhuai Wu, Yuri Burda, Ruslan Salakhutdinov, and Roger Grosse. On the
quantitative analysis of decoder-based generative models. In International
Conference on Learning Representations, 2017.
[271] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan
Salakhudinov, Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural
image caption generation with visual attention. In International Conference on
Machine Learning, pages 20482057, 2015.
[272] Tianbao Yang and Qihang Lin. Rsg: Beating subgradient method without
smoothness and strong convexity. Journal of Machine Learning Research,
19(1):133, 2018.
[273] John I Yellott. The relationship between luces choice axiom, thurstones theory
of comparative judgment, and the double exponential distribution. Journal of
Mathematical Psychology, 15(2):109144, 1977.
[274] Matthew D. Zeiler. ADADELTA: An Adaptive Learning Rate Method. arXiv
e-prints, page arXiv:1212.5701, December 2012.
[275] Yizhe Zhang, Xiangyu Wang, Changyou Chen, Ricardo Henao, Kai Fan, and
Lawrence Carin. Towards unifying hamiltonian monte carlo and slice sampling.
In Advances in Neural Information Processing Systems, pages 17411749, 2016.
[276] Xingyu Zhou. On the Fenchel duality between strong convexity and Lipschitz
continuous gradient. arXiv e-prints, page arXiv:1803.06573, March 2018.
[277] Constantin Zalinescu. On uniformly convex functions. Journal of Mathematical
Analysis and Applications, 95(2):344374, 1983.
[278] Constantin Zalinescu. Convex Analysis in General Vector Spaces. World
Scientific, 2002.
	Introduction
	Integrals and Optima
	Gradient Estimation and the Gumbel max trick
	Maximum Likliehood via Variational Objectives
	Gradient-based Optimization and Momentum
	Overview
	The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables
	Filtering Variational Objectives
	Hamiltonian Descent
	Abstract
	Introduction
	Notation and Convex Analysis Review
	Related Literature
	Continuous Dynamics
	Hamiltonian Systems
	Continuously Descending the Hamiltonian
	Continuous Hamiltonian Descent on Convex Functions
	Partial Lower Bounds
	Optimization Algorithms
	Implicit Method
	First Explicit Method, with Analysis via the Hessian of f
	Second Explicit Method, with Analysis via the Hessian of k
	First Explicit Method on Non-Convex f
	Kinetic Maps for Functions with Power Behavior
	Power Kinetic Energies
	Matching power kinetic k with assumptions on f
	Matching relativistic kinetic k with assumptions on f
	Conclusion
	Dual Space Preconditioning for Gradient Descent
	Abstract
	Introduction
	Related literature
	Convex analysis background
	Convex conjugate and Legendre functions
	Relative smoothness and relative strong convexity
	Analysis of the dual preconditioned scheme
	Motivation
	Relative conditions in the dual space
	Convergence rates under dual relative smoothness
	Applications
	Exponential Penalty Functions
	p-norm Regression
	Discussion
	Special cases and related methods
	Conclusions
	Conclusions
	Appendix to Hamiltonian Descent
	Proofs for convergence of continuous systems
	Proofs for partial lower bounds
	Proofs of convergence for discrete systems
	Implicit Method
	First Explicit Method
	Second Explicit Method
	Explicit Method on Non-Convex f
	Proofs for power kinetic energies
	Bibliography
