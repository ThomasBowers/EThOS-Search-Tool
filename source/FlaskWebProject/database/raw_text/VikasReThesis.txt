Essays in Behavioural Economics and
Incentive Design
Submitted by Vikas Chaudhary to the University of Exeter as a
thesis for the degree of Doctor of Philosophy in Economics,
August 2021.
This thesis is available for Library use on the understanding that it is
copyright material and that no quotation from the thesis may be published
without proper acknowledgment.
I certify that all material in this thesis which is not my own work has been
identified and that any material that has previously been submitted and
approved for the award of a degree by this or any other University has been
acknowledged.
Signature: . . . . . . . . . . . . . . . . . . . . . . . . . . .
Abstract
Chapter 1- The importance of the Probability of Winning in Risky Choices: Media reports say
that high earners and syndicates buy lottery tickets in bulk. Experimental evidence shows
that agents aggressively bid in auctions and contests. Do people try to trade-off probability of
winning with other basic risk dimensions (for example, cost) to achieve a subjective threshold
probability of winning (in environments they can) even when such choices are second-order
stochastic dominated? The literature on risky choices suggests so. In the main design of
this experiment, we deconstruct the expected value with variance and skewness of a lottery
with Bernoulli distribution to examine the decision-making process. Based on the results,
a proportion is classified as expected utility maximizer (EUM)1 while another proportion
seems to achieve a subjective threshold probability of winning (termed as target probability
of winning (TPW)). More TPWs prefer higher probabilities compared to EUMs in a constant
value lottery set which may explain the preference for negative skewness in experiments.
Additionally, we test two contest designs and find TPWs in the population which may explain
the puzzle of equilibrium effort more than risk-neutral Nash equilibrium in experiments.
Chapter 2- Reinforcement Learning in Contests: We study contests as an example of winner-
take-all competition with linearly ordered large strategy space. We study a model in which
each player optimizes the probability of winning above some subjective threshold. The envi-
ronment we consider is that of limited information where agents play the game repeatedly
and know their own efforts and outcomes. Players learn through reinforcement. Predictions
are derived based on the model dynamics and asymptotic analysis. The model is able to
predict individual behavior regularities found in experimental data and track the behavior at
aggregate level with reasonable accuracy.
Chapter 3- A Mechanism and Matching in a Social Dilemma: Cooperation can be achieved
via incentives from future interactions, specifically in the case of public monitoring. But,
today, our social and professional spheres keep shifting rapidly and we interact often with
strangers. We are interested in such sporadic interactions which can be modeled as a con-
tinuous Prisoners Dilemma in an environment of the symmetric market where the whole
population is competing among themselves to interact with other agents who will contribute
the most. The interaction is private, only the agents involved know how much they have
contributed to each others well-being, and partners may change in the next period. In
such an environment if the reputation of agents is not available, then there is no incentive
to cooperate. In this paper, we show that if an experience reporting mechanism facilitates
assortative matching, then cooperation and honest reporting is evolutionarily (neutral) stable.
1In this thesis whenever we say expected utility theory (EUT) or maximization (EUM) we mean any continu-
ous and smooth utility function for which second-order stochastic dominance holds.
Contents
Introduction 9
1 The importance of Probability of Winning in Risky Choices 10
1.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
1.2 Related Literature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
1.3 Towards Theoretical Formulation . . . . . . . . . . . . . . . . . . . . . . . . . 18
1.4 Main Design-Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
1.5 Experimental Design and Procedures . . . . . . . . . . . . . . . . . . . . . . . 21
1.6 Predictions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
1.7 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
1.8 Part 5 Results - Alternative Explanation . . . . . . . . . . . . . . . . . . . . . 32
1.9 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
2 Reinforcement Learning in Contests 39
2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
2.2 Static Equilibrium Characterization . . . . . . . . . . . . . . . . . . . . . . . . 43
2.3 Basic Learning Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
2.4 Asymptotic Analysis -Tullock Contests . . . . . . . . . . . . . . . . . . . . . . 47
2.5 Model Predictions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
2.6 Experimental Evidence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
2.6.1 Simulations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
2.7 Model Applicability to All Pay Auction . . . . . . . . . . . . . . . . . . . . . . 58
2.8 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
3 A Mechanism and Matching in a Social Dilemma 62
3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
3.2 Related Literature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
3.3 Basic Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
3.3.1 Continuous PD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
3.3.2 Reputation Mechanism . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
3.4 Properties of Reputation Mechanism . . . . . . . . . . . . . . . . . . . . . . . 70
3.5 Equilibrium Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
3.5.1 Full History . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
3.5.2 Finite History - Straightforward Mutants . . . . . . . . . . . . . . . . . 80
3.5.3 Finite History - Strategic Mutants . . . . . . . . . . . . . . . . . . . . . 84
3.5.4 Finite History - Strategic Mutants Simulations . . . . . . . . . . . . . 85
3.6 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
Appendices 92
A.1 (Chapter 1) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
A.1.1 News and Social Media Posts . . . . . . . . . . . . . . . . . . . . . . . 93
A.1.2 Further Charts and Data . . . . . . . . . . . . . . . . . . . . . . . . . . 97
A.1.3 Risk Aversion for Part 5, 4 and 3 . . . . . . . . . . . . . . . . . . . . . 100
A.1.4 Instructions Set . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104
A.1.5 z-Tree Screenshots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
A.2 (Chapter 2) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128
A.2.1 Further Simulations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128
A.2.2 Reference for Proposition 1 . . . . . . . . . . . . . . . . . . . . . . . . 129
A.2.3 Learning Model Simulation Code . . . . . . . . . . . . . . . . . . . . . 130
A.3 (Chapter 3) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136
A.3.1 Simulation Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136
References 145
Acknowledgements
I would like to express my appreciation to Prof. Julian Jamison for helping me submit this
thesis. I am also grateful to Prof. Oliver Huaser and Prof. Luke Lindsay for their comments.
I am thankful to everyone who gave constructive comments to help shape this thesis, and
supported me in working on this.
Declaration
I hereby declare that except where specific reference is made to the work of others, the
contents of this dissertation are original and have not been submitted in whole or in part
for consideration for any other degree or qualification in this, or any other university. This
dissertation is my own work and contains nothing which is the outcome of work done in
collaboration with others, except as specified in the text and acknowledgements. This thesis
does not exceed a limit of 100,000 words.
Thesis Introduction
Many of the situations we face are risky choices. Is it the case that people trade-off cost with
a probability of winning even when such a choice is second-order stochastically dominated?
A clearer experimental design contributes to the gap in the suggestive literature on risky
choices.
We learn from the experience of the outcomes that can change our beliefs and decisions
on the further course of action. How will agents learn in a severely limited information
environment when they are driven by the target probability of winning? A reinforcement
learning model with features of directional learning is the primary contribution to literature
in learning in games.
We look for authentic information to decide whether to cooperate with someone or not.
How can we achieve cooperation in a private interaction in a symmetric market (where the
population is competing among themselves to find a full-contributing partner) in an infre-
quent social dilemma between any two players? A simple strategy of honest reporting and
full contribution is shown to achieve cooperative equilibrium contributing to the literature
on cooperation in a social dilemma.
Further understandings of how these decisions are made can contribute significantly to
the field and provide insights into a range of real-life situations. The thesis aims to address
the above three questions and will now outline the overarching organization.
In the first chapter we experimentally test the idea of target-based decision making in
risky choices. People often seem to target levels of income; however, in some settings choices
are over probabilities of success tend to be the predominant factor. For example, in buying
lottery tickets when a prize is fixed, anecdotal evidence suggests people might be trading off
one risk dimension with another to reach their target probability of winning.
The main objective of the first chapter is to seek an answer to the question as to whether
subjects target a probability of winning in risky choices. Target probability of winning
(henceforth TPW), emphasizes that the probability of winning in any decision/game could
be an independently important determinant of decision making. If feasible, subjects will
try to reach their suitable probability of winning in the choice set. This means that they can
also choose an option that is second-order stochastically dominated. This is different from
the probability weighting function, Cumulative Prospect Theory, (henceforth CPT) where
subjects perceive the objective probabilities subjectively and a decision is a function of the
product of this subjective probability and the possible subjective value to be gained.
We use an experimental design to test if TPWs exist in a lab and find some positive results.
The first chapter reviews and contributes to relevant experimental literature on risky choices
that investigate how agents make decision under laboratory conditions, which provides
further support for TPW with a clearer design and serves as a basis for the second chapter
which asks how agents behavior can be in a repeated game if they are driven by TPW in a
severely limited information environment.
In the second chapter we consider an example of winner-take-all contests. Application
of winner-take-all contests range from lobbying, political polls, sports tournaments, patent
races, and pharmaceutical R&D. Many real-life situations can be modeled as a probabilistic
contest. Some well-known real-world probabilistic contests are raffles and lotteries. We are
interested in the competitive situations which can be represented by Tullock contests/all-pay
auctions where (almost) fixed players play the game repeatedly. Where agents are not able to
see the effort of others or even the aggregate effort, what they know is their own previous
effort and outcome in terms of winning or losing. In such environments, it is found that
agents increase effort after losing, decrease effort after winning, drops out after consecutive
losing and there is over-dissipation at the aggregate level. The focus in the second chapter is
on how such behaviors can be explained within a single model which can describe the agents
behavior and track the aggregate effort over time.
We introduce a decision-making approach agent possibly follows in this potentially highly
uncertain environment. The limited information available to agents in the environment under
consideration is their own efforts and outcomes. Some results we find in the experimental
literature on contests run in similar information setting suggests our models predictions
can track observed behavior quite well. This two-parameter learning model has features of
reinforcement and directional learning and can make a more plausible explanation for some
of the experimentally observed behavioral regularities. We analyze this behavioral model via
the discrete-time finite-space time-homogeneous Markov process.
The second chapter is a reinforcement learning model that has features of directional
learning, hence, contributes primarily to the literature on learning in games. This model
is developed for contests, hence, contributes to the literature in contests and applicable
to all-pay auctions as well. This also contributes to the economics literature on similarity,
literature attempting to explain drop-out behavior and a possible explanation to dynamic
behavior in contests like games with over and under dissipation in the intermediate and long
In the third chapter we focus on how cooperation can evolve in humans interacting pri-
vately in a symmetric market where any standard mechanism alone does not achieve stable
equilibrium. Consider the situation in contemporary times where our social and professional
spheres keep shifting rapidly, including some important yet short-term interactions happen-
ing online with strangers. While interacting with others in specific and significant situations
we try to know what kind of person they are. We do this by gathering information about the
person through the accessible sources of information we have. One way to stop free-riding is
to provide interested parties with information on what to expect from a possible interaction.
A reputation system that has good accuracy in predicting what to expect from interacting
with someone can help evolve cooperation.
Some of our interactions are where both the agents privately contribute to the well-being
of each other. What the two parties have contributed to each other is known only to the
economic agents involved. Take, for example, a population of freelancers who collaborate.
Collaboration could be required due to a complementary skill set, experience, or for efficiency
in general. Their cooperation is essential for the efficient completion of the project. Only
they would know how much each of them has contributed to the project. While they are
cooperating, they are also competing for reputation in the network. Their future valuable
interactions would be decided based on their relative reputation in the network. They may
have an incentive not to reveal the truth to others about their and their partners contribution
to the project.
We use this mechanism which generates a score (reputation) for the two agents based on
their contribution in stage 1 and reporting in stage 2. In the next period, agents are matched
assortatively based on these scores. We find that assortative matching facilitated by this
mechanism achieves full contribution and honest reporting as evolutionary (neutral) stable.
The results hold for matching based on the infinite history of scores as well as the finite
history of scores for a range of parametric values.
8 PhD Thesis
Third chapter can contribute towards a real-world implementable approach to achieve
cooperation with simple equilibrium strategies of honest reporting and full contribution
without any social norms like explicit punishment by players.
9 PhD Thesis
Chapter 1
The importance of Probability of Winning
in Risky Choices
1.1 Introduction
The purpose of this chapter is to find experimental evidence with a clearer design for target-
based decision-making in risky choices where subjects trade-off cost with the probability of
winning to reach their desirable chance of winning.
The idea of target-based decision-making is found in the literature with levels of income
as the main focus. Camerer et al. (1997) find that cab drivers keep daily targets on earning,
driving fewer hours on a good day and more hours on a bad day. The authors attribute this
driving behavior to bracketed thinking combined with aspirations. Similarly, studies show
that gamblers tend to shift bets toward long shots in the last race in an attempt to break
even on the day (McGlothlin, 1956). Here, breaking even is a clear and significant reference
point. Such mechanisms (and their combination) of decision-making can give rise to behavior
that is qualitatively different from what is predicted by standard models.
However, in some settings choices are over probabilities of success, not levels. For
example, buying lottery tickets when a prize is fixed. Anecdotal evidence suggests they might
be trading off the probability of winning with either cost or size of the prize to achieve a
target probability of winning. In a two-outcome probabilistic game, a sufficiently favorable
probability of winning each game (one at a time in case these are repeated) can serve as an
aspiration. In this case, only after winning can one achieve payoffs above endowment as a
reference point when there is no explicit need-based reference point on the (net) amount to
be won.
The objective of this chapter is to answer the following questions. A) Do subjects target a
probability of winning in risky choices? B) Do subjects prefer negative skewness relatively
over positive skewness in risky choices? C) Do subjects target a probability of winning in
contests? To help answer these questions, TPW decision-making will first be described;
then, some real-life examples of the behavior it predicts will be given. Subsequently, the
TPW decision rule (Section 3) will be introduced and some predictions it makes with that of
EUMs will be compared (Section 7). Following this, lab experiments will be used to test the
predictions and explore different aspects of behavior (Section 8).
TPW, target probability of winning, emphasizes that the probability of winning in any
decision/game could be an independently important determinant of decision making. If
feasible, subjects will base their decision to engage with a choice set on the relative probability
of winning. This means that they can also choose an option that is second-order stochastically
dominated. This is different from the probability weighting function (CPT) where subjects
perceive the objective probabilities subjectively and a decision is a function of the product of
1.1. INTRODUCTION CHAPTER 1. THE IMPORTANCE OF PROB-
ABILITY OF WINNING IN RISKY CHOICES
this subjective probability with the possible subjective value to be gained.
In the real world, people may have to win (complete task within constraints) multiple
games rather than trying to achieve the highest expected utility with a low probability of
winning in each game. Some intuitive real-life examples can be as follows: addressing an
acceptable research problem for your thesis (or tenure track) which can be completed in the
designated time rather than putting forward the best ideas you can, given the low probability
of getting it completed within the time frame in the latter scenario; in congested cities trying
to reach a nearby hospital in an emergency rather than the best hospital whose location in the
city is less accessible; doctoral students may not even apply to higher-ranking universities for
the position of assistant professors even when these applications require minimal effort.
In these situations, if there is a possibility of changing the chance of success by changing
the cost/effort, some people may make such a trade-off to reach a more desirable chance
of winning. Some intuitive real-life examples can be: doctoral students extend their thesis
by a year or take up a post-doc to improve their chance of getting a position of assistant
professor; students take up a full-time research assistantship after their undergraduate
studies to improve their chance of getting into a good doctoral program, among other
reasons; differences in long-term investment decisionsat later age, people choose a lower
proportion of equities in their portfolio to increase the chances of the net return on the
investment to be above risk-free return at the time of retirement even though equities would
give the highest expected value.
Similarly, in real-life scenarios, keeping everything in one basket to achieve the highest
second-order dominant choiceeven if one can substantially increase the chance of winning
by increasing the effort (cost) but cant make it certainmay not be preferred by some as it
does not leave them with anything if it is a loss. An intuitive example in real life can be seen
in professional choices where very few people opt for taking up a career path that requires
long-term focused effort and (irreversible) opportunity cost on multiple fronts even when
success can transform their careers in the desired direction.
So far, only intuitive examples that are debatable in terms of whether expected utility or
any other mainstream behavioral theory can explain such decisions have been given. It is only
the decision-maker who has a true insight into how they make those decisions. The following
example is more appropriate for allowing discussion based on only observed behavior. There
seems to be a general strategy popular among the lottery players that one should buy more
tickets to increase the chance of winning. This is when, in general, the expected value of the
lottery is negative. There are media reports which claim the following:
1) High earners buy in bulk.
2) People form a syndicate to buy tickets in bulk (and a Wikipedia page suggests that people
do it to enhance their chance of winning).
3) Winners tend to follow the strategy of buying in bulk.
Some players find such advice reasonable.1 It might be the case that when individually
they think they are not able to achieve this then some of them form a syndicate to achieve
this. Buying in a syndicate strengthens the belief that they enhance the chance of winning by
trading off the size of the prize they will share as a reasonable way to approach this game
since the expected value remains the same.
Our main task in this experiment (Part 5) teases out EUMs from possible TPWs and
deconstructs the expected value with variance and skewness. Hence, a clearer experiment
to address aspects of questions A and B is achieved. Evidence on TPWs may resolve the
differences between one set of findings in (finance and psychology) literaturewhich finds
1See Appendix for a screenshot of news cited here (Figure 1.36-1.42).
11 PhD Thesis
1.1. INTRODUCTION CHAPTER 1. THE IMPORTANCE OF PROB-
ABILITY OF WINNING IN RISKY CHOICES
preference for statistical moments (corresponding to some utility maximization) as the
underlying approach subjects take when making a decision in risky choicesand another
set of findings (psychology and finance) where subjects make a decision based on some
contextual behavioral approaches. A possible reason for the two different sets of findings is
that these designs (due to their contextual nature) are dominated by one type of subjects.
This could leave the overall finding confounded which may also be the reason that some
experiments show a preference for positive preference and some for negative. The behavior
in these studies cannot be distinguished, whether subjects are making choices based on
moments (which can be rationalized as expected utility maximization) or the probability of
winning (a behavioral approach).
If the probability of winning is an important determinant in decision making (at least for
some substantial proportion of the population) in risky choices then, among others, it can
contribute to insights into two areas of economics research which are contests and all-pay
auctions where a limited discussion has occurred so far. If subjects are using this approach
to make a decision, then their decision will reflect qualitatively different choices compared
to what is predicted by the standard theory. This motivates us to come up with two (Part 3
and 4) additional tasks to address aspects of question C. In these tasks, the environment is
varied including the information available to the subjects. A challenge of testing any decision-
making approach is that it is contextual; subjects may use a different approach to decision
making depending on how they understand technically the same game presented differently2.
If TPWs are present in contests then it may give insights into behavioral regularities such as
over-dissipation and dropout.
In summary, there are three tasks (Part 5, 4, and 3) to study our questions. In Part 5 (the
lottery task) the subjects choose from a set of lotteries that vary in cost and probability of
winning for a fixed prize which is the same across all the lottery sets (LS). The purpose of
this task is to directly test if subjects make decisions based on EUM or TPW in risky choices.
In Part 4 (the response curve task) subjects play as a second-mover to the pre-populated
choices of the opponent. The purpose of this task is to understand the subjects response
curve. In Part 3 (the contest task) group size varies in the contests. The objective of this task
is to explore how subjects respond to change in the number of subjects in Tullock contests.
These are all one-shot tasks. After quizzes (Part 1), subjects played contests for ten periods
(Part 2) with limited feedback. Part 6 measures risk aversion.
A summary of the results is as follows. In Part 1, most of the subjects can calculate the
probability of winning if the opponents bid is given and can calculate the payoffs based
on whether they would win or lose. In Part 2, most of the subjects actively participated,
responded to the outcome (winning and losing) and the change in the number of subjects in
the game. In Part 3, around 33% subjects consistently decreased their probability of winning
and bid amount upon an increase in subjects in the game while 55% subjects consistently
increased either bid amount or probability of winning. In Part 4, around 15% of subjects
behaved as predicted by expected utility theory, 23% subjects behaved as predicted by the
target probability of winning. In Part 5, 23% of the subjects behavior is as predicted by an
expected utility theory, 36% as predicted by the target probability of winning. In Part 6, it
is found that around 8% of subjects are either risk-neutral or mildly risk-seeking or mildly
risk-averse, 92% of subjects are risk-averse.
Contribution: This chapter contributes to the experimental literature on risky choices that
investigate the probability of winning as an independent criterion in decision making under
laboratory conditions (for example, Edwards, 1953; Edwards, 1954; Slovic and Lichtenstein,
1968; Payne and Braunstein, 1971; Payne, 2005). It implicitly contributes to the literature
2Framing Effect, Kahneman and Tversky (1979), is a testimonial to this
12 PhD Thesis
1.2. RELATED LITERATURE CHAPTER 1. THE IMPORTANCE OF PROB-
ABILITY OF WINNING IN RISKY CHOICES
on bracket-based decision making (for example, Camerer et al. (1997)) and adds to the evi-
dence on non-value-based aspirations. The main contribution is a clearer design (which can
tease out EUMs) to test for subjects making trade-offs with basic risk dimensions to achieve
their TPW. To our knowledge, there has been no previous study that tries to investigate
for heterogeneity of decision making (EUM vs TPW) within one task3. It is found that a
substantial proportion of subjects can be categorized as EUM while a substantial propor-
tion of the population is TPW. This chapter also contributes to the literature on skewness
preference and finds that agents tend to prefer negative skewness over positive skewness. It
may resolve an apparent contrast between the finance literature which suggests that subjects
are EUMs with a preference for skewness and the psychology literature which suggests that
subjects use behavioral approaches to makes decisions which are contextual. This chapter
also contributes to finding such behavioral types in contests. To our knowledge, there are
no previous attempts4 to investigate TPW types in contests. Given more of these behavioral
types have a high target probability of winning, this could be another possible explanation
for the regular finding of over-dissipation in these games. Experiments can be designed to
test such behavioral types in all-pay-auction as well.
The remaining chapter is organized as follows. In the T owards T heoretical Formulation
section, we attempt to come up with a decision rule which can capture the central idea of the
behavior conjectured. In the Related Literature section, some literature is reviewed on risky
choices which suggest that the above behavior is a possibility. In the Main DesignFeatures
section, the design aspects of the main design, which can tease out two broad behavior
types, are discussed. In Experimental Design and P rocedures section, the design of whole
experiment is explicated. In Research Questions, the important questions we would like to
obtain an answer for from this experiment are listed. In the Results section, the insights
from the experimental data are discussed. In the Discussion section, further questions and
designs to test the robustness and understanding of the findings are considered.
1.2 Related Literature
There is some evidence in the literature that the probability of winning is important in
risky choices suggesting that subjects make a trade-off with cost, wherever feasible, to reach
their suitable probability of winning in the decision making (for example, Edwards, 1953;
Edwards, 1954; Slovic and Lichtenstein, 1968; Payne and Braunstein, 1971; Payne, 2005).
But these pieces of evidence are either debated due to their design or find their explana-
tion in some mainstream theories. Edwards (1954) puts forward subjective probability
which now is known as probability weighting to explain Edwards (1953, 54), while Decidue
(2008) explains Payne (2005) with a discontinuous value function. Similarly, the evidence on
preference for skewness is inconclusive. Both Golec and Tamarkin (1998) and Garret and
Sobel (1999) find evidence for positive skewness, whilst Symmonds et al. (2011) and Taleb
(2004) find evidence for negative skewness. This could be because skewness is correlated
with either expected value or variance or other moments which may also be present in the
distribution, thereby finding its explanation in EUM rather than as an independent criterion
of decision making. There seems to be no literature which studies evidence for the probability
3This is in a spirit similar to Harrison and Rutstrom (2009) who find support for data in the experiment
being generated by two types of the decision process with two different underlying theories
4To our knowledge this is the first study that has hypothesized probability of winning as an independent
criterion of decision making in winner-take-all competitions. An early version of the current second chapter
(with the previous title Learning in Contests) with these insights was poster-presented in GW4 Game Theory
Workshop in May 2016 and in conference Contests: Theory and Evidence in June 2017
13 PhD Thesis
1.2. RELATED LITERATURE CHAPTER 1. THE IMPORTANCE OF PROB-
ABILITY OF WINNING IN RISKY CHOICES
of winning in contests before conducting this experiment. A topic-wise detailed literature
review is conducted below.
Suggestive Literature on TPW
The probability of winning can be an important psychological factor of risk. Allais (1952/1979)
emphasizes the factors of psychological risk in his example of a traveler who may choose a
gamble which gives him the greatest chance of winning an amount equal to the price of the
ticket they need to return home. It emphasizes the shape of a utility function, probability
weighting, and dispersion as psychological factors of risk and expected utility as a monetary
factor of risk. The example suggests that the probability of winning can be another important
factor in decision-making in risky choices.
There is some suggestive evidence from zero expected value lottery tickets, in a lab
experiment, that agents have a preference for the probability of winning. Edwards (1953)
designed the lotteries which have the same expected value to understand what makes subjects
deviate from the expected value. A set of gambles is designed with a monotonically increasing
probability of winning and decreasing prize value such that the expected value of all the
gambles remained the same5. Three such sets are designed. The first is in the domain of gain
having a positive expected value. The second is in the domain of losses having the negative
expected value. The third is neutral having an expected value as close to zero. Each set of
gambles had three parts, Part A, B, and C. Part A is based just on imagining. Part B is based
on worthless-chip. Part C is based on real gambling. The choices of all the parts are highly
correlated. In the real gambling choices based on the choice distribution of all the subjects it
is concluded that in positive and zero expected value gamble sets, subjects have a preference
for the probability of winning. The middle gambles (with probability 3/8, 4/8, and 5/8) are
chosen more than other gambles. In the negative expected value set, the choices generally
decreased as the probability of winning increased.
Similar suggestive evidence is found in the non-zero expected value gamble experiment
in the lab. Edwards (1954), following Edwards (1953), designed an experiment to identify
variables that determine choices among bets which differ from one another in expected value.
Three sets of gambles are designedpositive, negative, and near zero. Based on the results,
it can be concluded that subjects do not consistently prefer bets with higher expected values
to bets with lower expected values and part of the variation can be predicted by probability
preferences. Edwards (1954) gives subjective probability as a possible explanation for the
probability preference; that is, the objective probabilities stated in the gambles are perceived
subjectively by the subjects and the subjects are trying to maximize subjective (weighted
probability) expected utility6.
Subjects may use an alternative approach to understand and take gambles which can
make choices qualitatively different from EUT. They may evaluate a gamble using some
basic independent dimensions and seek a preferred trade-off between those. Slovic and
Lichtenstein (1968) characterize a gamble as a multidimensional stimulus with four basic risk
dimensionsthe probability of winning, amount to win, probability of losing, and amount
to lose. This design approach focuses on the relative importance of the basic risk dimensions
and how people use them. It gives an alternative explanation of subjects preferring low
probability of winning gambles in Coombs and Pruitt (1960), possibly because these (zero
expected value) gambles have the higher winning amount or lower losing amount (their
argument does not apply for the preference for gambles with a high probability of winning).
They use the same explanation (amount of winning and amount of losing) to explain variance
5One of the lottery sets in the main design in our experiment is similar to this
6In our main design, behavior in three lottery sets can help to gauge the possibility of such an explanation
14 PhD Thesis
1.2. RELATED LITERATURE CHAPTER 1. THE IMPORTANCE OF PROB-
ABILITY OF WINNING IN RISKY CHOICES
preference. Their results show that subjects bids were influenced considerably more by
variation in the probability of winning and amount of losing than by variation in the amount
of winning or probability of losing when subjects have to choose between the duplex gamble.
It is seen that subjects ratings or bids monotonically increased as the probability of winning
increases and monotonically decreases as the probability of losing increases. These results
are attributed to an information processing model where subjects believe the probability of
winning is more important than other risk dimensions rather than subject having specific
preference7 for probability.
Similarly, Tversky (1969, 3/33) argued that one subject may conceptualize (two-outcome)
gambles in terms of odds and stakes, while another may view them in terms of their ex-
pectation, variance, and skewness. Tversky used specially constructed sets of gambles to
demonstrate that subjects use a choice process termed as lexicographic semiorder (LS)
that is qualitatively incompatible with expected utility maximization. Payne and Braunstein
(1971) explore the relative merit of the basic risk dimensions and the underlying distribu-
tions as explanations for decisions. Their study involves the use of pairs of gambles that
display different values for the risk dimensions, but which are equal in their underlying
distributions8. To explain the results, they propose an information processing model in
which subjects first examine probability information and use it to exclude gambles having an
unacceptable probability of winning.
Some known behavior effects could be specific examples suggesting that the probability
of winning does matter for some in decision-making in risky choices. Kahneman and Tversky
(1979) show possibility effect (no chance of winning less preferred to some chance of
winning) and certainty effect (some chance of losing less preferred to no chance of losing)
in decision making where subjects choose the option with lower mathematical expectation.
It possibly reflects (in two extreme cases) that the probability of winning and losing can be
an important factor in decision making and if subjects can trade-off expected value/cost with
(substantial) increased chance of winning, they do so.
Lopes (1981) questions the interpretation of expected utility theory by von Neumann and
Morgenstern regarding whether subjects combine values (utilities) and probabilities ever,
except in the long run. She discusses the idea of whether the only rational measure of the
worth of a gamble is its expected value (utility). She argues that in decision making it is
reasonable to consider the probability of success in the short-run compared to expectation-
based decision-making in long-run situations. Lopes (1987, 1996, 1995, 1999) cites multiple
experimental studies conducted after Lopes (1981) which confirm the difference between
the choices made by the subjects when they playing a one-shot game compared to the same
game being played repeatedly. It concludes that an adequate descriptive theory of risk-
taking will need to be a dual criterion theory. It formulates the SP/A theory that combines a
decumulative weighting process with a process that maximizes the probability of achieving
an aspiration level. It claims that the dual criterion theory does a creditable job of describing
both preferences and reasoning patterns across a wide variety of behavioral phenomena.
Some more recent evidence confirms that the probability of winning can drive the decision-
making in risky choices. Payne (2005) does a simple test of the expected utility model, the
original prospect theory, and cumulative prospect theory in a value allocation task. Subjects
are provided with an opportunity to improve a gamble (for example: $100, 0.20; $50, 0.20;
$0, 0.20; -$25, 0.20; -$50, 0.20) such that they can change the overall probability of gain or
loss. Subjects are given a value (say $38) and must choose one of the two options (say $100
7Preference here means the intrinsic preference across the games (e.g., risk preferences). To avoid such
confusion of terminology we preferred to use the abbreviation TPW (i.e., target probability of winning)
8Note, a tradeoff between EV and probability of winning is not tested in this design but is present in our
main design
15 PhD Thesis
1.2. RELATED LITERATURE CHAPTER 1. THE IMPORTANCE OF PROB-
ABILITY OF WINNING IN RISKY CHOICES
and $ 0 from the above gamble) they would like to add to this value. The experiment finds
that subjects were highly sensitive to changes in outcome values that either increased the
overall probability of a strict gain or decreased the overall probability of a strict loss. It is
concluded that the experimental result supports the hypothesis that subjects focus on the
overall probability of success, which is in contrast with expected utility and prospect theory.
Venkatraman, Payne, and Huettel (2014) use the value allocation task in multi-outcome
gambles involving possibilities of both gains and losses and find that subjects often maximize
the overall probability of winning. Zeisberger (2016), in a series of experiments, finds that
people pay explicit attention to the probability of losing and their willingness to take risks
and choice behavior is considerably influenced by loss probabilities, while performance
feedback seems unable to mitigate this effect.
EUM and Skewness
The economics approach to decision-making in risky choices is the expected utility maxi-
mization which depends on the curvature of the utility function (Pratt, 1964). The finance
approach to decision-making in risky choices is based on statistical moments of the un-
derlying distribution. Markowitz (1952) shows that the approach of mean-variance is the
expected utility maximization if the distribution is normal. Roy (1952) defines the safety
first principle that calculates the probability of returns of the portfolio going below the
desired threshold as a measure of downside risk. The optimum portfolio will be the one that
minimizes this probability. Such a measure of downside risk has been further generalized as
a probability loss risk measure for making risky choices. The major problem with probability
loss risk measure is its failure to distinguish increasing downside risk from other properties
of distributions (e.g., mean, variance, skewness).9
Tsiang (1972) asserts that as the ratio of risk (standard deviation) to the mean value of
total wealth increases, the accuracy of mean-variance analysis decreases, and higher-order
central moments in a particular third would have to be taken into consideration for an
appropriate utility function for a risk-averse individual.10 It shows that subjects with such
utility functions will have a preference for positive skewness (U  > 0) if the phenomenon of
increasing absolute risk aversion is regarded as absurd. Scott and Horvath (1980) show that
investors exhibiting positive marginal utility of wealth for all wealth levels, consistent risk
aversion at all wealth levels, and strict consistency of moment preference will have a positive
preference for positive skewness (i.e. U  > 0 )
In the wide range of economic models (e.g., gambles, auctions, and contests) individuals
decisions under risk can be understood as trade-offs between mean, variance, and skewness.
Chiu (2010) establishes a skewness-comparability condition on probability distributions
that is necessary and sufficient for any decision-makers preferences over the distributions
to depend on their means, variances, and third moments only. The study generalizes the
condition for two distributions to be comparable in terms of downside risk, establishing that
all Bernoulli distributions are mutually skewness comparable. The degree of skewness is
determined only by the probability of the lower possible outcome. The utility preferences
9The main design (Part 5) of this experiment does indeed distinguish it from the first two moments.
10As mentioned in Tsiang (1972), according to Arrow an appropriate utility function for a risk-averse in-
dividual should have the following essential properties: (a) U (y) > 0, i.e., the marginal utility of wealth
is positive; (b) U(y) < 0, i.e., the marginal utility of wealth decreases with an increase of wealth; (c)
d[U(y)/U (y)]/dy < 0, i.e., marginal absolute risk-aversion should, if anything, decrease with an increase
in wealth; (d) d[yU(y)U (y)]/dy > 0, i.e., marginal relative (proportional) risk-aversion should, if anything,
increase with an increase in wealth. These properties are satisfied by a negative exponential function, constant
elasticity utility function, log function, among others. Polynomials as utility functions cannot satisfy these
requirements at the same time.
16 PhD Thesis
1.2. RELATED LITERATURE CHAPTER 1. THE IMPORTANCE OF PROB-
ABILITY OF WINNING IN RISKY CHOICES
can be described by the preference over three moments.
Evidence on Preference for Skewness
In the below papers there is differing evidence on preference for skewness in lab experiments
as well as natural empirical data. The evidence ranges from a preference for positive skewness
to negative skewness and a preference for skewness to no skewness. Coombs and Bowen
(1971) construct gambles with the same underlying mathematical expectation and variance
but different skewness. The choices made by the subjects show that their decision (perceived
risk) was a function of skewness as well as mathematical expectation and variance even
under multiple plays.
Some studies (Arditti, 1967; Levy and Sarnat ,1972; Krauss and Litzenberger, 1976) have
found the coefficient for the second moment to be positive and statistically significant. This
was interpreted to mean that a higher return tends to go together with prospects with a
higher variance 11 and investors prefer a positive asymmetry. The coefficient for the fourth
moment was significant only in few cases, and the coefficient for higher moments was always
insignificant.
Golec and Tamarkin (1998) find that betting behavior at horse tracks is explained by
expected utility functions which consider mean, variance, and skewness of the returns. It
finds that bettors are risk-averse, but are attracted to the positive skewness of returns offered
by low probability, high variance bets. Garret and Sobel (1999) find that lottery players are
also risk-averse but favor positive skewness. In contrast with risk-measures focusing only on
the chance of poor outcomes, Symmonds et al. (2011) use a multi-outcome gamble to test
preference for statistical moments. The study finds the mean-variance-skewness model as
the best fit. The majority of subjects were variance averse and seeking negative skewness. In
line with this conclusion from the perspective of financial markets, Taleb (2004) also lists the
areas where traders have a preference for negative skewness.
Brunner, Levinsky and Qiu (2011) experimentally test skewness preference at the individ-
ual level. The experimental approach allows to directly control the payoff distributions faced
by the subjects. Their definition of skewness preferences follows the definition of Tsiang
(1972) that an expected utility maximizer reveals skewness preferences if the third derivative
of the utility function is positive. The subjects choose one of the two gambles provided each
time. The researchers find that skewness of the distribution has a significant impact on
the decisions. Around 40% prefer skewness (positive and negative) and around 10% avoid
skewness.
Astebro, Mata, and Santos-Pinto (2015) study how the presence of skewness influences
the risk attitudes of experimental subjects. Using three sets (with different non-negative
skewness) of ten pairs of choices (similar to Holt and Laury, 2002) each with multiple out-
comes, they find that when the choice task includes a positively skewed lottery, subjects
make riskier choices. Additionally, estimated parameters of power utility (crra) function find
no evidence for risk-loving; rather, skew seeking is attributed to optimism and likelihood
sensitivity.
One-Shot vs Multi-Period Games
Even if subjects are expected to be utility maximizers, they can behave differently in the
one-shot game to how they would in a repeated game. Ross (1999) shows that there is a large
class of utility functions (including crra utility functions, but excluding exponential and
risk-neutral utility functions) that accepts a long enough sequence of independent good bets
11In the main design higher mean are separated with higher variance for two sets of choices
17 PhD Thesis
1.3. TOWARDS THEORETICAL
FORMULATION
CHAPTER 1. THE IMPORTANCE OF PROB-
ABILITY OF WINNING IN RISKY CHOICES
due to the law of large numbers, any one of which considered individually would be rejected.
This motivates us to prefer a one-shot design as our starting point of investigation. In our
design, subjects play lotteries and raffles which have more choices than simply accepting or
rejecting: they can choose their bid amount.
1.3 Towards Theoretical Formulation
In an attempt to give more clarity to what is being conjectured we formulate a decision rule,
stated below, to express the behavior. The domain of the decision rule is an environment
where there is a feasibility of probability-cost trade-off with the framing of two-possible
outcomes as winning and losing.12 Given a lottery game, the TPW is taken as exogenous13. It
possibly depends on various factors like the endowment of the agent, the cost structure of the
lottery, and the number of total lotteries and prize money. What we conjecture is that if these
factors remain approximately the same then a TPW subject will make a similar choice. Our
basic conjecture emphasizes that people try to achieve a threshold probability of winning
even if it is a second-order stochastically dominated choice.14
This is in no way to claim that the stated decision rule is exactly how people make
decisions in risky choices. Not everyone is the same and decision-making also depends on the
environment. For example, in the case of a choice between a pair of gamble options, it may
not be feasible for subjects to achieve their TPW; in which case, they may decide differently
while making a choice. For example, the value of the amount which can be won/lost or a
substantial difference/absolute values of probabilities of the two paired gambles may become
salient.
Decision Rule: min(n) s.t. nP (L)  T PW
where
n is the number of lottery tickets
P (L) is the probability of winning achieved by one lottery (L) ticket
T PW is the target probability of winning agent desires for this lottery game
It is not possible to fully ascertain how exactly subjects arrive at their target probability
of winning. Yet, this decision-making can be captured even without such knowledge if an
environment is considered that has a possibility of a more continuous trade-off between
the two. In such an environment, for a specific game, a simulation of such behavior can be
generated as agents try to achieve some minimum chance of winning and see whether they
go beyond that as it is costly. Figure 1.0 (x-axis: cost, y-axis: the probability of winning)
captures such an environment.
Now, returning to the lottery example, let us discuss the decision rule and prediction it
makes which is in contrast to two mainstream theories. Can individuals buying lotteries
in bulk, but not spending all their spendable income on it, be explained using expected
utility maximization (EUM) or cumulative prospect theory (CPT)? It seems that these lottery
players make a trade-off decision between cost and chance of winning. Although players
12It might apply to limited multi-outcome games and lotteries but will require separate study to investigate.
13This is a critical simplification and beyond the scope of this work
14As Roy (1952) notes, a valid objection to much economic theory is that it is set against a background of ease
and safety. To dispel this artificial sense of security, theory should take account of the often close resemblance
between economic life and navigation in poorly charted waters or man-oeuvres in a hostile jungle. Decisions
taken in practice are less concerned with whether a little more of this or of that will yield the largest net increase
in satisfaction than with avoiding known rocks of uncertain position or with deploying forces so that, if there is
an ambush round the next corner, total disaster is avoided. If economic survival is always taken for granted, the
rules of behavior applicable in an uncertain and ruthless world cannot be discovered.
18 PhD Thesis
1.3. TOWARDS THEORETICAL
FORMULATION
CHAPTER 1. THE IMPORTANCE OF PROB-
ABILITY OF WINNING IN RISKY CHOICES
Figure 1.0: Indifference Curves
may enhance their chance of winning by buying more lottery tickets, at the same time they
do not pour all their money into doing that. Hence, in the first-order they seem to value the
chance of winning up to a certain threshold and in second-order value the cost. In the case of
buying in bulk in a syndicate, the trade-off is between the chance of winning and the size of
the prize they will share as an individual.
Let us consider these lotteries with a prize value equal to 100 which is equal to the
endowment (spendable income). The TPW indifference curve would be a vertical line
starting from the point in the lottery curve that gives this probability of winning. The family
of curves will be parallel vertical lines on the right. Note, if the cost does not greatly differ
across lottery curves then indifference curves for a specific TPW across different lottery
curves will be close. 15 The indifference curves (Figure 1.0) for EUMs are inclined straight
lines. It is seen that for these lotteries with slightly different cost structures (compared to
constant expected value (=100) lottery line LS 1) the choice predictions for LS 2 and 3 are at
the extreme opposite end. In general for a family of lottery sets (LS 2 and 3), any point on the
curve can be considered as an element of the set of lotteries represented by this curve, the
EUM prediction will be one of the endpoints. The indifference curves for prospect theory are
based on the convex weighting function with loss aversion equals 2.25 and endowment as
the reference point. These predict that the high probability end lotteries will be chosen.
Nonetheless, it is important to note that there can be various qualitatively different
indifference curves for PT due to different parametric functionals. Although it is difficult to
rule out the possibility that another existing EUM or CPT model can explain this behavior
(some of such (non-standard) CPT models and their limitations will be discussed), we find
that a CPT model with parametric values of Tversky and Kahneman (1992), as used in the
literature on risky choice, with an endowment as the reference point16 gives a prediction
similar to standard EUM. A list of possibility of alternative explanations are discussed in
Section 8 of this chapter for the results in the main design.
15This is an approximation in itself. This means that if cost changes substantially players are likely to consider
this factor. However, with a small margin of change, they will choose a similar probability of winning.
16One of the challenges in applying PT is that it is not clear what should be the reference point (Barberis,
2013). In a lottery with only two possible outcomes (single prize), it seems reasonable to take endowment as a
reference point.
19 PhD Thesis
1.4. MAIN DESIGN-FEATURES CHAPTER 1. THE IMPORTANCE OF PROB-
ABILITY OF WINNING IN RISKY CHOICES
1.4 Main Design-Features
As reviewed above, on the one hand, literature in finance suggests that subjects are expected
to be utility maximizers, attracted to the first and the third moment and with an aversion
towards the second moment. On the other hand, literature in psychology finds subjects
having some preference for the probability of winning (equivalent to skewness in Bernoulli
distribution), preference for statistical moments, and use of some information processing
approach to make decisions.
The main task central to our study is framed such that variance and skewness for any
lottery do not vary across the three lottery sets (LS1-LS3) and only expected value changes
as shown in Table 1.1-1.3 below. In these tables apart from self-explanatory headers, EV
denotes the expected wealth of the gamble (including the endowment of 100), Var denotes
variance of wealth and Skew denotes its skewness.
Table 1.1: Lottery Set 1
Table 1.2: Lottery Set 2
Table 1.3: Lottery Set 3
LS 2 and 3 are second-order stochastically dominant lotteries in the respective lottery
sets. There is no second-order stochastic dominance among lotteries in LS 1. In this task,
between two adjacent lotteries, the risk dimensions (probability of winning, probability of
losing, winning amount, and losing amount) change gradually over lottery sets with moments
varying in a pattern easily detectable even if values are difficult to calculate.
In one of the lottery sets, LS 1, the expected value is zero for all the lotteries in the set.
The pair LS 2-3 can help categorize EUMs and LS 1 can test if they choose positive skewness
20 PhD Thesis
1.5. EXPERIMENTAL DESIGN AND
PROCEDURES
CHAPTER 1. THE IMPORTANCE OF PROB-
ABILITY OF WINNING IN RISKY CHOICES
over negative skewness since for each value of variance there are two lotteries in the set, one
with positive skewness and another with negative skewness.
A design approach of choices between pair-wise gambles is good in understanding whether
something is a significant factor in decision making. Many real-life situations are not the
choices between a pair, rather a series of options varying such that there is a trade-off among
the decision-making factors. In this sense, a lottery set has an advantage over pairwise choices
to better depict a realistic situation.
A lottery set design also has an advantage over pairwise choices given to subjects by
removing possibilities of different salience guiding choices between different pairs. For
example, the choice between a pair which is close to each other in any lottery sets can be
due to the salience of the amount to be won or amount to be lost while the choice between
any pair far apart in any lottery set could be due to the probability of winning (losing). This
design makes lotteries easier to compare among themselves.
There is evident salience in LS 2 and 3. In the first lottery (L1) in LS 2, the proportional
cost (per unit probability of winning) is minimum. The case is similar to the last lottery (L9)
in LS 3, in which the proportional cost of the last lottery is minimum. These two lotteries are
the optimal choice for a non-risk-seeking (measured risk preference found in the experiment)
expected utility maximizer.
The expected value (first moment) in LS 1 does not change, in LS 2 it decreases monotoni-
cally and in LS 3 it monotonically increases from top to bottom lottery. Variance is symmetric
and decreases on either side away from the middle lottery. The skewness of the middle lottery
is zero and is symmetric and increasing in magnitude as one moves away from the middle
lottery on either side with top lotteries positively skewed and bottom lotteries negatively
skewed. The variance and skewness of each lottery are the same across the three lottery sets.
In LS 2 and LS 3, optimal choice based on expected value and variance is in the opposite
direction to skewness. The subjects who are predominantly driven by the probability of
winning should not change their choice across the lottery sets LS 1 to 3. There is no lottery
with the certainty of outcome in either of the lottery sets. This is to eliminate choices due to
the certainty effect and avoid any abrupt change in the pattern of the moments.
The lottery set provides a series of probability of winning (losing) options, unlike many
other studies where designs have limited values of probability of winning. This is to frame a
gradual trade-off between cost and probability of winning for a fixed prize which captures
many real-life situations. While there is a gradual trade-off between the expected value and
probability of winning in LS 2 these are in the same direction in LS 3.
These features can help to gauge which are major risk dimensions of decision-making if
subjects are driven by some behavioral approach. The fact that only two dimensions, cost,
and the probability of winning, change gradually, as one moves down any lottery with the
salience of L1 in LS 2 and L9 in LS 3, makes the task simple to comprehend. A presence
of increasing or decreasing stochastically dominant lotteries and separately measuring risk
aversion allows these subjects to be classified into broad types.
1.5 Experimental Design and Procedures
This is a within-subject design. As discussed above, the main task is framed as lotteries
with a fixed prize where subjects have to choose one of the nine lotteries in each lottery set.
The other two tasks are framed as a raffle where the choices made relate to the number of
tickets (bid amount) they want to buy. Three sessions are conducted on consecutive days
from 5 to 7 June 2018 with 24 students in each. All the subjects are undergraduate students.
All the sessions lasted for around 75 minutes. The average earning is around 15.5 pounds.
21 PhD Thesis
1.5. EXPERIMENTAL DESIGN AND
PROCEDURES
CHAPTER 1. THE IMPORTANCE OF PROB-
ABILITY OF WINNING IN RISKY CHOICES
For further details refer to the instructions sheet in the Appendix section. The descriptive
statistics of the payment subjects received can be found in the Appendix (Table 1.43). The
experiment is implemented using the experimental software z-Tree (Fischbacher, 2007).
The experiment consists of six parts. In Part 1, subjects are provided with an explanation
about the game of raffle by asking them six basic questions on calculating the probability
of winning and payoff of the raffle upon winning and losing. The answers along with the
explanations were provided after each set of three questions so that the underlying principles
of the game were clear. In Part 2, a recap on the raffle game and its procedure was provided
before we matched subjects in a fixed group of two for five rounds followed by a fixed group
of three for five rounds. Subjects received the information on the outcome of each round. The
purpose of this part is to give subjects experience of the game with a change in group size.
This is to ensure that subjects have a good understanding of the game as the following part
consists of a one-shot game. In Part 3, the subjects played the game of raffle with a change in
group size. In the first two games, their group size doubled in the second period, while in
the last two games their group size halved in the second period. The change in group size is
made such that there is a minimum change in the number of new partners encountered. This
part is designed is to understand how the subjects change their probability of winning and
bid amount as their group size changes. In Part 4, subjects are matched with another subject
and have to submit their response against the set of pre-populated opponents bid amounts.
These choices are used to categorize the subjects response curve. In Part 5, subjects have to
choose a lottery from the set of lottery choices that have varied costs and the probability of
winning ranging from 0.1 to 0.9. There are three sets of such lotteries. In Part 6, the subjects
risk aversion is measured.
The primary parts of the experiment (Part 3, 4 and 5) were organized as one-shot games.
This preference for one-shot games over the repeated game is to decrease the impact of any
learning as it is not clear what and how they will learn. It is unclear how the winning and
losing will impact the decision-making approach subjects may have. It is suspected that
subjects may approach the game differently when making decisions in repeated games.
One may question if the subjects understood the one-shot games. The explanatory
instructions are given, but the constraint remained that examples could not be given in the
instructions as this may have an anchoring effect. In the questionnaire at the end of the
experiment, subjects were asked if the instructions were clear to them. The response in
the survey suggests that the instructions for any part of the experiment were clear to most
of the subjects. The analysis includes data only for the subjects who have stated that they
understood that part of the experiment.
One general concern is whether the order of the parts of the experiment impacts the
results and why the order is not randomized over three sessions. Even though the underlying
contests game is the same, after Part 2 all games are one-shot and framed differently; hence,
it is less likely that any significant learning would take place. Subjects may enhance their
understanding of the contest game as they proceed in the session which means that it is
better to put easier experiments first. The experiment is not concerned with comparing any
treatments; rather, the focus remains on the proportion of EUM and TPW types in each part.
The rationale of Parts 1 and 2 is largely to give subjects an understanding of the game. It is
logical for Part 3 to follow Part 1 and 2. As Part 4 is on raffles and is more complicated in
terms of instructions so it followed Part 3. Part 5 is on lotteries, so it followed all the parts
which were based on raffles. Part 6 is to measure risk-aversion, so it is kept at last. In Part 2,
the order of contests is not changed, having group size 2 with contests having group size 3.
The reason is that only three sessions are run and no inferences are drawn from the parts
which are designed for enhancing the understanding by giving subjects some experience of
the game.
22 PhD Thesis
1.5. EXPERIMENTAL DESIGN AND
PROCEDURES
CHAPTER 1. THE IMPORTANCE OF PROB-
ABILITY OF WINNING IN RISKY CHOICES
In Part 3, subjects are simply asked what they consider their target probability of winning
to be. One may like to extend further and get incentivized elicitation of the sum of other
subjects effort and use both the inputs to check for the consistency of beliefs. However,
using a modulus or square error to elicit opponents effort will distort the incentives of the
primary contest. The risk symmetric Nash equilibrium does not change if such an elicitation
mechanism is applied but based on data from other experiments in the Tullock contest it
is known that subjects deviate from standard theoretical predictions, in general, they are
overbid. In this case, subjects know that they would possibly incur losses in predicting
other subjects efforts. So in response, they may change their actual bidding behavior. If
they are loss averse, then they may further reduce their bids. If subjects are relative payoff
maximizers then they may increase their bid even further so that the opponent incurs losses
while predicting. Moreover, under behavioral factors (e.g., optimism and pessimism) at play,
the consistency approach is not appropriate. Lets say the subject believes (in a two-person
contest) that the opponent would put 50 and her actual bid amount is 50 and she puts in
her probability of winning to be 70%. These choices can be explained by assuming that the
subject is optimistic rather than being interpreted as inconsistent. The value of interest is
70%, not 50%. These are the reasons for simply asking subjects to input their estimated
probability of winning assuming that is what they aimed for.
In Part 3, the assumption is that the joy-of-winning does not change as group size changes.
This rules out a possible explanation of subjects increasing their bid amount as group size
increases. The reason for not having a design for capturing the joy of winning was to keep
the scope of the experiment within its set objectives. Nonetheless, subjects are asked in
the questionnaire if they experience any joy in winning. The distribution is listed in the
Appendix (Table 1.44). It is found that almost half of the subjects say that they do not have
any joy in winning and the other half say they have somewhat. In Part 4, subjects are matched
with another subject across raffles, so the joy of winning is the same across each raffle which
should not change the shape of the response curve. In Part 5, subjects are playing against a
computer and the interest is in how close the lottery choices are in the three different lottery
sets. In LS 1 and 3, the EUM predictions for agents with some additive joy-of-winning are
incorporated in the last lottery (L9).
It is useful to think about how the target probability of winning is different from the
joy of winning and whether it can explain the behavior observed in different parts of the
experiment. In the three main parts of the experiment, the inference is drawn based on their
relative behavior in different raffles/lotteries in that part. It is reasonable to assume that
the joy of winning does not change significantly for different raffles/lotteries in any part of
the experiment which rules it out as a possible explanation for the difference in behavior.
Described below is each part in more detail including the tables used. For further details
please see the instructions sheet and z-tree screenshots.
Part 1 (Quizzes) Two quizzes with 3 questions each are framed in such a way that subjects
understand the underlying game of raffle across the parts of the experiment and how to
calculate the probability of winning, compute the payoffs upon winning and losing, and
interchange notation of percentage and real number to express probability.
Part 2 (Experience) This part of the experiment aimed to give subjects some experience of
the raffle game. In the first five rounds, subjects were matched in a fixed group of two players,
and in the last five rounds, they were re-matched in a fixed group of three players. The sub-
jects knew about the number of tickets they bought and the outcome at the end of each round.
Part 3 (Group Size Change) In this part of the experiment, four games of two periods each
23 PhD Thesis
1.5. EXPERIMENTAL DESIGN AND
PROCEDURES
CHAPTER 1. THE IMPORTANCE OF PROB-
ABILITY OF WINNING IN RISKY CHOICES
were played as shown in the table below. In the first two games, the group size was doubled
in period 2 by amalgamating the two groups from period 1. In the last two games, the group
size was decreased to half by splitting the groups formed in period 1. A separate screen was
used to obtain the input for each game each period. In the first period of each game, subjects
were told about what follows in period 2 and in the second period of the game subjects were
told about what they choose in period 1. This is done to give them a perception of actual
periods. The outcome of each game is presented only at the end of all the games in this part.
This is done to avoid any impact of intermediate winning and losing during this part of the
experiment. Subjects played Part 3 based on whatever understanding and learning they had
by the end of Part 2. Table 1.4 below is used in the instructions sheet to describe this part of
the experiment.
Table 1.4: Design summary for Part 3
Part 4 (Response Curve) This part of the experiment aims to capture the response curve of
the subjects. Subjects are presented with the pre-populated choice of the opponent in each of
the six raffle games. Subjects are matched with another subject and all are given the same
pre-populated choices to enter their choices against them. It is created in this way to give
subjects a perception that they will be winning or losing against another subject rather than
a computer. All the choices are entered on one screen. Table 1.5 below describes this part of
the experiment.
Table 1.5: Design summary for Part 4
Part 5 (Inverted Lotteries Sets) A brief description of the main features of this part is as
follows. Three sets of lotteries are constructed each having the same prize value. Each set has
nine lotteries to choose from with their probabilities ranging from 0.1 to 0.9. The variance
and skewness are the same across the three lottery sets. The expected value is the same for all
lotteries in LS 1. The expected value is highest for the first lottery (L1) in LS 2 and decreases
towards the last lottery choice (L9). The expected value is highest for the last lottery (L9) in
24 PhD Thesis
1.6. PREDICTIONS CHAPTER 1. THE IMPORTANCE OF PROB-
ABILITY OF WINNING IN RISKY CHOICES
LS 3 which decreases towards the first lottery choice (L1). Table 1.6 describes this part of the
experiment.
Table 1.6: Summary design for Part 5
Part 6 (Risk Preference) This part of the experiment aims to measure the risk aversion of
the subjects using Holt and Laury (2002). Table 1.7 (Appendix) describes this part of the
experiment.
1.6 Predictions
Following are the predictions for some parts of the experiment design. The order of the parts
is changed compared to the one in the experiment to examine first the part which is central
to our experiment.
P rediction 1 : In Part 5 (Inverted Lotteries Sets), if subjects are driven by EUM (smooth and
continuous standard utility functions) and they are not risk-seeking then, irrespective of the
risk preference, they will choose L1 in LS 2 and L9 in LS 3.
P rediction 2 : In Part 5 (Inverted Lotteries Sets), if subjects are driven by TPW then their
choices will remain largely stable across all three lottery sets.
P rediction 3 : In Part 5 (Inverted Lotteries Sets), if subjects have a preference for positive
skewness then they will choose lotteries in the upper half of the LS 1 and if they prefer
negative skewness then they will choose in the lower half of the LS 1. This is because the
expected value of all the lotteries in LS 1 is the same with variance decreasing as one moves
away from the middle lottery. For any value of variance, there are two choices, one with
positive skewness and the other with negative skewness.
P rediction 4 : In Part 4 (Response Curve), expected utility theory predicts that subjects
best response curve will be like the shape of a rightly skewed inverted parabola while TPW
predicts that subjects response will be in a straight line of the positive slope until they drop
out. The below graph (Figure 1.8) shows the prediction of two competing theories for discrete
values greater than equal to 10. The red curve is the response predicted by standard theory
while other curves are based on subjects been driven by the TPW.
P rediction 5 : In Part 3 (Group Size Change), the expected utility theory predicts that
subjects decrease their bid amount as the group size increases. Regarding TPW prediction,
this depends on whether subjects increase or decrease their target probability of winning as
25 PhD Thesis
1.7. RESULTS CHAPTER 1. THE IMPORTANCE OF PROB-
ABILITY OF WINNING IN RISKY CHOICES
Figure 1.8: Best Response curve and TPW predictions
the group size increases. TPW predicts an increase in bid amount if subjects do not decrease
their minimum probability. The inference is drawn based on the direction of change rather
than the point prediction.
1.7 Results
In this section, the main results of this experiment are briefly discussed. The results of each
section will be covered in an order different from the experiment conducted; rather, the first
parts are designed to make subjects understand followed by more degree of objectivity in
other parts. First results are presented on how subjects performed in the Quizzes (Part 1)
then the description of the behavior in Part 2 is designed to give experience to the subjects.
Then the distribution of the risk-preference is measured in Part 6. Following this, choices
made in Part 5 are analyzed which is central to this experiment. Subsequently, the qualitative
behavior in Part 4 is studied. This is followed by an analysis of the impact of group size
change on the bid amount and probability of winning of subjects in Part 3. As previously
mentioned, all the results are based only on the choices of subjects who stated that they were
clear about the instructions for that part of the experiment.
Part 1 (Quizzes) Quiz 1 and Quiz 2 each consist of 3 basic questions (see Instructions in
Appendix) related to the underlying contests. Quiz 1 was followed by a detailed explanation
of the answers before the start of Quiz 2 which has similar questions to those of Quiz 1.
Table 1.9 shows the number of questions answered correctly. Out of 72, 62 subjects answered
all the questions correctly in both the quizzes. In Quiz 1, 64 subjects answered all the
questions correctly, which increased to 69 in Quiz 2. This suggests that almost all the subjects
understood how to calculate the probability of winning and how to calculate payoff upon
winning and losing.
Table 1.9: Response summary for Quizzes
Part 2 (Experience) The average behavior of the subjects in the 10 rounds is given in Table
1.10 and the distribution of tickets (bid amount) is shown in Figure 1.11 (Appendix).
26 PhD Thesis
1.7. RESULTS CHAPTER 1. THE IMPORTANCE OF PROB-
ABILITY OF WINNING IN RISKY CHOICES
Table 1.10: Summary of results in Part 2
Part 6 (Measuring Risk-Preference) Out of 72, 57 subjects were clear about the instructions
for this part of the experiment and entered the choices which are valid17. The distribution
of choices made is in Table 1.12. Very few subjects are found to be possibly risk-seeking
(switching point  4).
Table 1.12: Risk distribution of subjects in Part 6
Part 5 (Inverted Lotteries Sets) In this part the choice behavior of decision-makers is ex-
amined and attempt to find if they are EUM or TPW. Further, it is examined if they prefer
negative skewness over positive skewness. Out of 72, 56 subjects were clear about the instruc-
tions. Table 1.45 shows some statistics and distributions of the choices made in the three sets
of lottery tables. Table 1.46 shows the choice distribution of the subjects in the three lottery
sets.
As the lotteries in LS 2 and 3 follow second-order stochastic dominance and given that
almost all subjects are either risk-averse or risk-neutralas per expected utility theory
predictions subjects should choose L1 in LS 2 and L 9 in LS 3. Figure 1.13 shows the joint
distribution of choices in LS 2 and LS 3.
Figure 1.13: Joint distribution of choices in LS2 and LS3
Table 1.14 shows the classification of subjects into two main types (EUM & TPW). The
variable Diff (LS3 - LS2) is the difference between the choice made in lottery set 3 and 2. If
subjects are EUM, then the predicted difference in choices should be 8 (as subjects are not
risk-seeking) while if their behavior is driven by TPW then the difference should be zero.
The data shows that only 5% of subjects have made a choice such that LS3 < LS2 which
17The choices are considered to be valid if the subject has switched only once with the order of switch from
Option A to Option B and last choice as Option B.
27 PhD Thesis
1.7. RESULTS CHAPTER 1. THE IMPORTANCE OF PROB-
ABILITY OF WINNING IN RISKY CHOICES
Table 1.14: Classification of Types
strengthens the belief that subjects are not making the choices randomly. The direction
of the stochastically dominant lottery appears to impact the choices of the subjects. From
the frequency table of the difference in choices between LS 2 and 3, one can categorize the
behavior as follows: subjects with a difference of 0 to 2 are categorized as driven by the
target probability of winning, while subjects with a difference of 6 to 8 are categorized as
expected utility maximizers. The subjects with a difference of 3 to 5 are categorized as others.
The proportion of the population is estimated in each category using maximum likelihood
estimation and approximating it to normal distribution. Table 1.15 shows the estimated
mean values of the proportion along with the confidence interval for each category.
Table 1.15: Proportion estimation of each type in Part 5.
Result 1 In Part 5 (Inverted Lotteries Sets), the proportion of subjects driven by expected
utility maximization are approximately 23%. The criterion used for EUM classification is:
any choice in LS 1 and difference in lottery choices between LS 2 and 3 is  6. The predicted
difference is 8 as these lottery sets have dominant lotteries on opposite ends.
Result 2 In Part 5 (Inverted Lotteries Sets), the criterion for classifying an agent as TPW is
the difference between the choices in LS 2 and 3 is less than equal to 2 (irrespective of choice
in LS 1). This criterion takes TPW counts to 27 which is 48%.
Robustness Check
A robustness check is done using how many agents have LS 1 choices within the choices
they made in LS 2 and 3, this criterion makes TPW agents count to 25 which is 45%. It is
found that due to the choices of 2 subjects in LS 1 they are categorized as TPW based on the
second criterion but not based on the first criterion.18
18Note, pairwise comparison between two sets for consistency of type by categorizing TPW based on any two
lottery sets and comparing it with the third lottery set is not appropriate as the first lottery set does not have
28 PhD Thesis
1.7. RESULTS CHAPTER 1. THE IMPORTANCE OF PROB-
ABILITY OF WINNING IN RISKY CHOICES
Another robustness check controls for possible status quo bias. Our definition of status
quo bias is that agents make the same choices across LS 1 to 3 that is either all L4 or all L5 or
all L619. It reduces the estimate to 36%.
A further robustness check is to see if the decision rule holds for the subjects classified
as TPW. What can falsify it? If subjects classified as TPW strongly respond to the cost
difference between LS 2 and 3 at either end. Below is the graph (Figure 1.16) drawn for the
smaller to middle probabilities TPWs and for middle to higher20 probabilities TPWs. The
difference is examined if it is correlated with the lottery number in LS 2. For smaller to
middle probabilities lotteries, the graph is almost flat. For middle to higher probabilities
lotteries, it seems that the difference increases as the probability increases. But one should
notice the salience effect. Out of 7 L7 choices made in LS 2, the corresponding choices in LS
3 are L9 for 6 cases and L8 for 1 case. This shows that agents jump to choice L9 in LS 3 due
to salience which causes the positive slope but the R square explained is low.
Table 1.16: Decision Rule Robustness
Result 3 In Part 5 (Inverted Lotteries Sets), the design of LS 1 helps to answer whether more
subjects prefer positive skewness over negative skewness. Note, the measured values of risk
aversion in Part 6 shows that almost all of the subjects are risk-averse. For subjects who
are not risk-seeking, the CRRA utility is almost unchanged over the lotteries L 1-9 in LS 1
as shown in Table 1.19 (Appendix). From Table 1.20 the ratio of the population preferring
negative skewness to positive skewness for each type can be calculated. The ratio (=1.5) is
least for the EUM types and highest (=3.0) for TPW types.
Figure 1.21 shows the histogram of the choices in LS 1. It validates the assertion that
subjects preferred positive skewness over negative skewness when the expected value and
variance are the same. Note, it cannot be inferred that EUM subjects prefer positive or
negative skewness per se. This is because the middle lotteries have relatively high variance
which could be the reason why these subjects choose skewed lotteries. EUM types have the
least proportion of preference for zero skewness. This serves as another robustness check
that our classification of types is not arbitrary rather is directionally consistent with the
theoretical predictions that the risk-averse subjects of EUM types should prefer positive
skewness and largely prefer positive over negative. It still leaves the puzzle of why any EUM
type prefers negative skewness at all, as it does not find its explanation in the theory. It could
any second-order stochastically dominant lottery.
19Note, any other lottery has different costs across the three lottery sets. For details see status quo bias under
section Alternative Explanations
20Note, omitted are L8 and 9 in LS 2 because the difference between LS 3 and 2 cannot reach 2. If those are
included then the regression line will be forced to be flatter.
29 PhD Thesis
1.7. RESULTS CHAPTER 1. THE IMPORTANCE OF PROB-
ABILITY OF WINNING IN RISKY CHOICES
be that these subjects mostly have a utility function (for example, crra) such that there is no
significant difference in utilities between positive and negative skewness options for a given
variance in LS 1.
Table 1.20: Distribution of preference for skewness in Lottery Set 1
Figure 1.21: Histogram showing the preference for skewness in Part 5
In Part 5, there is no certainty lottery option (probability of winning equals 0 or 1) in any
lottery sets. This is done to avoid the first option in LS 2 and the last option in LS 3 with
an abrupt change in distribution pattern in that lottery set. One drawback of an absence
of certainty option in a lottery set is that subjects can choose lottery 1 in LS 2 and LS 3 be-
cause these are the least cost option preferred by subjects who do not want to play this game.
The data indicate that there are no observations with all choices as L1 across lottery sets LS1-3.
Part 4 (Response Curve) The purpose of this part of the experiment is to understand the
response curve of the subjects. Out of the total 72 subjects, 53 stated that they have clarity
on the instructions for this part of the experiment. The results are shown in Table 1.22. The
classification criterion used is as follows:
EU : If the subject first increases then decreases the bid amount, it is classified as an EU type.
The qualitative shape of the response curve is the only criterion used for classification.
T PW : If the subject increases the bid amount (unless drops out) as the pre-populated bid
amount of the opponent increases then she is classified as TPW type.
Others : If the subjects are difficult to classify into either of the two above types then they are
grouped under the type Others.
Result 4 In experiment Part 4, the proportion of subjects driven by the target probability
of winning (TPW) is approximately 23%, while the subjects driven by expected utility (EU)
represent 15%.
30 PhD Thesis
1.7. RESULTS CHAPTER 1. THE IMPORTANCE OF PROB-
ABILITY OF WINNING IN RISKY CHOICES
Table 1.22: Proportion estimation of each type in Part 4.
The above qualitative classification (based on the manual observation) considers only re-
sponses that can be classified into one of the categories. The minimum probability can be
estimated for the cases classified as TPW. The data for this part of the experiment is included
in the Appendix (Table 1.47) for self-verification.
Part 3 (Group Size Change) Out of the total of 72 subjects, 63 stated that they have clarity
on the instructions for this part of the experiment. Figure 2.24 (Appendix) illustrates the
distribution of the difference in subjects bid amount as the group size changes in the four
games. The difference is calculated as the bid amount high group size minus low group size
irrespective of the initial group size. The interest is in the direction of change in the bid
amount (increases or decreases) in the four games rather than the point prediction. This is
because the expected utility theory predicts a decrease in bid amount as group size increases
and the target probability of winning conjecture predicts an increase in bid amount if subjects
do not decrease their minimum probability.
Based on the direction of change in the four games, subjects are categorized as consistent
if their direction of change was the same for at least three out of four games. Table 1.25
shows the number of subjects who are consistent in either direction, including no change,
along with their measured risk preference. The number of subjects found consistent in the
direction of change is the same for the increase and decrease, and their mean value of risk
preference is similar. Out of the total of 63 subjects who have clarity on the instructions, 49
subjects are consistent. Note, that the probability of a behavior being classified as consistent
when it is random is (13 
27 which is low.
Table 1.25: Number of subjects and risk for each consistency type.
Result 5 In Part 3, approximately 25% of subjects increase their bid amount in at least
three of the four games as their group size decreases. Similarly, approximately 25% of sub-
jects decrease their bid amount in at least three of the four games as their group size increases.
Result 6 In Part 3, 48 subjects are consistent in terms of their directional change (including
no change). In Table 1.25 38 subjects are consistently decreasing their probability of winning
(based on their judgment) upon an increase in the number of subjects in the game. In Table
1.27 14 subjects who decrease their bid amount and probability of winning upon the increase
in the number of subjects in the game cannot be rejected as EUM types. 23 subjects either
increase bid amount or probability of winning or both and cannot be rejected as TPW types.
31 PhD Thesis
1.8. PART 5 RESULTS - ALTERNATIVE
EXPLANATION
CHAPTER 1. THE IMPORTANCE OF PROB-
ABILITY OF WINNING IN RISKY CHOICES
Table 1.27: Consistent change in probability of winning
Table 1.28: Joint distribution of consistent change in probability of winning and bid amount
1.8 Part 5 Results - Alternative Explanation
Risk Aversion
There are many subjects classified as TPW choosing middle lotteries that have the least
skewness, maximum variance and median expected values, which suggests that these sub-
jects are not driven by any expected utility maximization but are targeting the probability of
winning. Similar results are found in Edwards (1953, 1954) and in Tversky (1969) where it is
presumed that subjects choose middle lotteries considering them as fair bets. The other
subjects classified as TPW which have opted for choices corresponding to lower probabilities
of winning can be rationalized as EUT only if they are risk-seeking. But, the measured risk
aversion in Part 6 has a scale of monetary distribution higher than these individual lotteries
and almost all of the subjects are found to be risk-averse. This implies any explanation
assuming subjects have convex utility functions at these lower monetary distributions is
inconsistent. The TPWs do not opt for the maximum probability of winning across lottery
sets either, which could indicate that these subjects just want to win the game. It is not clear
how and why they are targeting such a probability of winning. The subjects who are classified
as intermediate (neither EUT nor TPW) are possibly the ones who value both expected utility
and probability of winning (within this setup) as argued by Roy (1952).
Cumulative Prospect Theory
The design of lotteries in LS 2 and 3 being second-order stochastically dominant, TPW
subjects are ruled out from being expected utility maximizers. One of the widely used de-
scriptive models of risky choice is prospect theory. While the reference point of the individual
in these models can be anything, the generally considered reference point is the subjects
endowment. The expected payoff is the subjective value of the two possible outcomes com-
bined with their weighted probabilities. The probability weights will remain the same for
any lottery across the lottery sets. If it can be shown that the stochastic order of the lotteries
within each lottery set has some order for widely accepted parameters then it can be ruled
out that the behavior can be explained using cumulative prospect theory. The functional
forms used for the value function (v) and probability weighting functions (w+ and w-) are as
32 PhD Thesis
1.8. PART 5 RESULTS - ALTERNATIVE
EXPLANATION
CHAPTER 1. THE IMPORTANCE OF PROB-
ABILITY OF WINNING IN RISKY CHOICES
proposed by Tversky and Kahneman (1992). Based on these value functions and standard
parametric values the calculated expected value for the three lottery sets is as shown in Table
1.29-1.31. The expected value monotonically decreases before increasing. Thus, for a wide
range of parametric values, the highest should be either the first lottery or the last lottery.
Therefore, it seems that TPW cannot be explained using cumulative prospect theory, which
concurs with the findings of Wu and Gonzalez (1996). Both the models are examined for the
reference point as either 100 and 150.
Table 1.29: Expected value (based on PT) of Lottery Set 1
Table 1.30: Expected value (based on PT) of Lottery Set 2
Table 1.31: Expected value (based on PT) of Lottery Set 3
Discontinuous Value Function
Diecidue and Van (2008), based on experimental evidence in other papers (including Payne,
2005), theorize aspiration levels as a relevant aspect of decision making in value allocation
tasks. They develop a model that includes the overall probabilities of success and failure
33 PhD Thesis
1.8. PART 5 RESULTS - ALTERNATIVE
EXPLANATION
CHAPTER 1. THE IMPORTANCE OF PROB-
ABILITY OF WINNING IN RISKY CHOICES
relative to the aspiration level into an expected utility representation. This turns out to be
equivalent to the expected utility with a discontinuous utility function. The discontinuous
value function around the aspiration level as a reference point exhibits the extreme form
of loss aversion. For two possible outcome prospects that involve the aspiration level, the
model shows that subjects are always risk-averse from above and risk-seeking from below.
The functional form of the model is given as:
V (x) =
j=1pju(xj) +P (x+)P (x)
where
n is the number of prospects which in our case is 2 for every lottery (L)
pj is the probability of prospect j,
u(xj) is the utility of prospect xj ,
P (x+) is the overall probability of success,
P (x) is the overall probability of failure,
, R+
and aspiration level is taken as zero (there is no endowment in the value allocation task)
The above functional form for our set-up of two possible outcomes with endowment as
aspiration level
V (x) = pv(x+) + (1 p)v(x) +P (x+)P (x)
where
v(x+) = x+0.88,
v(x) = x0.88,
and other notations are as defined as above
Can it explain the TPW behavior? Irrespective of the parameters of the overall gain and
overall loss probabilities (the last two parts of the equation above), it can be said that as
one move from top to bottom lotteries (in all three lottery sets) the net value from this part
of the functional form increases monotonically. Based on the first two components of the
value function and parametric values used above it can be seen (as shown in Table 1.32-1.34)
that the expected value almost monotonically increases in LS 1 and 3 and monotonically
decreases in LS 2. When the values from all four parts of the value function will be combined,
it will give the last lottery (and not middle lotteries) in LS 1 and 3 as the preferred choice for
a range of parametric values which is not supported by the experimental data.
Table 1.32: Expected value (based on Diecidue and Van (2008)) of Lottery Set 1
Non-Standard Weighting Functions
34 PhD Thesis
1.8. PART 5 RESULTS - ALTERNATIVE
EXPLANATION
CHAPTER 1. THE IMPORTANCE OF PROB-
ABILITY OF WINNING IN RISKY CHOICES
Table 1.33: Expected value (based on Diecidue and Van (2008)) of Lottery Set 2
Table 1.34: Expected value (based on Diecidue and Van (2008)) of Lottery Set 3
Can non-standard weighting functions explain this experimental data. To answer this
take three weighting functionsone which has abrupt jump ( w(p) = 0 for p <= p & w(p) = 1
for p > p), another which has concave smooth jump (w(p) = 0 for p <= p & w(p) = p0.5
for p > p) and another which has convex smooth jump (w(p) = 0 for p <= p & w(p) = p2
for p > p). The concave and convex functions are chosen to be able to judge a combined
weighting function for any arbitrary switching point to give a general weighting function. A
general weighting function is convex for a range of small probabilities and concave above
that. This is done for three values of loss aversion parameter (l=1, 1.75 and 2.25) and a set
of jump probabilities (p={0.1, 0.2, 0.3, 0.4, 0.5, 0.6}). The predictions of the EUM and CPT
models are calculated with these weighting functions.
The prediction for the EUM model is calculated with the subjective perception for winning
defined as per the weighting function and the remaining probability for losing. This indicates
that the abrupt and concave weighting function gives a prediction that is the same as TPW,
while convex weighting predicts the first or the last lottery as the highest value. Note, in
general, weighting functions are found to be concave for small probabilities and convex for
large probabilities. Also, what drives the prediction in the abrupt model is the increasing cost
as all probabilities below the jump point are perceived to be zero. Similarly, a CPT model
with an abrupt jump and reference point as zero (or below) also gives the same prediction as
that of TPW.
A CPT model with the above weighting functions and endowment (=100) as the reference
point predicts behavior similar to TPW around lotteries L6-9 regardless of p at which jump
is taken as shown in Table 1.35. This is a limited range and as one can see the middle lottery
(L5) which has the highest proportion in experimental data is not predicted. This feature
seems to be robust for a range of parameters as what is driving this result is the fact that for
p < p only negative values will be reflected in the expected value of a value function.
Priority Heuristics
Brandstatter, Gigerenzer and Hertwig (2006) generalize the framework of fast and frugal
heuristics as the priority heuristic can explain many experimental pieces of evidence that are
35 PhD Thesis
1.8. PART 5 RESULTS - ALTERNATIVE
EXPLANATION
CHAPTER 1. THE IMPORTANCE OF PROB-
ABILITY OF WINNING IN RISKY CHOICES
Table 1.35: Jump Weighting Function
different from expected utility maximization. It does so in gambling environments that were
designed to demonstrate the empirical validity of theories of risky choice that assume both
weighting and summing. It is a simple heuristic that forgoes summing and therefore does
not make trade-offs. It proposes the priority rule of reasons in the order of minimum gain,
probability of minimum gain, maximum gain and probability of maximum gain. Although it
is clearly stated that the priority heuristics does not apply in cases where one of the gambles
dominates the other, in general, it predicts the top lottery to be chosen as both the minimum
amount and the probability of minimum amount are higher than the immediate lower lottery.
Status Quo Bias
The simplicity of this design is that only one dimension (cost) changes across the sets which
makes a comparison of the lottery within the set and across the sets easy. Still, the status
quo bias due to decision avoidance (Dean, 2008) can be conjectured as a possible reason for
subjects classified as TPW who chose the same lottery across lottery sets. Except for middle
lottery L5, all lotteries across the lottery sets are different. One can consider L4 and L6 to be
close enough across the lottery sets. Considering these three lotteries to be the same across
the three lottery sets, there is a total of 7 subjects who chose these lotteries. These subjects
could also be considered as targeting cost or making choices considering these lotteries as
fair (Tversky, 1969). If all these subjects are considered not to be TPW, then it reduces the
estimate to 36%.
Security-Potential/Aspiration Theory
SP/A dual criterion theory that combines a decumulative weighting process (the security
potential part of SP/A) with a process that maximizes the probability of achieving an as-
piration level describes both preferences and reasoning patterns across a wide variety of
behavioral phenomena. This model captures the idea of the probability of attaining a certain
aspiration level as one of the dual criteria used for decision-making. It can be thought that
subjects aspire to win (receive the outcome higher than endowment) in the two possible
outcome games rather than aspiring to any specific value. Unlike this model, TPW subjects
are cost-sensitive. This can be confirmed from the choices that the TPW subjects make in
LS 3 where the lowest lottery (L9) has maximum expected utility and highest probability
of winning but all these subjects do not make it as their choice possibly because its cost is
highest in that set.
Spiteful Behavior
Herrmann and Orzen (2008) investigate the importance of spiteful rivalry in Tullock contests.
In the Fehr-Schmidt model, spiteful agents dislike disadvantageous inequality but enjoy
36 PhD Thesis
1.9. DISCUSSION CHAPTER 1. THE IMPORTANCE OF PROB-
ABILITY OF WINNING IN RISKY CHOICES
advantageous inequality. They find subjects overinvest to some extent even in decision tasks
when there are no other players and social preferences can play no role. This is similar to the
results in the main design of this experiment. In another task, the best response function for
each player is elicited in a one-shot setting. The difference in response curves  increasing
vs hump-shaped  is attributed to spite and excessive rivalry between players. In this
experiment, both the results, that is, decision task in main design with probabilities and
response curves (more like a decision task) are explained by the same underlying behavior
types. The increasing response curve is classified as TPW and the humped-shaped response
curve as EUMs. This makes the inference of behavior types as TPW and EUMs more robust.
1.9 Discussion
The support for TPW is found with a clearer main task that can separate EUMs from TPWs
and some important alternative explanations for the choices made. The risk preference
measured helps to rule out risk-based residual explanations for such behavior. Similarly,
these tasks help to rule out other possible explanations in the literature like joy-of-winning,
CPT, discontinuous value function, status quo bias, SP/A, priority heuristics as discussed
in Section 1.8. Non-standard weighting functions are simulated. While these weighting
functions are conceptually different than TPW these do support the possibility of such
choices being optimized behavior. There is also supportive evidence for these behavioral
types in contests tasks tested.
This chapter contributes to decision-making literature in risky choices including areas
in lotteries, lottery-like financial decision making and winner-take-all competitions. Some
more specific applied areas for illustration purposes are as follows. It can contribute to the
literature on pricing by measuring the trade-offs between risk dimensions, for example, a
salesperson deciding on how much minimum secret discount to offer a customer to make a
B2B sales in a limited information environment. Similarly, it can contribute to the literature
on salary negotiation in a market with structural differences like perceived taste-based and
statistical discrimination.
Further to above, we would suggest that this decision-making approach holds wherever
there is a possibility where subjects can trade-off the chance of winning with the cost and the
two possible outcomes are winning and losing. Subjects would follow this decision-making
irrespective of whether it increases or decreases the expected payoff. This can be seen in
Part 5 (LS 3) where lower lotteries have a higher probability of winning as well as a higher
expected value, these subjects do not choose the last lottery. This shows that their behavior
is primarily driven by two of the basic dimensions of the game which are a probability of
winning and cost, with priority given to the probability of winning up to a target and then to
the cost. This is in line with the results in Slovic and Lichtenstein (1968) but is different from
the priority heuristic in Brandstatter, Gigerenzer and Hertwig (2006). In this experimental
design, the trade-offs are with cost. Lottery buying in a syndicate is an example of a trade-off
with the size of the prize. Similarly, trade-offs with other risk dimensions can be studied with
different designs.
A one-shot game provides a starting point to investigate the underlying decision-making
approach; however, there are a number of ways in which further research could address the
limitations of the present research. The parts of this experiment could be extended further
as a repeated game with and without intermediate feedback to examine any difference in
behavior. Further studies are required to understand the general decision-making process
of these subjects who are being classified as TPW type which can answer how and why
these subjects make such decisions of the target probability of winning and the theoretical
37 PhD Thesis
1.9. DISCUSSION CHAPTER 1. THE IMPORTANCE OF PROB-
ABILITY OF WINNING IN RISKY CHOICES
equilibrium predictions are in the presence of these types.
To robustly corroborate the results one can design an experiment with a series of lotteries
which are transformations of LS 1-3 for a fixed common value prize and endowment. If
subjects make choices in 20 such lotteries (LS 1-3), this can allow for more robust statistical
tests. Additionally, a graphical alternative of the above design, which can help understand
how subjects choose TPW, could be achieved by giving subjects various cost vs probability
of winning (p=f(c)) graphs for a fixed common value prize and endowment. The pattern of
their choices across various such graphs can reveal if they have a kink or can offer insights
into how they value a probability of winning and cost.
Another design that uses equivalent designs of LS 1-3 with cost and prize replaced by
values of two possible outcomes can be used to test the generality of the result by changing
the context. Such a design can have different results due to a change in frame. The frame of
two possible positive outcomes (without any endowment) is in the gain domain while the
present framing is in the mixed domain if subjects have endowment as a reference point.
To further test the robustness the sequence of lotteries can be randomized in every lottery
set (in Part 5) such that no lottery is at the same row across the lottery sets. This can filter
out the possibility that subjects merely choose a row of a position on the computer screen.
Nevertheless, this might make comparing lotteries difficult/costly for subjects.
For empirical validation, one can collect applications for the position of assistant profes-
sors in top universities and one stratum lower-ranking universities. These applications are
generally costless other than the manual effort of applying. The distribution of applications
from all strata of universities can be examined to see if students generally fail to even apply
to relatively higher-ranking universities where they believe that they will not get through.
Similarly, applications for any other competitive positions where application cost is low can
be examined.
Much remains unknown about TPW behavioral types. Venkatraman, Payne and Huet-
tel (2014) and Zeisberger (2016) have labeled it as a heuristic. We propose to repeat the
experiments (those of Venkatraman et al. (2014) and our main design) by giving subjects
a calculator in one treatment and with additional information on statistical moments in
another treatment. If the proportion of these types do not fall considerably then one needs
to run a series of designs to further investigate how these subjects approach such decisions.
Based on the present results it is not clear whether this decision-making approach is being
used by the subjects (who are classified as TPW) as a heuristic (see, for example, Tversky
and Kahneman, 1973; Tversky and Kahneman, 1974) or more of these subjects choosing
higher probabilities is a reflection of behavioral tendencies which in real-life might give some
evolutionary (survival) advantage. The non-standard jump weighting functions predictions
are similar to TPW, showing that the decision-making approach can also be viewed as opti-
mization. In this experimental design, incentive-based joy-of-winning is not captured. One
can do this and correlate it with the TPW types. It can give insight into the question if both
types might have a similar evolutionary origin.
In the next chapter, we show that the decision rule has utility representation and how
agents may learn in a severely limited information environment of repeated contests when
driven by TPW. The reinforcement learning model predictions track the experimental data
available from other studies reasonably well. This strengthens support for TPW.
38 PhD Thesis
Chapter 2
Reinforcement Learning in Contests
2.1 Introduction
We hypothesize that in winner-take-all contests agents optimize their subjective thresh-
old probability of winning when they have limited information about their environment.
Applications of winner-take-all contests range from lobbying, political polls, sports tourna-
ments, patent races and pharmaceutical R&D. A contemporary example of such a limited
information environment is remote working in many organizations.
There is experimental evidence in a number of experiments on decision theory indicating
that subjects prioritize the probability with which they win as opposed to being expected
utility maximizers. Roy (1952) introduced the safety first principle where the investor
chooses a portfolio such that it minimizes the probability of returns going below the minimum
rate of return (this minimum in winner-take-all contests can be winning the contests). In
a similar vein, Edwards (1953) designed lotteries that have the same expected value to
understand what makes subjects deviate from the expected value. His findings point to an
overall preference for the probability of winning as middle gambles are more frequently
chosen. Likewise, evidence is found in Edwards (1954) and Tversky (1969). Slovic and
Lichtenstein (1968) find that agents place greater importance on the probability of winning
and payoffs rather than other factors in risky choices. Lopes (1995) surveys literature that
suggests that agents give importance to the probability of achieving aspiration levels (this
aspiration in winner-take-all contests can be winning the contests). She rightly incorporates
this as one of the criteria in her SP/A dual theory of decision-making in risky choices.
Payne (2005) finds that a substantial proportion of agents make choices to increase their
probability of positive payoff (or decrease their probability of negative payoff) in value
allocation tasks. Equally, Venkatraman, Payne and Huettel (2014) use the value allocation
task in multi-outcome gambles involving possibilities of both gains and losses and find that
subjects often maximize the overall probability of winning. Zeisberger (2016), in a series
of experiments, notably finds that people pay explicit attention to the probability of losing.
Subjects willingness to take risks and choice behavior is considerably influenced by loss
probabilities, and performance feedback seems unable to mitigate this effect.
One of the winner-take-all contests is Tullock contest, Tullock (1980). In the standard
Tullock contest the probability of winning of a player is given by P (a) = a/(a+ b), where a is
the amount of effort player A has put in and b is the amount of effort player B has put in ( if
a = b = 0 then P(a)=P(b) = 0.5). The optimal effort for both the players can be calculated by
the Nash equilibrium.
We are interested in competitive situations which can be represented by Tullock contests
where (almost) fixed players play the game repeatedly. Where agents are not able to see the
effort of others or even the aggregate effort, what they know is their own previous effort and
2.1. INTRODUCTION CHAPTER 2. REINFORCEMENT LEARN-
ING IN CONTESTS
outcome in terms of winning or losing. This can serve as a base case for real-world situations
where signals are not good enough to form beliefs on others (for example, employees working
remotely in large departments). In such limited information environments where there is
stochastic behavior by other players, low chance of positive reinforcement (success) at lower
bid amounts (as predicted by the standard model) that further decreases as the number of
players increases, a large action space that can allow agents vary probability of winning, and
suggests that agents would play games in such environments differently.
In literature, foresight-free stochastic learning models (Roth and Erev, 1995; Camerer
and Ho, 1999) have successfully explained many repeated games where agents learn to
make choices predicted by Nash equilibrium starting from random choices. Sobel (2000)
discusses the conditions which enable agents to learn the equilibrium in the game. It states
that when an agent starts with a sensible model, has a stationary environment and it is
costless to obtain and process information then eventually agents learn enough about the
environment to make optimal decisions. These conditions do not hold for the environment
of interest in this chapter. The information is limited, the environment is not stationary and
it is costly to obtain information (since losing is costly). Erev and Roth (2007) experimentally
find that foresight-free learning models fail to explain the data in individual choice tasks
which suggests high sensitivity to feedback. Payoff variability slows learning in games and
this effect can be considerably magnified in multiplayer games with large strategy space.
Charness and Levin (2005) experimentally find that agents make better decisions when
the direction of Bayesian updating and reinforcement learning are the same. The situation
in our environment is that the agent should be bidding low but the chances of winning
at lower efforts are lowered when other agents bid higher, hence positive reinforcement is
less likely. This may lead to agents moving towards higher effort levels and staying there
before dropping out. Teodorescu and Erev (2014) experimentally demonstrate that it is not
the uncontrollability of the environment rather the low frequency of reward which causes
learned helplessness. This finding may, for a large strategy space, translate into the agent
eventually dropping out after consecutively losing even after putting in a high effort. In such
an environment, repeated experience of outcomes may lead to agents becoming risk-averse
(March, 1996) and stabilize at any effort (risk) level. Conversely, it is also likely that some
agents do not learn to be risk-averse as evidence presented in one-armed restless bandit
problems in Biele, Erev and Ert (2009).
Another type of learning theory, started by Reinhard Selten, is directional learning (for
example, Selten and Stoecker, 1986) where agents have a sequential dependency in repeated
decision tasks. Within this theory Ockenfels and Selten (2005) formulate a quantitative
model, impulse balance equilibrium, to explain behavior in the first-priz auction. An im-
portant factor is that losing in a first prize auction is not costly. On this basis, Grosskopf
(2003) discusses the need for models that have both reinforcement and directional learning
components to be able to better explain experimental evidence. She rightly emphasis this
because such models can be important when losing is costly as reinforcement would also play
a role. In the case of Tullock contests, the probability of any effort level leading to winning
is linearly ordered. This means that if one strategy has worked then it is not the case that
all other strategies would not have worked. All other strategies above the chosen strategy
must have worked and maybe some strategies below it might have worked too. Similarly,
if one strategy has not worked last period then definitely all the below strategies would
not have worked and maybe some strategies just above the chosen strategy might not have
worked either. This is the argument we put forward as strategy similarity1 (Sarin and Vahid;
1The three parameters of similarity can be: 1) How far a strategy is located in strategy space from the last
period strategy, 2) Whether a strategy is above or below the last period chosen strategy and 3) Whether the
agent has won or lost in the last period
40 PhD Thesis
2.1. INTRODUCTION CHAPTER 2. REINFORCEMENT LEARN-
ING IN CONTESTS
2004; Goldstone and Son, 2012). This means that the model has a feature of directional
learning where the agent ex-post looks at what might have been better last time and adjusts
the decision in this direction. What the agent will be learning is their subjective probability
of each strategy to win the game. The agentdepending on whether she has won or lost in
the last periodcan adapt in that direction.
We introduce a decision-making approach that agents possibly follow in this highly
uncertain environment. The limited information available to agents in the environment
under consideration is their own efforts and outcomes. The assumption is that they start
with the effort which, as per their initial subjective probabilities of winning, gives them
their target probability of winning. The players play the game with one period in mind. At
each period, the agent chooses the lowest strategy which offers them their target probability
of winning without taking into account the possible future times she may face the same
game situation again. Such isolated decision-making could be due to short-sightedness or
believing that the game is not stationary. Players may be approaching the game thinking that
to be able to make a profit one has to win. On this basis, after choosing their strategy the
agents receive the outcome in terms of winning or losing the game. They incorporate the
experience they receive in the last period in their decision for the next period. The agent uses
this outcome information to update her subjective probabilities of winning for some of the
strategies available.
The strategy similarity argument is used where agents ex-post update the similar strategies
similarly. This means that the agent treats strategies that are a few steps away similar to
the strategy she played last period. If she wins she increases the subjective probabilities
of winning for all the strategies above the chosen strategies and some strategies below it.
Similarly, if she loses then she decreases her subjective assessment of the probability of
winning for all the strategies below it and a few strategies above it. The agent does not
necessarily update all the strategies, but she updates strategies for more than just the one
chosen. This similarity-based updating results in the agent switching to immediate below
strategy after consecutive wins on any strategy. Likewise, she switches to the immediate
above strategy after consecutive losses. The learning speed is exogenous, symmetric (the
following analysis assumes this, but the model and framework of the asymptotic analysis are
general enough for any pair of asymmetric learning speeds) for winning and losing, and does
not change with time.
The model predictions are that agents decrease effort after winning, increase effort
after losing, drop out after consecutive losing in the direction of increasing effort. Thus
intermediate- and long-run aggregate efforts. can be probabilistic in terms of over or under
dissipation.
Some results found in the experimental literature on contests run in similar information
setting suggests our models predictions can perform reasonably well in tracking observed
behavior. This two-parameter learning model has features of reinforcement and directional
learning and can provide a more plausible explanation for some of the experimentally
observed behavioral regularities (increasing effort after losing, decreasing effort after winning,
and dropping out after consecutive losing and over-dissipation at the aggregate level) in
Tullock contests. Based on the proposed model of behavioral decision-making, the artificial
agents are simulated in the repeated contest with the same settings as in the experiment.
The model can capture the frequent individual and approximate aggregate behavior over
the period. This behavioral model will be analyzed via the discrete-time finite-space time-
homogeneous Markov process. It will be shown that the game reaches an absorbing state
where one agent wins with minimal effort and others drop out. In case there is a positive
probability that agents after dropping out bounce back, then the game reaches a limiting
distribution.
41 PhD Thesis
2.1. INTRODUCTION CHAPTER 2. REINFORCEMENT LEARN-
ING IN CONTESTS
Contribution: The chapter is on the reinforcement learning model that has features of
directional learning and contributes to the literature on learning in games. This chapter adds
a new dimension to decision making in winner-take-all repeated games2. This model is for
contests, hence, contributes to the literature in contests. This chapter adds to the economics
literature on similarity (for example, Sarin and Vahid, 2004; Grosskopf, Sarin and Watson,
2015). It adds to the literature that attempts to explain dropout behavior in winner-take-all
competitions (for example, Muller and Schotter, 2010). In the environment considered, it
can explain the behavioral regularities such as increasing effort after losing, decreasing effort
after winning, dropping out and overall over-dissipation in a dynamic game. The strength
of the contribution lies in the fact that the model can explain all the behavioral regularities
together, which does not seem possible using the predominant theories (as discussed in
Section 6- Experimental Evidence).
The remainder of the chapter is organized as follows. Below are definitions of some terms.
Section 2 briefly outlines the static equilibrium characterization. Section 3 introduces the
basic learning model. Section 4 provides an analysis of the model for Tullock contests. Sec-
tion 5 states model predictions. Section 6 provides simulations of the models and compares
them to experimental findings. Section 7 states the applicability of the learning model to the
all-pay auction. Section 8 concludes with further discussion of the model.
Definitions
Target Probability of Winning (i): This is an exogenous probability of winning agent i aims
to achieve. Agent tradeoffs cost with the probability of winning is considered to decide on
how much effort to expend to have a sufficient chance of winning the game with a positive
profit. This is considered as the intrinsic behavior of the agent for this game.
Over-dissipation: A game is considered to be over-dissipated if the aggregate effort in the
game is higher than that of risk-neutral symmetric Nash equilibrium of the standard model.
Strategy Set: It is a set containing all possible effort levels (pure strategies/actions) agent can
j chosen by agent i is denoted as eij . The strategies are linearly ordered i.e ej > ek  j > k.
Subjective Probability of Winning (SPW): Agents for every strategy (effort level) have subjective
probability of winning. Formally, pij(t) is the agent is subjective probability of winning by
playing the strategy eij at period t. This is not the actual probability of winning, and not any
function of effort of the agent and efforts of other agents. 3
Dropout: An agent is considered to have dropped out if she played for a few consecutive
periods and then does not enter the game at least for the next few periods. Formally, pij < 
ieij
Learning Speed: It is the rate at which agents update their SPWs every period. Formally, if
learning speed of agent i is (0  i  1) then its pij(t) 
0,i ,2i ,3i , ..,1
2To our knowledge this is the first model to bring in the probability of winning as an independent criterion
of decision making in winner-take-all competitions. An early version of this chapter was presented in GW4
Game Theory Workshop in May 2016 and in conference Contests: Theory and Evidence in June 2017
3A justification for such an approach is that a) in a severely limited information environment (of knowing
only their own effort and outcome) b) agents convert a strategic game into a decision problem and c) utilize
linearly ordered effort space to form such beliefs.
42 PhD Thesis
2.2. STATIC EQUILIBRIUM
CHARACTERIZATION
CHAPTER 2. REINFORCEMENT LEARN-
ING IN CONTESTS
Bounce Back: An agent is considered to have bounced back if she puts in positive effort after
dropping out. Formally, bounce back is a process where the agent after dropping out enters
the game again with a new set of arbitrary SPWs and a small probability of bouncing back to
any arbitrary SPW is denoted as  in any period after dropout. If  > 0 agent restarts decision
making from SPWs, she bounces back. Once agents have dropped out, they do not bounce
back unless there is an exogenous perturbation.
2.2 Static Equilibrium Characterization
It is supposed that agents face the one shot game and have the finite set of n effort levels
(e), e N0 with emin as lowest effort level and emax as highest effort level. This effort set is
denoted as e = {emin, e2, ..ej .., emax} where ej < ek  j < k.
For the above effort set e let us define a binary relation ej < ek with different possible cases.
Case C1: Both ej and ek are s.t. p
j and p
k  
i then ej % ek.
Case C2: Both ej and ek are s.t. p
j and p
k < 
i then ej % ek.
Case C3: When ej and ek are s.t. p
j < 
i and pik  
i then ek % ej .
The above binary relation is complete now it needs to be shown that it is transitive as well.
Case T1: Let us take ej , ek and el are s.t. p
j , p
k and p
l  
i then ej % ek and ek % el from Case
C1 above. Similarly, from Case C1 above it is known ej % el hence transitivity holds.
Case T2: Let us take ej , ek and el are s.t. p
j , p
k and p
l < 
i then ej % ek and ek % el from Case
C2 above. Similarly, from Case C2 above it is known ej % el hence transitivity holds.
Case T3: Let us take ej , ek and el are s.t. p
j and p
k < 
i and pil  
i then ej % ek from Case
C2 and el % ek from Case C3 above. Similarly, from Case C3 above it is known el % ej hence
transitivity is not violated.
From Case C2 and T2 above it can be seen that for any effort set e such that pij < 
i  ej in
e = {emin, e2, ..ej .., emax}, emin is the choice.
The above binary relation is found to be complete and transitive. Therefore it is a preference
relation. For the setup (effort levels are distinct and ordered), independence of irrelevant
alternative axiom is satisfied and indifference axiom of choice correspondence is not appli-
cable. From this, it can be said that the preference relation has a utility representation and
rationalizes the below decision rule.
Decision Rule: The agent is problem is:(
eij s.t p
j  
otherwise, if @ j s.t.pij  
i then eij = emin
where,
eij is the effort choice of agent i,
43 PhD Thesis
2.2. STATIC EQUILIBRIUM
CHARACTERIZATION
CHAPTER 2. REINFORCEMENT LEARN-
ING IN CONTESTS
pij is the subjective probability with which agent i thinks to win by putting in effort e
Below, a structure is laid out for a pure strategy static equilibrium for different cases of i
i . Agents know the number of players, m, in the game. This decision rule can be
understood as the best response function where these types of agents have converted a game
into a decision problem.
Case 1: If  i, i  1m and
i  1 then eij = emin is a stable point.
It holds in case emin = 0. This is because in Tullock contests if  i, eij = 0 then probability of
winning for each agent is 1m . This means that agents will achieve their minimum probability
at the minimum possible effort level and hence do not have an incentive to unilaterally
deviate. Argument for the case when emin > 0 is trivial as  i, if eij = emin then probability of
winning for each agent is 1m and there is no incentive for an agent to increase their costly
effort level.
Case 2: If
i  1 then at least one stable point exists.
i < 1 and  i s.t i > 1m then at least one stable exists.
Lets us take an example. Say in the case of two agents having i s as 0.1 and 0.6, respectively.
Agent 1 chooses the minimum non-zero effort level, which is 1. This means Agent 2 chooses
the effort level of 2. So, effort tuple (1,2) is a stable point as it gives Agent 1 33.3% winning
probability and Agent 2 as 66.6%. Note, Agent 1 cannot go below this effort level. This can
be generalized for any set of i s.
Lets try another example. Say in case of 2 agents having i s as 0.4 and 0.55, respectively. In
this case, the stable point is (2,3). This can be generalized for m agents.
Lets try another example. Say in case of 3 agents having i s as 0.05, 0.30 and 0.60, respec-
tively. Agent 1 chooses the minimum non-zero effort level, which is 1. Then Agent 2 chooses
6 and Agent 3 as 11. Hence, (1,6,11) is a stable point. This can be generalized.
i = 1 then at least one stable point may or may not exist.
When i s expressed as r.R(= (R1, ...Ri ...,Rm)) s.t highest common factor for R1, ...Ri ...,Rm is 1
and Ri N. For simplicity the assumption made is that i s when expressed as percentages
are integers then,
r.R are all NE r Z+ s.t. emin  r.Ri  emax i and
i  emax 4
For example, lets say in case of 2 agents having i s as 0.4 and 0.6, respectively. The corre-
sponding R = (2,3). Then {(2,3), (4,6), .. (40,60)} is a set of stable points for emax = 100
Case 3: If
i > 1 then no stable point exists.
For example, lets say in the case of 2 agents having Ri s like 0 and 101, respectively. Then
4The latter condition may not be true as such, for example, in the below example lets say R1=49 and R2=51
and emax < 100. Here, no stable point will exist.
44 PhD Thesis
2.3. BASIC LEARNING MODEL CHAPTER 2. REINFORCEMENT LEARN-
ING IN CONTESTS
the second agent can never attain her TPW because even if agent 1 chooses the lowest effort
as 0, the maximum probability with which agent 2 can win is 100. Similarly, if agents have
Ri s as 2 and 9 then (0,0) as an equilibrium effort is ruled out. Lets say agent 1 chosen effort
equals 1 then the minimum effort agent 2 will choose is 9 which does not satisfy agent 1 TPW.
So, agent 1 will (virtually) respond by choosing 2.25 (for the purpose of argument lets say
efforts are allowed to be real numbers). Likewise, the virtual responses can be calculated and
it can be seen that there is no effort tuple that satisfies the condition.
2.3 Basic Learning Model
A decision-maker i chooses from a finite set of effort levels (pure strategies/ actions) e =
{e1, e2, ..ej .., en}. The effort levels are linearly ordered. The cost of ej > ek j > k. Every effort
level has a probability of winning given by
eijm
i=1 e
, where m is the number of players in the
game5. The game is a winner-take-all. If an agent puts in the effort of ej then it costs her ej
amount regardless of whether she wins or loses. In the case that she wins, she receives the full
prize money which equals the highest possible effort level en. The assumption is that agents
face the same game repeatedly with one repetition of the decision problem per unit of time
(period). Agents know the number of players in the game. We formulate a decision-making
approach in these games; agents are not identical, and they have their own characteristics.
Agents have subjective winning probability (SPW) for each effort level denoted by, for agent
i, P i = {pi1, ..p
j ..,p
For every agent, based on its learning speed (0  i  1) its pij(t) 
0,i ,2i , ..,1
They do not explicitly6 form beliefs about the effort levels of other agents. These are their
perception rather than any estimation or actual probability of winning. The initial SPWs are
denoted by P i(0) which may have been formed by hearsay, strategy labels, or other factors.
The assumption is that P i(0) is given. Since the effort levels are linearly ordered so are the
SPWs, that is pij  p
k j > k  i.
Agents are not driven by expected payoffs rather their exogenous target probability of
winning (TPW)7. The model does not say how agents form their target probability of winning.
This is their approach to choose the effort level to be playeda strategy that gives them a
good enough chance of winning. Their TPW is denoted as, for agent i, i .
The agent chooses the smallest effort level which can ensure her TPW. On the one hand,
they know their own effort level last period and the outcome. On the other hand, they neither
know the effort of the other agents in the game nor the outcome of other agents. They are
playing the game with one period in mind and use their experience in the following period
to update their SPWs. Let q and r are positive integers. If they win they increase their SPWs
for all the effort levels above the chosen effort level and q effort levels below the chosen. If
they lose, they decrease their SPWs for all the effort levels below the chosen effort level and r
effort levels above the chosen. The effort level zero is considered as dropout which agents
5This is standard Tullock contest other than that the agent is not allowed to choose zero effort level as an
active strategy. The probability of winning will change for all-pay auctions but the basic learning model does
not rely on this probability.
6It cannot be ruled out that agents implicitly form beliefs about effort levels of other agents. SPWs can be
an outcome of their implicit beliefs about other agents efforts. The model does not say how these SPWs are
formed. What we are saying is that they do not approach the game as a fictitious play where they best respond
based on the beliefs about other agents.
7In our model only TPW and cost matters
45 PhD Thesis
2.3. BASIC LEARNING MODEL CHAPTER 2. REINFORCEMENT LEARN-
ING IN CONTESTS
(passively) select when no SPW satisfies her TPW.
The basic model can be divided into two qualitatively different models. One where  = 0;
this is termed as unperturbed model where agents do not bounce back. The other where
 > 0; this is termed as perturbed model where agents do bounce back with some positive
probability to an arbitrarily new set of SPWs.Both the models will be analyzed for their
asymptotic results. The model can be stated as below.
Initial Values:
If pij(t) < 
i  j  {0,1,2, ..,n} then;

Unperturbed Model :
P r( at least one l  {0,1,2, ..,n} s.t. pil (t + 1)  
i) = 0
Perturbed Model :
P r( at least one l  {0,1,2, ..,n} s.t. pil (t + 1)  
i) = 
where,0 <   1
(2.1)
The initial SPWs for each strategy could be the outcome of an agents perception or previous
experiences in similar situations. The model does not say how these initial SPWs are formed
or what factors impact these including the environment of the game. Given the strategies are
ordered linearly, so will be the SPWs:
eij(t)  e
k(t) if j  k (2.2)
Decision Rule: The agent is problem every period t is :
eij s.t p
j(t)  
i (2.3)
Updating Rule:
If agent i wins after playing strategy k:
pij(t + 1) =
min
pij(t) +
for j  k  q
pij(t), otherwise
(2.4)
If agent i loses after playing strategy k:
pij(t + 1) =
max
pij(t)
for j  k + r
pij(t), otherwise
(2.5)
Boundary Conditions:
It is assumed that the agents cannot move below the smallest positive effort level. This means
that when an agent has reached the smallest positive effort level, she just updates her SPWs
but does not make a decision to go further downwards based on any new SPWs. One can
46 PhD Thesis
2.4. ASYMPTOTIC ANALYSIS -TULLOCK
CONTESTS
CHAPTER 2. REINFORCEMENT LEARN-
ING IN CONTESTS
also say that zero effort level is not part of the strategy space; rather, it is considered as the
condition of dropout. Similarly, once the agent has dropped out she does not move to any
positive effort level unless there is an exogenous perturbation.
The pij(t) is updated every period based on the outcome, that is a win or lose. The SPW
update on strategies starts from q below and r above strategies w.r.t. the last period strategy,
in the case of winning and losing respectively. The updating rule above states how an agent
transitions from one strategy to another based on the outcome, given all strategies are not
exhausted, which means that the SPW for at least the highest strategy is above the threshold.
When all strategies are exhausted, the agent has a small  probability of bouncing back to
any arbitrary SPW and restarts decision making as stated below.
2.4 Asymptotic Analysis -Tullock Contests
We are interested in understanding the asymptotic state of the game. Given pij are dependent
on its immediate previous value, it serves as state space in a discrete-time, finite-state Markov
chain.8 State-space can be defined as SPW profile
p11, ..,p
n; ..;p
1 , ..,p
. If the learning speed
of the agent is high, then there will be fewer elements in the SPW set of that agent. The
transition probability will depend on the SPW profile and target probabilities of all agents,
which will not change with time. This means that the Markov process is time-homogeneous.
A simplifying assumption made is that q=r=1.
Absorbing State: In this model, the term absorbing state is used in its usual meaning with a
specific9 description that in this state one agent wins by putting in minimum effort and other
agents have dropped out and hence no agent change their SPWs thereafter.
Winning Set: The set Ai which has an agent i as the final winner with all possible dropout
SPWs (pij = 1j and p
j < 
kk , i&j )10 for other agents who have dropped out is called
winning set i.
Unperturbed Model:
Lemma 1: An initial state where all agents have their SPW above their TPW for at least one of
the effort levels, there is a positive probability that process can be absorbed in any of the winning sets.
Proof: If all agents have at least one SPW above the threshold, then all agents will put in a
positive effort, which gives a positive probability for every agent to win. There is a positive
probability that agent i wins every period, which means that there is a positive probability
that the process will be absorbed in the winning set Ai .
Lemma 2: In the unperturbed model, from every state, there is a possible path to reach at least one
of the winning sets.
8Note, this is a non-standard Markov model where TPW cannot be modeled as part of the transition matrix.
The complexity of the setup makes deriving analytical results difficult due to intractability. Indeed no literature
was found providing a method of analysis for such a process.
9The specific description is only to bring contextual clarity, other than that the term holds its usual meaning
10A general element in set Ai can be written as (1,1,1, ...1(ntimes);< 
k , ...,< k(ntimes); ..;< l , ...,<
l(ntimes)..(f or total m 1players))
47 PhD Thesis
2.4. ASYMPTOTIC ANALYSIS -TULLOCK
CONTESTS
CHAPTER 2. REINFORCEMENT LEARN-
ING IN CONTESTS
Proof: From Lemma 1 it is known that this is true for any state where all the agents have
SPW for at least one strategy above the threshold. A similar argument could be made for
any general state. For any given state, consider those agents who have SPW for at least one
strategy above the threshold 11. The way the model has been defined is that not all the agents
will drop out. All these agents will put in a positive effort and will have a positive probability
of winning in this period. There is a positive probability that any of these agents wins every
following period, which means that there is a positive probability that the process will be
absorbed in the corresponding winning set.
Proposition 1: If agents do not bounce back (unperturbed model) then the process will be ab-
sorbed in one of the winning sets.
Proof: In Lemma 2 it is shown that from any state it is possible to reach at least one of
the absorbing states. There are only a finite number of states; hence, the process will be
absorbed.12 With the dropouts not bouncing back, the transition matrix could be identified
as absorbing Markov process with at least m (number of players) absorbing states with at
least one state associated with the dropout of each agent. Using the theorem for absorbing
the Markov process, it could be stated that the process will be absorbed by one of the states in
a finite time. This means that the aggregate effort in the game will decrease, below the Nash
equilibrium of the standard model, after some time. Depending on the relative parameters of
the agents in the group, there would be different trajectories of aggregate effort with different
average effort (across periods) for each trajectory.
Proposition 213:Agents dropout probability increases with an increase in her relative  (TPW)
and relative  (learning speed).
Support: The above proposition is supported by the below simulation results. 1000 2-
players games with different parametric values (generated randomly) are simulated to study
the significance of relative values of parameters in an agents probability of dropout. The
parameters are classified into four categories as shown in Table 2.1 below. The simulation
results are shown in Table 2.2 in which it is clear that as the agents relative TPW or relative
learning speed increases the dropout probability of the agent increases. The results hold for
any value of the sum of TPW of the two agents.
Table 2.1: Parameter classification for a 2-players game.
11A special initial condition can arise where all the SPWs for all the agents are below their TPWs. Here it can
be assumed that the game is restarted.
12Refer to Appendix for mathematical theorem used.
13Holt and Roth (2004) conclude with the following: Looking ahead if game theorys next 50 years are
to be as productive, the challenges facing game theorists include learning to incorporate more varied and
realistic models of individual behavior into the study of strategic behavior and learning to better use analytical,
experimental, and computational tools in concert to deal with complex strategic environments. Given the
non-standard modeling of the learning model in Tullock contests as the Markov process, it calls for use of
computational methods to achieve more insightful results. Our learning model and simulation results are in
this spirit.
48 PhD Thesis
2.4. ASYMPTOTIC ANALYSIS -TULLOCK
CONTESTS
CHAPTER 2. REINFORCEMENT LEARN-
ING IN CONTESTS
Table 2.2: Dropout frequency based on parameter classification for a 2-players game.
Proposition 3: As the aggregate target probability of winning increases in the game, agents drop
out sooner.
Support: The above proposition is supported by the simulation results. Tables 2.3 and 2.4
show the number of agents who drop out after a fixed period for various values of aggregate
target probability of winning. It is observed that as the aggregate TPW increases the number
of agents dropping out, within a fixed time, increases.
Table 2.3: Number of dropouts after 1000 periods in a 3 players game for various values of
aggregate TPW.
Table 2.4: Number of dropouts after 1000 periods in a 3 players game for various values of
aggregate TPW.
Perturbed Model:
In the unperturbed model, we assumed that agents do not bounce back. However, it is possi-
ble and at times seen in real life that agents who drop out do bounce back. This possibility
is incorporated into the perturbed model analyzed in the next section, although this model
does not explain why the agent bounces back. One possible reason could be that agents
experiment. Now, the attempt is to understand what will happen to the aggregate effort levels
if all the agents bounce back with some positive probability. No pattern is assumed in terms
of where agents bounce back, rather the bounce back is allowed with an unrestricted set of
new SPWs for the dropout agents. Although the bounced back agents SPW is unrestricted
(only defined by its learning speed), the states to which it can bounce back in the state space
are restricted. This is because the agents who have not dropped out will not change their
SPWs. These states will depend on where the other agents are present in the period when the
agent bounces back. If a player is allowed to bounce back with an arbitrary set of new SPWs,
49 PhD Thesis
2.4. ASYMPTOTIC ANALYSIS -TULLOCK
CONTESTS
CHAPTER 2. REINFORCEMENT LEARN-
ING IN CONTESTS
then agents may not stay in the same set of states to which the process was confined when no
bounce-back phenomenon occurred.
Assumption: It is assumed that irrespective of a dropout SPW of the agent, all agents have a
positive probability of bouncing back to any arbitrary set of SPWs. This means that there is a
positive probability of reaching any of the absorbing states within any winning sets.
Proposition 4: If players after dropping out bounce back with positive probability (perturbed
model) to an arbitrary set of new SPWs, then a limiting distribution will be reached.
Proof: The state space is such that at least one of the winning sets is reachable from every state.
The model is such that the process does not stay at the same state next period. Proposition
1states that the process will be absorbed if there is no bounce back. Thus, irrespective of
where the process starts, it reaches one of the absorbing states.
Assume that each agent drops out at a unique SPW. This means that if the agent has
dropped out, she will have a unique SPW which will not change unless she bounces back.
Nevertheless, this is only a simplifying assumption. Later it is argued what happens if
dropout SPWs are not unique.
Let Ti be the set of states to which other agents except i can bounce after dropping out.
This includes all the states where all agents (except i) drop out. This also includes the state
(Ai) where the agent i is the final winner and is putting in minimum effort to win the game.
The state-space can be defined as S =
1im
Ti  T , where Ti is for the agent i and T is the
union of states which are not reached by any agent after bouncing back. For each i there
are (m 1) sequence in which other agents can drop out to make i reach Ai . It is not known
for sure that in the state when the last agent drops out whether the agent i has reached her
highest SPW or not. If not, then agent i continues to decrease their effort and increase their
SPWs to reach Ai .
Now lets take the extreme case where each sequence of dropout for any final winner
i consists of a unique set of states, transitioning from one to another and reach Ai . Our
objective is to show that there is one recurrent class.
From the state, Ai , all the agents except i have some positive probability to bounce to any
set of SPWs above their threshold at which they dropped out. States to which agents (except
i) can bounce will be part of some chain corresponding to the dropout sequence of player i
as the final winner. Hence, all the states following the bounce-back state will become a part
of a communication class.
The SPW profile (with each player above threshold SPWs) to which agents have bounced
back has a positive probability for any player to be the final winner (from Proposition 1).
Hence, at least one sequence of dropouts for each player being the final winner will be
the part of this communication class. Since without bounce back all the states in the state
space lead to some sequence of dropout (absorbing process), there cannot be any other
recurrent class not communicating with this communication class. Hence there will be a
single recurrent class. The model is such that the agent can transition to two states with
unequal probabilities, in general, so an irreducible chain is aperiodic. Hence, an irreducible
aperiodic chain will reach a limiting distribution.
In Proposition 1, it is assumed that there is only one absorbing state in each winning
set. Here it is argued that even if there are multiple absorbing states in all the winning sets,
then also the limiting distribution will be reached. This is because each absorbing state
within the winning set has a positive probability to bounce back to any arbitrary set of SPWs.
The states which could be reached from one absorbing state in a winning set are possible
to be reached by any other absorbing state in that winning set. This means that there will
50 PhD Thesis
2.5. MODEL PREDICTIONS CHAPTER 2. REINFORCEMENT LEARN-
ING IN CONTESTS
be a positive probability of being absorbed in any of the absorbing states within a winning set.
2.5 Model Predictions
Agents will decrease effort on consecutive winning while they increase the effort on consecu-
tive losing. This is because agents draw directional feedback from outcomes and update their
SPWs accordingly. The prediction does not depend on the number of strategies for which
they update their SPWs until they update one strategy below the winning strategy and one
strategy above the losing strategy.
Following overbidding, some players will drop out (they may bounce back). This can
be understood as learned helplessness. When agents are not able to win even after trying
harder, they quit. In terms of the model, after agents lose, they decrease their SPWs for some
strategies higher than the lost strategy and with consecutive losses moves to higher strategy.
This continues as long as there is some strategy for which SPW is greater than or equal to
The aggregate effort will be higher than the risk-neutral symmetric Nash equilibrium
prediction at the intermediate run and will then start falling if they do not bounce back. In
a winner-take-all, only one agent wins whilst the remaining agents lose and increase their
effort. This dynamic continues until agents start dropping out and then aggregate effort
starts to decrease. From the proof of Proposition 4, it can be said that the average aggregate
effort is probabilistic if agents bounce back. In this case, the aggregate effort could be higher
or lower compared to the Nash equilibrium of the standard model.
The objective of this learning model is to be able to explain how agents may learn if
they are driven by the probability of winning. One may like to ask whether this model with
specific values of the parameter can lead agents to reach close to the equilibrium predicted
by the standard model. It can be a valuable exercise to analyze this model for the case when
learning speeds are asymmetric for the case of positive and negative reinforcement. We
believe that when agents have i = n i , where i is the learning speed in case of positive
reinforcement and  i is the learning speed in case of negative reinforcement, then the agents
will reach around some effort levels and stay close to it for a long time. If they start with the
initial effort levels predicted by the standard model and their minimum probability as 1m
then they will stay close to equilibrium predicted by the standard model for a reasonable time.
2.6 Experimental Evidence
Now discussion will follow on what some known models considered relevant in the related
literature predict and whether these can explain the above behavioral regularities together.
Next, simulations are run for this learning model to see if it can track the experimental data.
The primary data considered is from one of the experiments in Falluchi, Renner and
Sefton (2013) (FRS, henceforth) where the Tullock contest is played by fixed players with
their own feedback. There are ten games played among three agents for sixty periods each.
Prize money is 1000 ECU (=  0.15 ) and agents are given the endowment of 1000 ECU at
the start of each period. Agents know their own efforts and outcomes each period, but do not
know the efforts and outcomes of other players. The contest success function, prize money,
endowment, number of periods in the game, and number of players are common knowledge.
Again mentioning briefly the behavioral regularities observed in this experimental data.
Over-dissipation is a consistent finding across the games. Following over-dissipation, some
51 PhD Thesis
2.6. EXPERIMENTAL EVIDENCE CHAPTER 2. REINFORCEMENT LEARN-
ING IN CONTESTS
players drop out over the course of the game. In terms of individual behavior, it is frequently
observed that agents decrease effort on consecutive winning while they increase the effort on
consecutive losing (Table 2.6). There are agents whose participation is limited throughout the
experiment. Conversely, some agents persist with remarkably high efforts despite losing so
frequently that they make a net loss. Many agents start with a considerably high level of effort.
It is observed in these games that on average, agents start high compared to risk-neutral
symmetric Nash equilibrium prediction and further increase their efforts in the next few
periods before aggregate efforts start decreasing. Similar data from one of the experiments in
Mago, Samak and Sheremeta (2016) (MSS, henceforth) is also studied. In this experiment,
15 Tullock contests are played, each having four fixed players for 20 periods with their own
feedback only. The endowment and prize money is 80 francs (15 francs = US$1) and the
experimental setting is common knowledge. The agents behavior in this experiment is
similar to FRS. The aggregate data in MSS are less noisy while the individual data are noisier
compared to FRS.
Table 2.6: Impact of winning and losing on the next period effort decision of the agent in
Standard Model
The behavioral regularities observed in the experimental data cannot be explained by the
standard model. If agents are playing as per contest reaction function based on forming
beliefs on other agents effort, then the direction of dropout (if at all) should be preceded by
a decrease in effort, not an increase in effort as more often observed in the data. The agent
should not put in any effort more than the prize value divided by the number of agents, for a
risk-neutral case (Figure 2.6).
Figure 2.7: The contest reaction function for any beliefs of other agents aggregate effort,
assuming risk neutrality(r=0). Reaction function is given by V
(1r)
(x+y)2
= yr where y = ei and
x = ei
Risk Preferences
52 PhD Thesis
2.6. EXPERIMENTAL EVIDENCE CHAPTER 2. REINFORCEMENT LEARN-
ING IN CONTESTS
Millner and Pratt (1991) find that risk preference can explain the differences in the effort
levels while Potters, Vries, and Winden (1998) find the opposite. However, broadly speaking,
being more risk-averse means less effort and risk-seeking means higher effort. In Figure
2.8 below, the general equation for symmetric NE with risk as a parameter and its graph is
provided for the case when agents have a CRRA utility function. One can argue that agents
are playing symmetric Nash equilibrium with different risk levels. While it can explain high
effort by high risk-seeking preference, it cannot explain the dropout preceded by high efforts.
One may further argue that agents are risk-seeking and when they learn that they cannot
win, they drop out. A simple objection to this explanation is the observation that agents who
do not drop out fail to stabilize at any effort level. This raises the question as to whether it is
only agents who drop out who stabilize. Agents frequently change the direction of their effort
level even after many periods in the game, which makes a risk-preference-based explanation
on its own unconvincing. Another treatment from FRS is that of full feedback, where it is
observed that very few agents drop out. This suggests that the absence of feedback makes
agents approach the game differently. Cornes and Hartley (2003) explain over-dissipation
using heterogeneity in risk aversion of agents maximizing earnings. In Chapter One we show
that TPWs and EUMs have the same risk distribution and this learning model explains other
behavioral regularities in addition to over-dissipation.
Figure 2.8: A general expression and graph showing effort for symmetric Nash equilibrium
with different values of risk for CRRA utility function. On X-axis is the r (risk preference)
and Y-axis is the optimal effort.
Joy-of-Winning and Relative Payoff Maximization
In the literature, several behavioral explanations have been proposed for over-dissipation in
the Tullock contest. Sheremeta (2010) finds that in addition to monetary incentives subjects
derive a non-monetary utility from winning which leads to higher effort levels. This can
partially explain the higher effort levels, but cannot explain the dynamic behavior of agents
53 PhD Thesis
2.6. EXPERIMENTAL EVIDENCE CHAPTER 2. REINFORCEMENT LEARN-
ING IN CONTESTS
over the course of the multiple period games. MSS state the residual reason for overbidding
is, in addition to the utility from winning, that the subjects care about their relative payoffs.
It seems less likely for the experiment in consideration that anonymous players in the lab
can track the relative payoffs when no feedback is present on players cumulative winning in
the game.
Mistakes and Judgmental Biases
Another set of explanations are based on the argument that subjects are prone to mistakes,
judgmental biases such as non-linear probability weighting (Baharad and Nitzam, 2008), and
the hot hand fallacy. These mistakes add noise to the Nash equilibrium solution and thus
may cause overbidding in contests. While, in general, this can be true that for the first few
periods of the game, data under consideration show that agents put in higher effort even
after many periods in the game and respond to winning and losing in a predictable way. This
suggests that high bids are not mere mistakes; rather, a deliberate choice. In a game with a
large number of agents, winning should be a rare event which means that with consecutive
losing (experience) an agent will be under-weighting the probability of winning which should
make agents decrease the effort and not increase it.
Regret Theory and Impulse Balance Equilibrium
In first-price auction literature, regret theory (Engelbrecht-Wiggans and Katok, 2007; Filiz-
Ozbay and Ozbay, 2007) and impulse balance equilibrium (Ockenfels and Selten 2005) can
explain the directional changes in case of winning and losing every period. The environment
under consideration is fundamentally different in two aspects: first, losing is not costly in
the first-price auction; secondly, agents can deduce whether the last period winning bid was
higher or lower than their bid. The aggregate bidding behavior is different from the all-pay
auction in which bimodal bidding is witnessed.
Aspiration
A model where agents have exogenous aspiration levels as the reference point can also
explain this increase and decrease in the effort after losing and winning, respectively. One
can see that mathematically target probability of winning and aspiration look equivalent. In
a repeated game, lets say an agent has a target probability of winning equal 0.5 and the prize
money is 1000 then aspiration (reference point) will be 500. Another (mis)interpretation of
target probability in the repeated game can be the number of times agents want to win; in
the above example, it will be 50%. As can be seen from the data, agents appear to play the
game as a one-shot incorporating their experience. In such a case, aspiration in a one-shot
game with a fixed prize is not appealing. The fundamental difference is the psychological
interpretation of these as well. The higher value of the parameter target probability of
winning can be psychologically classified as the need for security while the higher aspiration
can be classified as a desire for achievement.
If the agents are driven by an endogenous aspiration as a reference point (Borgers and Sarin,
2000) they would increase the effort on winning and decrease effort on losing. The two
theories (learning model based on a target probability of winning vs endogenous aspiration)
predict the opposite behavior. If it can be understood that in which environment one will
be true, it can help understand the agents decision-making approach and can prescribe the
efficient prize allocation scheme for different environments.
54 PhD Thesis
2.6. EXPERIMENTAL EVIDENCE CHAPTER 2. REINFORCEMENT LEARN-
ING IN CONTESTS
Collusion
In an all-pay auction with fixed matching treatment, Lugovskyy, Puzzello and Tucker (2010)
argued that agents may learn how to collude by reducing their effort. This may seem an ex-
planation for the case when agents under-dissipate but in the limited feedback environment
collusion seems less likely.
If McKelvey and Palfreys (1995) model is applied to the data under consideration, it will not
be able to explain since over time many agents move further away from NE, and dropout be-
havior cannot be explained convincingly. Lima, Matros and Turocy (2014) combine cognitive
hierarchy and quantal response equilibrium to model responses of agents that can explain
over-dissipation in contests experiment. In FRS data studied in this chapter, the responses
are though stochastic but skewed in one predictable direction based on whether it is win or
lose and moves substantially across the effort levels including dropout.
Evolutionary Stable Strategy
Hehenkamp, Leininger, Possajennikov (2004) explain over-dissipation by arguing that agents
play an evolutionarily stable strategy. While a high target probability of winning may give
agents an evolutionary advantage if winning every game (not net payoff) is the criterion of
survival, a static model cannot explain the dynamic behavior observed in this data.
2.6.1 Simulations
The objective of the simulation exercise is to see if it can track the aggregate data and
understand the possible trajectories for various values of the parameters (Fig 2.15-2.17 in
Appendix). Given the probabilistic nature of the game and the difficulty of estimating the
parameters of an individual player, the period-wise comparison of the actual behavior of an
agent in the experimental data with the artificial agent in the simulation cannot be made.
What can be said is the behavioral regularities observed and aggregate dynamics found in the
experimental data are also present in the simulation data. The artificial agent is driven by the
model while a real agent would be subject to sharp changes and some random behavior. This
calls for qualitative behavior match for individual behavior rather than strict quantitative
comparison. A simplifying assumption made is that q=r=1.
Based on this descriptive model of decision-making, simulations are run for the three-
player game played for 60 periods, which is then compared with experimental data. It is as-
sumed that agents do not bounce back. A total of 1000 games are simulated for 60 periods, i
assigned for each agent randomly from [0.2, 0.4] for each i  {0.05,0.075,0.1,0.125,0.15,0.175}.
Here all agents have the same learning speed. Further,simulations are run for the cases where
agents are assigned the learning speed randomly from the uniform distribution of [0.05,
0.1], [0.1, 0.15] and [0.05, 0.15]. The experimental data is of 10 games for 60 periods with
3 players each. The objective is to see whether the model-based simulation can track the
experimental data, assuming experimental data is representative of the large sample behavior
despite its size. Nevertheless, it needs to be emphasized that as the size of the data increases,
it is expected to see smoother aggregate behavior. The code used to simulate is given in the
appendix; below, the results are stated directly. The following are the simulation results for
55 PhD Thesis
2.6. EXPERIMENTAL EVIDENCE CHAPTER 2. REINFORCEMENT LEARN-
ING IN CONTESTS
aggregate effort compared period-wise with experimental data followed by comparison in
blocks of periods. Figures 2.9 to 2.12 refer to the FRS data described above. Figures 2.13 and
2.14 refer to MSS data described in the literature review.
Figure 2.9: Average effort across groups in FRS and various simulations based on the proposed
behavioral model.
Figure 2.10: The above graph is drawn for data in blocks of ten periods each.
Table 2.11: The table shows mean deviation (MD), mean absolute deviation (MAD) and
root mean square error (RMSE) between experimental data and simulation data for various
learning speeds for period-wise data.
56 PhD Thesis
2.6. EXPERIMENTAL EVIDENCE CHAPTER 2. REINFORCEMENT LEARN-
ING IN CONTESTS
Table 2.12: The table shows mean deviation (MD), mean absolute deviation (MAD) and
root mean square error (RMSE) between experimental data and simulation data for various
learning speeds for data in blocks of 10 period each.
Figure 2.13: The graph shows, period wise, the average effort across groups in MSS and
simulations based on the proposed behavioral model.
Figure 2.14: The above graph is drawn for data in blocks of five periods each.
Table 2.11 shows mean deviation, mean absolute deviation, and root means square error for
the aggregate effort for different learning speeds. These are for i in the interval [0.2, 0.4]. It
can be seen that the learning speed attributed to agents from the random uniform distribution
of [10, 15], and all agents with 12.5 learning speed tracks the data better. It is expected that
as the experimental data size would increase, the aggregate behavior observed for all the
games (black dotted line above) would become smoother. Looking at the simulations for
57 PhD Thesis
2.7. MODEL APPLICABILITY TO ALL PAY
AUCTION
CHAPTER 2. REINFORCEMENT LEARN-
ING IN CONTESTS
various parameters does suggest that the gradual fall in an aggregate effort at later periods
is a common behavior. In initial periods depending on the parameters, the aggregate effort
may rise before starting to decline. These simulations show that the decision-making model
proposed is general enough to capture the various trajectories of aggregate effort for different
values of parameters.
It is seen in the simulations that for low and medium values of the parameters (refer
to simulations for the six-player game provided in Appendix) there is an increase in the
aggregate effort before it starts falling. The decline in the aggregate effort is primarily due to
the dropouts and in later periods due to the consistent winning of the single-player remaining.
This is because agents start with low- or middle-level strategies, and they end up competing
for prizes before becoming exhausted with strategies. What is happening is that agents
are trying to search for effort levels where they could have a good chance to win, which is
confirmed only when they win. In cases where parameters are high, the aggregate effort is
predicted to start high and fall steeply. This is because the agents would start with higher
strategies and end up dropping out sooner as their strategies would be exhausted faster.
The difference in the game dynamics when the number of agents increases can be the
marginal difference in the probability of winning when one agent bids very high and others
are bidding low or average. If the number of players is low, then there is a significant increase
in the probability of a win by increasing effort. If the number of players is high, the increase
in the probability of a win is not significant. This is important in such a scenario in which
agents are trying to locate the effort level whereby they can win by a good probability and
they are revising decisions every period. So, if increasing the effort does not translate into
winning (chances of which would be higher in the case of a large number of agents) then it is
likely that the agent drops out soon. The model parameters(i ,i) which fit the experimental
data closely are higher for the four-player game compared to the three-player game.
With the maximum effort being 100, the deviation of aggregate effort by three agents
averaged over 60 periods is not negligible. The deviations substantially decrease when
the comparison is done in blocks of ten periods. From the charts, it can be seen that the
simulations approximate the aggregate experimental data better when compared period-wise
for the MSS compared to FRS. This may be due to the increased size of the data in MSS. The
deviations further decrease to this data set when the comparison is done in blocks of five
periods. This suggests that the model is approximately fit for aggregate behavior and thus
explains the individual behavioral regularities.
2.7 Model Applicability to All Pay Auction
An all-pay auction is another example of a winner-take-all competition that has a linearly
ordered strategy set. The basic learning model applies to all-pay auctions that have the same
information environment. The difference lies in the transition probabilities in determining
the winner post-selection of effort levels by the players in the game each period.
In the case of Tullock contests without perturbation, it is found that any agent has a
positive probability of eventually dropping out when both exert positive efforts. This was
due to the probabilistic nature of prize allocation where the probability of winning was
proportional to the ratio of effort to the total effort. In the case of an all-pay auction, the prize
allocation is deterministic. For any set of efforts in a game, the player with the maximum
effort receives the prize with certainty unless there is a tie. The nature of the learning rule
is such that the agent after losing increases effort, while the agent after winning decreases
effort. This allows for the possibility that agents may keep cycling in a few states without any
58 PhD Thesis
2.8. DISCUSSION CHAPTER 2. REINFORCEMENT LEARN-
ING IN CONTESTS
perturbation and may not eventually drop out. One needs to find parametric conditions that
lead to such cyclic behavior.
At the preliminary stage, we try to seethrough simulationsif such cyclic behavior is
found or not. If so, then in some two-player games none of the two agents will drop out even
after a very long time. Table 2.5 shows the number of agents who drop out after 1000, 10000,
and 100000 periods. It can be seen that there is no substantial increase in the number of
agents dropping out even after the number of periods increased by 100 times, around 25% of
games have no agent dropping out. This proportion of games is indicative that agents cycle
between some states.
Table 2.5: Number of dropouts after 1000, 10000 and 100000 periods in a 2-players all pay
auction game.
2.8 Discussion
A big picture summary of the model is that agents convert a game to a decision problem and
learn through reinforcement. With reinforcement learning their effective choice set changes
and hence their effort choice based on decision rule.
We have made two behavioral assumptions in this model. One is that the agents use the
approach of the target probability of winning, not payoff to make their decisions in contests.
The second is on how agents update their subjective winning probabilities upon winning and
losing. In this model, strategy similarity gives the reinforcement learning model a feature of
directional learning.
In the Tullock contest, agents can choose a target probability of winning which they like.
They do this by choosing the level of effort, making the trade-off between cost (potential
profit if they win) and the probability of winning.
The deterministic model has two parameters; a target probability of winning (i) and
learning speed (i). Although initial SPWs and learning speed would impact the transition
and asymptotic state that the process would reach, what drives the behavior is the parameter
TPW and strategy similarity.
The intuition based on which we postulate TPW is that agents in a probabilistic environ-
ment think only if they win will they make a positive profitwhile if they lose, they will
make a loss. Either agents are not farsighted, or they do not believe that the environment
is stationary, so they gather directional feedback only from the outcome of the last period.
Agents rapid change in effort choices can be captured by this model. That is possibly the
reason they do not put in the same effort across all the periods of the game.
This deterministic model can explain the observed individual pattern of behavior qual-
itatively, which is in contrast to what the standard model would predict, hence it serves
its basic purpose. It could be observed in the three-player experimental data that agents
chosen strategy (effort) profile can be explained by the deterministic model. Note, agents
after changing the SPWs may still be at the same effort levels.
We have modeled the behavioral parameter TPW as an exogenous intrinsic behavior.
The change in effort level is explained by a change in SPWs. It could be interpreted that in
this memory-less model, agents infer that after winning their minimum probability is met
and in case of losing they infer the opposite. This occurs independently from the behavior
of other agents in the game which is acceptable as the behavior of the other agents is not
59 PhD Thesis
2.8. DISCUSSION CHAPTER 2. REINFORCEMENT LEARN-
ING IN CONTESTS
observable. Indeed, the number of agents in the game is known but their target probabilities
are unknown and unobservable for other agents.
In this model, we have assumed learning speed to be symmetric, for positive and negative
reinforcement. In FRS data, we see that the degree of positive reinforcement is stronger.
This may be because in this environment reinforcement is based on its information value
to guide direction. Winning gives agents a stronger sense of direction than losing. The
model predicts that there can be variation in aggregate efforts. A survey of the experimental
contest literature by Dechenaux, Kovenock and Sheremeta (2015) has highlighted that on
average agents spend considerably more than the equilibrium prediction of the standard
model. Shogren and Baik (1991) find that agents expend the amount closer to the rational
model prediction, while in Millner and Pratt (1989) and Sheremeta (2010) agents expend
significantly more.
It can explain dropout behavior in a dynamic setting. Muller and Schotter (2010) find
that low-ability workers drop out and exert little or no effort, while high-ability workers
make excessive efforts. In a static model, this is explained by assuming loss aversion on the
part of the subjects, such explanation will fall short in a dynamic setting.
Further study shows that there is an overall decrease in expenditures when subjects
repeatedly play (Fallucchi, Niederreiter and Riccaboni, 2020) and some agents choose to stay
at zero bid for a period of time or for the remaining periods in the game after bidding for
some initial periods.
The specific game would impact the target probability of winning agents may have. For
example, in a first prize auction, the agent may have a higher value of probability of winning
compared to the Tullock contest, which may further increase in the case of an all-pay auction.
Conversely, in the literature (Muller and Schotter, 2010), the dropout behavior in equilibrium
is explained using loss-aversion in the case of an all-pay auction. The dropout in this model is
explained as a dynamic behavior where agents learn that their target probability of winning
is not achievable. Similarly, the remarkably high effort could be explained as agents trying to
ensure their minimum probability before they learn to drop out.
The behavioral regularities (increasing effort after losing and decreasing effort after
winning) and aggregate behavior in the four-player game in MSS is similar to the three-player
game in FRS. The simulation can track the data for MSS with parameter values slightly
higher. This suggests that agents use the target probability of winning to make a decision
that has uncertainty in the severely limited information game as the underlying cause of such
behavior. This model also explains why agents keep exerting effort even if their cumulative
profit is negative. Some agents do not vary their effort substantially over the periods in the
game, these could be explained by assuming their learning speed is low. The asymptotic
prediction (Proposition 1) can explain why generally there are not more than two (out of
three) players bidding consistently later at the end of the sixty-period game.
This decision-making model is deterministic where agents do not ever stay in the same
state. One may like to change it to a stochastic model where the process transitions, with
higher probability to the state predicted by the deterministic model, stay at the same state
with some probability and transitions in the direction opposite to what the deterministic
model predicts with a lower probability. The reason for preferring a deterministic model
over the stochastic model is the ease of modeling the behavior and highlighting the decision-
making process that the agent experiences.
One objection to our model of TPW can be that sum of TPW of subjects in the game does
not necessarily add to one. Tversky and Kahneman (1974) give examples to support that a
sum of probabilities not being additive is not completely unusual. This paper state that the
use of heuristics can lead to systematic errors in the judgment of probabilities. They provide
examples of some of the approaches, along with the evidence from the lab, used by people
60 PhD Thesis
2.8. DISCUSSION CHAPTER 2. REINFORCEMENT LEARN-
ING IN CONTESTS
(including experts in the field) to make real-life decisions.
In this model, the agents do not change their target probability of winning over the
periods of the game. The choice of efforts by the agents observed in the simulation data is the
result of their SPWs and TPW. One may further develop a model where TPW evolves with
learning and analyze it to find intermediate predictions.
Here, all the players are considered to be TPW type. The experimental evidence in
Chapter One suggests it is a mixed population. It will be interesting to study game dynamics
when two types (EUM and TPW) are in some proportion. It seems that TPW will be wiped
out in the evolutionary process but it might have interesting results if intermediate winning
scores are also important.
The model is developed for Tullock contests; nevertheless, we believe it can be adapted to
other winner-take-all games and indicate its applicability to all-pay-auction. Similarly, its
predictions can also be found in different informational environments since agents could be
playing the game similarly. Fallucchi, Niederreiter and Riccaboni (2020) find that in Tullock
contests, many players learn more from their own past payoffs despite having information
feedback on total bid amounts.
Morgan, Orzen and Sefton (2012) find that a key difference between the theory and actual
behavior in contests is the variability of investment decisions over time and the endogenous
entry could not discipline the market. The paper also summarizes findings of over and under
dissipation in different experimental studies in Tullock contests. The trajectory of aggregate
dissipations appears similar to what our model tries to explain for standard Tullock contests.
The model in this chapter contribute to an understanding of over and under dissipation. It is
a matter of further analysis to see if it can be adapted to explain the trajectory in the case of
endogenous entry in the market games.
61 PhD Thesis
Chapter 3
A Mechanism and Matching in a Social
Dilemma
3.1 Introduction
The most important unanswered question in evolutionary biology, and more generally in
the social sciences, is how cooperative behavior evolved and can be maintained in human
or other animal groups and societies (Robert May in his Presidential Address to the Royal
Society in 2005).
From the perspective of game theory in economics, this chapter seeks to address an aspect
of the above conundrum posed in Mays seminal speech. The focus of this chapter is how
cooperation can be evolved in humans in a private interaction in a symmetric market where
any standard mechanism alone does not achieve stable equilibrium. A simple equilibrium
strategy achieving stable equilibrium without the requirement of following social norms
(e.g, explicit punishment) seems to be a real-world implementable. One such strategy can
be honest reporting and full contribution. It will be shown that an enforcement mechanism
facilitating assortative matching achieves honest reporting and full contribution as evolution-
arily neutral stable equilibrium. It will simulate how accuracy in trust ratings in an online
environment can best be achieved. 1
Consider the situation in contemporary times where our social and professional spheres
keep shifting rapidly, involving people interacting with many unknown people for a lim-
ited time including some important interactions happening online with strangers. While
interacting with others in specific and significant situations we try to know what kind of
person they are. We do this by gathering information about the person through the accessible
sources of information we have. There are a few critical limitations to this approach. It is
limited by time and information sources we may have. Sources may be biased, although we
cannot remove the subjectivity with which people perceive others. We may not have time to
process these sets of information and may lack the skills and experience to derive a useful
interpretation from these.
One way in which cooperation can be achieved is through bottom-up self-regulation
(Helbing, (2013) assuming there is a proportion of the society which wants to stay cooperative.
One way to stop free-riding is to provide them with information on what to expect from
a possible interaction. A reputation system that has good accuracy in predicting what to
expect from interacting with someone can help to enhance cooperation (Milinski et al., 2002;
Brandt et al., 2003). Some of our real-world economic systems do have such reputation
1The first version of this chapter was presented in November 2017 in the Brown Bag Seminar Series in
the Economics Department at the University of Exeter with the title A Reputation Mechanism in a Social
Dilemma
3.1. INTRODUCTION CHAPTER 3. A MECHANISM AND
MATCHING IN A SOCIAL DILEMMA
systems to facilitate interactions among strangers. For example, seller-buyer ratings and
driver-passenger ratings. Although these may be manipulable, in general, they are widely
acknowledged to play a role in improving efficiency and providing the necessary trust for
transactions to happen compared to environments where none existed (Bolton et al., 2004;
Cook et al., 2009; Bolton et al., 2013).
In the real world, explicit punishment seems difficult to achieve as people trade based on
whether it is profitable, regardless of history per se. Auction Web sites such as eBay are good
examples of an implicit form of punishment (Resnick and Zeckhauser, 2002) since these
websites provide a forum where buyers can report their personal experiences with sellers
and vice versa. These reports presumably lead to assortative matching (fully contributing
buyers avoid less fully contributing sellers) and hence incentive to contribute fully to the
trade even though there is no mechanism to check if the reports are honest. Here the market
is asymmetric as buyers and sellers are from different populations and they do not compete
between themselves. Sellers compete among themselves to obtain a good buyer and buyers
compete among themselves to obtain a good seller.
Some of our interactions are bilateral, a two-sided incentive problem, where both the
agents privately contribute to the well-being of each other. What the two parties have
contributed to each other is known only to the economic agents involved, for example, driver-
passenger interaction. In such an environment there is an absence of accurate information
to build a reputation. In this specific example, the two sides do not have incentives to
misreport about each other, assuming no other behavioral factors are at play and there is no
subjectivity in the evaluation. One may like to use simple reporting for building a reputation
in such interactions. If the environment is such that agents have reasons to misreport, like
an implicit competition to gain a relative reputation, then simple reporting is manipulable.
Moreover, when behavioral factors are at play (tendency to report negatively when it is not
costly or altruistic people who give only high evaluations irrespective of their experience)
and frequency of interactions is low then these reports may not give an accurate estimate of
the reputation.
Collaboration could be required due to a complementary skill set, experience, or efficiency
in general. Take, for example, a population of freelancers (Fiverr, Upwork, Freelancer.com,
etc) who collaborate. Their cooperation is essential for the efficient completion of the project.
Only they would know how much each of them has contributed to the project. While they
are cooperating, they are also competing for reputation in the network. Call this a symmetric
market where each agent in the population is competing with others to interact with agents
who most contribute. Their future valuable interactions would be decided based on their
relative reputation in the network. They have an incentive not to reveal the truth to others
about their and their partners contribution to the project. Another example of a symmetric
market is same-gender dating and a closer example is researchers collaborating around the
world regardless of their geographical location.
Another example could be seen in social interactions where people meet unknown people
in their workplace or residential societies. They collaborate for social utilitymutual help
allows them to meet personal fulfillment. These interactions could be classified in terms
of economic behavior. Costly efforts are expended to help someone in the expectation that
others would reciprocate help in the future when they need it. In globalized societies many
interactions are sporadic and not enough is known about the new people we meet. Without
any credible reputation mechanism, it becomes rational not to put any costly effort into
helping someone. The trade-offs of the environment are similar to cooperation vs natural
selection and cooperation can be evolved only when a mechanism exists to support it (Nowak
2006).
In an environment where agents interact with the same person infrequently, direct
63 PhD Thesis
3.1. INTRODUCTION CHAPTER 3. A MECHANISM AND
MATCHING IN A SOCIAL DILEMMA
reciprocity is not feasible. Indeed, online interactions seem to be moving us in that direction.
Nowak (2006) states that indirect reciprocity is one of the five rules for cooperation to evolve.
Sigmund and Nowak (1998) find that image scoring can evolve indirect reciprocity leading
to cooperation as a stable equilibrium. Ohtsuki et al. (2015) develop a hybrid model where
there are public interactions and private interactions that have some probability of being
revealed. They find that for a range of parametric values, cooperation can be achieved as ESS.
Our environment allows only for private interaction. In a symmetric market, agents have an
incentive not to reveal the truth: agents lack an incentive to cooperate in this environment.
However, Kandori (1992) shows that if there is a reputation mechanism, then cooperation
can evolve with the help of social norms.
So, lets say there is a population where agents are scored based on the reports provided
by the other agents they have interacted with. The reports are on how much the agents have
contributed to each other. The nature of the interaction is such that both agents could put
in costly effort to increase the well-being of the other member. Now our research question
is first if there can be a mechanism that can be used to report the experience that both the
interacting members had with each other and if so, will assortative matching facilitated by
such a mechanism achieve stable cooperative equilibrium? A quantitative form of such a
mechanism is proposed which is similar to the qualitative idea of matching reports introduced
by Ben-Porath and Kahneman (1996).
This mechanism is used to generate a score (reputation) for the two agents based on their
contribution in stage one and reporting in stage two. In the next period, agents are matched
assortatively based on these scores. Assortative matching facilitated by this mechanism
achieves full contribution and honest reporting as evolutionary (neutral) stable (Smith and
Price, 1973). The results hold for matching based on the infinite history of scores as well as
the finite history of scores for a range of parametric values.
This reputation mechanism converts the environment of no monitoring to a case of a
social planner (a software application) monitoring to some degree. It is not public monitoring
as the reputation scores are not seen by any agent in the population. What agents know is
that they would be matched based on their reputation. However, they do not see the exact
score of the opponent, nor do they exactly know their own score. This kind of restricted
information sharing may seem unwanted, but it will be shown that the full information
sharing is not evolutionarily (neutral) stable. One may argue that in real-world side payment
(after contribution in stage one and before reporting in stage two) is possible which may
make the reputation mechanism ineffective. It is not obvious to us if the side payment would
certainly lead to a breakdown of the mechanism.
Contribution: This chapter is in the spirit of economists working as engineers (Roth 2002)
to design real-world implementable solutions. The literature in biology (see, for example,
Nowak, 2006) finds that cooperation can evolve through indirect reciprocity. In the standard
model of indirect reciprocity, some agents can accurately observe others interactions and
truthfully report them. The environment of concern is that of private interaction and a sym-
metric market. Our approach to design a reputation mechanism for a continuous prisoners
dilemma (PD)in which an optimal outcome for one party is dependent on their cooperation
with othersis similar to the abstract idea of information matching in game-theoretic litera-
ture (see, for example, Ben-Porath and Kahneman, 1996). The mechanism by itself does not
yield cooperation (full contribution) as a unique equilibrium in this environment. When this
mechanism is used to facilitate assortative matching, full contribution and honest reporting
as evolutionarily (neutral) stable equilibrium is achieved. A stable equilibrium that does
not require sophisticated strategies or explicit punishment strategies as social norms (see,
for example, Kandori (1992)) on part of the players is what, we believe, can serve towards a
64 PhD Thesis
3.2. RELATED LITERATURE CHAPTER 3. A MECHANISM AND
MATCHING IN A SOCIAL DILEMMA
real-world implementable solution.
The chapter is organized as follows, in Related Literature review of some of the literature
is done from which the chapter borrows and contributes to, in Basic Model section the two-
stage games including the reputation mechanism is introduced, P roperties of Reputation
Mechanism section studies unilateral deviations to understand what equilibrium it can
achieve standalone, Equilibrium Analysis section utilizes ESS as a solution concept to study
the equilibrium which can be achieved when the mechanism facilitates assortative matching
and Discussion section further comments on the basic model. Few terms are explicated as
follows:
Strategy: The decision on the amount of contribution an agent makes in the continuous PD,
and the decision on how much to report as self-contribution and how much as a partners
contribution. Formally, strategy of agent i matched with agent j is defined as a triplet
(ci ,CSi(ci , cj),GSi(ci , cj))
Incumbent: The initial population has the strategy of full contribution and honest reporting.
Mutant: Any small proportion of the population deviating from the strategy of full contribu-
tion and honest reporting.
Reputation-Length: The number of periods for which awarded scores are considered for
matching, is denoted by T .
Visibility Condition: It is assumed that agents do not see the awarded scores of the agent
they are matched with.
Evolutionarily Stable Strategy: If all the mutations (strategies different from incumbent)
asymptotically decrease their proportion in the population, then the incumbent strategy is
evolutionarily stable.
EvolutionaryNeutral Stable Strategy: If there is no mutation (strategy different from incum-
bent) that can increase its proportion in the population asymptotically, then the incumbent
strategy is evolutionary neutral stable.
Social Dilemma: A social dilemma is a situation in which all individuals would be bet-
ter off cooperating but fail to do so because not cooperating is individually rational.
3.2 Related Literature
Evolutionary Stable Strategy and Assortative Matching:
The problem of sustaining cooperation in a social dilemma has been long studied in
biology. Eshelt and Cavalli-Sforza (1982) generalize the concept of evolutionary stability
under the assumption of non-random encounters imposed by population and social structure,
including within-family encounters, group selection, and neighbor effect. They call this
assortment structural. They also study the evolutionary effects of non-random encounters
due to the active choice of companions and discuss the co-evolution of this type of selective
assortment, together with that of sociality. In the setup, full exogenous assortative matching
65 PhD Thesis
3.2. RELATED LITERATURE CHAPTER 3. A MECHANISM AND
MATCHING IN A SOCIAL DILEMMA
is considered without any assortativity arising due to population structure.
In the standard framework of indirect reciprocity, one of the five rules of cooperation
by Nowak (2006), the interactions are observed by a subset of the population who might
inform others. Nowak and Sigmund (1998) show that the simple model of image scoring can
evolve indirect reciprocity. It assumes that the accurate image score of each agent is known
to the other members of the society or it is observed by a few members of the society which
they report honestly. Nax et al. (2015) show the importance of image scores is also in group
scoring, without which cooperation will not evolve. The paper assumes that a mechanism
exists which assorts and matches future interactions based on the accurate image scores of a
proportion in groups. Ohtsuki et al. (2015) develop a hybrid model where there are public
and private interactions that have some probability of getting revealed. They find that for
a range of parametric values, cooperation can be achieved as ESS. In the absence of private
interaction getting truthfully revealed, the cooperation cannot be stabilized. Our setup
focuses on this challenge. In the environment under consideration, third-person observation
is not feasible. Hence, there is a need for an artificial mechanism that can build a reputation.
Bergstrom (2002) and Nax and Rigos (2016) show assortative matching as one of the
approaches to sustain cooperation in social dilemmas. Without any reputation mechanism,
one cannot achieve such assortative matching in the population. It will be easily seen that
our reputation mechanism with random matching cannot achieve a stable equilibrium. Fu
et al. (2008) theoretically establish that natural selection can give rise to homophily when
multiple observable phenotypes are present. In human populations in the absence of relevant
phenotypes that can be seen as a sign of cooperative behavior, a reliable artificial mechanism
is needed to stabilize cooperation. It will be shown that this mechanism along with assortative
matching serves this purpose.
Reputation/ Repeated Games/ Social Norms/Networks:
In game theory literature, it is shown how cooperation can be achieved due to incentives
for future interactions, specifically in the case of public monitoring. In terms of information,
the environment of interest is that of perfect private monitoring where each players action
is perfectly observed by her partner with (initially) sporadic random matching. This is
examined by Kandori (1992) and Ben-Porath and Kahneman (1996). In the former paper, it
is shown that efficient outcome is achieved without communication in repeated prisoners
dilemma with random matching, employing contagion strategies, while Ben-Porath and
Kahneman (1996) use communication (reporting after stage game) to sustain cooperation as
sequential equilibrium.
It is known that a good reputation facilitated by informal institutions encourages good and
honest behavior in the trade community in the absence of commercial law (Milgrom, North
and Weingast, 1990; Grief, 1993). Okuno-Fujiwara and Postlewaite (1995) and Kandori (1992)
use social norms to achieve cooperation as equilibrium assuming honest updating of actions
and status of each agent. Specifically, Okuno-Fujiwara and Postlewaite (1995) establish
equilibrium using social norms in a population of self-interested agents in social dilemma
games for random matching. This does not formally model how the past information of all
agents behavior is being revealed to the other agents; rather, it is assumed that the action and
status of the agents are correctly seen by some social planner who is updating the status of all
the agents. Additionally, Kandori (1992) talks about contagious equilibrium and its fragility
and extends the study of social norms when there is a mechanism or institution which
systematically processes some information among community members. The processing
of information is treated as exogenous, and it is assumed that information is transmitted
honestly. Based on the mechanism which facilitates local information processing, a robust
66 PhD Thesis
3.2. RELATED LITERATURE CHAPTER 3. A MECHANISM AND
MATCHING IN A SOCIAL DILEMMA
efficient equilibrium is achieved. Ben-Porath and Kahneman (1996) study a repeated game in
a scenario where teammates can make a public announcement about the contribution of each
other (including themselves). If the reports do not match, both the players get min-maxed by
their future partners. The infinite sequence of payoff generates cooperation as a sequential
equilibrium. In contrast, the chapter is interested in finite reputation length.
Another way of achieving cooperation is through non-random interaction that could be
due to active choice or due to population structure. When agents are spatially restricted
in their interactions it can lead to repeated interaction with the same small set of agents
(Brandt, Hauert and Sigmund, 2003). The population structures could be due to physical
geography (neighbors) or sharing/avoiding the same environment or belonging to the same
social group. Ohtsuki et al. (2006) describe a simple rule that is a good approximation for
several graphs (including cycles, spatial lattices, random regular graphs, random graphs,
and scale-free networks). They find that natural selection favors cooperation, if the benefit
of the altruistic act, b, divided by the cost, c, exceeds the average number of neighbors, k
(b/c > k). Rand, Arbesman and Christakis (2011) experimentally show that the approach of
making and breaking a link in the network can stabilize cooperation. It can be interesting
to see how different structures (networks) enhance the possibility of cooperation. Capraro,
Jordan and Rand (2014) suggest based on their finding in a one-shot continuous-strategy
Prisoners Dilemma (i.e. one-shot two-player Public Goods Game) that there are players
using heuristics to decide the degree of cooperation with underlying pro-social preferences.
Our focus is on an (implicit) active choice in which agents interactions are not restricted
by any geographical structure or social structure or using heuristics but driven only by the
scores they have.
Empirical Evidence:
Following Kandori (1992), experiments are done to test the efficiency of various reputa-
tion labeling mechanisms. Wedekind and Milinski (2000) experimentally find that image
scoring promotes cooperative behavior in situations where direct reciprocity is unlikely
but, again, in a private interaction what image score can be based on. Stahl (2013) tests a
color-coded mechanism and found that cooperation steadily increased with experience in
repeated interaction. His mechanism is such that it revealed whether the agent cooperated or
defected during interaction in the last period in a pairwise random matching. This shows
that reputation-revealing mechanisms matter and leaves an open question on a possible
mechanism for private interaction in a symmetric market. Gunnthorsdottir et al. (2010)
designed and experimentally tested a Group-based Meritocracy Mechanism where indi-
viduals first decide the level of their contribution depending on which they are grouped
assortatively to receive the share of their contribution for a one-shot game. Near-efficient
equilibrium is a part of equilibria with positive experimental evidence. This shows that
assortative matching can lead to efficient equilibrium and leaves an open question on what
the basis of assortativity can be in a private interaction that can lead to efficient equilibrium
upon matching.
In sum, the existing literature shows that in an environment where agents interact with
the same person infrequently for their well-being, without any supporting mechanism no
contribution is the equilibrium. A natural mechanism of direct reciprocity does not work for
this environment and indirect reciprocity cannot be assumed to have honest reporting where
the population needs to compete for reputation among itself. These papers have addressed
important social dilemmas through their study of reputation and mechanisms; however, they
have not been able to address how full contribution can be achieved for the environment of
interest, that is, symmetric markets where the interaction is private and it is infrequent with
67 PhD Thesis
3.3. BASIC MODEL CHAPTER 3. A MECHANISM AND
MATCHING IN A SOCIAL DILEMMA
the same person. This is therefore a gap in the literature to which this chapter contributes to.
In this chapter, it is shown that a simple mechanism facilitating assortative matching can
solve this problem, the analysis helps attain a better understanding of how full contribution
can be achieved in this environment and what can break it down.
3.3 Basic Model
As previously noted, the social interactions are represented as a continuous Prisoners
Dilemma (PD) (Killingback, Doebeli and Knowlton, 1999). Public goods game is also an
option to model this and in section 3.1.1 it is shown as a robustness check. Our choice of such
a game as a starting point allows us to substitute the variables of the PD in the reputation
mechanism variables as is. This simplifies the theoretical model. It is assumed that there is
no communication and hence no possibility of explicit collusion. Two agents interact where
they simultaneously decide to contribute their costly effort to the other agent. This costly
effort is multiplied by a factor greater than one and is added to the payoff of the other agent.
All the agents are endowed equally. In stage two, after playing a continuous PD in stage one,
agents report on each others contributions.
It will be examined that how in a one-shot bilateral interaction the reputation mechanism
gives agents no incentive to manipulate the reports of each other about their contributions.
The reports of both the agents are used to calculate the score each agent is awarded in
their interaction. Reputation is defined as the history of the awarded scores agents received
from past interactions. Agents are matched assortatively based on the awarded scores. An
evolutionary framework is used to analyze the equilibrium. It will be shown that agents
are implicitly competing for a positively awarded score. This is because the awarded score
determines who interacts with them in the future.
In an evolutionary framework, complete assortative matching is considered, in which
agents with the same reputation would be matched to interact with each other. The reputation
mechanism is such that the reputation score does not monotonically increase with an average
contribution by the agents, but the decrease in the future opportunity of interacting with
fully contributing agents would mean a decrease in an expected gain in the future. It will
be shown that unilateral deviation from revealing the truth about the contributions is not
profitable. If either of the agents does not truthfully reveal the value they or their partner
has createdgiven the partner reveals the truththen it would, in turn, lead to a decrease in
their own scores. In equilibrium, there is no incentive to manipulate the score of the matched
partner at the cost of her own score, even if it makes her score higher relative to her present
partner. A simplifying assumption is made that over the infinite horizon all agents play a
game of identical parameters  and e.
3.3.1 Continuous PD
In stage one, Agent 1 decides to contribute costly effort c1 for Agent 2 from her endowment e.
Simultaneously, Agent 2 decides to contribute costly effort c2 for Agent 1 from his endowment
e. The multiplying factor is  > 1, e > 0 and 0  ci  e. The following equations state the
payoffs of two agents interacting in stage one:
P1 =   c2  c1 + e
P2 =   c1  c2 + e
Without any reputation mechanism, the sporadic interaction has equilibrium contribution
as (0,0) with payoffs as (e,e). The Pareto optimal equilibrium contribution is (e,e) with payoffs
68 PhD Thesis
3.3. BASIC MODEL CHAPTER 3. A MECHANISM AND
MATCHING IN A SOCIAL DILEMMA
as (  e,  e).
A payoff matrix for Agent 1 and Agent 2 matched to each other for any variable contri-
bution c against endowment e can be expressed as below. Where C stands for cooperation
and D stands for defection. Cooperation means full contribution (e), defection means any
contribution (c) less than full contribution. Let  = e c+ e ,  = e,  = c c+ e and  = c
where  >  >  > .
Payoff
(1,2)
C ,  , 
D ,   , 
Table 3.1: Payoff Matrix
One way to build a reputation is to have a social planner as a monitor which can see the
contributions and calculate the reputation score for each agent, thereby achieving full contri-
bution in equilibrium with assortative matching (and random matching with social norms).
In the real world, it is not possible to have a monitor in private interaction, the environment
of interest. The objective is to design a mechanism that can replace the requirement for such
a monitor. Below the mechanism which builds the reputation of agents in stage two of the
game is stated.
In this section, it is shown that the PGG is an affine transformation of continuous PD and can
be used to model the interactions of interest. This makes the mechanism application more
general.
P1 =   (c1 + c2) c1 + e
P2 =   (c1 + c2) c2 + e
where, 1/2 <  < 1
P1 = (1)(
  c2
c1(1)
) + e
P2 = (1)(
  c1
c2(1)
) + e
P1 = (1)(  c2  c1) + e
P2 = (1)(  c1  c2) + e
where,
=  > 1. Here,  is equivalent to  in the continuous PD above and (1) is
the multiplier of the affine transformation of the equation in the continuous PD above. The
decision-making on the contribution is dependent on the relative cost of own contribution
and revenue from opponent contribution. The multiplier (1  ) is common to both the
factors, hence, will not change the decision-making in the two games. Note, the  and  in
this section are different from the ones taken in the payoff matrix above.
69 PhD Thesis
3.4. PROPERTIES OF REPUTATION
MECHANISM
CHAPTER 3. A MECHANISM AND
MATCHING IN A SOCIAL DILEMMA
3.3.2 Reputation Mechanism
In stage two, the two agents who interacted with each other submit the report of each others
contribution. It is assumed that there is no subjectivity in the evaluation of the contributions
which agents made to each others well-being during the interaction in stage one. Lets say ci
is the value contributed by Agent i to Agent j and cj is the value contributed by Agent j to
Agent i, and these are mutually known and unknown to any third party.
Our objective is to find a mechanism that can sustain cooperation where the initial en-
vironment is that of sporadic interactions with no monitoring. The following design is
considered for agents to report the contributions based on which an awarded score of the
individuals can be generated. Each agent is asked to report on the previous interaction via
two scores: Claimed Score (CS) and Given Score (GS), both of these can take values from zero
to endowment.
CSi : Agent i claims that she had made this much contribution to Agent j
GSi : Agent i gives this score to Agent j
CSj : Agent j claims that she had made this much contribution to Agent i
GSj : Agent j gives this score to Agent i
The generalized mechanism is stated below; it uses the above two score inputs to compute
the Awarded Score (AS) for each agent as follows:
Awarded Score: ASi =min[CSi ,GSj]|CSi GSj | |CSj GSi |
Awarded Score: ASj =min[CSj ,GSi]|CSj GSi | |CSi GSj |
Throughout this article, the above mechanism will be analyzed using the parametric values
 = 1 and  = 1. This is without loss of any generality as the following analysis will bring out
that the critical aspect of the mechanism is the deviations between the reports of the agents.
Awarded Score: ASi =min[CSi ,GSj] |CSi GSj |  |CSj GSi |
Awarded Score: ASj =min[CSj ,GSi] |CSj GSi |  |CSi GSj |
3.4 Properties of Reputation Mechanism
To understand the above mechanism, it is assumed for this part (before moving to section 5)
that the payoff of an agent is her awarded score. The line of thought applied to argue for the
equilibrium in this two person game is to keep one agents report as honest and check if a
possible deviation of other agent, one dimension at a time, is profitable or not.
Below it is shown why the agent has no incentive to manipulate the Claimed Score (CS) in
stage two of any interaction if their opponent has reported honestly. It is argued that there is
no incentive for the agent to manipulate the score she claims from the other agent. Agent 1
knows that Agent 2 would be scoring her equal to c1 otherwise Agent 2s score would fall
given Agent 1 reveals truthfully. Agent 1 would not want to claim her score to be less than
c1 because if her claimed score (CS1) goes below the score given by Agent 2 (GS2) then she
would end up with a lower score. Similarly, if Agent 1 claims more than c1 then she knows
that her score would fall. Overclaiming and underclaiming would lead to Agent 1s score
falling below c1.
Once it is shown that the honest reporting is an equilibrium it is further shown that this
equilibrium is not unique. The agents can coordinate and report falsely to achieve good score
70 PhD Thesis
3.4. PROPERTIES OF REPUTATION
MECHANISM
CHAPTER 3. A MECHANISM AND
MATCHING IN A SOCIAL DILEMMA
mutually regardless of the actual contributions. This section brings out the limitation of this
mechanism in achieving full-contribution in equilibrium.
Lemma 1: Unilateral deviation from truth-telling of self-contribution is not profitable when payoffs
are measured in terms of awarded score.
Part 1.a: Unilateral deviation from truth-telling by over-reporting of self-contribution is not
profitable when payoffs are measured in terms of awarded score.
Proof: Lets say Agent 2 claims more than the value she has created for Agent 1. But Agent
1 gives the score to Agent 2 as per the value she received from Agent 2. Assuming Agent 1
claimed and received score are consistent with the value she has created for Agent 2, then
the awarded score of Agent 2 will be as follows:
AS2 =min[CS2,GS1] |CS2 GS1|  |CS1 GS2|
= GS1  (CS2 GS1)
The term in the bracket is positive
 AS2 < GS1
Hence, if Agent 2 manipulates and submits the Claimed Score (CS) to be higher than the
value she created then her awarded score would fall. Now, lets see the impact of Agent 2
manipulating (over-claiming) her score on the awarded score of Agent 1.
AS1 =min[CS1,GS2] |CS1 GS2|  |CS2 GS1|
= GS2(= CS1) 0 (CS2 GS1)
= GS2  (CS2 GS1)
The term in the bracket is positive
 AS1 < GS2
The awarded score of Agent 1 also decreases by the same amount as of Agent 2, so Agent 2
does not even gain relatively in awarded score w.r.t. Agent 1. Note, agents are competing for
the relative score in the population to be matched with full contributors, so any fall in the
absolute score is not desirable.
Part 1.b: Unilateral deviation from truth-telling by under-reporting of self-contribution is
not profitable when payoffs are measured in terms of awarded score.
Proof: Lets say Agent 2 claims less than the value she has created for Agent 1. But Agent
1 gives the score to Agent 2 as per the value she received from Agent 2. Assuming Agent 1
claimed and received score are consistent with the value she has created for Agent 2, then
the awarded score of Agent 2 will be as follows:
AS2 =min[CS2,GS1] |CS2 GS1|  |CS1 GS2|
= CS2  (GS1 CS2)
The term in the bracket is positive
 AS2 < CS2
Hence, if Agent 2 manipulates and submits the Claimed Score (CS) to be lower than the
value she created, then her awarded score would fall. Now, lets see the impact of Agent 2
manipulating (under-claiming) her score on the awarded score of Agent 1.
71 PhD Thesis
3.4. PROPERTIES OF REPUTATION
MECHANISM
CHAPTER 3. A MECHANISM AND
MATCHING IN A SOCIAL DILEMMA
AS1 =min[CS1,GS2] |CS1 GS2|  |CS2 GS1|
= GS2(= CS1) 0 (GS1 CS2)
= GS2  (GS1 CS2)
The term in the bracket is positive
 AS1 < GS2
The awarded score of Agent 1 also decreases by the same amount as of Agent 2, so Agent
2 does not even gain relatively in awarded score w.r.t. Agent 1. Note, agents are competing
for the relative score in the population to be matched with full contributors, so any fall in the
absolute score is not desirable.
Now, let us argue why there is no incentive for the agent to manipulate the Given Score
(GS) she will award to the other agent. If Agent 2 gives a score of less than c1 to Agent 1,
then her score would get lowered than c2, assuming another agent reveals truthfully. Since
awarded scores will determine the relative position of the agents in the population, there
seems no reason why Agent 2 would give Agent 1 a score of more than c1. But even if she
does, there will be a decrease in scores of both the agents due to the functional form of
Awarded Score.
Lemma 2: Unilateral deviation from truth-telling of the partners contribution is not profitable
when payoffs are measured in terms of awarded score.
Part 2a: Unilateral deviation of truth-telling by under-reporting partners contribution is not
profitable when payoffs are measured in terms of awarded score.
Proof: Lets say Agent 2 manipulates and gives Agent 1 a score lower than the actual contri-
bution. Agent 1 gives and expects the score as per the actual contribution. Lets check if the
manipulation of Agent 2 impacts his own awarded score.
AS2 =min[CS2,GS1] |CS2 GS1|  |CS1 GS2|
= GS1(= CS2) 0 (CS1 GS2)
= GS1  (CS1 GS2)
The term in the bracket is positive
 AS2 < GS1
Now, lets check if the manipulation of Agent 2 impacts awarded score of Agent 1.
AS1 =min[CS1,GS2] |CS1 GS2|  |CS2 GS1|
= GS2  (CS1 GS2)
The term in the bracket is positive
 AS1 < GS2
The awarded score of Agent 1 decreases due to Agent 2 manipulating. Note, GS2 will be
less than the actual contribution by Agent 1. This means if Agent 2 manipulates, then his
own awarded score decreases by the amount of manipulation (CS1 GS2) to Agent 1s score.
In this case, by manipulating, she may be able to attain a relative advantage w.r.t. Agent 1
because the impact of his manipulation in the awarded score of Agent 1 is twice. Still, this is
not desirable in general for Agent 2 as a decrease in his absolute awarded score would lead to
a relative loss of reputation score in comparison to other agents in the population.
72 PhD Thesis
3.4. PROPERTIES OF REPUTATION
MECHANISM
CHAPTER 3. A MECHANISM AND
MATCHING IN A SOCIAL DILEMMA
Part 2b: Unilateral deviation of truth-telling by over-reporting partners contribution is not
profitable when payoffs are measured in terms of awarded score.
Proof: Lets say Agent 2 manipulates and gives Agent 1 more score than the actual contribu-
tion. Agent 1 gives and expects the score as per the actual contribution. Lets check if the
manipulation of Agent 2 impacts his own awarded score.
AS2 =min[CS2,GS1] |CS2 GS1|  |CS1 GS2|
= GS1(= CS2) 0 (GS2 CS1)
= GS1  (GS2 CS1)
The term in the bracket is positive
 AS2 < GS1
Now, lets check if the manipulation of Agent 2 impacts awarded score of Agent 1.
AS1 =min[CS1,GS2] |CS1 GS2|  |CS2 GS1|
= CS1  (GS2 CS1)
The term in the bracket is positive
 AS1 < CS1
The awarded score of Agent 1 decreases due to Agent 2 manipulating. Note, GS2 will be
more than the actual contribution by Agent 1. This means if Agent 2 manipulates, then his
own awarded score decreases by the amount of manipulation (CS1 GS2) he did to Agent
1. In this case, by manipulating, he may be able to get the relative advantage w.r.t. Agent 1
because the impact of his manipulation in the awarded score of Agent 1 doubles. Still, this is
not desirable in general for Agent 2 as a decrease in his absolute awarded score would lead to
a relative loss of reputation score in comparison to other agents in the population.
If agents submit a higher Claimed Score (CS) or lower Given Score (GS), they will lower
their Awarded Score. Although manipulation in Given Score may increase the score relative
to Awarded Score of the matched agent in that interaction, this would decrease both the
agents score below the contribution levels c1 and c2 respectively. Given that there are other
agents in the population, this manipulation is still not desirable as agents are implicitly
competing to pair themselves with agents having perfect reputation scores to interact with in
the future.
This methodology is not able to check the implicit collusion/coordination to report falsely
irrespective of what happened in stage 1. If agents start mutually claiming and giving
high scores irrespective of the true contributions, they can inflate their reputation in the
population.
Lemma 3: Dishonest reporting is a Nash equilibrium.
Proof: Lets consider a case where both the agents contribute 50 (out of 100) to each others
well-being but report their own and their partners contribution as 75. Both will achieve
an awarded score of 75. From the Awarded Score equation, it can be seen that neither of
the two players has an incentive to deviate from (75,75). Hence, implicit collusion is an
equilibrium. The increase in awarded scores of both due to collusion would result in false
higher reputation scores in the population, which may break down the desirable effect of the
reputation mechanism.
Proposition 1: Truth telling is a Nash equilibrium and it is not unique.
73 PhD Thesis
3.5. EQUILIBRIUM ANALYSIS CHAPTER 3. A MECHANISM AND
MATCHING IN A SOCIAL DILEMMA
Proof: This mechanism has two aspects of reportingself-reporting and partners reporting.
Lemma 1 and 2 establishes that truth-telling is in equilibrium. Lemma 3 shows that dishonest
reporting is in equilibrium as well.
The reputation mechanism has awarded scores increasing non-monotonically with the con-
tribution of the agent. The following example illustrates this.
Recall:
AS1 =min[CS1,GS2] |CS1 GS2|  |CS2 GS1| (awarded score of Agent 1)
AS2 =min[CS2,GS1] |CS2 GS1|  |CS1 GS2| (awarded score of Agent 2)
Thus if,
e is Agent 1s contribution and
c is Agent 2s contribution
What follows is:
CS1 = e,
GS1 = c,
CS2 = c,
GS2 = e  (where,  is the amount of understatement Agent 2 made about Agent 1s contri-
bution),
AS1 =min[e,e ] |e  (e )|  |c  c|
= e 
= e  2
AS2 = c  |c  c|  |e  (e )|
= c 
Lets us assume e=100. If c=90 then AS1 = AS2=80. It can be seen that even though Agent 1
made a greater contribution than Agent 2, they both receive the same awarded score.
A discussion below returns on the basic evolutionary framework to analyze why agents
can, in equilibrium, choose to contribute full endowment and report honestly resulting in
achieving Pareto efficiency.
3.5 Equilibrium Analysis
In the previous section it is shown that the mechanism by itself cant achieve full-contribution
in all equilibria. In this section, it is further explored by analyzing what equilibria can be
achieved if the scores from the mechanism are used to match the agents assortatively. While
discussing the reputation mechanism above scores as a payoff are considered to understand
the properties of the mechanism. This section brings in the Continuous Prisoners Dilemma
game played in stage one followed by reporting in stage two. The stage one game has a cost
and actual payoffs. Stage two reporting is used to award scores to agents based on which they
would be matched in the next period.
74 PhD Thesis
3.5. EQUILIBRIUM ANALYSIS CHAPTER 3. A MECHANISM AND
MATCHING IN A SOCIAL DILEMMA
The success of any strategy (contribution and reporting) is determined by the relative pro-
portion of the population in the long term. The growth of any strategy (fitness) depends on
the last period payoff of the strategy. The identification of the agents (strategy) by the social
planner (software in the laboratory or the real world) for matching purposes is based on their
scores.
Agents play Stage one game in period t. The payoff of the parent i in period t is:
P ti =   c
j  c
i + e
Agents play Stage one game in period t + 1. The payoff of the offspring of i in period t + 1 is:
P t+1i =   c
k  c
i + e
where,
 > 1 is the multiplying factor,
cti is the contribution by the parent i in the period t
ctj is the contribution by the parent j matched with parent i in the period t
ct+1i is the contribution by the offspring of i matched with k in period t + 1
ct+1k is the contribution by k matched with offspring of i in the period t + 1
The assortative matching at any period t is based on the immediate past T periods awarded
scores agents have received. The permutation of the awarded scores in T periods is consid-
ered for matching. Formally, the matching condition can be stated as follows:
ASi = AS
j  t  T    t  1.
ASi = AS
k  t  T + 1    t.
where,
T is the reputation-length
ASti is the awarded score of the parent i with whoever she was matched within period t.
While discussing the reputation mechanism, it is shown that the awarded score depends
on her reporting strategy and the reporting strategy of her partner which can depend on
her actual contribution. The matching condition considers the immediate history of T ( 1)
periods. Hence, the awarded score received by the parent in period t will be considered in
the matching condition of her offspring and will impact the payoff of her offspring in period
t + 1. Agents can make a decision only for the period they live in. The strategy of the parent
is replicated to the offspring. The population proportion of the strategy is determined by the
relative payoff (fitness) of the strategy at any period. This evolutionary framework will be
used to analyze the equilibrium. The analysis will be done for various mutations against the
incumbents who fully contribute and report honestly and show that incumbents strategy is
evolutionary neutral stable.
The scores from the reputation mechanism are used to facilitate the complete assortative
matching. A complete assortative matching (Jensen and Rigos, 2018) is when agents of
exact scores for the reputation length are matched. The matching function is such that for
any proportion of the population having the same reputation scores, they have a uniform
probability of matching among themselves. 2
It could be observed that in the population with a distribution of scores as x with perfect
monitoring and complete assortative matching, the payoff function (Schuster and Sigmund,
2Given the strategy space is finite, population size is large and the group size is two, even if the population
is odd the probability of an agent not being matched is so low that it does not make a difference in her choice of
the strategy.
75 PhD Thesis
3.5. EQUILIBRIUM ANALYSIS CHAPTER 3. A MECHANISM AND
MATCHING IN A SOCIAL DILEMMA
1983) of the agents is i(x) = i . This means that the payoff function is independent of the
population distribution if there is no reporting, and the amount of contribution is the only
strategy agents use. Hence, full contribution and honest reporting is chosen as incumbents
strategy and examine whether various possible mutations can invade the population or
not. One may like to argue that the strategy of honest reporting is a punishment strategy
when there is a assortative matching. However, we hold the view that there is no explicit
punishment strategy where the agent has to punish the defector on behalf of other exposed
incumbents failing which she would be punished.
Only simple mutations are considered, by this it is meant that mutation is one-period, not
continuous, and defined by its contribution and reporting strategy for one-period. Complex
mutations like different strategies in different periods are not considered. Below are the
categorization of mutations and the scores they receive after being matched with incumbents
and themselves.
Straightforward Mutants: These mutants contribute less and report regardless of whether
they meet incumbents or mutants. This can result in mutants and exposed incumbents
having a different awarded score or same score but lower than unexposed incumbents. When
mutants get matched among themselves their score will be lower than full. These are the last
four cases in Table 3.2. Table 3.2 shows that any mutation against the unexposed incumbent
would lead to lower scores of the mutants and exposed incumbents. The table is shown only
for the over-reporting self-contribution and under-reporting partners contribution. Since
Awarded Score computation has minimum and modulus functions, the awarded score would
always be less than perfect for both the matched partners if any of them unilaterally deviates
from full contribution and honest reporting (irrespective of the direction).
Table 3.2: Score Matrix for Straightforward Mutants
Following are the notations used in Table 3.2,
FC- full contribution (e)
LC- less than full contribution (ci)
HSR- honest self-report
HPR- honest report about partner
DSR- dishonest self-report (+i)
DPR- dishonest report about partner (i)
Note, the first four cases in the above table are the mutants that contribute fully but report
dishonestly. They will not be able to invade the incumbent population neither would they
die out in proportion. If such a mutation happens, then these will become segregated from
the incumbent population but their fitness will be the same as that of incumbents. Their
proportion in the population will remain the same as that at the time of mutation. These
mutants are evolutionarily (neutral) stable as well. This shows why the full contribution and
76 PhD Thesis
3.5. EQUILIBRIUM ANALYSIS CHAPTER 3. A MECHANISM AND
MATCHING IN A SOCIAL DILEMMA
honest reporting can only be evolutionarily (neutral) stable.
Strategic Mutants: These mutants contribute less and report conditional on whether they
are matched with incumbents or mutants. The observed contribution of the matched agent in
stage one of the game can reveal whether the mutant is matched with another mutant or an
incumbent. Lets consider a small group of mutants who always contribute low (c < e). When
these mutants are matched with incumbents (observing contribution e by the opponent) they
submit the report (me: e, opponent: c), they swap the contributions in the report. They claim
that they have contributed fully, and the matched partner has contributed what they have
contributed. When these mutants are matched among themselves (observing contribution
c by the opponent) they report (me: e, opponent: e) themselves and the matched agent
as contributing fully. This results in mutants having the same score as that of exposed
incumbents when matched with incumbents. When these are matched among themselves,
they receive a full score and stay with the unexposed incumbents.
From the awarded score point of view, there can be two types of mutations, one where
the mutants awarded score is different from the matched incumbent (henceforth exposed
incumbent). The other type is where mutants and the exposed incumbents that they were
matched with receive the same awarded score. As could be seen in Table 3.3, the strategic
mutants always receive the same awarded score when they meet incumbents. The strategic
mutants when matching themselves receive full scores like incumbents matching among
themselves. Table 3.3 illustrates the received awarded score of the incumbents and mutants
based on the type of mutation. It highlights the Mutants Qualitative Types in terms of their
score being equal to invaded incumbents or not. Where Si and Sj denote the awarded score
of incumbent and mutant, respectively.
Table 3.3: Mutants Qualitative Types
3.5.1 Full History
First, it is argued that this reputation mechanism can work when full history is considered
for matching. Then it will be examined if this mechanism can achieve cooperation with finite
history for different types of one-shot mutations. Considered are the mutations against the
incumbents who contribute fully and report honestly.
Proposition 2: When the full history of the awarded score is used for matching agents assortatively,
then full contribution and honest reporting is evolutionarily (neutral) stable.
Proof: Each possible type of mutation is argued for separately. As shown in the figure above,
there are three cases to be considered from a mutant segregation point of view. The two cases
will arise from straightforward mutation and the third will arise from strategic mutation as
shown in the above figure.
77 PhD Thesis
3.5. EQUILIBRIUM ANALYSIS CHAPTER 3. A MECHANISM AND
MATCHING IN A SOCIAL DILEMMA
Case 1: In the first case the mutants and exposed incumbent receive a different awarded score.
In this case, mutants will segregate themselves from incumbents and exposed incumbents.
The population will get divided into four sub-populations, namely, unexposed incumbents,
exposed incumbents, mutants matched with exposed incumbents and mutants matched
among themselves. Given the full history is considered, these groups will keep matching
among themselves and the unexposed incumbents will not get exposed further. Given the
contribution of mutants is lower than the incumbents, their fitness will decrease and they
will be contained in the long term.
Case 2: In the second case mutants and exposed incumbents receive the same score but
mutants matching among themselves results in them having lower scores than full. In this
case, the population will get divided into three sub-populations of unexposed incumbents,
mutants and exposed incumbents together and mutants matched among themselves. The
mutants would never be able to come back to the major population of unexposed incumbents.
They would keep invading the exposed incumbents and grow before the exposed incumbents
die out. Then the mutants would be left to be matched with each other and hence would start
declining and eventually die out in terms of proportion. Given the contribution of mutants is
lower than the incumbents, their fitness will decrease and they will be contained in the long
term.
Case 3: In the third case, mutants and exposed incumbents receive the same score but
mutants matching among themselves results in them having full scores. Lets say the initial
mutation size is  then in any period t, t = 0
2t residual mutants will invade the unexposed
incumbents after the initial mutation has occurred at t = 0. After invading the unexposed
incumbents, they will get segregated and will never be able to return to the unexposed in-
cumbents. The size of the mutants invading unexposed incumbents decreases exponentially.
Assuming  is small then in a finite time its size will become critically low. The mutants in
sub-populations will grow and their fitness will be lower as argued in the above two cases.
Hence, in this case, the mutants will also be contained in the long term. Detailed proof for
case 3 is discussed below.
Proof for Case 3: To prove this case two separate arguments are built. The first is that mutants
decay exponentially in the population having a history of perfect awarded scores. It will be
shown that the size of the sub-population falling away from the primary population every
period depending on the size of the mutants in the primary population. The second is that
mutants in every sub-population first have fitness advantage followed by fitness disadvantage.
The following abbreviations are used:
UI- unexposed incumbents
EI- exposed incumbents
At t=0, mutation happens with unexposed incumbent population= 1  and mutants popu-
lation= . They match randomly. The matching process with the resulting awarded score
and fitness is as follows:
Matching at t=1:
agent  partner: agents size , partner s size , agents awarded history , agents f itness
UI UI : 1  , 1  , e , e
78 PhD Thesis
3.5. EQUILIBRIUM ANALYSIS CHAPTER 3. A MECHANISM AND
MATCHING IN A SOCIAL DILEMMA
M M:  ,  , e , c  c+ e
M EI :  , 1  , s , e  c+ e
EI M: 1  ,  , s , c
The resulting size of the unexposed incumbents in the primary population having perfect
history is equal to (1 )2e. The resulting size of the mutants in the primary population
having perfect history is equal to ()2c. Remember,  = e  c+ e ,  = e,  = c  c+ e and
 = c where  >  >  > . The ratio of mutants to unexposed incumbents in the perfect
history population for next period matching is given as follows:
Ratio (M/UI) = 
(1)2 = (
(1) )
2( )
Matching at t=2:
agent  partner: agents size , partner s size , agents awarded history , agents f itness
UI UI : (1 )2 , (1 )2 , ee , 
M M: 2 , 2 , ee , 
M EI : 2 , (1 )2 , es , 
EI M: (1 )2 , 2 , es , 
The resulting size of the unexposed incumbents in the primary population having a perfect
history is equal = (1  )43. The resulting size of the mutants in the primary population
having a perfect history is equal to ()43. The ratio of mutants to unexposed incumbents in
this population for next period matching is given as follows:
Ratio (M/UI) = 
(1)43 = (
(1) )
4( )
This ratio can be generalized for any period t as:
Ratio (M/UI) = ( (1) )
2t( )
Given,  < 1/2 = 1 < 1, and from payoff matrix it is known that (
 ) < 1
= limt( (1) )
2t( )
2t1 = 0
The above expression shows that the size of the mutant (compared to unexposed incum-
bents) in the primary population having perfect history will decrease from  to 0. From
the above case2 it is known that within any sub-population falling away from the primary
population, the mutants will first grow and then will be left to be matched with each other.
Given that there is always a positive proportion of unexposed incumbents in the primary
population, the relative fitness of mutants in any sub-population will cause it to decline.
This shows that when the full history is used to match the agents assortatively then full
contribution and honest reporting is evolutionarily (neutral) stable.
Full histories are rarely meaningfully accessible in the real world. People have a finite
life and should have the opportunity to learn from their mistakes. They should not bear
the consequences of the systems errors forever. Therefore, it is important to find out if
incumbents strategy of full contribution and honest reporting can be evolutionarily stable
under finite history. This possibility will be analyzed separately for the two types of mutants
discussed earlier. First, the straightforward mutants are considered followed by the strategic
79 PhD Thesis
3.5. EQUILIBRIUM ANALYSIS CHAPTER 3. A MECHANISM AND
MATCHING IN A SOCIAL DILEMMA
mutants.
3.5.2 Finite History - Straightforward Mutants
The eight possibilities were shown in the table above. Our focus is on the evolution of full
contribution. The discussion will follow on the four possibilities of mutants who contribute
less than fully and show that a one-period reputation is enough to make incumbents strategy
evolutionarily (neutral) stable.
Straightforward Mutants Type 1: Mutants who contribute less and report self-contribution
and matched agents contribution honestly
Straightforward Mutants Type 2: Mutants who contribute less, over-report self-contribution
and honestly report matched agents contribution
Straightforward Mutants Type 3: Mutants who contribute less, honestly report self-contribution
and under-report matched agents contribution
Straightforward Mutants Type 4: Mutants who contribute less, over-report self-contribution
and under-report matched agents contribution
Straightforward Mutants Type 1: Mutants who contribute less and report self-contribution
and matched agents contribution honestly represent a trivial case and by observation, it
can be concluded that these agents would get a higher payoff in the period of mutation. But
in the following periods, they would be matched among themselves and get a lower payoff
compared to the incumbent. Hence, their fitness would decline and they would die out.
The exposed incumbent would be able to match with the original incumbents after a finite
reputation period (T=1) and hence would make a higher payoff than the mutants.
StraightforwardMutants Type 2: Mutants who contribute less, over-report self-contribution
and honestly report matched agents contribution. Below it is shown that mutants would
receive a lower score than the exposed incumbents even though their payoff in the period
of mutation is higher than the exposed incumbent. After mutation, there would exist two
populations with lower scores than the majority of the incumbents. One is the exposed
incumbents, while the other is the mutants. Due to complete assortative matching, the next
period mutant would match the mutant while the exposed incumbent would match the
exposed incumbent. The exposed incumbent would be able to merge to the larger incumbent
population after the reputation period (T) while mutants would keep matching each other
and decline in their fitness. If T=1 then these mutants are contained.
AS1 =min[CS1,GS2] |CS1 GS2|  |CS2 GS1| (awarded score of incumbent)
AS2 =min[CS2,GS1] |CS2 GS1|  |CS1 GS2| (awarded score of mutant)
e is Incumbents contribution
c is Mutants contribution
CS1 = e
GS1 = c
CS2 = c+  ( is mutants overstatement of her contribution)
GS2 = e
80 PhD Thesis
3.5. EQUILIBRIUM ANALYSIS CHAPTER 3. A MECHANISM AND
MATCHING IN A SOCIAL DILEMMA
Incumbents awarded score-
AS1 =min[CS1,GS2] |CS1 GS2|  |CS2 GS1|
=min[e,e] |e  e|  |c+   c|
= e  
Mutants awarded score-
AS2 =min[CS2,GS1] |CS2 GS1|  |CS1 GS2|
=min[c+ ,c] |c+   c|  |e  e|
= c  
From the above calculations, it can be seen that when incumbents are matched with these mu-
tants both receive different awarded scores. Hence, they become segregated after matching.
Below are the calculations for the awarded score of these mutants when they are matched
among themselves.
c is Mutant 3 contribution
c is Mutant 4 contribution
CS3 = c
GS3 = c+  (where,  is the amount of overstatement Mutant 3 made his contribution)
CS4 = c
GS4 = c+  (where,  is the amount of overstatement Mutant 4 made his contribution)
AS3 =min[CS3,GS4] |CS3 GS4|  |CS4 GS3| (awarded score of mutant 3)
AS4 =min[CS4,GS3] |CS4 GS3|  |CS3 GS4| (awarded score of mutant 4)
AS3 =min[c,c+ ] |c  (c+ )|  |c  (c+ )| (awarded score of mutant 3)
= c    
= c  2
Given Mutant 4 is same as Mutant 3,
AS4 = c  2
Note, these mutants matching among themselves will become segregated and form a different
sub-population that can never invade the unexposed population.
Straightforward Mutants Type 3: Mutants who contribute less, report self-contribution
honestly and under-report matched agents contribution. The calculation below shows that
the mutants awarded score would be higher than the incumbents awarded score except
when  = ec. If they have a different score, mutants would match mutants while exposed in-
cumbents would match exposed incumbents. This means that an exposed incumbent would
merge with the original incumbent after a finite reputation period (T=1) while mutants
would keep matching among themselves and keep declining in fitness, eventually dying out.
AS1 =min[CS1,GS2] |CS1 GS2|  |CS2 GS1| (awarded score of incumbent)
AS2 =min[CS2,GS1] |CS2 GS1|  |CS1 GS2| (awarded score of mutant)
e is Incumbents contribution
c is Mutants contribution
81 PhD Thesis
3.5. EQUILIBRIUM ANALYSIS CHAPTER 3. A MECHANISM AND
MATCHING IN A SOCIAL DILEMMA
CS1 = e
GS1 = c
CS2 = c
GS2 = e  (where,  is the amount of understatement Agent 2 made about Agent 1s contri-
bution)
AS1 =min[e,e ] |e  (e )|  |c  c| (incumbent)
= e 
= e  2
AS2 = c  |c  c|  |e  (e )| (mutant)
= c 
From the above calculations, it can be seen that when the incumbent is matched with a
specific mutant both get different awarded scores. Hence, they will be segregated after
matching. Below are the calculations of the awarded score of these mutants when they are
matched among themselves.
c is Mutant 3 contribution
c is Mutant 4 contribution
CS3 = c
GS3 = c  (where,  is the amount of understatement Mutant 3 made about Mutant 4s
contribution)
CS4 = c
GS4 = c  (where,  is the amount of understatement Mutant 4 made about Mutant 3s
contribution)
AS3 =min[CS3,GS4] |CS3 GS4|  |CS4 GS3| (awarded score of mutant 3)
AS4 =min[CS4,GS3] |CS4 GS3|  |CS3 GS4| (awarded score of mutant 4)
AS3 =min[c,c ] |c  (c )|  |c  (c )| (awarded score of mutant 3)
= c 
= c  3
Given Mutant 4 is same as Mutant 3,
AS4 = c  3
Note, these mutants matching among themselves will be segregated and form a different
sub-population. These can never invade the unexposed population. These will not even be
part of the exposed population.
If  = ec, the mutants awarded score will be the same as the incumbents awarded score.
Then three sub-populations would be formed. The first is of the unexposed incumbent, the
second of a mixed group of exposed incumbents and mutants having the same score and the
third sub-population consisting entirely of mutants. In the next period after matching, the
second sub-population will be further segregated into three groups. The first group is the
one where exposed incumbents match exposed incumbents. The second group will be the
one where mutants meet exposed incumbents. The third group is the one where mutants
meet mutants.
The awarded score of the first group will be a full score (e) for this period as the incum-
82 PhD Thesis
3.5. EQUILIBRIUM ANALYSIS CHAPTER 3. A MECHANISM AND
MATCHING IN A SOCIAL DILEMMA
bents contribute fully and report honestly. They will merge with the unexposed incumbents
if the reputation length is one period. The awarded score of the exposed incumbents and
mutants in the second group will be the same as calculated after the first period of mutation
(above). They will have the same histories. The awarded score of the third group will be is
c  3 as calculated above.
The second group will keep splitting into three sub-groups into the following periods.
The first sub-group will merge with the unexposed incumbents. The second sub-group will
have a mixed population. The third sub-group will consist only of mutants that will become
segregated. In the second sub-group, given payoff of the exposed incumbents is lower than
mutants, slowly the exposed incumbent would decline, and this group will start shrinking.
In the following periods, only mutants would be left to be matched among themselves.
Eventually, mutants would be eliminated due to lower fitness compared to unexposed incum-
bents. The mutants cannot grow unboundedly because they are prevented from matching
with the unexposed incumbents and have limited exposed incumbents to rematch with.
StraightforwardMutants Type 4: Mutants who contribute less and over-report self-contribution
and under-report matched agents contribution. The matching analysis and scores of these
mutants turn out to be similar to the Type 3 mutants. Below is the analysis for the matching
process and calculate the awarded scores.
AS1 =min[CS1,GS2] |CS1 GS2|  |CS2 GS1| (incumbent)
AS2 =min[CS2,GS1] |CS2 GS1|  |CS1 GS2| (mutant)
e is Incumbents contribution
c is Mutants contribution
CS1 = e
GS1 = c
CS2 = c+  (where,  is the amount of overstatement of Agent 2s contribution)
GS2 = e  (where,  is the amount of understatement of Agent 1s contribution by Agent 2)
AS1 =min[CS1,GS2] |CS1 GS2|  |CS2 GS1| (incumbent)
=min[e,e ] |e  (e )|  |c+   c|
= e  
= e    2
AS2 =min[CS2,GS1] |CS2 GS1|  |CS1 GS2| (mutant)
=min[c+ ,c] |c+   c|  |e  (e )|
= c   
Now, it can be the case that AS1 = AS2 if c = e. In that case, they will stay in the same sub-
population after matching but will be segregated from unexposed incumbents. The matching
process in the following periods will be the same as in the case of Type 3 mutants. Below is the
calculation for the awarded score of these mutants when they are matched among themselves.
c is Mutant 3 contribution
c is Mutant 4 contribution
AS3 =min[CS3,GS4] |CS3 GS4|  |CS4 GS3| (awarded score of mutant 3)
AS4 =min[CS4,GS3] |CS4 GS3|  |CS3 GS4| (awarded score of mutant 4)
83 PhD Thesis
3.5. EQUILIBRIUM ANALYSIS CHAPTER 3. A MECHANISM AND
MATCHING IN A SOCIAL DILEMMA
CS3 = c+  (where,  is the amount of overstatement of Agent 3s contribution)
GS3 = c  (where,  is the amount of understatement of Agent 4s contribution by Agent 3)
CS4 = c+  (where,  is the amount of overstatement of Agent 4s contribution)
GS4 = c  (where,  is the amount of understatement of Agent 3s contribution by Agent 4)
AS3 =min[CS3,GS4] |CS3 GS4|  |CS4 GS3| (awarded score of mutant 3)
=min[c+ ,c ] |c+   (c )|  |c+   (c )|
= c     
= c  2  3
Given Mutant 4 is same as Mutant 3,
AS4 = c  2  3
Note, these mutants matching among themselves will be segregated and form a different
sub-population that can never invade the unexposed population.
3.5.3 Finite History - Strategic Mutants
The environment is defined in this basic model as one in which agents are not able to
observe the score of the matched participants nor do they know their own exact scores.
This is a feasible scenario; Although agents know their contributions and their partners
contribution, they do not know what their partner has reported about them. Based only on
the contributions, they cannot calculate their exact next period reputation (although it does
not matter in this model as mutants live only for one period). All they know is that they
are being matched every period with the agent having the same score as theirs. And their
strategies would impact their scores which would impact the matching of their off-springs.
Further analysis is done on how the population grows when such a mutation happens.
Lets say that in period t = 0 the mutation happens and the (1 ) incumbents along with
(1 ) mutants fall away in the same pool (their scores being equal) and become segregated
from the remaining population of unexposed incumbents. The 2 mutants which matched
among themselves and reported each other as fully contributing stay with the unexposed
population.
Below is the analysis of how these segregated sub-populations will get matched further.
Lets say that in period t = 0 exposed incumbent and mutants receive the score of s. In
period t = 1 the mixed sub-population will randomly match to form the following groups.
G1- (EI-EI) : s,e
G2- (M-EI) : s,s
G3- (M-M) : s,e
G1 and G3 have the same history of scores and will again fall in the same pool to be matched
randomly among themselves. G2 has a different score from G1 and G3. It consists of mutants
and exposed incumbent, to be matched again randomly.
In Period 2 following groups will be formed.
Group with history s,s further divides into,
84 PhD Thesis
3.5. EQUILIBRIUM ANALYSIS CHAPTER 3. A MECHANISM AND
MATCHING IN A SOCIAL DILEMMA
G1- (EI-EI) : s,s,e
G2- (M-EI) : s,s,s
G3- (M-M) : s,s,e
For the next period matching, the above G1 and G3 populations will be in the same sub-
population and match randomly to form a further three groups. G2 will form further three
groups as it has mutants and exposed incumbents having the same history of scores.
Group with history s,e further divides into, G1- (EI-EI) : s,e,e
G2- (M-EI) : s,e,s
G3- (M-M) : s,e,e
In the next period, G1 and G3 populations will be in the same sub-population and match
randomly to form further three groups. G2 will form further three groups as it has mutants
and exposed incumbents having the same history of scores.
Looking at the above pattern it can be generalized that after t periods following t-length
histories sub-populations will be found:
The simulations are run in the following section to examine for what parametric values (, T )
can the finite history sustain evolutionary stability.
3.5.4 Finite History - Strategic Mutants Simulations
In this section, some simulation results are presented for the strategic mutation in the basic
model. A similar approach of computer experiments is taken by Tesfatsion (1997) due to the
complexity of analysis of non-standard mutations and the matching process. The case of the
strategic mutation is one where it is speculated whether the cooperation would evolve or
not within finite reputation length. The model is simulated for limited reputation history.
The simulations are run considering mutations of 5%(), various lengths of the reputation
history, multiplier () for continuous prisoners dilemma and mutants contribution (c). All
the simulations are for single one-shot mutations. Below are some of the simulation results.
In all the tables below, the columns represent the length of reputation history from 1 to 10
and rows represent a multiplier () of continuous PD.
In Tables 3.4 and 3.5, it can be seen how many periods it takes for the mutant population
to fall below the initially infused 5%. Table 3.4 is for the case when mutants contribution
(c) in a continuous PD is 0 and Table 3.5 is for the case when mutants contribution in a
continuous PD is 50%. The tables show in which cases mutants take over (MTO) and in
which they get contained.
Proposition 3: For a small mutation size (), there exists  for which strategic mutations get
85 PhD Thesis
3.5. EQUILIBRIUM ANALYSIS CHAPTER 3. A MECHANISM AND
MATCHING IN A SOCIAL DILEMMA
contained in a finite reputation length (T ).
Table 3.4
Table 3.5
In Table 3.6 and 3.7, it can be seen what proportion of mutants remain 100 periods after
mutation of  = 5%. Table 3.6 is for the case when mutants contribution (c) in a continuous
PD is 95% and Table 3.7 is for the case when c = 99%. It shows in which cases mutants take
over (MTO) and in which they get contained. When multiplier  = 1, mutants take over (MTO)
for any reputation length. The number of periods required for mutants to fall below the
initially infused (5%) level monotonically decreases with reputation length and multiplier ().
Proposition 4: For a small mutation size (), for  > 2, the maximum value of the lower bound of
reputation length in which strategic mutants get contained is given by 2/( 2).
The number in the tables confirms that the analytical result holds for these parametric values.
For example,  = 2.5 gives T = 4, both the tables show that for reputation period T  4 the
mutants get contained.
Table 3.6
In Tables 3.8 and 3.9, it is shown how many periods it takes for the mutant population to fall
below 0.005%. Table 3.8 is for the case when mutants contribution (c) in the continuous PD
is 0. Table 3.9 is for the case when mutants contribution in a continuous PD is 50%. The
tables show in which cases mutants take over (MTO) and in which they get contained.
86 PhD Thesis
3.5. EQUILIBRIUM ANALYSIS CHAPTER 3. A MECHANISM AND
MATCHING IN A SOCIAL DILEMMA
Table 3.7
Table 3.8
Table 3.9
In Table 3.10 and 3.11, a snapshot is seen of how the population grows in the first 10 periods
after 1% mutation. This is for the case when  = 2, c = 50% and reputation length T = 4. In
the above tables, matching is based on agents having the same history (not average score).
The corresponding values of the payoff matrix are  = 2.5, = 2, = 1.5 and  = 1. Table
3.10 displays the first five periods, while Table 3.11 displays the subsequent five periods.
The color band represents histories falling under the same average awarded a score for the
history of length T = 4.
Table 3.10
87 PhD Thesis
3.6. DISCUSSION CHAPTER 3. A MECHANISM AND
MATCHING IN A SOCIAL DILEMMA
Table 3.11
From the above simulation results, the model shows that the proposed reputation mecha-
nism can contain the one-shot mutations when assortative matching is done based on the
awarded scores from the reporting. It is found that even when the mutants contribute just
one strategy (out of 100) below the full contribution (c = 99%) the mutants get contained in
one-period reputation length when the multiplier () is high. With the reputation length
T  3, the cooperation can evolve for   2.5. For the case when mutants show a low contri-
bution level then cooperation can evolve even for lower values of reputation length (T ) and
multiplier ().
3.6 Discussion
A reputation mechanism is discussed which when used to match agents assortatively sustains
an evolutionarily (neutral) stable cooperative equilibrium in a continuous PD. The equilib-
rium of honest reporting and full-contribution is not unique, and the population can drift to
other full-contribution equilibrium. The main purpose of the mechanism and matching is to
get full-contribution in equilibrium and that is served. It is found that this can be achieved
in finite reputation length which is a more realistic situation compared to infinite reputation
length as in the real world case errors will happen in terms of contributionit may take
time to learn so initial mistakes should not mean a life-long imperfect reputation. These
results come with the assumption of perfect assortative matching in a large population. The
assortative matching allows for this stable equilibrium and this can be seen by considering the
other extreme of complete assortative matching which is random matching. If the matching
is random, then, as known, the NE is that of no contribution. Further research is required
to see if a variation of mechanism and matching can make full-contribution as NE or other
refinements.
Incentives for cooperation may be enhanced substantially if a player can rely on the
entire community to punish a partner who failed to cooperate. The ability of a player to
report to others when a partner defected can constitute a strong deterrent against defection
if this report triggers a punishment imposed by the whole community. The term community
enforcement is often used for this type of mechanism. A prerequisite for this mechanism to
work, however, is that the community of players is accurately informed of whether a partner
defected or not. When transactions cannot be observed by the public (private interaction),
then the mechanism depends on a players ability to truthfully report his or her experience
with a partner. In the current literature on community enforcement, truthful experience-
sharing is assured by assumption (Kandori, 1992; Okuno-Fujiwara and Postlewaite, 1995).
In a private interaction in a symmetric market, it is reasonable to believe that players will
88 PhD Thesis
3.6. DISCUSSION CHAPTER 3. A MECHANISM AND
MATCHING IN A SOCIAL DILEMMA
not truthfully report. It is shown that honest reporting can be achieved, even though the
market is symmetric if agents report using the mechanism and are assortatively matched
despite the possibility that agents may coordinate to misreport. What our model relaxes is
the requirement of community enforcement which has explicit punishment strategies to be
followed by the entire population for some periods for an agent who did not cooperate last
period. This is where the mechanism combined with the matching process fills the gap in the
literature, that is efficient equilibrium in this environment without explicit punishment. A
large population is assumed to make the assortative matching possible. The scores generating
from the reports in the mechanism fulfill the requirement of local information sharing and
simple and straightforward information structure, in terms of Kandori (1992), although these
scores are not displayed publicly.
In the mechanism proposed, players simultaneously report experiences and, if the re-
ports contradict each other, both players are punished. The reporting mechanism, thus,
is essentially a coordination game where any coordination of reports (honest or not) is a
Nash equilibrium. Although the mechanism is not dominant-strategy-incentive-compatible
(Vickrey, 1961; Clarke, 1971), it is Bayesian-Nash incentive-compatibility. However, in this
case, one needs to have good reasons to believe that players will select the truthful equilib-
rium over the untruthful ones. The mechanism along with the matching resolves this and
contributes to the gap in this literature. Note, a unilateral reporting mechanism (Annen,
2011) will not work in a symmetric market due to the incentive to misreport.
The role of a reporting mechanism for the formation of reputations can be usefully studied
based on a repeated-matching Prisoners Dilemma (PD) game. In each period, players are
randomly matched into pairs. The outcome of the game is observed by the matched players
only. In large populations, players interact infrequently with each other which produces
weak incentives for cooperative behavior. However, if there is an experience-sharing device
in place, the history of each player becomes common knowledge to all players. In contrast
to Ben-Porath and Kahnemans (1996) study, the mechanism and matching used in this
chapter generate honest reporting and full-contribution equilibrium in finite reputation
length and without explicit punishment strategy. This is a contribution to the literature as
finite reputation length and no explicit punishment strategy seem more implementable in a
real-world situation.
In indirect reciprocity literature (for example Nowak and Sigmund, 1998; Bergstrom,
2002; Nowak, 2006) the assortative matching leads to cooperative equilibrium. The assor-
tative matching is either due to structural reasons or active choice. The interest is in active
choice interactions. In this case, a subset of the population observes the interaction and
reports honestly to facilitate assortativity. The assumption of honest reporting does not
hold for the environment of private interaction and symmetric market. This is a gap in this
literature that the chapter fills as the mechanism stated when used to facilitate assortative
matching leads to honest reporting and full-contribution as an equilibrium.
In sum, this chapter relates and brings together multiple sets of literature, filling gaps
in each, to achieve honest reporting and full contribution, constituting an evolutionarily
(neutral) stable strategy when agents care about their (offspring) future payoff in a contin-
uous prisoners dilemma (2-person standard PGG), which offers insights into a real-world
implementable situation.
This mechanism is better than a qualitative mechanism such as labeling both the parties as
a defector in case their reports do not match, other aspects of the model remaining the same.
This can be seen in Table 3.2, in which the resulting scores of the incumbent and mutants
after the mutation shows thatin some of the casesthe mechanism can segregate mutants
from the incumbents. Given this, the mechanism differentiates itself from any qualitative
mechanism. The segregation means that the reputation length required for cooperation to
89 PhD Thesis
3.6. DISCUSSION CHAPTER 3. A MECHANISM AND
MATCHING IN A SOCIAL DILEMMA
evolve is 1. This also means that in case there are multiple types (in terms of contributions) of
mutations happening simultaneously, these will not combine and the time horizon required
for mutations to get contained will be reduced compared to such a qualitative mechanism. A
further simulation exercise can be done for comparison.
In this model, agents are assumed not to see the scores of the agent they are matched
with. What if there is a change in the information available to the agents in this model
by allowing the visibility of the scores to the agents, the matching process remaining the
same? Under this visibility condition, one example of the mutation can be found where
the mechanism discussed in this chapter breaks down and those can invade the incumbent
population. Hence, full cooperation and honest reporting would not be evolutionary neutral
stable. These are mutants that contribute at a low level and report honestly when the partner
has a full scorethey contribute fully and report honestly when they see that their partner
has a lower score. If the full history is considered for matching, then these mutants along
with the exposed incumbents would separate out and its proportion in the population would
remain roughly the same as at the beginning of the mutation. Mutants would not be able to
invade the population of incumbents. If the reputation period is finite, then these mutants
would be able to invade the population. Though in equilibrium they would oscillate between
cooperation and non-cooperation and their average payoff would be less than the Pareto-
optimal. It can be seen that the above mutation is unstable as well if multiple mutations are
allowed as it can be invaded by other non-cooperative mutants.
This is a simple model that can be further extended allowing for more realism. One can
allow for communication after stage one before reporting. Although, in the real world, one
does observe that communication generally breaks down when one party defects given there
is no uncertainty about defection. Communication may alter the game dynamics due to the
possibility of coordination before reporting including a demand for side payment. How
much side payment can the agent demand before reporting when the partner defects and can
that amount be more than the loss due to reputation loss which is equivalent to gain from
defection?
In this model, it is assumed that agents contributions are perfectly observed by each
other. It will be interesting to examine how equilibrium dynamics change in the case where
there is uncertainty about the contribution. It is also assumed that the assortative matching
based on awarded scores is perfect. It will be interesting to find out the minimum degree
of assortativity required to sustain a stable cooperative equilibrium since it is shown that
random matching does not allow for the mechanism to achieve full contribution. Given the
reputation mechanism is not monotonic, its stability analysis required examining each case
of mutations separately. The assortative matching process with the objective of having full-
contribution and honest reporting as equilibrium strategies made us choose an evolutionary
framework for the analysis.
This mechanism does not make awarded scores monotonically increasing in contribution
off the equilibrium path. This can make learning equilibrium strategies difficult. One can still
improve this aspect of the problem by using a generalized mechanism with other parametric
values (, ) such that it is monotonic with respect to one of the reporting manipulations
(over-reporting self-contribution or under-reporting partners contribution), whichever is
found more prevalent empirically. Further, a randomized forma of a generalized mechanism
can more clearly segregate straightforward mutations. This model assumes that agents act in
their self-interest and do not use social norms for any coordination.In this model, one-shot
mutations are considered and analyzed if those can invade the population. A higher degree
of stability requires continuous mutations to be contained. However, the mechanism does
not qualify for such a strong criterion of stability. Another interesting exercise can be to see
one-period multiple-type mutations occurring simultaneously. Intuitively, it seems that such
90 PhD Thesis
3.6. DISCUSSION CHAPTER 3. A MECHANISM AND
MATCHING IN A SOCIAL DILEMMA
simultaneous mutations would enhance the stability of the equilibrium.
The mechanism used here is appropriate only if higher effort levels are socially better.
This will not be the case; for example, in a threshold public good game (for threshold PGG
see, for example, Cartwright and Stepanova, 2017). There may be the highest desirable effort
level, even though higher efforts can be chosen. It would be worthwhile to study this in more
detail and find out if there is a way this mechanism can be implemented in such games.
Although in this chapter a complete graph is considered implicitly, it can be studied how
the assortative matching based on this mechanism can facilitate making and breaking of
the link to stabilize cooperation. It will be interesting to implement this mechanism and
scoring rules to experimentally test in which graphs and under how much assortativity it can
stabilize cooperation where interactions are private and the market is symmetric.
Nowak, Sasaki, Taylor and Fudenberg (2004) define ESS for a finite population with
stochastic replication and finds parametric conditions under which tit-for-tat can invade
always-to-defect. This approach can be studied in detail to examine if it can be used in the
environment of interest in this chapter to evolve cooperation.
91 PhD Thesis
Appendix
Appendix A
A.1 (Chapter 1)
A.1.1 News and Social Media Posts
Figure 1.36: Individuals buying in bulk.
Figure 1.37: Individuals buying in syndicate.
A.1. (CHAPTER 1) APPENDIX A.
Figure 1.38: Media promoting bulk buying.
Figure 1.39: Individual response to media promoting bulk buying 1.
94 PhD Thesis
A.1. (CHAPTER 1) APPENDIX A.
Figure 1.40: Individual response to media promoting bulk buying 2.
Figure 1.41: Individual response to media promoting bulk buying 3.
95 PhD Thesis
A.1. (CHAPTER 1) APPENDIX A.
Figure 1.42: Individual response to media promoting bulk buying 4.
96 PhD Thesis
A.1. (CHAPTER 1) APPENDIX A.
A.1.2 Further Charts and Data
Table 1.7: Summary design for Part 6
Figure 1.11: Tickets distribution in Part 2
Table 1.19: CRRA utility values with different risk preference for each lottery in Lottery Set 1
97 PhD Thesis
A.1. (CHAPTER 1) APPENDIX A.
Figure 1.24: Distribution of difference in bid amount with change in group size.
Table 1.43: Descriptive statistics of the payment subjects received.
Table 1.44: Distribution of winning utility of subjects.
98 PhD Thesis
A.1. (CHAPTER 1) APPENDIX A.
Table 1.45: Summary statistics of choices in three lottery sets in Part 5
Table 1.46: Choice distribution of three lottery sets in Part 5
99 PhD Thesis
A.1. (CHAPTER 1) APPENDIX A.
A.1.3 Risk Aversion for Part 5, 4 and 3
We further state and test predictions if the difference between TPW and EUM can be ex-
plained by the difference in measured risk preferences of these subjects as stated below.
P rediction 6 : In Part 5 (Inverted Lotteries Sets), if TPW is a conceptually different way of
decision making then the measured risk aversion of the subjects should not be able to explain
the difference in their classification as EUM and TPW.
P rediction 7 : In Part 4 (Response Curve), if TPW is a conceptually different way of decision
making then the measured risk aversion of the subjects should not be able to explain the
difference in their classification as EUM and TPW.
P rediction 8 : In Part 3 (Group Size Change), if TPW is a conceptually different way of
decision making then the measured risk aversion of the subjects should not be able to explain
the difference in their classification as EUM and TPW.
Result for Prediction 6 In Part 5 (Inverted Lotteries Sets), the risk preference for each type is
shown in Table 1.17. It appears that for the type TPW (0,2) the mean value of risk preference
is lower than the type EU (6,8). Wilcoxon Rank-Sum test is not able to reject that both the
samples are taken from the same distribution. The calculation for the test is shown in Table
Table 1.17: Mean risk of each type in Part 5
Table 1.18: Wilcox Risk Difference between EU and TPW Part 5
Result for Prediction 7 In experiment Part 4 the measured risk aversion cant explain the
difference in choices made by the types of subjects. Table 1.23 shows the mean risk for each
category and the result of the significance test of difference in the mean risk for each category.
100 PhD Thesis
A.1. (CHAPTER 1) APPENDIX A.
Table 1.23: Mean risk for each type in Part 4
Result for Prediction 8 In Part 3, the measured risk aversion cant explain the difference in
choices made by the types of subjects. In the below table we see the mean risk for each
category. Wilcoxon Rank-Sum test (Table 1.26) cant reject that both the samples are taken
from the same distribution.
Table 1.26: Wilcox risk difference between increase and decrease consistency type Part 3
101 PhD Thesis
A.1. (CHAPTER 1) APPENDIX A.
Table 1.47: Data from Part 4.
102 PhD Thesis
A.1. (CHAPTER 1) APPENDIX A.
Table 1.48: Data from Part 5.
103 PhD Thesis
A.1. (CHAPTER 1) APPENDIX A.
A.1.4 Instructions Set
104 PhD Thesis
GeneralInstructionsfortheExperiment
Welcome to the experiment. This is an experiment related to decisionmaking. Over
approximatelynext75minutes,youwillbeaskedtoparticipateinseveraltasks.Forsimply
showingup to this experiment youhave already received 3.You can earn considerably
more.During theexperiment,youmayearnECU (ExperimentalCurrencyUnit).The total
amountofECUthatyouwillhaveearnedduringtheexperimentwillbeconvertedintoat
theendoftheexperiment;50ECU=1.Youwillreceivetheseearningsincash,in,andin
privateattheendoftheexperiment.Pleasestayseateduntilweaskyoutoleave.
Therearesixindependentpartsofthisexperimentfollowedbyaquestionnaire.Thedetails
will be provided in the instructions for each part.Any examples in the instructions are
merelyforillustrationpurposes;youshouldnotinterpretthemastheadviceofanykind.
Pleasereadtheinstructionscarefully.Ifyouhaveanyquestions,pleaseraiseyourhand.One
of the experimenters will come to you and answer your questions. From now on
communicationwith other participants is not allowed. Do not use yourmobile phones
duringtheexperiment.
Remember,on thecomputer screenonceSubmitbutton is clickedyou cantgobackand
changeyourchoices.
InstructionsforPart1:
Thispartoftheinstructionsexplainstheraffletaskfollowedbytwoquizzes.Eachquizhas
threequestionstobeanswered.Allquestionsaremultiplechoicequestionsandthereisone
correctanswer.Ifyouchooseacorrectanswerthenyouwillscore100ECUforthatquestion,
anincorrectanswerwillscorezero.Yourscorefromoneofthesixquestionswillbechosen
randomlyfortheactualpayment.
There isarafflewhereaprizeworth100ECUcanbewon.Yourprobabilityofwinning is
thenumberofraffleticketsyoubuydividedbythenumberofraffleticketssoldintotal.For
instance,ifyoubuyXraffleticketsandtheotherparticipantsY,yourchanceofwinningisX
/(X+Y).Eachraffleticketcosts1ECU
Ifyouwin,yourincomefromtherafflewillbe:
Income=100(Endowment)No.ofticketsyoubought(0100)+100(PrizeValue)
Ifyoulose,yourincomefromtherafflewillbe:
Income=100(Endowment)No.ofticketsyoubought(0100)
Onthecomputerscreen,Quiz1willbefollowedbyitsanswersandexplanationsafteryou
haveclickedSubmitbutton.Similarly,Quiz2willbefollowedbyitsanswersand
explanations.
QUIZ1
Q1.Assume thatyoubought 10 tickets in a raffle and all otherparticipants in the raffle
bought90.Whatisyourchanceofwinningtheraffle?
(a)10/90
(b)10/100
(c)10/80
(d)80/90
Q2.Assumethatfromanendowmentof100ECUyoubought40raffletickets.Whatisyour
incomeifyouwin?(Hint:Ifyouwin,Income=EndowmentTicketCost+PrizeValue)
(a)80
(b)180
(c)160
(d)40
Q3.Assumeyourprobabilityofwinninginaraffleis0.8.Whatisyourchanceofwinning
expressedinpercentage?
(a)40%
(b)80%
(c)60%
(d)100%
PleaseWRITEDOWNyouranswersinthisinstructionssheetBEFOREclickingtheSubmit
buttononyourcomputerscreen.
QUIZ2
Q4.Assume that in a raffle you bought 60 tickets and all otherparticipants bought 140.
Whatisyourchanceofwinningtheraffle?
(a)60/160
(b)160/200
(c)60/200
(d)160/300
Q5.Assumethatfromanendowmentof100ECUyoubought40raffletickets.Whatisyour
incomeifyoulose?(Hint:Ifyoulose,Income=EndowmentYourTicketCost)
(a)60
(b)140
(c)30
(d)170
Q6.Assumeyourprobabilityofwinninginaraffleis0.3.Whatisyourchanceofwinning
expressedinpercentage?
(a)40%
(b)60%
(c)80%
(d)30%
PleaseWRITEDOWNyouranswersinthisinstructionssheetBEFOREclickingtheSubmit
buttononyourcomputerscreen.
InstructionsforPart2:
Inthispartoftheexperiment,youwillplay10roundsoftheraffledescribedinthequizzes
earlierandexplainedagainintheparagraphsbelow.Inthefirstfiverounds,youwillbeina
groupof size 2 and in the last five roundsgroup sizewillbe 3.Groupswillbe formed
randomlyatthebeginningofeachgroupsize.Youwillstayinthesamegroupforallfive
roundsofeachgroupsize.
In each round,youhave an endowmentof 100ECU andyou can buybetween 0 to 100
tickets.Similarly,othermembersofyourgroupwillalso choose tobuybetween0 to100
tickets.Youwillnotknowhowmuch theotherparticipantschoose.Thewinningprize is
100ECUandthecostofeachticketis1ECU.
Yourprobabilityofwinningisthenumberofticketsyoubuydividedbythetotalnumberof
theticketsbought inyourgroup(includingyou).Oneoftheparticipantsfromyourgroup
willbechosenrandomlyasawinner.YourincomeinECUinanyroundwillbeasfollows:
Ifyouwin,yourincomewillbe:
Income=100(Endowment)No.ofticketsyoubought(0100)+100(PrizeValue)
Ifyoulose,yourincomewillbe:
Income=100(Endowment)No.ofticketsyoubought(0100)
Outofthe10rounds,yourincomefromoneoftheroundswillbechosenrandomlyasyour
actualpaymentfromthispartoftheexperiment.
InstructionsforPart3:
Inthispartoftheexperiment,youwillbegroupedwithotherparticipants.Youwillplay4
games.Eachgamehas2raffles,oneinPeriod1followedbyanotherinPeriod2.
InGame 1, in Period 1 youwill be grouped in a group of size 2 and in Period 2 two
additionalparticipantswilljoinyourgroupandyourgroupsizewillincreaseto4,asshown
inTable1below.
InGame 2, in Period 1 youwill be grouped in a group of size 3 and in Period 2 three
additionalparticipantswilljoinyourgroupandyourgroupsizewillincreaseto6,asshown
inTable1below.
InGame3,inPeriod1youwillbegroupedinagroupofsize4andinPeriod2twogroup
memberswillleaveyourgroupandyourgroupsizewilldecreaseto2,asshowninTable1
below.
InGame4,inPeriod1youwillbegroupedinagroupofsize6andinPeriod2threegroup
memberswillleaveyourgroupandyourgroupsizewilldecreaseto3,asshowninTable1
below.
Ineachgame,atthestartofeachperiod,youwillgetanendowmentof100ECUandyou
havetodecidehowmuchofthisendowmentwillyouliketousetopurchasetickets.Each
ticketcosts1ECU.Youcanbuyfrom0to100raffletickets.Ineachgame,eachperiodthe
prizevalueis100ECU.
Youwillnotknowhowmanyticketstheothergroupmembershavebought.Youwillknow
theoutcomesofalltherafflesONLYattheendofPeriod2ofGame4.Notethatexactlyone
oftheparticipantswillwintheprizeineverygroup,foreachperiodandineverygame.
Additionally,youhavetoestimateyourprobabilityofwinning(P1P8)giventhenumber
of tickets you have purchased in each game in each period. Your estimation of the
probabilityofwinningwillnotbeusedincomputingyourincome.
Yourincome(inECU)ineachrafflewillbecalculatedasfollows:
Ifyouwin,yourincomewillbe:
Income=100(Endowment)No.ofticketsyoubought(0100)+100(PrizeValue)
Ifyoulose,yourincomewillbe:
Income=100(Endowment)No.ofticketsyoubought(0100)
Outof the total 8 rafflesyouplayed,your income fromoneof the raffleswillbe chosen
randomlyasyouractualpaymentforthispartoftheexperiment.
InstructionsforPart4:
Thispartoftheexperimentconsistsof6raffles(R1R6).Youwillbematchedwithanother
participant.Foreachraffle,youareendowedwith100ECU.Theendowmentcanbeusedto
purchaseraffletickets.Youcanbuybetween0to100tickets.Eachraffleticketcosts1ECU.
Theprizevalueofeachraffleis100ECU.
Bothyou andyourmatchedparticipant areprovidedwithTable 2. InTable 2, column 2
(MatchedParticipantsFixedTicketsChoice)hasfixedticketchoicesforeachraffleR1R6.
Againstthesefixedticketchoices,youhavetoenteranumberofticketsyouwanttobuyfor
each raffleR1R6.Youwillenteryourchoicesofanumberof tickets incolumn3ofyour
Table2.Similarly,yourmatchedparticipantwillenterherchoicesofanumberofticketsin
column 3ofherTable 2.Note,you cantmake any choices inyourmatchedparticipants
Table2andshecantmakeanychoicesinyourTable2.
The fixed ticketschoices inyourTable2willbeconsideredasyourmatchedparticipants
ticketchoicesinyourtable.Similarly,thefixedticketschoicesinyourmatchedparticipants
Table2willbeconsideredasyourticketschoicesinhertable.Thefixedticketschoicescant
bechanged.
Either your Table 2 or yourmatched participants Table 2will be selected for the actual
payment calculation. The unselected Table 2will be discarded. You and yourmatched
participanthaveanequalchanceofgettingyourTable2selected.
OnceeitheryoursoryourmatchedparticipantsTable2isselected,oneoftheraffles(R1R6)
willbe selected from this table foryours andyourmatchedparticipants actualpayment
calculation.Allsixrafflesareequallylikelytobechosen.
IfyourTable2isselected,yourprobabilityofwinningtherafflewillbeasgivenincolumn5
ofyourTable2andyourmatchedparticipantsprobabilityofwinningtherafflewillbeas
givenincolumn6ofyourTable2.
IfyourmatchedparticipantsTable2isselected,herprobabilityofwinningtherafflewillbe
asgiven in column5ofherTable2andyourprobabilityofwinning the rafflewillbeas
givenincolumn6ofherTable2.
Youractualpayment(inECU)isgivenasfollows:
IfyourTable2isselectedandarafflefromR1R6isselected:
Ifyouwin,youractualpaymentwillbe:
100(Endowment)YourTicketChoicefortheselectedraffle+100(PrizeValue)
Ifyoulose,youractualpaymentwillbe:
100(Endowment)YourTicketChoicefortheselectedraffle
IfyourmatchedparticipantsTable2isselectedandarafflefromR1R6isselected:
Ifyouwin,youractualpaymentwillbe:
100(Endowment)FixedTicketChoicefortheselectedraffle+100(PrizeValue)
Ifyoulose,youractualpaymentwillbe:
100(Endowment)FixedTicketChoicefortheselectedraffle
InstructionsforPart5:
Inthispartoftheexperiment,youarenotgroupedwithotherparticipants.Youmustchoose
oneofthelotteries(L1L9)fromeachLotteryTable1,LotteryTable2,andLotteryTable3.
ForeachLotteryTable,youareendowedwith100ECU.Theendowment canbeused to
purchaseanyoneofthelotteriesinthatLotteryTable.
ForeachofthelotteriesinalltheLotteryTables,itsprobabilityofwinning,costandprize
valueofthelotteryismentionedinthetable.Unlikeintheearlierpartsoftheexperiment,in
thispart, theprobabilityofwinning is fixed foreach lottery.Note:Only thevalues in the
CostcolumnaredifferentineachLotteryTable.
Once you havemade your choices, any one of the three Lottery Tableswill be selected
randomly.All Lottery Tables are equally likely to be chosen.After the Lottery Table is
selected,thelotteryyouhavechoseninthatLotteryTablewillbeconsideredfortheactual
paymentcalculation.
Youractualpayment(inECU)willbeasfollows:
Ifyouwin,youractualpaymentwillbe:
100(Endowment)CostofyourchosenlotteryintheselectedTable+100(PrizeValue)
Ifyoulose,youractualpaymentwillbe:
100(Endowment)CostofyourchosenlotteryintheselectedTable
InstructionsforPart6:
In thispart of the experiment, you arenot grouped.Youhave tomake 10 choices.Each
decisionisapairedchoicebetweenOptionAandOptionB.Foreachdecisionrow(D1
D10),youwillhavetochoosebetweenOptionAandOptionB.YoumaychooseAforsome
decisionrowsandBforotherrowsandyoumaychangeyourdecisionsandmakethemin
anyorder.
Now,please lookatdecisionD1at the top in thebelow table.OptionAhas twopossible
outcomesA1andA2.A1pays100ECUwithaprobabilityof0.1andA2pays80ECUwith
theremainingprobabilityof0.9.Similarly,OptionBhastwopossibleoutcomesB1andB2.
B1pays193ECUwithaprobabilityof0.1andB2pays5ECUwiththeremainingprobability
of0.9.
Theotherdecisionsaresimilar,exceptthatasyoumovedownthetable,thechancesofthe
higherpayingoutcome foreachoption increases. In fact, fordecisionD10 in thebottom
row,eachoptionpaysthehighestpayoffforsure,soyourchoicehereisbetween100or193
Todetermine your income, one of thedecisions (D1D10)will be selected randomly.All
decisionshaveanequalchanceofbeingselectedforyouractualpaymentcalculation.
Yourincome(inECU)willbeasfollows:
IfyouhavechosenA,yourincomewillbeeither100or80basedontheprobabilitiesstated
inthetable.
IfyouhavechosenB,yourincomewillbeeither193or5basedontheprobabilitiesstatedin
thetable.
PrA1 A1 PrA2 A2 PrB1 B1 PrB2 B2
D1 0.1 100 0.9 80 0.1 193 0.9 5
D2 0.2 100 0.8 80 0.2 193 0.8 5
D3 0.3 100 0.7 80 0.3 193 0.7 5
D4 0.4 100 0.6 80 0.4 193 0.6 5
D5 0.5 100 0.5 80 0.5 193 0.5 5
D6 0.6 100 0.4 80 0.6 193 0.4 5
D7 0.7 100 0.3 80 0.7 193 0.3 5
D8 0.8 100 0.2 80 0.8 193 0.2 5
D9 0.9 100 0.1 80 0.9 193 0.1 5
D10 1 100 0 80 1 193 0 5
Decision
OptionA OptionB
Questionnaire:
Pleaseanswerthefollowingquestions.Theseanswerswillbeanonymousandthereareno
rightandwronganswers.Themoreexhaustiveyouwillbeinansweringthesequestions,the
moreyouwillbehelpingtheresearchstudy.Pleasewriteclearly
1. YourAge?_____
2. YourGender?Tick
1) Female,2)Male,3)NotListed,4)PreferNottoAnswer
3. YourNationality?__________________
4. Yoursubjectofstudy?(Eg:Economics)______________________
5. Yourlevelofstudy?Tickonebelow
1) UndergraduateYear1,2)Year2,3)Year3,4)Year4,5)Masters,6)PhD
6. WhichPartsoftheexperimenthaveclearinstructions?Tickanybelow
1) Part1,2)Part2,3)Part3,4)Part4,5)Part5,6)Part6
7. Werequizzeshelpfulinmakingyouunderstandthegameofraffle?Tickone
1) notatall,2)somewhat,3)verymuch,4)absolutely
8. InwhichPartsyoufeltconfidentoftheapproachyoutookinmakingchoices?
Tickanybelow
1) Part1,2)Part2,3)Part3,4)Part4,5)Part5,6)Part6
9. Were results in one part of the experiment impacted yourdecisions in the
followingparts?Tickonebelow
1) Yes,2)No
Ifyes,howexactly:___________________________
10. Did you have some basic understanding of probability prior to this
experiment?Tickonebelow
1) Yes,2)No
11. Areyoucomfortablewithnumericcalculations?Tickonebelow
1) notatall,2)somewhat,3)verymuch,4)absolutely:itmakesmefeel
better.
12. Haveyouplayedanyraffle/lotterygamesearlier?Tickanybelow
1) Yesinreallife,2)YesinAnotherexperiment,3)Notatall.
13. In thegameswhereyouwere inagroup,didyourchoicemaking take into
considerationwhatchoicesothergroupmembersmaybemaking?Tickone
1) Notatall,2)Somewhat,3)FullyConsidered
14. Ingeneral,whichof the following factorsdidyouconsidered tomakeyour
choices?Tickanybelow
1) Income/ActualPayment,2)Cost,3)ProbabilityofWinning
15. Doyoulikewinningevenifyouendupmakinglossesintotal?Tickonebelow
1) notatall, 2)somewhat, 3)verymuch, 4)absolutely:winningatany
16. How you approached the game of raffle/lottery?Onwhat basiswere you
makingyourchoices/decisions?________________________________________
17. Wasthereacommonstrategyyoutooktomakedecisionsineverypartofthe
experiment?__________________________________________________________
18. Howwinning or losing in any raffle/lottery impacted your choices in the
followingraffle/lottery?________________________________________________
19. Anythoughts,commentsorsuggestionsonimprovementsintheexperiment?
_____________________________________________________________________
Thank you verymuch for your participation Please do not share anything about this
experimentwithanyonefornext twomonths.Thereareothersessions toberunand it
willcontaminatetheresearchstudy.
A.1. (CHAPTER 1) APPENDIX A.
A.1.5 z-Tree Screenshots
Figure 1.49: z-tree screenshot of Part 1 (Quizzes) - questions
Figure 1.50: z-tree screenshot of Part 1 (Quizzes) - solutions
119 PhD Thesis
A.1. (CHAPTER 1) APPENDIX A.
Figure 1.51: z-tree screenshot of Part 1 (Quizzes) - solutions
Figure 1.52: z-tree screenshot of Part 1 (Quizzes) - payment
120 PhD Thesis
A.1. (CHAPTER 1) APPENDIX A.
Figure 1.53: z-tree screenshot of Part 2 (Experience) - choice input
Figure 1.54: z-tree screenshot of Part 2 (Experience) - feedback
121 PhD Thesis
A.1. (CHAPTER 1) APPENDIX A.
Figure 1.55: z-tree screenshot of Part 2 (Experience) - payment
Figure 1.56: z-tree screenshot of Part 3 (Group Size Change) - choice input Game 1, Period 1
122 PhD Thesis
A.1. (CHAPTER 1) APPENDIX A.
Figure 1.57: z-tree screenshot of Part 3 (Group Size Change) - choice input Game 1, Period 2
Figure 1.58: z-tree screenshot of Part 3 (Group Size Change) - choice input Game 4, Period 1
123 PhD Thesis
A.1. (CHAPTER 1) APPENDIX A.
Figure 1.59: z-tree screenshot of Part 3 (Group Size Change) - choice input Game 4, Period 2
Figure 1.60: z-tree screenshot of Part 3 (Group Size Change) - payment
124 PhD Thesis
A.1. (CHAPTER 1) APPENDIX A.
Figure 1.61: z-tree screenshot of Part 4 (Response Curve) - choice input
Figure 1.62: z-tree screenshot of Part 4 (Response Curve) - payment
125 PhD Thesis
A.1. (CHAPTER 1) APPENDIX A.
Figure 1.63: z-tree screenshot of Part 5 (Inverted Lottery Sets) - choice input Lottery Table 1
Figure 1.64: z-tree screenshot of Part 5 (Inverted Lottery Sets) - choice input Lottery Table 2
126 PhD Thesis
A.1. (CHAPTER 1) APPENDIX A.
Figure 1.65: z-tree screenshot of Part 5 (Inverted Lottery Sets) - choice input Lottery Table 3
Figure 1.66: z-tree screenshot of Part 5 (Inverted Lottery Sets) - payment
127 PhD Thesis
A.2. (CHAPTER 2) APPENDIX A.
Figure 1.67: z-tree screenshot of Part 6 (Measuring Risk Aversion) - choice input
A.2 (Chapter 2)
A.2.1 Further Simulations
We further simulate the model to see trajectories of aggregate effort over the period of time
to find that the model can track different trajectories for various parametric values.
Figure 2.15: Simulations for the game having group size equals six, for different parametric
values. Where pT stands for ic and x stands for 
128 PhD Thesis
A.2. (CHAPTER 2) APPENDIX A.
Figure 2.16: Simulations for the game having group size equals six, for different parametric
values. Where pT stands for ic and x stands for 
Figure 2.17: Simulations for the game having group size equals six, for different parametric
values. Where pT stands for ic and x stands for 
A.2.2 Reference for Proposition 1
Theorem 2.15 [Page 83, Markov Processes - Advances in applied mathematics by James R
Kirkwood (2015)]: The probability that any state in a finite absorbing Markov chain is absorbed
after n steps approaches 1 as n goes to infinity.
Proof: We show that the probability that any state in an absorbing Markov chain is not
absorbed after n steps approaches 0 as n goes to infinity. Let i be a non-absorbing state. Since
the Markov state is absorbing, there is a positive integer mi for which there is a positive
probability pi that i has moved to an absorbing state after mi steps. Note that this implies
129 PhD Thesis
A.2. (CHAPTER 2) APPENDIX A.
that if l  mi , then the probability that i has been absorbed after l steps is greater than or
equal to pi . Thus, the probability that state i has not been absorbed after mi steps is less
than or equal to 1 pi . Repeat the aforementioned procedure for each transient state, and let
m =max {mi}, p =min {pi}. Then, beginning in any state, the probability the process has not
been absorbed after m steps is less than or equal to 1p. Thus, for any positive integer N, the
probability the process has not been absorbed after Nm steps is less than or equal to (1 p)N
and limN(1 p)N = 0. This also means that each entry of Qn goes to 0 as n becomes large.
A.2.3 Learning Model Simulation Code
130 PhD Thesis
C:\Users\Vikas\workspace\contestsimulation\src\contestsimulation\certaintyzone3.java Saturday, January 12, 2019 8:47 AM
*** Simulation code for Chapter:Learning in Contests, Section:Simulations
package contestsimulation;
*** @author Vikas
public class target probabilityzone3 {
public static void main(String[] args) {
// TODO Auto-generated method stub
//for(int k=0;k<10;k++){
int session=25;
int period=100;
int prizemoney=80;
int playercount=4;
int effortcount=11;
double[] agentpriorslope = new double[playercount]; // m in y=mx
double[] agentpT = new double[playercount]; // pT- target probability zone 
probability
double[] agentx = new double[playercount]; //
double[][] effortprior = new double[playercount][effortcount]; // prior for each 
effort
double[][][] effortchoice = new double [session][period][playercount]; //  effort 
choice
double[][][] effortoutcome = new double [session][period][playercount]; //  effort 
outcome
double[][] aggeffortchoice = new double [period][playercount]; //  agg effort choice
double[][] aggeffortoutcome = new double [period][playercount]; //  agg effort 
outcome
//this program is target probability experiment, where agent tries to reach target 
probability then updates her belief.
//first agent chooses slope of her prior 'm', y=mx then she chooses the level of 
target probability she is looking for 
//then she chooses how to update her priors depending on the result of her last 
round. 
//Then there are exogenous shocks if agents has stopped working.    
//for(int j=0;j<10;j++){
for(int s=0; s<session; s++)
//agent choosing the target probability zone and slope of its priorsand making sure 
the slope 
//if such that target probability zone choice is actionable
for(int agent=0;agent<playercount;agent++)
double a,b,c;
//do{
a=(Math.random()*20+20)/100; //20 to 40 % minimum target probability pT
C:\Users\Vikas\workspace\contestsimulation\src\contestsimulation\certaintyzone3.java Saturday, January 12, 2019 8:47 AM
b=(Math.random());//*60+40)/100; //40 to 100 % at 100 effort m
c=(Math.random()*5+10)/100;
//}while(b<a); // choose again if m<pT
agentpT[agent]=a;
agentpriorslope[agent]=b;
agentx[agent]=c;
//agent setting the its priors for each effort
for(int agent=0;agent<playercount;agent++)
for(int efforti=0;efforti<effortcount;efforti++)
effortprior[agent][efforti] = agentpriorslope[agent]*efforti*0.1;
for(int p=0; p<period;p++)
double totaleffort=0;
int winner=0;
int [] PWin= new int[playercount];
//agents making the strategy decision
for(int agent=0;agent<playercount;agent++)
for(int efforti=0;efforti<effortcount;efforti++)
if (effortprior[agent][efforti] >= agentpT[agent])
{ effortchoice[s][p][agent] = efforti*10;
break;
else { effortchoice[s][p][agent]=0;}
totaleffort=totaleffort + effortchoice[s][p][agent];
//generate probabilistic prize
double randomnumber=(Math.random()*totaleffort);
double teffort=0;
for (int agent=0;agent<playercount; agent++)
{ teffort=teffort + effortchoice[s][p][agent];
if(randomnumber<= teffort)
C:\Users\Vikas\workspace\contestsimulation\src\contestsimulation\certaintyzone3.java Saturday, January 12, 2019 8:47 AM
winner=agent;
break;
//screen output
for(int agent=0;agent<playercount;agent++)
if(agent==winner )
PWin[agent]=1;
PWin[agent]=0;
effortoutcome[s][p][agent]=PWin[agent];
//print screen
/*   System.out.printf("%4d  %4d   %5.1f  %5.1f  %5.1f  %5.1f  %5.1f  %5.1f %5.0f  
%5.0f %5.0f  %5.0f  %5.0f  %5.0f%n",
(s+1),
(p+1), 
(agentpriorslope[0]),
(agentpriorslope[1]),
(agentpriorslope[2]),
(agentpT[0]),
(agentpT[1]),
(agentpT[2]),
(effortchoice[s][p][0]), 
(effortchoice[s][p][1]),
(effortchoice[s][p][2]),
(effortoutcome[s][p][0]),
(effortoutcome[s][p][1]),
(effortoutcome[s][p][2])
// belief revision of monotonically higher(win) and lower(lose) strategies with 
immediate below(win)/above(lose) strategies treated as similar 
for(int agent=0;agent<playercount;agent++)
for(int efforti=0;efforti<effortcount;efforti++)
if (PWin[agent]==1 && ((efforti+1)*10 >= effortchoice[s][p][agent]))
effortprior[agent][efforti]=effortprior[agent][efforti]*(1+agentx[agent
C:\Users\Vikas\workspace\contestsimulation\src\contestsimulation\certaintyzone3.java Saturday, January 12, 2019 8:47 AM
if (effortprior[agent][efforti]>1)
{effortprior[agent][efforti]=1;}
if (PWin[agent]==0 && ( (efforti-1)*10 <= effortchoice[s][p][agent]))
effortprior[agent][efforti]=effortprior[agent][efforti]*(1-agentx[agent
if (effortprior[agent][efforti]<0)
{effortprior[agent][efforti]=0;}
// print belief revision
/* for(int agent=0;agent<playercount;agent++)
System.out.printf("%5.2f  %5.2f  %5.2f  %5.2f  %5.2f  %5.2f  %5.2f  %5.2f  
%5.2f  %5.2f%n", 
effortprior[agent][0],
effortprior[agent][1],
effortprior[agent][2],
effortprior[agent][3],
effortprior[agent][4],
effortprior[agent][5],
effortprior[agent][6],
effortprior[agent][7],
effortprior[agent][8],
effortprior[agent][9]);
for(int agent=0;agent<playercount;agent++){
for(int p=0;p<period;p++){
for(int s=0;s<session;s++){
aggeffortchoice[p][agent]=aggeffortchoice[p][agent]+effortchoice[s][p][agent];
aggeffortoutcome[p][agent]=aggeffortoutcome[p][agent]+effortoutcome[s][p][agent];
for(int p=0;p<period;p++){
System.out.printf("%4d  %5.1f  %5.1f  %5.1f   %5.2f  %5.2f   %5.2f%n",
(p+1),
(aggeffortchoice[p][0]/session),
(aggeffortchoice[p][1]/session),
(aggeffortchoice[p][2]/session),
(aggeffortoutcome[p][0]/session),
(aggeffortoutcome[p][1]/session),
(aggeffortoutcome[p][2]/session)
for(int agent=0;agent<playercount;agent++){
for(int p=0;p<period;p++){
aggeffortchoice[p][agent]=0;
C:\Users\Vikas\workspace\contestsimulation\src\contestsimulation\certaintyzone3.java Saturday, January 12, 2019 8:47 AM
aggeffortoutcome[p][agent]=0;
//}//closing for(int k;)
A.3. (CHAPTER 3) APPENDIX A.
A.3 (Chapter 3)
A.3.1 Simulation Code
136 PhD Thesis
1   /**
2    *** SIMULATIONS CODE FOR SECTION 6 *** 
3    */
4   package reputation_mechanism;
6   /**
7    * @author Vikas
8    *
9    */
10   public class HistoryBasedReputation {
12   /**
13    * @param args
14    */
15   public static void main(String[] args)
17   {
19   //System.out.println("Welcome to the World of Evolutionary Simulations");
21   double endowment_e =1;
22   double initial_population=10000;
24   for( double
initial_mutant_size_percentage=0.01;initial_mutant_size_percentage<=0.01;
initial_mutant_size_percentage+=0.01)
26   {
27   for( double mutant_contribution=0; mutant_contribution<endowment_e;
mutant_contribution+=0.5)
29   { for( double multiplier=2; multiplier<=6; multiplier+=2)
31   { double alpha = multiplier*endowment_e - mutant_contribution +
endowment_e;
32   double beta = multiplier*endowment_e;
33   double gamma = multiplier*mutant_contribution -
mutant_contribution + endowment_e;
34   double delta = multiplier*mutant_contribution;
35   //System.out.println("***"+"alpha="+ alpha + ", " + "beta="+ beta 
+ " ," + "gamma=" + gamma + ", " + "delta=" + delta + "***");
36   //System.out.println();
37   //System.out.println();
38   for( int reputation_period=10; reputation_period<=10;
reputation_period+=5)
40   {
41   int sequencerows = (int) Math.pow(2,reputation_period);
42   double score_s = Math.min(endowment_e,mutant_contribution) -
Math.abs(endowment_e-mutant_contribution) -
Math.abs(endowment_e-mutant_contribution);
43   double[][] reputation_block = new double
[sequencerows][reputation_period];
44   int simulation_periods_length = reputation_period*5;
45   double[][] incumbent_population = new double
[simulation_periods_length+1][sequencerows];
46   double[][] mutant_population = new double
[simulation_periods_length+1][sequencerows];
49   // Initialize the 0th reputation period as all 
eee...
50   // Given you have fixed the reputation period there are possible 
histories of e and s. 
51   // Now one can calculate the population of different (four based 
on the payoffs) incumbents and mutants in each history.
52   // Calculate their new history and next period population size 
and which will determine the matching for next period
55   for (int i=0; i<sequencerows; i++) // sequence of history {e,s} 
is generated
56   {
57   for (int j=0; j<=reputation_period-1; j++)
58   {
59   reputation_block[i][j] = (i/(int) Math.pow(2,
j))%2 ;
61   if (reputation_block[i][j]==0)
62   { reputation_block[i][j]= score_s;
63   }
65   if (reputation_block[i][j]==1)
66   { reputation_block[i][j]= endowment_e;
67   }
69   }
71   }
74   for (int i=0; i<sequencerows; i++) // initializing the 
population size for the first period of simulation
76   {
77   incumbent_population[0][i]=0;
78   incumbent_population[0][i]=0;
80   double match_counter=0;
83   for (int j=0; j<=reputation_period-1; j++)
84   {
85   if (reputation_block[i][j] == endowment_e)
86   {
87   match_counter++;
89   }
90   }
92   if (match_counter==reputation_period) // initial reputation 
is that of full.
94   {
96   incumbent_population[0][i] = initial_population -
initial_population*initial_mutant_size_percentage ;
97   mutant_population[0][i] =
initial_population*initial_mutant_size_percentage ;
99   }
101   }
103   // initializing the population size for the all the periods of 
simulation (except first) to zero 
105   for( int simulation_period=1;
simulation_period<simulation_periods_length; simulation_period++)
107   {
108   for (int i=0; i<sequencerows; i++)
110   {
111   incumbent_population[simulation_period][i]=0;
112   incumbent_population[simulation_period][i]=0;
113   }
114   }
117   for( int simulation_period=0;
simulation_period<simulation_periods_length; simulation_period++)
119   { double store_calculated_population;
120   double [] store_calculated_sequence = new double
[reputation_period];
122   for (int i=0; i<sequencerows; i++)
124   {
126   /* calculating incumbent population when "incumbent 
matching incumbent"*/
128   store_calculated_population = 0;
130   if ((incumbent_population[simulation_period][i] +
mutant_population[simulation_period][i])>0)
131   {
132   store_calculated_population =
beta*incumbent_population[simulation_period][i]*in
cumbent_population[simulation_period][i]/(incumben
t_population[simulation_period][i] +
mutant_population[simulation_period][i]);
133   }
135   // generate new sequence and find this sequence in 
the reputation block and add it to next simulation 
period.
137   for (int j=0 ;j<reputation_period-1; j++)
138   {
139   store_calculated_sequence[j] =
reputation_block[i][j+1] ;
140   }
142   store_calculated_sequence[reputation_period-1] =
endowment_e; // incumbent matching incumbent 
generates full score for that period
144   // find this new_stored_pooled_sequence in 
reputation block and increment the population in 
next simulation period.
146   for (int r=0; r<sequencerows; r++) // 
148   {
149   double match_counter=0;
152   for (int j=0; j<=reputation_period-1; j++)
153   {
154   if (store_calculated_sequence[j] ==
reputation_block[r][j])
155   {
156   match_counter++;
158   }
159   }
161   if (match_counter==reputation_period)
162   {
163   //System.out.println ("match counter="+ 
match_counter + " " + "newseq=" + newseq);
164   //System.out.println ("r=" + r + " "+ 
"incumbent_population[simulation_period+1][
r]=" + 
incumbent_population[simulation_period+1][r
] + " " + 
"new_incumbent_pooled_population[m]" + 
new_incumbent_pooled_population[m] + " " 
+ "pooled_nbr_of_sequence[r]=" + 
pooled_nbr_of_sequence[r]);
incumbent_population[simulation_period+1][r
incumbent_population[simulation_period+1][r
] + store_calculated_population;
166   //System.out.println ("After calculation" 
+ r + " "+ 
"incumbent_population[simulation_period+1][
r]=" + 
incumbent_population[simulation_period+1][r
167   //System.out.println ();
168   }
169   }
173   /* calculating incumbent population after "incumbent 
matching mutant"  */
175   store_calculated_population = 0;
177   if ((incumbent_population[simulation_period][i] +
mutant_population[simulation_period][i]) >0)
178   {
179   store_calculated_population =
delta*incumbent_population[simulation_period][i]
*mutant_population[simulation_period][i] /(
incumbent_population[simulation_period][i] +
mutant_population[simulation_period][i] ) ;
180   }
182   // generate new sequence and find this sequence in 
the reputation block and add it to next simulation 
period.
183   for (int j=0 ;j<reputation_period-1; j++)
184   {
185   store_calculated_sequence[j] =
reputation_block[i][j+1] ;
186   }
187   store_calculated_sequence[reputation_period-1] =
score_s;
189   // find this new_stored_pooled_sequence in 
reputation block and increment the population in 
next simulation period.
191   for (int r=0; r<sequencerows; r++) // 
193   {
194   double match_counter=0;
197   for (int j=0; j<=reputation_period-1; j++)
198   {
199   if (store_calculated_sequence[j] ==
reputation_block[r][j])
200   {
201   match_counter++;
202   }
203   }
205   if (match_counter==reputation_period)
206   {
207   //System.out.println ("match 
counter="+ match_counter + " " + 
"newseq=" + newseq);
208   //System.out.println ("r=" + r + 
"incumbent_population[simulation_p
eriod+1][r]=" + 
incumbent_population[simulation_pe
riod+1][r] + " " + 
"new_incumbent_pooled_population[m
new_incumbent_pooled_population[m]
 + " " + 
"pooled_nbr_of_sequence[r]=" + 
pooled_nbr_of_sequence[r]);
incumbent_population[simulation_pe
riod+1][r] =
incumbent_population[simulation_pe
riod+1][r] +
store_calculated_population;
210   //System.out.println ("After 
calculation" + " " + 
"incumbent_population[simulation_p
eriod+1][r]=" + 
incumbent_population[simulation_pe
riod+1][r]);
211   //System.out.println ();
212   }
213   }
217   /*  calculating mutant population when "mutant 
matching incumbent"  */
219   store_calculated_population = 0;
221   if ((incumbent_population[simulation_period][i] +
mutant_population[simulation_period][i]) >0)
222   {
223   store_calculated_population =
alpha*mutant_population[simulation_period][i]*i
ncumbent_population[simulation_period][i]/(incu
mbent_population[simulation_period][i] +
mutant_population[simulation_period][i]) ;
224   }
227   // generate new sequence and find this sequence in 
the reputation block and add it to next simulation 
period.
228   for (int j=0 ;j<reputation_period-1; j++)
229   {
230   store_calculated_sequence[j] =
reputation_block[i][j+1];
231   }
232   store_calculated_sequence[reputation_period-1] =
score_s;
234   // find this new_stored_pooled_sequence in 
reputation block and increment the population in 
next simulation period.
236   for (int r=0; r<sequencerows; r++) // 
238   {
239   double match_counter=0;
242   for (int j=0; j<=reputation_period-1;
243   {
244   if (store_calculated_sequence[j] ==
reputation_block[r][j])
245   {
246   match_counter++;
247   }
248   }
250   if (match_counter==reputation_period)
251   {
252   //System.out.println ("match 
counter="+ match_counter + " " + 
"newseq=" + newseq);
253   //System.out.println ("r=" + r + 
"mutant_population[simulation_peri
od+1][r]=" + 
mutant_population[simulation_perio
d+1][r] + " " + 
"new_mutant_pooled_population[m]" 
new_mutant_pooled_population[m] 
+ " " + 
"pooled_nbr_of_sequence[r]=" + 
pooled_nbr_of_sequence[r]);
mutant_population[simulation_perio
d+1][r] =
mutant_population[simulation_perio
d+1][r] +
store_calculated_population;
255   //System.out.println ("After 
calculationr=" + " "+ 
"mutant_population[simulation_peri
od+1][r]=" + 
mutant_population[simulation_perio
d+1][r]);
256   //System.out.println ();
257   }
258   }
261   /*  calculating mutant population when "mutant 
matching mutant"  */
263   store_calculated_population = 0;
265   if ((incumbent_population[simulation_period][i] +
mutant_population[simulation_period][i]) >0)
266   {
267   store_calculated_population =
gamma*mutant_population[simulation_period][i]*mu
tant_population[simulation_period][i]/(incumbent
_population[simulation_period][i] +
mutant_population[simulation_period][i]) ;
268   }
270   // generate new sequence and find this sequence in 
the reputation block and add it to next simulation 
period.
272   for (int j=0; j<reputation_period-1; j++)
273   {
274   store_calculated_sequence[j] =
reputation_block[i][j+1] ;
275   }
276   store_calculated_sequence[reputation_period-1] =
endowment_e;
278   // find this new_stored_pooled_sequence in 
reputation block and increment the population in 
next simulation period.
280   for (int r=0; r<sequencerows; r++) // 
282   {
283   double match_counter=0;
286   for (int j= 0 ; j<=reputation_period-1;
287   {
288   if (store_calculated_sequence[j] ==
reputation_block[r][j])
289   {
290   match_counter++;
292   }
293   }
295   if (match_counter==reputation_period)
296   {
297   //System.out.println ("match 
counter="+ match_counter + " 
" + "newseq=" + newseq);
298   //System.out.println ("r=" + 
r + " "+ 
"mutant_population[simulation_
period+1][r]=" + 
mutant_population[simulation_p
eriod+1][r] + " " + 
"new_mutant_pooled_population[
m]" + 
new_mutant_pooled_population[m
] + " " + 
"pooled_nbr_of_sequence[r]=" 
+ pooled_nbr_of_sequence[r]);
mutant_population[simulation_p
eriod+1][r] =
mutant_population[simulation_p
eriod+1][r] +
store_calculated_population;
300   //System.out.println ("After 
calculation"+ " "+ 
"mutant_population[simulation_
period+1][r]=" + 
mutant_population[simulation_p
eriod+1][r]);
301   // System.out.println ();
302   }
304   }
307   }
312   // sum the total population of incumbents and mutants 
across the sequences for each simulation period         
313   double total_incumbent_population_thisperiod =0;
314   double total_mutant_population_thisperiod =0;
316   for (int k=0; k<sequencerows; k++)
318   {
319   total_incumbent_population_thisperiod =
total_incumbent_population_thisperiod +
incumbent_population[simulation_period][k];
320   total_mutant_population_thisperiod =
total_mutant_population_thisperiod +
mutant_population[simulation_period][k] ;
321   }
323   System.out.println( initial_mutant_size_percentage + ","
+ mutant_contribution + "," + multiplier + "," +
reputation_period + "," + simulation_period + "," +
"Total" +"," + total_incumbent_population_thisperiod +
"," + total_mutant_population_thisperiod);
324   //System.out.println();
325   }
327   }
329   }
331   }
333   }
336   }
339   }
References
[1] Maurice Allais. The foundations of a positive theory of choice involving risk and a
criticism of the postulates and axioms of the American School (1952). In: Expected
utility hypotheses and the Allais paradox. 1979, pp. 27145.
[2] Kurt Annen. Lies and slander: truth-telling in repeated matching games with private
monitoring. In: Social Choice and Welfare 37.2 (2011), pp. 269285.
[3] Fred D Arditti. Risk and the required return on equity. In: The Journal of Finance
22.1 (1967), pp. 1936.
[4] Thomas Astebro, Jose Mata, and Lus Santos-Pinto. Skewness seeking: risk loving,
optimism or overweighting of small probabilities? In: Theory and Decision 78.2
(2015), pp. 189208.
[5] Eyal Baharad and Shmuel Nitzan. Contest efforts in light of behavioural considera-
tions. In: The Economic Journal 118.533 (2008), pp. 20472059.
[6] Nicholas C Barberis. Thirty years of prospect theory in economics: A review and
assessment. In: Journal of Economic Perspectives 27.1 (2013), pp. 17396.
[7] Elchanan Ben-Porath and Michael Kahneman. Communication in repeated games
with private monitoring. In: Journal of Economic Theory 70.2 (1996), pp. 281297.
[8] Theodore C Bergstrom. The Algebra of Assortative Encounters and the Evolution of
Cooperation. In: International Game Theory Review 5.03 (2003), pp. 211228.
[9] Guido Biele, Ido Erev, and Eyal Ert. Learning, risk attitude and hot stoves in restless
bandit problems. In: Journal of Mathematical Psychology 53.3 (2009), pp. 155167.
[10] Gary Bolton, Ben Greiner, and Axel Ockenfels. Engineering Trust: Reciprocity in the
Production of Reputation Information. In: Management Science 59 (2013), pp. 265
[11] Gary Bolton, Elena Katok, and Axel Ockenfels. How Effective Are Electronic Repu-
tation Mechanisms? An Experimental Investigation. In: Management Science 50.11
(2004), pp. 15871602.
[12] Tilman Borgers and Rajiv Sarin. Naive reinforcement learning with endogenous
aspirations. In: International Economic Review 41.4 (2000), pp. 921950.
[13] Eduard Brandstatter, Gerd Gigerenzer, and Ralph Hertwig. The priority heuristic:
making choices without trade-offs. In: Psychological Review 113.2 (2006), p. 409.
[14] Hannelore Brandt, Christoph Hauert, and Karl Sigmund. Punishment and Reputa-
tion in Spatial Public Goods Games. In: Proceedings of the Royal Society of London B:
Biological Sciences 270.1519 (2003), pp. 10991104.
[15] Tobias Brunner, Rene Levnsky, and Jianying Qiu. Preferences for skewness: evidence
from a binary choice experiment. In: The European Journal of Finance 17.7 (2011),
pp. 525538.
REFERENCES REFERENCES
[16] Colin Camerer and Teck Hua Ho. Experience-weighted attraction learning in normal
form games. In: Econometrica 67.4 (1999), pp. 827874.
[17] Colin Camerer et al. Labor supply of New York City cabdrivers: One day at a time.
In: The Quarterly Journal of Economics 112.2 (1997), pp. 407441.
[18] Valerio Capraro, Jillian J Jordan, and David G Rand. Heuristics guide the imple-
mentation of social preferences in one-shot Prisoners Dilemma experiments. In:
Scientific reports 4.1 (2014), pp. 15.
[19] Edward Cartwright and Anna Stepanova. Efficiency in a forced contribution thresh-
old public good game. In: International Journal of Game Theory 46.4 (2017), pp. 1163
1191.
[20] Gary Charness and Dan Levin. When optimal choices feel wrong: A laboratory study
of Bayesian updating, complexity, and affect. In: American Economic Review 95.4
(2005), pp. 13001309.
[21] W Henry Chiu. Skewness preference, risk taking and expected utility maximisation.
In: The Geneva Risk and Insurance Review 35.2 (2010), pp. 108129.
[22] Edward H Clarke. Multipart pricing of public goods. In: Public choice 11.1 (1971),
pp. 1733.
[23] K. S. Cook et al. eTrust: Forming Relationships in the Online World. In: (2009).
[24] Clyde H Coombs and James N Bowen. A test of VE-theories of risk and the effect of
the central limit theorem. In: Acta Psychologica 35.1 (1971), pp. 1528.
[25] Clyde H Coombs and Dean G Pruitt. Components of risk in decision making: Proba-
bility and variance preferences. In: Journal of Experimental Psychology 60.5 (1960),
p. 265.
[26] Richard Cornes and Roger Hartley. Risk aversion, heterogeneity and contests. In:
Public Choice 117.1 (2003), pp. 125.
[27] Mark Dean. Status quo bias in large and small choice sets. In: Unpublished working
paper (2008).
[28] Emmanuel Dechenaux, Dan Kovenock, and Roman M Sheremeta. A survey of ex-
perimental research on contests, all-pay auctions and tournaments. In: Experimental
Economics 18.4 (2015), pp. 609669.
[29] Enrico Diecidue and Jeroen Van De Ven. Aspiration level, probability of success and
failure, and expected utility. In: International Economic Review 49.2 (2008), pp. 683
[30] Ward Edwards. Probability-preferences among bets with differing expected values.
In: The American Journal of Psychology 67.1 (1954), pp. 5667.
[31] Ward Edwards. Probability-preferences in gambling. In: The American Journal of
Psychology 66.3 (1953), pp. 349364.
[32] Richard Engelbrecht-Wiggans and Elena Katok. Regret and feedback information in
first-price sealed-bid auctions. In: Management Science 54.4 (2008), pp. 808819.
[33] Richard Engelbrecht-Wiggans and Elena Katok. Regret in auctions: Theory and
evidence. In: Economic Theory 33.1 (2007), pp. 81101.
[34] Ido Erev and Alvin E Roth. Multi-agent learning and the descriptive value of simple
models. In: Artificial Intelligence 171.7 (2007), pp. 423428.
146 PhD Thesis
REFERENCES REFERENCES
[35] Ilan Eshel and Luigi Luca Cavalli-Sforza. Assortment of Encounters and Evolution
of Cooperativeness. In: Proceedings of the National Academy of Sciences 79.4 (1982),
pp. 13311335.
[36] Francesco Fallucchi, Jan Niederreiter, and Massimo Riccaboni. Learning and dropout
in contests: an experimental approach. In: Theory and Decision (2020), pp. 134.
[37] Francesco Fallucchi, Elke Renner, and Martin Sefton. Information feedback and
contest structure in rent-seeking games. In: European Economic Review 64 (2013),
pp. 223240.
[38] Emel Filiz-Ozbay and Erkut Y Ozbay. Auctions with anticipated regret: Theory and
Experiment. In: American Economic Review 97.4 (2007), pp. 14071418.
[39] Urs Fischbacher. z-Tree: Zurich toolbox for ready-made economic experiments. In:
Experimental Economics 10.2 (2007), pp. 171178.
[40] Feng Fu et al. The evolution of homophily. In: Scientific reports 2 (2012), p. 845.
[41] Thomas A Garrett and Russell S Sobel. Gamblers favor skewness, not risk: Further
evidence from United States lottery games. In: Economics Letters 63.1 (1999), pp. 85
[42] Robert L Goldstone and Ji Yun Son. Similarity. Oxford University Press, 2012.
[43] Joseph Golec and Maurry Tamarkin. Bettors love skewness, not risk, at the horse
track. In: Journal of Political Economy 106.1 (1998), pp. 205225.
[44] Avner Greif. Contract Enforceability and Economic Institutions in Early Trade: The
Maghribi Traders Coalition. In: The American Economic Review (1993), pp. 525548.
[45] Brit Grosskopf. Reinforcement and directional learning in the ultimatum game with
responder competition. In: Experimental Economics 6.2 (2003), pp. 141158.
[46] Brit Grosskopf, Rajiv Sarin, and Elizabeth Watson. An experiment on case-based
decision making. In: Theory and Decision 79.4 (2015), pp. 639666.
[47] Anna Gunnthorsdottir et al. Near-Efficient Equilibria in Contribution-Based Com-
petitive Grouping. In: Journal of Public Economics 94.11-12 (2010), pp. 987994.
[48] Glenn W Harrison and E Elisabet Rutstrom. Expected utility theory and prospect
theory: One wedding and a decent funeral. In: Experimental economics 12.2 (2009),
p. 133.
[49] Burkhard Hehenkamp, Wolfgang Leininger, and Alexandre Possajennikov. Evolu-
tionary equilibrium in Tullock contests: spite and overdissipation. In: European
Journal of Political Economy 20.4 (2004), pp. 10451057.
[50] Dirk Helbing. Economics 2.0: The Natural Step Towards a Self-Regulating, Participa-
tory Market Society. In: Evolutionary and Institutional Economics Review 10.1 (2013),
pp. 341.
[51] Benedikt Herrmann and Henrik Orzen. The appearance of homo rivalis: Social prefer-
ences and the nature of rent seeking. Tech. rep. CeDEx discussion paper series, 2008.
[52] Charles A Holt and Susan K Laury. Risk aversion and incentive effects. In: American
Economic Review 92.5 (2002), pp. 16441655.
[53] Charles A Holt and Alvin E Roth. The Nash equilibrium: A perspective. In: Proceed-
ings of the National Academy of Sciences 101.12 (2004), pp. 39994002.
[54] Martin Kaae Jensen and Alexandros Rigos. Evolutionary Games and Matching Rules.
In: International Journal of Game Theory (2018), pp. 129.
147 PhD Thesis
REFERENCES REFERENCES
[55] Daniel Kahneman and Amos Tversky. Prospect Theory: An Analysis of Decision
under Risk. In: Econometrica 47 (1979), pp. 263291.
[56] Michihiro Kandori. Social Norms and Community Enforcement. In: The Review of
Economic Studies 59.1 (1992), pp. 6380.
[57] Timothy Killingback, Michael Doebeli, and Nancy Knowlton. Variable Investment,
the Continuous Prisoners Dilemma, and the Origin of Cooperation. In: Proceedings
of the Royal Society of London B: Biological Sciences 266.1430 (1999), pp. 17231728.
[58] Alan Kraus and Robert H Litzenberger. Skewness preference and the valuation of
risk assets. In: The Journal of Finance 31.4 (1976), pp. 10851100.
[59] Haim Levy and Marshall Sarnat. Safety firstan expected utility principle. In:
Journal of Financial and Quantitative Analysis 7.3 (1972), pp. 18291834.
[60] Wooyoung Lim, Alexander Matros, and Theodore L Turocy. Bounded rationality
and group size in Tullock contests: Experimental evidence. In: Journal of Economic
Behavior & Organization 99 (2014), pp. 155167.
[61] Lola L Lopes. Algebra and process in the modeling of risky choice. In: Psychology of
Learning and Motivation. Vol. 32. 1995, pp. 177220.
[62] Lola L Lopes. Between Hope and Fear: The Psychology of Risk. In: Advances in
Experimental Social Psychology 20.2 (1987), pp. 255295.
[63] Lola L Lopes. Decision making in the short run. In: Journal of Experimental Psychol-
ogy: Human Learning and Memory 7.5 (1981), p. 377.
[64] Lola L Lopes. When time is of the essence: Averaging, aspiration, and the short run.
In: Organizational Behavior and Human Decision Processes 65.3 (1996), pp. 179189.
[65] Lola L Lopes and Gregg C Oden. The role of aspiration level in risky choice: A com-
parison of cumulative prospect theory and SP/A theory. In: Journal of Mathematical
Psychology 43.2 (1999), pp. 286313.
[66] Volodymyr Lugovskyy, Daniela Puzzello, and Steven Tucker. An experimental inves-
tigation of overdissipation in the all pay auction. In: European Economic Review 54.8
(2010), pp. 974997.
[67] Shakun D Mago, Anya C Samak, and Roman M Sheremeta. Facing your opponents:
Social identification and information feedback in contests. In: Journal of Conflict
Resolution 60.3 (2016), pp. 459481.
[68] James G March. Learning to be risk averse. In: Psychological Review 103.2 (1996),
p. 309.
[69] Harry Markowitz. Portfolio selection. In: The Journal of Finance 7.1 (1952), pp. 77
[70] William H McGlothlin. Stability of choices among uncertain alternatives. In: The
American Journal of Psychology 69.4 (1956), pp. 604615.
[71] Richard D McKelvey and Thomas R Palfrey. Quantal response equilibria for normal
form games. In: Games and Economic Behavior 10.1 (1995), pp. 638.
[72] Paul R Milgrom, Douglass C North, and Barry R Weingast*. The role of institutions
in the revival of trade: The law merchant, private judges, and the champagne fairs.
In: Economics & Politics 2.1 (1990), pp. 123.
[73] Manfred Milinski, Dirk Semmann, and Hans-Jurgen Krambeck. Reputation Helps
Solve the Tragedy of the Commons. In: Nature 415.6870 (2002), p. 424.
148 PhD Thesis
REFERENCES REFERENCES
[74] Edward L Millner and Michael D Pratt. An experimental investigation of efficient
rent-seeking. In: Public Choice 62.2 (1989), pp. 139151.
[75] Edward L Millner and Michael D Pratt. Risk aversion and rent-seeking: An extension
and some experimental evidence. In: Public Choice 69.1 (1991), pp. 8192.
[76] John Morgan, Henrik Orzen, and Martin Sefton. Endogenous entry in contests. In:
Economic Theory 51.2 (2012), pp. 435463.
[77] Wieland Muller and Andrew Schotter. Workaholics and dropouts in organizations.
In: Journal of the European Economic Association 8.4 (2010), pp. 717743.
[78] Heinrich H Nax and Alexandros Rigos. Assortativity Evolving from Social Dilem-
mas. In: Journal of Theoretical Biology 395 (2016), pp. 194203.
[79] Heinrich H Nax et al. Stability of cooperation under image scoring in group interac-
tions. In: Scientific reports 5 (2015), p. 12145.
[80] M Kathleen Ngangoue and Andrew Schotter. The Common-Probability Auction
Puzzle. In: (2019).
[81] Martin A Nowak. Five rules for the evolution of cooperation. In: science 314.5805
(2006), pp. 15601563.
[82] Martin A Nowak and Karl Sigmund. Evolution of Indirect Reciprocity by Image
Scoring. In: Nature 393.6685 (1998), p. 573.
[83] Martin A Nowak et al. Emergence of cooperation and evolutionary stability in finite
populations. In: Nature 428.6983 (2004), pp. 646650.
[84] Axel Ockenfels and Reinhard Selten. Impulse balance equilibrium and feedback in
first price auctions. In: Games and Economic Behavior 51.1 (2005), pp. 155170.
[85] Hisashi Ohtsuki, Yoh Iwasa, and Martin A Nowak. Reputation effects in public and
private interactions. In: PLoS computational biology 11.11 (2015), e1004527.
[86] Hisashi Ohtsuki et al. A Simple Rule for the Evolution of Cooperation on Graphs
and Social Networks. In: Nature 441.7092 (2006), p. 502.
[87] Masahiro Okuno-Fujiwara and Andrew Postlewaite. Social Norms and Random
Matching Games. In: Games and Economic Behavior 9.1 (1995), pp. 79109.
[88] John W Payne. It is whether you win or lose: The importance of the overall proba-
bilities of winning or losing in risky choice. In: Journal of Risk and Uncertainty 30.1
(2005), pp. 519.
[89] John W Payne and Myron L Braunstein. Preferences among gambles with equal
underlying distributions. In: Journal of Experimental Psychology 87.1 (1971), p. 13.
[90] Jan Potters, Casper G De Vries, and Frans Van Winden. An experimental examination
of rational rent-seeking. In: European Journal of Political Economy 14.4 (1998), pp. 783
[91] John W Pratt. Risk aversion in the small and in the large. In: 1964, pp. 122136.
[92] David G Rand, Samuel Arbesman, and Nicholas A Christakis. Dynamic social net-
works promote cooperation in experiments with humans. In: Proceedings of the
National Academy of Sciences 108.48 (2011), pp. 1919319198.
[93] Paul Resnick and Richard Zeckhauser. Trust Among Strangers in Internet Trans-
actions: Empirical Analysis of eBays Reputation System. In: The Economics of the
Internet and E-commerce 11.2 (2002), pp. 2325.
149 PhD Thesis
REFERENCES REFERENCES
[94] Stephen A Ross. Adding risks: Samuelsons fallacy of large numbers revisited. In:
Journal of Financial and Quantitative Analysis 34.3 (1999), pp. 323339.
[95] Alvin E Roth. The Economist as Engineer: Game Theory, Experimentation, and
Computation as Tools for Design Economics. In: Econometrica 70.4 (2002), pp. 1341
1378.
[96] Alvin E Roth and Ido Erev. Learning in extensive-form games: Experimental data and
simple dynamic models in the intermediate term. In: Games and Economic Behavior
8.1 (1995), pp. 164212.
[97] Andrew Donald Roy. Safety first and the holding of assets. In: Econometrica: Journal
of the Econometric Society (1952), pp. 431449.
[98] Rajiv Sarin and Farshid Vahid. Strategy similarity and coordination. In: The Eco-
nomic Journal 114.497 (2004), pp. 506527.
[99] Peter Schuster and Karl Sigmund. Replicator Dynamics. In: Journal of Theoretical
Biology 100.3 (1983), pp. 533538.
[100] Robert C Scott and Philip A Horvath. On the direction of preference for moments of
higher order than the variance. In: The Journal of Finance 35.4 (1980), pp. 915919.
[101] Reinhard Selten and Rolf Stoecker. End behavior in sequences of finite Prisoners
Dilemma supergames A learning theory approach. In: Journal of Economic Behavior
& Organization 7.1 (1986), pp. 4770.
[102] Roman M Sheremeta. Experimental comparison of multi-stage and one-stage con-
tests. In: Games and Economic Behavior 68.2 (2010), pp. 731747.
[103] Jason F Shogren and Kyung H Baik. Reexamining efficient rent-seeking in laboratory
markets. In: Public Choice 69.1 (1991), pp. 6979.
[104] Paul Slovic and Sarah Lichtenstein. Relative importance of probabilities and payoffs
in risk taking. In: Journal of Experimental Psychology 78.3p2 (1968), p. 1.
[105] J Maynard Smith and George R Price. The Logic of Animal Conflict. In: Nature
246.5427 (1973), p. 15.
[106] Joel Sobel. Economists models of learning. In: Journal of Economic Theory 94.2
(2000), pp. 241261.
[107] Dale O Stahl. An Experimental Test of the Efficacy of a Simple Reputation Mechanism
to Solve Social Dilemmas. In: Journal of Economic Behavior and Organization 94 (2013),
pp. 116124.
[108] Mkael Symmonds et al. Deconstructing risk: Separable encoding of variance and
skewness in the brain. In: Neuroimage 58.4 (2011), pp. 11391149.
[109] Nassim Nicholas Taleb. Bleed or Blowup? Why Do We Prefer Asymmetric Payoffs?
In: The Journal of Behavioral Finance 5.1 (2004), pp. 27.
[110] Kinneret Teodorescu and Ido Erev. Learned helplessness and learned prevalence:
Exploring the causal relations among perceived controllability, reward prevalence,
and exploration. In: Psychological Science 25.10 (2014), pp. 18611869.
[111] Leigh Tesfatsion. A Trade Network Game with Endogenous Partner Selection. In:
Computational Approaches to Economic Problems. Springer, 1997, pp. 249269.
[112] Sho-Chieh Tsiang. The rationale of the mean-standard deviation analysis, skewness
preference, and the demand for money. In: 1972, pp. 354371.
150 PhD Thesis
REFERENCES REFERENCES
[113] Gordan Tullock. Toward a Theory of the Rent-Seeking Society. Efficient Rent Seeking.
Texas A & M University, 1980.
[114] Amos Tversky. Intransitivity of preferences. In: Psychological Review 76.1 (1969),
p. 31.
[115] Amos Tversky and Daniel Kahneman. Advances in prospect theory: Cumulative
representation of uncertainty. In: Journal of Risk and Uncertainty 5.4 (1992), pp. 297
[116] Amos Tversky and Daniel Kahneman. Availability: A heuristic for judging frequency
and probability. In: Cognitive psychology 5.2 (1973), pp. 207232.
[117] Amos Tversky and Daniel Kahneman. Judgment under uncertainty: Heuristics and
biases. In: science 185.4157 (1974), pp. 11241131.
[118] Amos Tversky and Daniel Kahneman. Rational choice and the framing of decisions.
In: Journal of Business (1986), S251S278.
[119] Vinod Venkatraman, John W Payne, and Scott A Huettel. An overall probability of
winning heuristic for complex risky decisions: Choice and eye fixation evidence. In:
Organizational Behavior and Human Decision Processes 125.2 (2014), pp. 7387.
[120] William Vickrey. Counterspeculation, auctions, and competitive sealed tenders. In:
The Journal of finance 16.1 (1961), pp. 837.
[121] Claus Wedekind and Manfred Milinski. Cooperation through image scoring in
humans. In: Science 288.5467 (2000), pp. 850852.
[122] George Wu and Richard Gonzalez. Curvature of the probability weighting function.
In: Management science 42.12 (1996), pp. 16761690.
[123] Stefan Zeisberger. Do People Care About Loss Probabilities? In: Available at SSRN
2169394 (2016).
151 PhD Thesis
	Introduction
	The importance of Probability of Winning in Risky Choices
	Introduction
	Related Literature
	Towards Theoretical Formulation
	Main Design-Features
	Experimental Design and Procedures
	Predictions
	Results
	Part 5 Results - Alternative Explanation
	Discussion
	Reinforcement Learning in Contests
	Introduction
	Static Equilibrium Characterization
	Basic Learning Model
	Asymptotic Analysis -Tullock Contests
	Model Predictions
	Experimental Evidence
	Simulations
	Model Applicability to All Pay Auction
	Discussion
	A Mechanism and Matching in a Social Dilemma
	Introduction
	Related Literature
	Basic Model
	Continuous PD
	Reputation Mechanism
	Properties of Reputation Mechanism
	Equilibrium Analysis
	Full History
	Finite History - Straightforward Mutants
	Finite History - Strategic Mutants
	Finite History - Strategic Mutants Simulations
	Discussion
	Appendices
	(Chapter 1)
	News and Social Media Posts
	Further Charts and Data
	Risk Aversion for Part 5, 4 and 3
	Instructions Set
	z-Tree Screenshots
	(Chapter 2)
	Further Simulations
	Reference for Proposition 1
	Learning Model Simulation Code
	(Chapter 3)
	Simulation Code
	References
